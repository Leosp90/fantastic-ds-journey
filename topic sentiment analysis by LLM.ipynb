{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "a354873f-368d-48f9-b789-581108d59db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import openai\n",
    "from bertopic.representation import OpenAI\n",
    "\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c9f2b-df28-459c-b211-5f9c787d78a8",
   "metadata": {},
   "source": [
    "### data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ff5b1-0d09-4209-89cf-49facc418385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_df = pd.read_csv(\"C:\\\\Users\\\\Liu Shi Peng\\\\Documents\\\\feb_2_aug_topic_classification\\\\tgpt_feb_2_jul_topic_mapping_by_llama.csv\")\n",
    "# topic_df.shape\n",
    "\n",
    "# df = pd.read_csv(\"C:\\\\Users\\\\Liu Shi Peng\\\\Documents\\\\feb_2_aug_topic_classification\\\\feb_2_aug_topic_db.csv\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "095d062f-af39-4a4e-a28c-b749d157d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:\\\\Users\\\\Liu Shi Peng\\\\Documents\\\\feb_2_aug_topic_classification\\\\feb_2_aug_topic_db.csv\"\n",
    "with open (file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "    csv_file = csv.reader(f)\n",
    "    header = next(csv_file)\n",
    "    rows = [row for row in csv_file]\n",
    "df = pd.DataFrame(rows, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "f60a6c93-637d-4dc8-81da-ae8959c9d17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>user_query_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>topic_y</th>\n",
       "      <th>month</th>\n",
       "      <th>session_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27ad4be3-ab42-4fe9-945f-b4a700edf3b1</td>\n",
       "      <td>but the put open interest is higher than call ...</td>\n",
       "      <td>6/16/2024 14:07</td>\n",
       "      <td>Options Trading Strategies</td>\n",
       "      <td>6</td>\n",
       "      <td>926e5219-2760-4c63-85f3-a8d217115fc5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0cada57b-ef39-418d-9486-46d3135b6c1e</td>\n",
       "      <td>Which stocks will increase 2% on next two week...</td>\n",
       "      <td>6/16/2024 14:12</td>\n",
       "      <td>Options Trading Strategies</td>\n",
       "      <td>6</td>\n",
       "      <td>0fce8430-4a6f-4ad9-8c7e-35b9973ac490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4b1b1dac-04c4-4c69-a809-05cabef3529f</td>\n",
       "      <td>RapidAdvanceSolutions.com - What do I use for ...</td>\n",
       "      <td>7/3/2024 1:28</td>\n",
       "      <td>Non-Financial/Unrelated Topics</td>\n",
       "      <td>7</td>\n",
       "      <td>3e3ad757-8cc9-4bc7-b3d3-7173e6f41901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4616cd71-3846-4b3d-9d38-54e9bc7655b3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>6/16/2024 14:12</td>\n",
       "      <td>Non-Financial/Unrelated Topics</td>\n",
       "      <td>6</td>\n",
       "      <td>0fce8430-4a6f-4ad9-8c7e-35b9973ac490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07dbbadf-37fd-4cf2-9d99-4d5d73d872d5</td>\n",
       "      <td>Im on a demo account</td>\n",
       "      <td>6/16/2024 15:20</td>\n",
       "      <td>Trading Terminology &amp; Tools</td>\n",
       "      <td>6</td>\n",
       "      <td>52dc0c3a-4f46-418b-84e5-b4b36d6921f1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             message_id  \\\n",
       "0  27ad4be3-ab42-4fe9-945f-b4a700edf3b1   \n",
       "1  0cada57b-ef39-418d-9486-46d3135b6c1e   \n",
       "2  4b1b1dac-04c4-4c69-a809-05cabef3529f   \n",
       "3  4616cd71-3846-4b3d-9d38-54e9bc7655b3   \n",
       "4  07dbbadf-37fd-4cf2-9d99-4d5d73d872d5   \n",
       "\n",
       "                                        user_query_x       created_at  \\\n",
       "0  but the put open interest is higher than call ...  6/16/2024 14:07   \n",
       "1  Which stocks will increase 2% on next two week...  6/16/2024 14:12   \n",
       "2  RapidAdvanceSolutions.com - What do I use for ...    7/3/2024 1:28   \n",
       "3                                                Yes  6/16/2024 14:12   \n",
       "4                             Im on a demo account   6/16/2024 15:20   \n",
       "\n",
       "                          topic_y month                            session_id  \n",
       "0      Options Trading Strategies     6  926e5219-2760-4c63-85f3-a8d217115fc5  \n",
       "1      Options Trading Strategies     6  0fce8430-4a6f-4ad9-8c7e-35b9973ac490  \n",
       "2  Non-Financial/Unrelated Topics     7  3e3ad757-8cc9-4bc7-b3d3-7173e6f41901  \n",
       "3  Non-Financial/Unrelated Topics     6  0fce8430-4a6f-4ad9-8c7e-35b9973ac490  \n",
       "4     Trading Terminology & Tools     6  52dc0c3a-4f46-418b-84e5-b4b36d6921f1  "
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "c55e189c-44cc-48b1-bd3a-00de0acdc1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['message_id', 'user_query_x', 'created_at', 'topic_y', 'month',\n",
       "       'session_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "d74beeb6-cae1-4271-8282-487e79b70ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105839, 3)"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.loc[df['month'].isin(['8','9']), ['session_id', 'user_query_x', 'created_at']]\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "0b926c93-43ca-4fa2-b090-0074a868f41b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1_sorted = df1.sort_values(by = ['session_id', 'created_at'], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "4dbf50fe-d865-4adf-9725-f1ffd0e83423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>user_query_x</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>339355</th>\n",
       "      <td>00010369-2ee4-459b-9080-43c6793c32ff</td>\n",
       "      <td>What's the least expensive crypto coin to trad...</td>\n",
       "      <td>8/3/2024 2:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339356</th>\n",
       "      <td>00010369-2ee4-459b-9080-43c6793c32ff</td>\n",
       "      <td>The crypto market which one cost the less in \"...</td>\n",
       "      <td>8/3/2024 2:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339358</th>\n",
       "      <td>00010369-2ee4-459b-9080-43c6793c32ff</td>\n",
       "      <td>Thank you, you're always a big help</td>\n",
       "      <td>8/3/2024 2:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394108</th>\n",
       "      <td>0003c604-45a5-4f1d-8b87-b684ef3641e4</td>\n",
       "      <td>Ciao</td>\n",
       "      <td>8/22/2024 17:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356592</th>\n",
       "      <td>00054a16-b0cb-4c2e-84c1-505a7e859899</td>\n",
       "      <td>Get me trades for today to buy for weekend gai...</td>\n",
       "      <td>8/9/2024 19:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356600</th>\n",
       "      <td>00054a16-b0cb-4c2e-84c1-505a7e859899</td>\n",
       "      <td>Which of these is highest probability, high bu...</td>\n",
       "      <td>8/9/2024 19:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356611</th>\n",
       "      <td>00054a16-b0cb-4c2e-84c1-505a7e859899</td>\n",
       "      <td>Give me your best picks for trades on blnd and...</td>\n",
       "      <td>8/9/2024 19:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356624</th>\n",
       "      <td>00054a16-b0cb-4c2e-84c1-505a7e859899</td>\n",
       "      <td>If they go up what is the profit possible on e...</td>\n",
       "      <td>8/9/2024 19:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356630</th>\n",
       "      <td>00054a16-b0cb-4c2e-84c1-505a7e859899</td>\n",
       "      <td>Possible profit from option on blnd and fivn a...</td>\n",
       "      <td>8/9/2024 19:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424434</th>\n",
       "      <td>0006538f-33de-4fb9-8f57-19835fb2e110</td>\n",
       "      <td>Top 3 Call option contracts in the AI hardware...</td>\n",
       "      <td>9/1/2024 4:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398844</th>\n",
       "      <td>0007bc3e-d9ec-4a4f-b79c-b2f7069d0a4e</td>\n",
       "      <td>What stock made the most money today</td>\n",
       "      <td>8/23/2024 20:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398848</th>\n",
       "      <td>0007bc3e-d9ec-4a4f-b79c-b2f7069d0a4e</td>\n",
       "      <td>What penny stocks wil make me $100 in a day?</td>\n",
       "      <td>8/23/2024 20:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369715</th>\n",
       "      <td>000cf849-868c-444a-adf9-1bc84655da4b</td>\n",
       "      <td>Weekly Swing Trade: Find me an options trade t...</td>\n",
       "      <td>8/14/2024 16:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326673</th>\n",
       "      <td>000cf849-868c-444a-adf9-1bc84655da4b</td>\n",
       "      <td>Weekly Swing Trade: Find me an options trade t...</td>\n",
       "      <td>9/4/2024 12:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345484</th>\n",
       "      <td>000e11e4-592b-414a-8825-2a217a663853</td>\n",
       "      <td>What is the current market sentiment for ARM. ...</td>\n",
       "      <td>8/6/2024 4:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347109</th>\n",
       "      <td>000e3867-a447-47da-81fa-91a9c22b3675</td>\n",
       "      <td>Whats my stop loss and take profit for LCID s...</td>\n",
       "      <td>8/6/2024 18:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347342</th>\n",
       "      <td>000e3867-a447-47da-81fa-91a9c22b3675</td>\n",
       "      <td>Can you tell me which stock has potential trad...</td>\n",
       "      <td>8/6/2024 19:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376739</th>\n",
       "      <td>0011fb82-cf49-4349-b7cf-760b191d0f4d</td>\n",
       "      <td>Does the healthcare stock THC have strong fund...</td>\n",
       "      <td>8/16/2024 12:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376730</th>\n",
       "      <td>0011fb82-cf49-4349-b7cf-760b191d0f4d</td>\n",
       "      <td>Thank you.  That answers my questions.</td>\n",
       "      <td>8/16/2024 12:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414107</th>\n",
       "      <td>00127f9d-ef7c-4124-a35b-c98f30c2eca2</td>\n",
       "      <td>Is royal Caribbean a good buy?</td>\n",
       "      <td>8/28/2024 19:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414123</th>\n",
       "      <td>00127f9d-ef7c-4124-a35b-c98f30c2eca2</td>\n",
       "      <td>Entry point for rcl</td>\n",
       "      <td>8/28/2024 19:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370824</th>\n",
       "      <td>0012afcd-3c27-4fdf-b89d-87a47bb188ea</td>\n",
       "      <td>What is the future of CCJ? is it a good invest...</td>\n",
       "      <td>8/14/2024 21:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370825</th>\n",
       "      <td>0012afcd-3c27-4fdf-b89d-87a47bb188ea</td>\n",
       "      <td>Yes</td>\n",
       "      <td>8/14/2024 21:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370833</th>\n",
       "      <td>0012afcd-3c27-4fdf-b89d-87a47bb188ea</td>\n",
       "      <td>Yes</td>\n",
       "      <td>8/14/2024 21:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370839</th>\n",
       "      <td>0012afcd-3c27-4fdf-b89d-87a47bb188ea</td>\n",
       "      <td>Create a Call options for CCJ with the highest...</td>\n",
       "      <td>8/14/2024 21:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370841</th>\n",
       "      <td>0012afcd-3c27-4fdf-b89d-87a47bb188ea</td>\n",
       "      <td>How much capital would I need to execute this ...</td>\n",
       "      <td>8/14/2024 22:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391654</th>\n",
       "      <td>001421e7-6ff9-4f2f-8bcf-db982063e6ff</td>\n",
       "      <td>What are best indicators to use for trading Na...</td>\n",
       "      <td>8/22/2024 0:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328976</th>\n",
       "      <td>00153215-eecc-4c26-a6cd-64c88416a210</td>\n",
       "      <td>Give me the best stock to buy today and sell t...</td>\n",
       "      <td>9/4/2024 23:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426755</th>\n",
       "      <td>00166ab8-3cf1-49e7-a46c-8d439c3f9e18</td>\n",
       "      <td>A Federal Reserve rate cut can have several im...</td>\n",
       "      <td>9/2/2024 15:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426757</th>\n",
       "      <td>00166ab8-3cf1-49e7-a46c-8d439c3f9e18</td>\n",
       "      <td>What is the outlook for NVDA this month (Septe...</td>\n",
       "      <td>9/2/2024 15:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399960</th>\n",
       "      <td>0018bc1e-6601-45f4-8d22-1ae771d7b4b0</td>\n",
       "      <td>When is the best stock to trade that is near i...</td>\n",
       "      <td>8/24/2024 8:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399961</th>\n",
       "      <td>0018bc1e-6601-45f4-8d22-1ae771d7b4b0</td>\n",
       "      <td>But these are nowhere near their 52 week glow</td>\n",
       "      <td>8/24/2024 8:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399962</th>\n",
       "      <td>0018bc1e-6601-45f4-8d22-1ae771d7b4b0</td>\n",
       "      <td>What are the best stocks to trade that are 5% ...</td>\n",
       "      <td>8/24/2024 8:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399967</th>\n",
       "      <td>0018bc1e-6601-45f4-8d22-1ae771d7b4b0</td>\n",
       "      <td>What are the best stocks to trade that are 5% ...</td>\n",
       "      <td>8/24/2024 8:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399968</th>\n",
       "      <td>0018bc1e-6601-45f4-8d22-1ae771d7b4b0</td>\n",
       "      <td>List only the tickers</td>\n",
       "      <td>8/24/2024 8:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399970</th>\n",
       "      <td>0018bc1e-6601-45f4-8d22-1ae771d7b4b0</td>\n",
       "      <td>Give me the absolute best trade to make right ...</td>\n",
       "      <td>8/24/2024 8:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350157</th>\n",
       "      <td>00194254-c894-46b1-a3d5-3fe6bacfb612</td>\n",
       "      <td>Give me a buying entry level for spy based on ...</td>\n",
       "      <td>8/7/2024 18:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338680</th>\n",
       "      <td>001a6dc8-c4ad-4a17-b6f7-3021e0c4bef0</td>\n",
       "      <td>options on amzon</td>\n",
       "      <td>8/2/2024 20:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378093</th>\n",
       "      <td>001b0556-f343-4666-91dc-509bb020d8e0</td>\n",
       "      <td>Stock alert</td>\n",
       "      <td>8/16/2024 17:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378089</th>\n",
       "      <td>001b0556-f343-4666-91dc-509bb020d8e0</td>\n",
       "      <td>Stock alert up</td>\n",
       "      <td>8/16/2024 17:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378129</th>\n",
       "      <td>001b0556-f343-4666-91dc-509bb020d8e0</td>\n",
       "      <td>Stocks alert today up</td>\n",
       "      <td>8/16/2024 17:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379159</th>\n",
       "      <td>001b0556-f343-4666-91dc-509bb020d8e0</td>\n",
       "      <td>Stocks  alerts up</td>\n",
       "      <td>8/17/2024 0:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379165</th>\n",
       "      <td>001b0556-f343-4666-91dc-509bb020d8e0</td>\n",
       "      <td>Stock alert down</td>\n",
       "      <td>8/17/2024 0:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334449</th>\n",
       "      <td>001dd34f-fe17-4ad2-bd53-67f2fde2d62b</td>\n",
       "      <td>Is it smart to offset a covered call that is o...</td>\n",
       "      <td>8/19/2024 17:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346985</th>\n",
       "      <td>001dd34f-fe17-4ad2-bd53-67f2fde2d62b</td>\n",
       "      <td>Pltr</td>\n",
       "      <td>8/19/2024 17:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398921</th>\n",
       "      <td>001e036e-5214-4897-9e3e-eac649566d98</td>\n",
       "      <td>Dimers top mlb games  for today</td>\n",
       "      <td>8/23/2024 21:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398918</th>\n",
       "      <td>001e036e-5214-4897-9e3e-eac649566d98</td>\n",
       "      <td>What are top predictions  for mlb today</td>\n",
       "      <td>8/23/2024 21:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344592</th>\n",
       "      <td>00212408-ca30-4aab-a09f-01b4ae0bf54d</td>\n",
       "      <td>What is the best put option i can make today</td>\n",
       "      <td>8/5/2024 19:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356834</th>\n",
       "      <td>0024482e-70d5-4744-878d-f14d884f6ef4</td>\n",
       "      <td>Will spy open up or down on Monday and will it...</td>\n",
       "      <td>8/9/2024 21:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426136</th>\n",
       "      <td>0024c4ae-6cf8-4438-94ba-5a38d7535f19</td>\n",
       "      <td>Best options strategies</td>\n",
       "      <td>9/2/2024 4:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  session_id  \\\n",
       "339355  00010369-2ee4-459b-9080-43c6793c32ff   \n",
       "339356  00010369-2ee4-459b-9080-43c6793c32ff   \n",
       "339358  00010369-2ee4-459b-9080-43c6793c32ff   \n",
       "394108  0003c604-45a5-4f1d-8b87-b684ef3641e4   \n",
       "356592  00054a16-b0cb-4c2e-84c1-505a7e859899   \n",
       "356600  00054a16-b0cb-4c2e-84c1-505a7e859899   \n",
       "356611  00054a16-b0cb-4c2e-84c1-505a7e859899   \n",
       "356624  00054a16-b0cb-4c2e-84c1-505a7e859899   \n",
       "356630  00054a16-b0cb-4c2e-84c1-505a7e859899   \n",
       "424434  0006538f-33de-4fb9-8f57-19835fb2e110   \n",
       "398844  0007bc3e-d9ec-4a4f-b79c-b2f7069d0a4e   \n",
       "398848  0007bc3e-d9ec-4a4f-b79c-b2f7069d0a4e   \n",
       "369715  000cf849-868c-444a-adf9-1bc84655da4b   \n",
       "326673  000cf849-868c-444a-adf9-1bc84655da4b   \n",
       "345484  000e11e4-592b-414a-8825-2a217a663853   \n",
       "347109  000e3867-a447-47da-81fa-91a9c22b3675   \n",
       "347342  000e3867-a447-47da-81fa-91a9c22b3675   \n",
       "376739  0011fb82-cf49-4349-b7cf-760b191d0f4d   \n",
       "376730  0011fb82-cf49-4349-b7cf-760b191d0f4d   \n",
       "414107  00127f9d-ef7c-4124-a35b-c98f30c2eca2   \n",
       "414123  00127f9d-ef7c-4124-a35b-c98f30c2eca2   \n",
       "370824  0012afcd-3c27-4fdf-b89d-87a47bb188ea   \n",
       "370825  0012afcd-3c27-4fdf-b89d-87a47bb188ea   \n",
       "370833  0012afcd-3c27-4fdf-b89d-87a47bb188ea   \n",
       "370839  0012afcd-3c27-4fdf-b89d-87a47bb188ea   \n",
       "370841  0012afcd-3c27-4fdf-b89d-87a47bb188ea   \n",
       "391654  001421e7-6ff9-4f2f-8bcf-db982063e6ff   \n",
       "328976  00153215-eecc-4c26-a6cd-64c88416a210   \n",
       "426755  00166ab8-3cf1-49e7-a46c-8d439c3f9e18   \n",
       "426757  00166ab8-3cf1-49e7-a46c-8d439c3f9e18   \n",
       "399960  0018bc1e-6601-45f4-8d22-1ae771d7b4b0   \n",
       "399961  0018bc1e-6601-45f4-8d22-1ae771d7b4b0   \n",
       "399962  0018bc1e-6601-45f4-8d22-1ae771d7b4b0   \n",
       "399967  0018bc1e-6601-45f4-8d22-1ae771d7b4b0   \n",
       "399968  0018bc1e-6601-45f4-8d22-1ae771d7b4b0   \n",
       "399970  0018bc1e-6601-45f4-8d22-1ae771d7b4b0   \n",
       "350157  00194254-c894-46b1-a3d5-3fe6bacfb612   \n",
       "338680  001a6dc8-c4ad-4a17-b6f7-3021e0c4bef0   \n",
       "378093  001b0556-f343-4666-91dc-509bb020d8e0   \n",
       "378089  001b0556-f343-4666-91dc-509bb020d8e0   \n",
       "378129  001b0556-f343-4666-91dc-509bb020d8e0   \n",
       "379159  001b0556-f343-4666-91dc-509bb020d8e0   \n",
       "379165  001b0556-f343-4666-91dc-509bb020d8e0   \n",
       "334449  001dd34f-fe17-4ad2-bd53-67f2fde2d62b   \n",
       "346985  001dd34f-fe17-4ad2-bd53-67f2fde2d62b   \n",
       "398921  001e036e-5214-4897-9e3e-eac649566d98   \n",
       "398918  001e036e-5214-4897-9e3e-eac649566d98   \n",
       "344592  00212408-ca30-4aab-a09f-01b4ae0bf54d   \n",
       "356834  0024482e-70d5-4744-878d-f14d884f6ef4   \n",
       "426136  0024c4ae-6cf8-4438-94ba-5a38d7535f19   \n",
       "\n",
       "                                             user_query_x       created_at  \n",
       "339355  What's the least expensive crypto coin to trad...    8/3/2024 2:01  \n",
       "339356  The crypto market which one cost the less in \"...    8/3/2024 2:02  \n",
       "339358                Thank you, you're always a big help    8/3/2024 2:03  \n",
       "394108                                               Ciao  8/22/2024 17:08  \n",
       "356592  Get me trades for today to buy for weekend gai...   8/9/2024 19:25  \n",
       "356600  Which of these is highest probability, high bu...   8/9/2024 19:30  \n",
       "356611  Give me your best picks for trades on blnd and...   8/9/2024 19:35  \n",
       "356624  If they go up what is the profit possible on e...   8/9/2024 19:38  \n",
       "356630  Possible profit from option on blnd and fivn a...   8/9/2024 19:41  \n",
       "424434  Top 3 Call option contracts in the AI hardware...    9/1/2024 4:59  \n",
       "398844               What stock made the most money today  8/23/2024 20:45  \n",
       "398848       What penny stocks wil make me $100 in a day?  8/23/2024 20:47  \n",
       "369715  Weekly Swing Trade: Find me an options trade t...  8/14/2024 16:44  \n",
       "326673  Weekly Swing Trade: Find me an options trade t...   9/4/2024 12:56  \n",
       "345484  What is the current market sentiment for ARM. ...    8/6/2024 4:49  \n",
       "347109  Whats my stop loss and take profit for LCID s...   8/6/2024 18:29  \n",
       "347342  Can you tell me which stock has potential trad...   8/6/2024 19:46  \n",
       "376739  Does the healthcare stock THC have strong fund...  8/16/2024 12:38  \n",
       "376730             Thank you.  That answers my questions.  8/16/2024 12:39  \n",
       "414107                     Is royal Caribbean a good buy?  8/28/2024 19:02  \n",
       "414123                               Entry point for rcl   8/28/2024 19:04  \n",
       "370824  What is the future of CCJ? is it a good invest...  8/14/2024 21:49  \n",
       "370825                                                Yes  8/14/2024 21:50  \n",
       "370833                                                Yes  8/14/2024 21:54  \n",
       "370839  Create a Call options for CCJ with the highest...  8/14/2024 21:55  \n",
       "370841  How much capital would I need to execute this ...  8/14/2024 22:00  \n",
       "391654  What are best indicators to use for trading Na...   8/22/2024 0:57  \n",
       "328976  Give me the best stock to buy today and sell t...   9/4/2024 23:26  \n",
       "426755  A Federal Reserve rate cut can have several im...   9/2/2024 15:17  \n",
       "426757  What is the outlook for NVDA this month (Septe...   9/2/2024 15:18  \n",
       "399960  When is the best stock to trade that is near i...   8/24/2024 8:04  \n",
       "399961      But these are nowhere near their 52 week glow   8/24/2024 8:04  \n",
       "399962  What are the best stocks to trade that are 5% ...   8/24/2024 8:05  \n",
       "399967  What are the best stocks to trade that are 5% ...   8/24/2024 8:11  \n",
       "399968                              List only the tickers   8/24/2024 8:11  \n",
       "399970  Give me the absolute best trade to make right ...   8/24/2024 8:14  \n",
       "350157  Give me a buying entry level for spy based on ...   8/7/2024 18:20  \n",
       "338680                                   options on amzon   8/2/2024 20:01  \n",
       "378093                                        Stock alert  8/16/2024 17:41  \n",
       "378089                                     Stock alert up  8/16/2024 17:42  \n",
       "378129                              Stocks alert today up  8/16/2024 17:48  \n",
       "379159                                  Stocks  alerts up   8/17/2024 0:14  \n",
       "379165                                  Stock alert down    8/17/2024 0:24  \n",
       "334449  Is it smart to offset a covered call that is o...  8/19/2024 17:22  \n",
       "346985                                               Pltr  8/19/2024 17:22  \n",
       "398921                    Dimers top mlb games  for today  8/23/2024 21:19  \n",
       "398918            What are top predictions  for mlb today  8/23/2024 21:20  \n",
       "344592       What is the best put option i can make today   8/5/2024 19:50  \n",
       "356834  Will spy open up or down on Monday and will it...   8/9/2024 21:04  \n",
       "426136                           Best options strategies     9/2/2024 4:34  "
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_sorted.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "1999c08d-73e6-488f-b652-fe29c524eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_time = df1_sorted.groupby('session_id')['created_at'].apply(min).reset_index(name='session_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "1f2c3cb3-7cb7-4cd9-9167-e12ff6bf5991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35267, 3)"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_session = df1_sorted.groupby('session_id')['user_query_x'].apply(list).reset_index(name='quer_list')\n",
    "df_session['session_time'] = session_time['session_time']\n",
    "df_session.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "d26d5067-114b-48e1-9ffa-b2e4438b0b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>quer_list</th>\n",
       "      <th>session_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00010369-2ee4-459b-9080-43c6793c32ff</td>\n",
       "      <td>[What's the least expensive crypto coin to tra...</td>\n",
       "      <td>8/3/2024 2:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0003c604-45a5-4f1d-8b87-b684ef3641e4</td>\n",
       "      <td>[Ciao]</td>\n",
       "      <td>8/22/2024 17:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00054a16-b0cb-4c2e-84c1-505a7e859899</td>\n",
       "      <td>[Get me trades for today to buy for weekend ga...</td>\n",
       "      <td>8/9/2024 19:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0006538f-33de-4fb9-8f57-19835fb2e110</td>\n",
       "      <td>[Top 3 Call option contracts in the AI hardwar...</td>\n",
       "      <td>9/1/2024 4:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0007bc3e-d9ec-4a4f-b79c-b2f7069d0a4e</td>\n",
       "      <td>[What stock made the most money today, What pe...</td>\n",
       "      <td>8/23/2024 20:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000cf849-868c-444a-adf9-1bc84655da4b</td>\n",
       "      <td>[Weekly Swing Trade: Find me an options trade ...</td>\n",
       "      <td>8/14/2024 16:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000e11e4-592b-414a-8825-2a217a663853</td>\n",
       "      <td>[What is the current market sentiment for ARM....</td>\n",
       "      <td>8/6/2024 4:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000e3867-a447-47da-81fa-91a9c22b3675</td>\n",
       "      <td>[Whats my stop loss and take profit for LCID ...</td>\n",
       "      <td>8/6/2024 18:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0011fb82-cf49-4349-b7cf-760b191d0f4d</td>\n",
       "      <td>[Does the healthcare stock THC have strong fun...</td>\n",
       "      <td>8/16/2024 12:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00127f9d-ef7c-4124-a35b-c98f30c2eca2</td>\n",
       "      <td>[Is royal Caribbean a good buy?, Entry point f...</td>\n",
       "      <td>8/28/2024 19:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             session_id  \\\n",
       "0  00010369-2ee4-459b-9080-43c6793c32ff   \n",
       "1  0003c604-45a5-4f1d-8b87-b684ef3641e4   \n",
       "2  00054a16-b0cb-4c2e-84c1-505a7e859899   \n",
       "3  0006538f-33de-4fb9-8f57-19835fb2e110   \n",
       "4  0007bc3e-d9ec-4a4f-b79c-b2f7069d0a4e   \n",
       "5  000cf849-868c-444a-adf9-1bc84655da4b   \n",
       "6  000e11e4-592b-414a-8825-2a217a663853   \n",
       "7  000e3867-a447-47da-81fa-91a9c22b3675   \n",
       "8  0011fb82-cf49-4349-b7cf-760b191d0f4d   \n",
       "9  00127f9d-ef7c-4124-a35b-c98f30c2eca2   \n",
       "\n",
       "                                           quer_list     session_time  \n",
       "0  [What's the least expensive crypto coin to tra...    8/3/2024 2:01  \n",
       "1                                             [Ciao]  8/22/2024 17:08  \n",
       "2  [Get me trades for today to buy for weekend ga...   8/9/2024 19:25  \n",
       "3  [Top 3 Call option contracts in the AI hardwar...    9/1/2024 4:59  \n",
       "4  [What stock made the most money today, What pe...  8/23/2024 20:45  \n",
       "5  [Weekly Swing Trade: Find me an options trade ...  8/14/2024 16:44  \n",
       "6  [What is the current market sentiment for ARM....    8/6/2024 4:49  \n",
       "7  [Whats my stop loss and take profit for LCID ...   8/6/2024 18:29  \n",
       "8  [Does the healthcare stock THC have strong fun...  8/16/2024 12:38  \n",
       "9  [Is royal Caribbean a good buy?, Entry point f...  8/28/2024 19:02  "
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_session.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "191ce72e-1cee-4938-b6bc-b6c8ad68fd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35267"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = df_session['quer_list'].tolist()[:100000]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b2509-fb93-4b2b-8601-ec4d50c0d2fe",
   "metadata": {},
   "source": [
    "### LLM sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "b3b662ec-0731-4e4a-b659-ae934f734337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "MAX_CONCURRENCY = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "9b5c1cc6-5c78-4f00-9246-9ec53a6cbd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "semaphore = asyncio.Semaphore(MAX_CONCURRENCY)\n",
    "server_host = \"http://100.84.186.114:8080\"\n",
    "# model = \"hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4\"\n",
    "model=\"gpt-3.5-turbo\"\n",
    "# api_key = \"TSGS@LLM\"\n",
    "api_key=\"sk-proj-DlLitL7d2xVm...\"\n",
    "async_client = AsyncOpenAI(\n",
    "    #base_url=server_host + \"/v1\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b246db-dc55-4630-903b-73753db40036",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# objective_prompt = \"\"\"\n",
    "#         Objective:\n",
    "#         You are a professional stock market analyst.\n",
    "#         Given user input, your task is to analyze sentiment and generate sentiment score for target company.\n",
    "#         Additionally, please provide a short, concise and clear reasoning for entire anlysis.\n",
    "#         \"\"\"\n",
    "\n",
    "#         steps_prompt = f\"\"\"\n",
    "#         Instructions:\n",
    "#         1. Read headline and preview thoroughly to understand full context.\n",
    "#         2. Identify the target company being analyzed based on given ticker name {ticker}. Ensure you concentrate on the target company's portrayal without being influenced by other companies' sentiment.\n",
    "#         3. Assign an overall sentiment for {ticker}. Please choose an answer from 'Positive, Neutral, Negative'. \n",
    "#         4. Assign a sentiment score from -100 to 100 for {ticker}.\n",
    "#             * -100 to -1 represents Negative sentiment (lower score indicates stronger negativity)\n",
    "#             * 0 represents Neutral sentiment\n",
    "#             * 1 to 100 represents Positive sentiment (higher score indicates stronger positivity)\n",
    "#         5. Reflect on your analysis and make changes if necessary.\n",
    "#         6. Generate final output following below output format.\n",
    "#         \"\"\"\n",
    "\n",
    "#         output_prompt = f\"\"\"\n",
    "#         Output Format:\n",
    "#         [Sentiment]: Positive, Neutral, or Negative\n",
    "#         [Score]: ...\n",
    "#         [Reasoning]: ...\n",
    "#         \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e3586-90ab-47ac-b018-7406e7f1075e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# prompt = f\"\"\"\n",
    "# You are an NLP expert specializing in sentiment analysis. \n",
    "# Your task involves analyzing user queries, which may be either new messages or follow-up messages.\n",
    "# Your job is:\n",
    "# step 1: to classify if each query is \"new message\" or \"followup message\";\n",
    "# step 2: for followup message, perform sentiment analysis and assign 'positive', 'negative' or 'neutral', following this definition:\n",
    "#     * 'positive': user seems satisfied with the answer given, by saying 'yes','thank you','go ahead', 'implemend it', or the tone sounds positive;\n",
    "#     * 'negative': user seems unsatisfied with the answer given, by saying 'no' or the tone sounds negative;\n",
    "#     * 'neutral': user seems neither obviously happy or unhappy, or you cannot tell from the  user query\n",
    "    \n",
    "# Please generate output in one word: positive, neutral, or negative\n",
    "# followed by a reasoning analysis of less than 10 words. \n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# # [message type]: new or followup\n",
    "# # [sentiment]: positive, neutral, or negative\n",
    "# prompt += \"You are given following user query: {message}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4c7dc7-d5df-407b-9e9b-891ee0b5db95",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# prompt = f\"\"\"\n",
    "# You are an NLP expert specializing in sentiment analysis. \n",
    "# Your task involves analyzing user queries, which may be either new messages or follow-up messages.\n",
    "\n",
    "# Definitions:\n",
    "# New Message: A query that does not reference previous interactions or context. For example:\n",
    "#     \"What are the current trends in the stock market?\"\n",
    "#     \"Can you explain how options trading works?\"\n",
    "# Follow-up Message: A query that references or builds upon previous interactions. For example:\n",
    "#     \"Can you provide more details on that strategy?\"\n",
    "#     \"I liked your last analysis; can we proceed with it?\"\n",
    "\n",
    "# Tasks:\n",
    "# 1. Classify the Query:\n",
    "#     Determine if the query is a \"new message\" or a \"follow-up message.\"\n",
    "# 2. Sentiment Analysis for Follow-up Messages: If the query is a follow-up message, perform sentiment analysis and classify it as 'positive', 'negative', or 'neutral' based on the following criteria:\n",
    "#     Positive: The user expresses satisfaction. Examples include:\n",
    "#         \"Thank you for that insightful analysis!\"\n",
    "#         \"Yes, let's implement this strategy!\"\n",
    "#     Negative: The user expresses dissatisfaction. Examples include:\n",
    "#         \"No, that advice didn’t work.\"\n",
    "#         \"I’m not happy with those results.\"\n",
    "#     Neutral: The user is neither clearly satisfied nor dissatisfied, or the sentiment is unclear. Examples include:\n",
    "#         \"Can you clarify that recommendation?\"\n",
    "#         \"I’m not sure about the next steps.\"\n",
    "# Output:\n",
    "#     Provide the sentiment classification in one word: positive, neutral, or negative.\n",
    "#     Include a brief reasoning analysis of less than 10 words.\n",
    "    \n",
    "# Example Queries:\n",
    "#     Query: \"What are the current trends in the stock market?\"\n",
    "#     Output: New message\n",
    "#     Query: \"Can you provide more details on that strategy?\"\n",
    "#     Output: Neutral\n",
    "#     Query: \"Thank you for your market insights!\"\n",
    "#     Output: Positive - User expresses satisfaction.\n",
    "#     Query: \"No, that advice didn’t work.\"\n",
    "#     Output: Negative - User expresses dissatisfaction.\n",
    "#     Query: \"I’m not sure about the next steps.\"\n",
    "#     Output: Neutral - Unclear sentiment.\n",
    "# \"\"\"\n",
    "\n",
    "# prompt += \"You are given following user query: {message}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a108c8-334e-4008-96b5-902413f8d40a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are an NLP expert specializing in sentiment analysis based on a session of user queries. \n",
    "Each session may consists of multiple user queries, each query may be a question or a response to an answer provided by a machine, but only the user's text is provided. \n",
    "Focus on identifying the sentiment conveyed in each session, whether it is positive, negative, or neutral.\n",
    "\n",
    "Please base on the following criteria:\n",
    "    Positive: The user expresses satisfaction. Examples include:\n",
    "        \"Thank you for that insightful analysis!\"\n",
    "        \"Yes, let's implement this strategy!\"\n",
    "    Negative: The user expresses dissatisfaction. Examples include:\n",
    "        \"No, that advice didn’t work.\"\n",
    "        \"I’m not happy with those results.\"\n",
    "    Neutral: The user is neither clearly satisfied nor dissatisfied, or the sentiment is unclear. Examples include:\n",
    "        \"What are the current trends in the stock market?\"\n",
    "        \"Can you clarify that recommendation?\"\n",
    "        \"I’m not sure about the next steps.\"\n",
    "Output:\n",
    "    Strictly limit your sentiment classification answer in one word: positive, neutral, or negative.\n",
    "\n",
    "    \n",
    "Example Queries:\n",
    "    Query: \"What are the current trends in the stock market?\"\n",
    "    Output: Neutral\n",
    "    Query: \"Can you provide more details on that strategy?\"\n",
    "    Output: Neutral\n",
    "    Query: \"Thank you for your market insights!\"\n",
    "    Output: Positive\n",
    "    Query: \"No, that advice didn’t work.\"\n",
    "    Output: Negative\n",
    "    Query: \"I’m not sure about the next steps.\"\n",
    "    Output: Neutral\n",
    "\"\"\"\n",
    "#    Include a brief reasoning analysis of less than 10 words.\n",
    "    # Query: \"What are the current trends in the stock market?\"\n",
    "    # Output: Neutral - Just a question.\n",
    "    # Query: \"Can you provide more details on that strategy?\"\n",
    "    # Output: Neutral - Just a question.\n",
    "    # Query: \"Thank you for your market insights!\"\n",
    "    # Output: Positive - User expresses satisfaction.\n",
    "    # Query: \"No, that advice didn’t work.\"\n",
    "    # Output: Negative - User expresses dissatisfaction.\n",
    "    # Query: \"I’m not sure about the next steps.\"\n",
    "    # Output: Neutral - Unclear sentiment.\n",
    "prompt += \"You are given- Query: {message} Output:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "3c4cd3b4-5351-4280-832c-0547c68f7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are an NLP expert specializing in sentiment analysis based on a session of user queries. \n",
    "Each session may consists of multiple user queries, each query may be a question or a response to an answer provided by a machine, but only the user's text is provided. \n",
    "Focus on identifying the sentiment conveyed in each session, whether it is positive, negative, or neutral.\n",
    "The sentiment should be purely based on reaction to the quality or relevance of the machine's answers, and ignore any sentiment related to external factors such as market conditions, portfolios, or trading.\n",
    "\n",
    "Please base on the following criteria:\n",
    "    Positive: The user expresses satisfaction in at least one of the query. Examples include:\n",
    "        \"Thank you for that insightful analysis!\"\n",
    "        \"Yes, let's implement this strategy!\"\n",
    "    Negative: The user expresses dissatisfaction in at least one of the query. Examples include:\n",
    "        \"No, that advice didn’t work.\"\n",
    "        \"I’m not happy with those results.\"\n",
    "    Neutral: The user is neither satisfied nor dissatisfied, or the sentiment is unclear. Examples include:\n",
    "        \"What are the current trends in the stock market?\"\n",
    "        \"Can you clarify that recommendation?\"\n",
    "        \"I’m not sure about the next steps.\"\n",
    "        \n",
    "**Output Requirements:**\n",
    "1. **Sentiment:** Provide a **one-word** sentiment classification: **Positive**, **Neutral**, or **Negative**.\n",
    "2. **Reasoning:** Provide a concise and insightful explanation for the sentiment. If the sentiment is positive, summarize what user is happy with; if negative,  what user is not happy about and how do you suggest to improve \n",
    "\n",
    "**Formatting:**\n",
    "- Start with \"Sentiment:\" followed by the classification.\n",
    "- Continue with \"Reasoning:\" followed by the explanation.\n",
    "\n",
    "**Examples for Guidance:**\n",
    "**Session 1:**\n",
    "    Query 1: \"What are the current trends in the stock market?\"\n",
    "    Query 2: \"Thank you for the detailed overview.\"\n",
    "    Query 3: \"whats the outlook for PLUG?\"\n",
    "**Output:**\n",
    "  Sentiment: Positive\n",
    "  Reasoning: user expresses satisfaction with the detailed overview after asking about stock market trend\n",
    "\n",
    "**Session 2:**\n",
    "    Query 1: \"Give me the best stocks to trade.\"\n",
    "    Query 2: \"No, that is not about right.\"\n",
    "**Output:**\n",
    "  Sentiment: Negative\n",
    "  Reasoning: user expresses dissatisfaction with the advice given, by saying the recommended stocks to trade are not right. \n",
    "\"\"\"\n",
    "\n",
    "prompt += \"You are given- **session**: {message} Overall Sentiment:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f085cc6c-7dd8-4530-a5bf-3d10071a89e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message = ['How many shares of Tsla was sold',\n",
    "#  'How many shares of TSLA was sold yesterday',\n",
    "#  'How many of those share were a sell order ']\n",
    "# print(prompt.format(message = message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "4def937a-770f-445e-af92-8fb191fe87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_review(text):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            input = [{\"role\": \"user\", \"content\": prompt.format(message=text)}]\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=input,\n",
    "                temperature=0,\n",
    "                logprobs = True\n",
    "            )\n",
    "            res = response.choices[0].message.content\n",
    "            #logprobs = response.choices[0].logprobs.content[0].logprob\n",
    "            return res #, logprobs\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Failed {text}\")\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "ccc4f702-e2b2-4724-8755-a59334518a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(docs):\n",
    "    tasks = []\n",
    "    for i, text in enumerate(docs):  # Use enumerate to get both index and text\n",
    "        tasks.append((i, process_review(text)))  # Store index and task together\n",
    "\n",
    "    # Use list comprehension to gather results and maintain index association\n",
    "    res = await tqdm.gather(*(task[1] for task in tasks))\n",
    "\n",
    "    # print('res:', res)\n",
    "    # print(range(len(res)))\n",
    "    # print(range(len(tasks)))\n",
    "    length = len(docs)\n",
    "\n",
    "    final_output = [(tasks[i][0], res[i]) for i in range(length)]\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "cde87b35-9c16-4706-8515-619ec579134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message = [\n",
    "#     ['How many shares of Tsla was sold, bcoz the market is really really positive','yes, thank you for that']\n",
    "#     ,['Why is the stock market is down today ', 'when is inflation rate is coming out on this month and what can be effect for us stock market']\n",
    "#     ,['What is the outlook for PLUG', 'Thank you', 'What is the outlook for WRAP?', 'what is the long term outlook for WRAP']\n",
    "# ,['Fast turnaround stock market investment', 'Short-term trading', 'Yes please', 'Any other suggestions?', 'I would like more detailed analysis please', 'Thanks bud']\n",
    "#     ,['Please provide me with the top 3 stocks that had inside days on Friday, July 19, 2024', \"That isn't true none of these stocks were inside on the day for Friday July 19,2024\", 'Please provide me with the top 3 stocks that had inside days on Friday, July 19, 2024', 'What are the top 3 stocks to buy for trade options on Monday July 22,2024', 'What are the top 3 small cap stocks to buy for trade options on Monday July 22,2024']       \n",
    "#     ,['What stocks are best to swing trade today', 'Yes', 'What target price should I buy surmidics', 'What target price should I buy ainos', 'How to turn 100 into 10,000 today', 'What is the most stock is most profitable to day trade right now', 'Show me a daytrading strategy for these stocks']\n",
    "# ,[\"Give me the past 5 quarters' EPS for TSLA.\", 'what the best stock to but for profit ', 'what is the price prediction for terra luna ', 'what is the price prediction for terra luna classic ', 'what is the price prediction for bcel atreca ', 'what will happen to bbbyq shorts', 'will bbbyq share holders be compensated', 'what is the price prediction for amanx']\n",
    "# ,['Do a comparison between NVDA and AMD.', 'fastest momentm stock with minimum risk today', 'fastest momentum stock low risk today May 15, 2024', 'User\\nfastest momentum stock low risk today  with large volume May 15, 2024']\n",
    "# ]\n",
    "# await main(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "070be628-a4d3-427a-a03e-590a667d573f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 376/35267 [00:19<25:09, 23.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 79874 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Failed ['Clean the following code, optimize it, and add try exception  to it\\n\\nimport logging\\nimport os\\n\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\n# Formats Position\\nformat_position = lambda price: (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n# Formats Currency\\nformat_currency = lambda price: \\'${0:.2f}\\'.format(abs(price))\\n\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\" Displays training results\\n    \"\"\"\\n    if val_position == initial_offset or val_position == 0.0:\\n        logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: USELESS  Train Loss: {:.4f}\\'\\n                     .format(strategy, result[0], result[1], format_position(result[2]), result[3]))\\n        return False\\n    else:\\n        logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: {}  Train Loss: {:.4f})\\'\\n                     .format(strategy, result[0], result[1], format_position(result[2]), format_position(val_position),\\n                             result[3], ))\\n        return True\\n\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\" Displays eval results\\n    \"\"\"\\n    if profit == initial_offset or profit == 0.0:\\n        logging.info(\\'{}: USELESS\\\\n\\'.format(model_name))\\n    else:\\n        logging.info(\\'{}: {}\\\\n\\'.format(model_name, format_position(profit)))\\n\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\"Reads stock data from csv file\\n    \"\"\"\\n\\n    df = pd.read_csv(stock_file)\\n    if date_field != \"Date\":\\n        df[\\'Date\\'] = df[date_field]\\n\\n    # df = df.sort_values([date_field], ascending=False)\\n    # df[\\'diff\\'] = df[adjcp_field].diff(periods=5)\\n    df[\\'adjcp\\'] = df[adjcp_field]\\n    df = df.sort_values([date_field], ascending=True)\\n    # Add Indicators and Pattern Recognition Functions\\n    df = add_indicators(df)\\n    return df\\n\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\"Reads stock data from csv file\\n    \"\"\"\\n    df = pd.read_csv(stock_file)\\n    df = df.sort_values([date_field])\\n    return list(df[adjcp_field])\\n\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\"\\n    split the dataset into training or testing using date\\n    :param df: (df) pandas dataframe\\n    :param start:\\n    :param end:\\n    :return: (df) pandas dataframe\\n    \"\"\"\\n    data = df[(df.index >= start) & (df.index < end)]\\n    data.reset_index()\\n    return data\\n\\n\\ndef switch_k_backend_device():\\n    \"\"\" Switches `keras` backend from GPU to CPU if required.\\n\\n    Faster computation on CPU (if using tensorflow-gpu).\\n    \"\"\"\\n    if keras.backend == \"tensorflow\":\\n        logging.debug(\"switching to TensorFlow for CPU\")\\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n\\n\\ndef get_features():\\n    all_feature_name = [\\n        # \\'CDL2CROWS\\',\\n        # \\'CDL3BLACKCROWS\\',\\n        # \\'CDL3INSIDE\\',\\n        # \\'CDL3LINESTRIKE\\',\\n        # \\'CDL3OUTSIDE\\',\\n        # \\'CDL3STARSINSOUTH\\',\\n        # \\'CDL3WHITESOLDIERS\\',\\n        # \\'CDLABANDONEDBABY\\',\\n        # \\'CDLADVANCEBLOCK\\',\\n        # \\'CDLBELTHOLD\\',\\n        # \\'CDLBREAKAWAY\\',\\n        # \\'CDLCLOSINGMARUBOZU\\',\\n        # \\'CDLCONCEALBABYSWALL\\',\\n        # \\'CDLCOUNTERATTACK\\',\\n        \\'CDLDARKCLOUDCOVER\\',\\n        # \\'CDLDOJI\\',\\n        # \\'CDLDOJISTAR\\',\\n        \\'CDLDRAGONFLYDOJI\\',\\n        \\'CDLENGULFING\\',\\n        # \\'CDLEVENINGDOJISTAR\\',\\n        \\'CDLEVENINGSTAR\\',\\n        # \\'CDLGAPSIDESIDEWHITE\\',\\n        \\'CDLGRAVESTONEDOJI\\',\\n        \\'CDLHAMMER\\',\\n        \\'CDLHANGINGMAN\\',\\n        # \\'CDLHARAMI\\',\\n        # \\'CDLHARAMICROSS\\',\\n        # \\'CDLHIGHWAVE\\',\\n        # \\'CDLHIKKAKE\\',\\n        # \\'CDLHIKKAKEMOD\\',\\n        # \\'CDLHOMINGPIGEON\\',\\n        # \\'CDLIDENTICAL3CROWS\\',\\n        # \\'CDLINNECK\\',\\n        # \\'CDLINVERTEDHAMMER\\',\\n        # \\'CDLKICKING\\',\\n        # \\'CDLKICKINGBYLENGTH\\',\\n        # \\'CDLLADDERBOTTOM\\',\\n        # \\'CDLLONGLEGGEDDOJI\\',\\n        # \\'CDLLONGLINE\\',\\n        # \\'CDLMARUBOZU\\',\\n        # \\'CDLMATCHINGLOW\\',\\n        # \\'CDLMATHOLD\\',\\n        # \\'CDLMORNINGDOJISTAR\\',\\n        \\'CDLMORNINGSTAR\\',\\n        # \\'CDLONNECK\\',\\n        # \\'CDLPIERCING\\',\\n        # \\'CDLRICKSHAWMAN\\',\\n        # \\'CDLRISEFALL3METHODS\\',\\n        # \\'CDLSEPARATINGLINES\\',\\n        # \\'CDLSHOOTINGSTAR\\',\\n        # \\'CDLSHORTLINE\\',\\n        # \\'CDLSPINNINGTOP\\',\\n        # \\'CDLSTALLEDPATTERN\\',\\n        # \\'CDLSTICKSANDWICH\\',\\n        # \\'CDLTAKURI\\',\\n        # \\'CDLTASUKIGAP\\',\\n        # \\'CDLTHRUSTING\\',\\n        # \\'CDLTRISTAR\\',\\n        # \\'CDLUNIQUE3RIVER\\',\\n        # \\'CDLUPSIDEGAP2CROWS\\',\\n        # \\'CDLXSIDEGAP3METHODS\\',\\n        \\'MOMENTUM\\',\\n        \\'DX\\',\\n        \\'FASTK\\',\\n        \\'FASTD\\',\\n        \\'DX\\',\\n        \\'CCI\\',\\n        \\'WILLR\\',\\n        \\'MFI\\',\\n        \\'ROC\\',\\n        \\'ADX\\',\\n        \\'PPO\\',\\n        \\'RSI\\',\\n        \\'STDDEV\\',\\n        \\'SIN\\',\\n        \\'COS\\',\\n        \\'TAN\\'\\n    ]\\n    return all_feature_name\\n\\n\\ndef add_indicators(df):\\n    # Pattern Recognition Functions\\n    # df[\\'CDL2CROWS\\'] = talib.CDL2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3BLACKCROWS\\'] = talib.CDL3BLACKCROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3INSIDE\\'] = talib.CDL3INSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3LINESTRIKE\\'] = talib.CDL3LINESTRIKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3OUTSIDE\\'] = talib.CDL3OUTSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3STARSINSOUTH\\'] = talib.CDL3STARSINSOUTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3WHITESOLDIERS\\'] = talib.CDL3WHITESOLDIERS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLABANDONEDBABY\\'] = talib.CDLABANDONEDBABY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLADVANCEBLOCK\\'] = talib.CDLADVANCEBLOCK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLBELTHOLD\\'] = talib.CDLBELTHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLBREAKAWAY\\'] = talib.CDLBREAKAWAY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCLOSINGMARUBOZU\\'] = talib.CDLCLOSINGMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCONCEALBABYSWALL\\'] = talib.CDLCONCEALBABYSWALL(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCOUNTERATTACK\\'] = talib.CDLCOUNTERATTACK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDARKCLOUDCOVER\\'] = talib.CDLDARKCLOUDCOVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLDOJI\\'] = talib.CDLDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLDOJISTAR\\'] = talib.CDLDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDRAGONFLYDOJI\\'] = talib.CDLDRAGONFLYDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLENGULFING\\'] = talib.CDLENGULFING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLEVENINGDOJISTAR\\'] = talib.CDLEVENINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLEVENINGSTAR\\'] = talib.CDLEVENINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLGAPSIDESIDEWHITE\\'] = talib.CDLGAPSIDESIDEWHITE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLGRAVESTONEDOJI\\'] = talib.CDLGRAVESTONEDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHAMMER\\'] = talib.CDLHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHANGINGMAN\\'] = talib.CDLHANGINGMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHARAMI\\'] = talib.CDLHARAMI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHARAMICROSS\\'] = talib.CDLHARAMICROSS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIGHWAVE\\'] = talib.CDLHIGHWAVE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIKKAKE\\'] = talib.CDLHIKKAKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIKKAKEMOD\\'] = talib.CDLHIKKAKEMOD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHOMINGPIGEON\\'] = talib.CDLHOMINGPIGEON(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLIDENTICAL3CROWS\\'] = talib.CDLIDENTICAL3CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLINNECK\\'] = talib.CDLINNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLINVERTEDHAMMER\\'] = talib.CDLINVERTEDHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLKICKING\\'] = talib.CDLKICKING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLKICKINGBYLENGTH\\'] = talib.CDLKICKINGBYLENGTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLADDERBOTTOM\\'] = talib.CDLLADDERBOTTOM(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLONGLEGGEDDOJI\\'] = talib.CDLLONGLEGGEDDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLONGLINE\\'] = talib.CDLLONGLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMARUBOZU\\'] = talib.CDLMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMATCHINGLOW\\'] = talib.CDLMATCHINGLOW(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMATHOLD\\'] = talib.CDLMATHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLMORNINGDOJISTAR\\'] = talib.CDLMORNINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLMORNINGSTAR\\'] = talib.CDLMORNINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLONNECK\\'] = talib.CDLONNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLPIERCING\\'] = talib.CDLPIERCING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLRICKSHAWMAN\\'] = talib.CDLRICKSHAWMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLRISEFALL3METHODS\\'] = talib.CDLRISEFALL3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSEPARATINGLINES\\'] = talib.CDLSEPARATINGLINES(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSHOOTINGSTAR\\'] = talib.CDLSHOOTINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSHORTLINE\\'] = talib.CDLSHORTLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSPINNINGTOP\\'] = talib.CDLSPINNINGTOP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSTALLEDPATTERN\\'] = talib.CDLSTALLEDPATTERN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSTICKSANDWICH\\'] = talib.CDLSTICKSANDWICH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTAKURI\\'] = talib.CDLTAKURI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTASUKIGAP\\'] = talib.CDLTASUKIGAP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTHRUSTING\\'] = talib.CDLTHRUSTING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTRISTAR\\'] = talib.CDLTRISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLUNIQUE3RIVER\\'] = talib.CDLUNIQUE3RIVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLUPSIDEGAP2CROWS\\'] = talib.CDLUPSIDEGAP2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLXSIDEGAP3METHODS\\'] = talib.CDLXSIDEGAP3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # Price action\\n    # https://ta-lib.github.io/ta-lib-python/func_groups/momentum_indicators.html\\n    # rsi,stochRsi,cci,williamsr,mfi,roc,atr,adx\\n    df[\\'MOMENTUM\\'] = talib.MOM(df[\\'Close\\'], timeperiod=10)\\n    df[\\'DX\\'] = talib.stream_DX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'FASTK\\'], df[\\'FASTD\\'] = talib.STOCHRSI(df[\\'Close\\'], timeperiod=14, fastk_period=5, fastd_period=3,\\n                                              fastd_matype=0)\\n    df[\\'CCI\\'] = talib.CCI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'WILLR\\'] = talib.WILLR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'MFI\\'] = talib.MFI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'], timeperiod=14)\\n    df[\\'ROC\\'] = talib.ROC(df[\\'Close\\'], timeperiod=10)\\n    # df[\\'ATR\\'] = talib.ATR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'ADX\\'] = talib.ADX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'PPO\\'] = talib.PPO(df[\\'Close\\'], fastperiod=12, slowperiod=26, matype=0)\\n    df[\\'RSI\\'] = talib.RSI(df[\\'Close\\'], timeperiod=14)\\n    df[\\'STDDEV\\'] = talib.STDDEV(df[\\'Close\\'], timeperiod=14, nbdev=1)\\n    df[\\'SIN\\'] = talib.SIN(df[\\'Close\\'])\\n    df[\\'COS\\'] = talib.COS(df[\\'Close\\'])\\n    df[\\'TAN\\'] = talib.TAN(df[\\'Close\\'])\\n\\n    df = df.fillna(0)\\n\\n    return df\\n', 'ValueError: Exception encountered when calling Sequential.call().\\n\\nInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 24, but received input with shape (32, 23)\\n\\nArguments received by Sequential.call():\\n  \\x95 inputs=tf.Tensor(shape=(32, 23), dtype=float32)\\n  \\x95 training=False\\n  \\x95 mask=None\\n', 'ValueError: Exception encountered when calling Sequential.call().\\n\\nInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 24, but received input with shape (32, 23)\\n\\nArguments received by Sequential.call():\\n  \\x95 inputs=tf.Tensor(shape=(32, 23), dtype=float32)\\n  \\x95 training=False\\n  \\x95 mask=None\\n\\nimport logging\\nimport os\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\n# Formats Position\\nformat_position = lambda price: (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n# Formats Currency\\nformat_currency = lambda price: \\'${0:.2f}\\'.format(abs(price))\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\"Displays training results\"\"\"\\n    try:\\n        if val_position == initial_offset or val_position == 0.0:\\n            logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: USELESS  Train Loss: {:.4f}\\'\\n                         .format(strategy, result[0], result[1], format_position(result[2]), result[3]))\\n            return False\\n        else:\\n            logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: {}  Train Loss: {:.4f}\\'\\n                         .format(strategy, result[0], result[1], format_position(result[2]), format_position(val_position), result[3]))\\n            return True\\n    except Exception as e:\\n        logging.error(f\"Error in show_train_result: {e}\")\\n        return False\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\"Displays eval results\"\"\"\\n    try:\\n        if profit == initial_offset or profit == 0.0:\\n            logging.info(\\'{}: USELESS\\\\n\\'.format(model_name))\\n        else:\\n            logging.info(\\'{}: {}\\\\n\\'.format(model_name, format_position(profit)))\\n    except Exception as e:\\n        logging.error(f\"Error in show_eval_result: {e}\")\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\"Reads stock data from csv file\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        if date_field != \"Date\":\\n            df[\\'Date\\'] = df[date_field]\\n        df[\\'adjcp\\'] = df[adjcp_field]\\n        df = df.sort_values([date_field], ascending=True)\\n        df = add_indicators(df)\\n        return df\\n    except Exception as e:\\n        logging.error(f\"Error in load_data: {e}\")\\n        return pd.DataFrame()\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\"Reads stock data from csv file\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        df = df.sort_values([date_field])\\n        return list(df[adjcp_field])\\n    except Exception as e:\\n        logging.error(f\"Error in get_stock_data: {e}\")\\n        return []\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\"Splits the dataset into training or testing using date\"\"\"\\n    try:\\n        data = df[(df.index >= start) & (df.index < end)]\\n        data.reset_index(drop=True, inplace=True)\\n        return data\\n    except Exception as e:\\n        logging.error(f\"Error in data_split: {e}\")\\n        return pd.DataFrame()\\n\\ndef switch_k_backend_device():\\n    \"\"\"Switches `keras` backend from GPU to CPU if required.\"\"\"\\n    try:\\n        if keras.backend == \"tensorflow\":\\n            logging.debug(\"Switching to TensorFlow for CPU\")\\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n    except Exception as e:\\n        logging.error(f\"Error in switch_k_backend_device: {e}\")\\n\\ndef get_features():\\n    \"\"\"Returns a list of all feature names\"\"\"\\n    return [\\n        \\'CDLDARKCLOUDCOVER\\', \\'CDLDRAGONFLYDOJI\\', \\'CDLENGULFING\\', \\'CDLEVENINGSTAR\\',\\n        \\'CDLGRAVESTONEDOJI\\', \\'CDLHAMMER\\', \\'CDLHANGINGMAN\\', \\'CDLMORNINGSTAR\\',\\n        \\'MOMENTUM\\', \\'DX\\', \\'FASTK\\', \\'FASTD\\', \\'CCI\\', \\'WILLR\\', \\'MFI\\', \\'ROC\\', \\'ADX\\',\\n        \\'PPO\\', \\'RSI\\', \\'STDDEV\\', \\'SIN\\', \\'COS\\', \\'TAN\\'\\n    ]\\n\\ndef add_indicators(df):\\n    \"\"\"Adds technical indicators to the dataframe\"\"\"\\n    try:\\n        df[\\'CDLDARKCLOUDCOVER\\'] = talib.CDLDARKCLOUDCOVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n        df[\\'CDLDRAGONFLYDOJI\\'] = talib.CDLDRAGONFLYDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n        df[\\'CDLENGULFING\\'] = talib.CDLENGULFING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n        df[\\'CDLEVENINGSTAR\\'] = talib.CDLEVENINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n        df[\\'CDLGRAVESTONEDOJI\\'] = talib.CDLGRAVESTONEDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n        df[\\'CDLHAMMER\\'] = talib.CDLHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n        df[\\'CDLHANGINGMAN\\'] = talib.CDLHANGINGMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n        df[\\'CDLMORNINGSTAR\\'] = talib.CDLMORNINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n        df[\\'MOMENTUM\\'] = talib.MOM(df[\\'Close\\'], timeperiod=10)\\n        df[\\'DX\\'] = talib.DX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n        df[\\'FASTK\\'], df[\\'FASTD\\'] = talib.STOCHRSI(df[\\'Close\\'], timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)\\n        df[\\'CCI\\'] = talib.CCI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n        df[\\'WILLR\\'] = talib.WILLR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n        df[\\'MFI\\'] = talib.MFI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'], timeperiod=14)\\n        df[\\'ROC\\'] = talib.ROC(df[\\'Close\\'], timeperiod=10)\\n        df[\\'ADX\\'] = talib.ADX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n        df[\\'PPO\\'] = talib.PPO(df[\\'Close\\'], fastperiod=12, slowperiod=26, matype=0)\\n        df[\\'RSI\\'] = talib.RSI(df[\\'Close\\'], timeperiod=14)\\n        df[\\'STDDEV\\'] = talib.STDDEV(df[\\'Close\\'], timeperiod=14, nbdev=1)\\n        df[\\'SIN\\'] = talib.SIN(df[\\'Close\\'])\\n        df[\\'COS\\'] = talib.COS(df[\\'Close\\'])\\n        df[\\'TAN\\'] = talib.TAN(df[\\'Close\\'])\\n        df = df.fillna(0)\\n        return df\\n    except Exception as e:\\n        logging.error(f\"Error in add_indicators: {e}\")\\n        return df', 'What can I add to the following code to smooth the data in order for the DRL to train and learn better\\n\\nimport logging\\nimport os\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\n# Formats Position\\nformat_position = lambda price: (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n# Formats Currency\\nformat_currency = lambda price: \\'${0:.2f}\\'.format(abs(price))\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\"Displays training results\"\"\"\\n    try:\\n        if val_position == initial_offset or val_position == 0.0:\\n            logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: USELESS  Train Loss: {:.4f}\\'\\n                         .format(strategy, result[0], result[1], format_position(result[2]), result[3]))\\n            return False\\n        else:\\n            logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: {}  Train Loss: {:.4f}\\'\\n                         .format(strategy, result[0], result[1], format_position(result[2]), format_position(val_position), result[3]))\\n            return True\\n    except Exception as e:\\n        logging.error(f\"Error in show_train_result: {e}\")\\n        return False\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\"Displays eval results\"\"\"\\n    try:\\n        if profit == initial_offset or profit == 0.0:\\n            logging.info(\\'{}: USELESS\\\\n\\'.format(model_name))\\n        else:\\n            logging.info(\\'{}: {}\\\\n\\'.format(model_name, format_position(profit)))\\n    except Exception as e:\\n        logging.error(f\"Error in show_eval_result: {e}\")\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\"Reads stock data from csv file\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        if date_field != \"Date\":\\n            df[\\'Date\\'] = df[date_field]\\n        df[\\'adjcp\\'] = df[adjcp_field]\\n        df = df.sort_values([date_field], ascending=True)\\n        df = add_indicators(df)\\n        return df\\n    except Exception as e:\\n        logging.error(f\"Error in load_data: {e}\")\\n        return pd.DataFrame()\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\"Reads stock data from csv file\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        df = df.sort_values([date_field])\\n        return list(df[adjcp_field])\\n    except Exception as e:\\n        logging.error(f\"Error in get_stock_data: {e}\")\\n        return []\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\"Splits the dataset into training or testing using date\"\"\"\\n    try:\\n        data = df[(df.index >= start) & (df.index < end)]\\n        data.reset_index(drop=True, inplace=True)\\n        return data\\n    except Exception as e:\\n        logging.error(f\"Error in data_split: {e}\")\\n        return pd.DataFrame()\\n\\ndef switch_k_backend_device():\\n    \"\"\"Switches `keras` backend from GPU to CPU if required.\"\"\"\\n    try:\\n        if keras.backend == \"tensorflow\":\\n            logging.debug(\"Switching to TensorFlow for CPU\")\\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n    except Exception as e:\\n        logging.error(f\"Error in switch_k_backend_device: {e}\")\\n\\n\\ndef get_features():\\n    \"\"\"Returns a list of all feature names\"\"\"\\n    all_feature_name = [\\n        # \\'CDL2CROWS\\',\\n        # \\'CDL3BLACKCROWS\\',\\n        # \\'CDL3INSIDE\\',\\n        # \\'CDL3LINESTRIKE\\',\\n        # \\'CDL3OUTSIDE\\',\\n        # \\'CDL3STARSINSOUTH\\',\\n        # \\'CDL3WHITESOLDIERS\\',\\n        # \\'CDLABANDONEDBABY\\',\\n        # \\'CDLADVANCEBLOCK\\',\\n        # \\'CDLBELTHOLD\\',\\n        # \\'CDLBREAKAWAY\\',\\n        # \\'CDLCLOSINGMARUBOZU\\',\\n        # \\'CDLCONCEALBABYSWALL\\',\\n        # \\'CDLCOUNTERATTACK\\',\\n        \\'CDLDARKCLOUDCOVER\\',\\n        # \\'CDLDOJI\\',\\n        # \\'CDLDOJISTAR\\',\\n        \\'CDLDRAGONFLYDOJI\\',\\n        \\'CDLENGULFING\\',\\n        # \\'CDLEVENINGDOJISTAR\\',\\n        \\'CDLEVENINGSTAR\\',\\n        # \\'CDLGAPSIDESIDEWHITE\\',\\n        \\'CDLGRAVESTONEDOJI\\',\\n        \\'CDLHAMMER\\',\\n        \\'CDLHANGINGMAN\\',\\n        # \\'CDLHARAMI\\',\\n        # \\'CDLHARAMICROSS\\',\\n        # \\'CDLHIGHWAVE\\',\\n        # \\'CDLHIKKAKE\\',\\n        # \\'CDLHIKKAKEMOD\\',\\n        # \\'CDLHOMINGPIGEON\\',\\n        # \\'CDLIDENTICAL3CROWS\\',\\n        # \\'CDLINNECK\\',\\n        # \\'CDLINVERTEDHAMMER\\',\\n        # \\'CDLKICKING\\',\\n        # \\'CDLKICKINGBYLENGTH\\',\\n        # \\'CDLLADDERBOTTOM\\',\\n        # \\'CDLLONGLEGGEDDOJI\\',\\n        # \\'CDLLONGLINE\\',\\n        # \\'CDLMARUBOZU\\',\\n        # \\'CDLMATCHINGLOW\\',\\n        # \\'CDLMATHOLD\\',\\n        # \\'CDLMORNINGDOJISTAR\\',\\n        \\'CDLMORNINGSTAR\\',\\n        # \\'CDLONNECK\\',\\n        # \\'CDLPIERCING\\',\\n        # \\'CDLRICKSHAWMAN\\',\\n        # \\'CDLRISEFALL3METHODS\\',\\n        # \\'CDLSEPARATINGLINES\\',\\n        # \\'CDLSHOOTINGSTAR\\',\\n        # \\'CDLSHORTLINE\\',\\n        # \\'CDLSPINNINGTOP\\',\\n        # \\'CDLSTALLEDPATTERN\\',\\n        # \\'CDLSTICKSANDWICH\\',\\n        # \\'CDLTAKURI\\',\\n        # \\'CDLTASUKIGAP\\',\\n        # \\'CDLTHRUSTING\\',\\n        # \\'CDLTRISTAR\\',\\n        # \\'CDLUNIQUE3RIVER\\',\\n        # \\'CDLUPSIDEGAP2CROWS\\',\\n        # \\'CDLXSIDEGAP3METHODS\\',\\n        \\'MOMENTUM\\',\\n        \\'DX\\',\\n        \\'FASTK\\',\\n        \\'FASTD\\',\\n        \\'DX\\',\\n        \\'CCI\\',\\n        \\'WILLR\\',\\n        \\'MFI\\',\\n        \\'ROC\\',\\n        \\'ADX\\',\\n        \\'PPO\\',\\n        \\'RSI\\',\\n        \\'STDDEV\\',\\n        \\'SIN\\',\\n        \\'COS\\',\\n        \\'TAN\\'\\n    ]\\n    return all_feature_name\\n\\n\\ndef add_indicators(df):\\n    \"\"\"Adds technical indicators to the dataframe\"\"\"\\n    # Pattern Recognition Functions\\n    # df[\\'CDL2CROWS\\'] = talib.CDL2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3BLACKCROWS\\'] = talib.CDL3BLACKCROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3INSIDE\\'] = talib.CDL3INSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3LINESTRIKE\\'] = talib.CDL3LINESTRIKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3OUTSIDE\\'] = talib.CDL3OUTSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3STARSINSOUTH\\'] = talib.CDL3STARSINSOUTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3WHITESOLDIERS\\'] = talib.CDL3WHITESOLDIERS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLABANDONEDBABY\\'] = talib.CDLABANDONEDBABY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLADVANCEBLOCK\\'] = talib.CDLADVANCEBLOCK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLBELTHOLD\\'] = talib.CDLBELTHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLBREAKAWAY\\'] = talib.CDLBREAKAWAY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCLOSINGMARUBOZU\\'] = talib.CDLCLOSINGMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCONCEALBABYSWALL\\'] = talib.CDLCONCEALBABYSWALL(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCOUNTERATTACK\\'] = talib.CDLCOUNTERATTACK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDARKCLOUDCOVER\\'] = talib.CDLDARKCLOUDCOVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLDOJI\\'] = talib.CDLDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLDOJISTAR\\'] = talib.CDLDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDRAGONFLYDOJI\\'] = talib.CDLDRAGONFLYDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLENGULFING\\'] = talib.CDLENGULFING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLEVENINGDOJISTAR\\'] = talib.CDLEVENINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLEVENINGSTAR\\'] = talib.CDLEVENINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLGAPSIDESIDEWHITE\\'] = talib.CDLGAPSIDESIDEWHITE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLGRAVESTONEDOJI\\'] = talib.CDLGRAVESTONEDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHAMMER\\'] = talib.CDLHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHANGINGMAN\\'] = talib.CDLHANGINGMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHARAMI\\'] = talib.CDLHARAMI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHARAMICROSS\\'] = talib.CDLHARAMICROSS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIGHWAVE\\'] = talib.CDLHIGHWAVE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIKKAKE\\'] = talib.CDLHIKKAKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIKKAKEMOD\\'] = talib.CDLHIKKAKEMOD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHOMINGPIGEON\\'] = talib.CDLHOMINGPIGEON(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLIDENTICAL3CROWS\\'] = talib.CDLIDENTICAL3CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLINNECK\\'] = talib.CDLINNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLINVERTEDHAMMER\\'] = talib.CDLINVERTEDHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLKICKING\\'] = talib.CDLKICKING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLKICKINGBYLENGTH\\'] = talib.CDLKICKINGBYLENGTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLADDERBOTTOM\\'] = talib.CDLLADDERBOTTOM(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLONGLEGGEDDOJI\\'] = talib.CDLLONGLEGGEDDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLONGLINE\\'] = talib.CDLLONGLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMARUBOZU\\'] = talib.CDLMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMATCHINGLOW\\'] = talib.CDLMATCHINGLOW(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMATHOLD\\'] = talib.CDLMATHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLMORNINGDOJISTAR\\'] = talib.CDLMORNINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLMORNINGSTAR\\'] = talib.CDLMORNINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLONNECK\\'] = talib.CDLONNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLPIERCING\\'] = talib.CDLPIERCING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLRICKSHAWMAN\\'] = talib.CDLRICKSHAWMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLRISEFALL3METHODS\\'] = talib.CDLRISEFALL3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSEPARATINGLINES\\'] = talib.CDLSEPARATINGLINES(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSHOOTINGSTAR\\'] = talib.CDLSHOOTINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSHORTLINE\\'] = talib.CDLSHORTLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSPINNINGTOP\\'] = talib.CDLSPINNINGTOP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSTALLEDPATTERN\\'] = talib.CDLSTALLEDPATTERN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSTICKSANDWICH\\'] = talib.CDLSTICKSANDWICH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTAKURI\\'] = talib.CDLTAKURI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTASUKIGAP\\'] = talib.CDLTASUKIGAP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTHRUSTING\\'] = talib.CDLTHRUSTING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTRISTAR\\'] = talib.CDLTRISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLUNIQUE3RIVER\\'] = talib.CDLUNIQUE3RIVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLUPSIDEGAP2CROWS\\'] = talib.CDLUPSIDEGAP2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLXSIDEGAP3METHODS\\'] = talib.CDLXSIDEGAP3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # Price action\\n    # https://ta-lib.github.io/ta-lib-python/func_groups/momentum_indicators.html\\n    # rsi,stochRsi,cci,williamsr,mfi,roc,atr,adx\\n    df[\\'MOMENTUM\\'] = talib.MOM(df[\\'Close\\'], timeperiod=10)\\n    df[\\'DX\\'] = talib.stream_DX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'FASTK\\'], df[\\'FASTD\\'] = talib.STOCHRSI(df[\\'Close\\'], timeperiod=14, fastk_period=5, fastd_period=3,\\n                                              fastd_matype=0)\\n    df[\\'CCI\\'] = talib.CCI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'WILLR\\'] = talib.WILLR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'MFI\\'] = talib.MFI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'], timeperiod=14)\\n    df[\\'ROC\\'] = talib.ROC(df[\\'Close\\'], timeperiod=10)\\n    # df[\\'ATR\\'] = talib.ATR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'ADX\\'] = talib.ADX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'PPO\\'] = talib.PPO(df[\\'Close\\'], fastperiod=12, slowperiod=26, matype=0)\\n    df[\\'RSI\\'] = talib.RSI(df[\\'Close\\'], timeperiod=14)\\n    df[\\'STDDEV\\'] = talib.STDDEV(df[\\'Close\\'], timeperiod=14, nbdev=1)\\n    df[\\'SIN\\'] = talib.SIN(df[\\'Close\\'])\\n    df[\\'COS\\'] = talib.COS(df[\\'Close\\'])\\n    df[\\'TAN\\'] = talib.TAN(df[\\'Close\\'])\\n\\n    df = df.fillna(0)\\n\\n    return df\\n\\n\\n', 'show me the entire code', 'Add the followings:\\n\\na function to calculate the moving average.\\na function to calculate the exponential moving average.\\na function to apply Gaussian smoothing.\\n and Incorporated these smoothing functions into the add_indicators function.\\non the following code:\\n\\nimport logging\\nimport os\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\n# Formats Position\\nformat_position = lambda price: (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n# Formats Currency\\nformat_currency = lambda price: \\'${0:.2f}\\'.format(abs(price))\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\"Displays training results\"\"\"\\n    try:\\n        if val_position == initial_offset or val_position == 0.0:\\n            logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: USELESS  Train Loss: {:.4f}\\'\\n                         .format(strategy, result[0], result[1], format_position(result[2]), result[3]))\\n            return False\\n        else:\\n            logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: {}  Train Loss: {:.4f}\\'\\n                         .format(strategy, result[0], result[1], format_position(result[2]), format_position(val_position), result[3]))\\n            return True\\n    except Exception as e:\\n        logging.error(f\"Error in show_train_result: {e}\")\\n        return False\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\"Displays eval results\"\"\"\\n    try:\\n        if profit == initial_offset or profit == 0.0:\\n            logging.info(\\'{}: USELESS\\\\n\\'.format(model_name))\\n        else:\\n            logging.info(\\'{}: {}\\\\n\\'.format(model_name, format_position(profit)))\\n    except Exception as e:\\n        logging.error(f\"Error in show_eval_result: {e}\")\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\"Reads stock data from csv file\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        if date_field != \"Date\":\\n            df[\\'Date\\'] = df[date_field]\\n        df[\\'adjcp\\'] = df[adjcp_field]\\n        df = df.sort_values([date_field], ascending=True)\\n        df = add_indicators(df)\\n        return df\\n    except Exception as e:\\n        logging.error(f\"Error in load_data: {e}\")\\n        return pd.DataFrame()\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\"Reads stock data from csv file\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        df = df.sort_values([date_field])\\n        return list(df[adjcp_field])\\n    except Exception as e:\\n        logging.error(f\"Error in get_stock_data: {e}\")\\n        return []\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\"Splits the dataset into training or testing using date\"\"\"\\n    try:\\n        data = df[(df.index >= start) & (df.index < end)]\\n        data.reset_index(drop=True, inplace=True)\\n        return data\\n    except Exception as e:\\n        logging.error(f\"Error in data_split: {e}\")\\n        return pd.DataFrame()\\n\\ndef switch_k_backend_device():\\n    \"\"\"Switches `keras` backend from GPU to CPU if required.\"\"\"\\n    try:\\n        if keras.backend == \"tensorflow\":\\n            logging.debug(\"Switching to TensorFlow for CPU\")\\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n    except Exception as e:\\n        logging.error(f\"Error in switch_k_backend_device: {e}\")\\n\\n\\ndef get_features():\\n    \"\"\"Returns a list of all feature names\"\"\"\\n    all_feature_name = [\\n        # \\'CDL2CROWS\\',\\n        # \\'CDL3BLACKCROWS\\',\\n        # \\'CDL3INSIDE\\',\\n        # \\'CDL3LINESTRIKE\\',\\n        # \\'CDL3OUTSIDE\\',\\n        # \\'CDL3STARSINSOUTH\\',\\n        # \\'CDL3WHITESOLDIERS\\',\\n        # \\'CDLABANDONEDBABY\\',\\n        # \\'CDLADVANCEBLOCK\\',\\n        # \\'CDLBELTHOLD\\',\\n        # \\'CDLBREAKAWAY\\',\\n        # \\'CDLCLOSINGMARUBOZU\\',\\n        # \\'CDLCONCEALBABYSWALL\\',\\n        # \\'CDLCOUNTERATTACK\\',\\n        \\'CDLDARKCLOUDCOVER\\',\\n        # \\'CDLDOJI\\',\\n        # \\'CDLDOJISTAR\\',\\n        \\'CDLDRAGONFLYDOJI\\',\\n        \\'CDLENGULFING\\',\\n        # \\'CDLEVENINGDOJISTAR\\',\\n        \\'CDLEVENINGSTAR\\',\\n        # \\'CDLGAPSIDESIDEWHITE\\',\\n        \\'CDLGRAVESTONEDOJI\\',\\n        \\'CDLHAMMER\\',\\n        \\'CDLHANGINGMAN\\',\\n        # \\'CDLHARAMI\\',\\n        # \\'CDLHARAMICROSS\\',\\n        # \\'CDLHIGHWAVE\\',\\n        # \\'CDLHIKKAKE\\',\\n        # \\'CDLHIKKAKEMOD\\',\\n        # \\'CDLHOMINGPIGEON\\',\\n        # \\'CDLIDENTICAL3CROWS\\',\\n        # \\'CDLINNECK\\',\\n        # \\'CDLINVERTEDHAMMER\\',\\n        # \\'CDLKICKING\\',\\n        # \\'CDLKICKINGBYLENGTH\\',\\n        # \\'CDLLADDERBOTTOM\\',\\n        # \\'CDLLONGLEGGEDDOJI\\',\\n        # \\'CDLLONGLINE\\',\\n        # \\'CDLMARUBOZU\\',\\n        # \\'CDLMATCHINGLOW\\',\\n        # \\'CDLMATHOLD\\',\\n        # \\'CDLMORNINGDOJISTAR\\',\\n        \\'CDLMORNINGSTAR\\',\\n        # \\'CDLONNECK\\',\\n        # \\'CDLPIERCING\\',\\n        # \\'CDLRICKSHAWMAN\\',\\n        # \\'CDLRISEFALL3METHODS\\',\\n        # \\'CDLSEPARATINGLINES\\',\\n        # \\'CDLSHOOTINGSTAR\\',\\n        # \\'CDLSHORTLINE\\',\\n        # \\'CDLSPINNINGTOP\\',\\n        # \\'CDLSTALLEDPATTERN\\',\\n        # \\'CDLSTICKSANDWICH\\',\\n        # \\'CDLTAKURI\\',\\n        # \\'CDLTASUKIGAP\\',\\n        # \\'CDLTHRUSTING\\',\\n        # \\'CDLTRISTAR\\',\\n        # \\'CDLUNIQUE3RIVER\\',\\n        # \\'CDLUPSIDEGAP2CROWS\\',\\n        # \\'CDLXSIDEGAP3METHODS\\',\\n        \\'MOMENTUM\\',\\n        \\'DX\\',\\n        \\'FASTK\\',\\n        \\'FASTD\\',\\n        \\'DX\\',\\n        \\'CCI\\',\\n        \\'WILLR\\',\\n        \\'MFI\\',\\n        \\'ROC\\',\\n        \\'ADX\\',\\n        \\'PPO\\',\\n        \\'RSI\\',\\n        \\'STDDEV\\',\\n        \\'SIN\\',\\n        \\'COS\\',\\n        \\'TAN\\'\\n    ]\\n    return all_feature_name\\n\\n\\ndef add_indicators(df):\\n    \"\"\"Adds technical indicators to the dataframe\"\"\"\\n    # Pattern Recognition Functions\\n    # df[\\'CDL2CROWS\\'] = talib.CDL2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3BLACKCROWS\\'] = talib.CDL3BLACKCROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3INSIDE\\'] = talib.CDL3INSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3LINESTRIKE\\'] = talib.CDL3LINESTRIKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3OUTSIDE\\'] = talib.CDL3OUTSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3STARSINSOUTH\\'] = talib.CDL3STARSINSOUTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3WHITESOLDIERS\\'] = talib.CDL3WHITESOLDIERS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLABANDONEDBABY\\'] = talib.CDLABANDONEDBABY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLADVANCEBLOCK\\'] = talib.CDLADVANCEBLOCK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLBELTHOLD\\'] = talib.CDLBELTHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLBREAKAWAY\\'] = talib.CDLBREAKAWAY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCLOSINGMARUBOZU\\'] = talib.CDLCLOSINGMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCONCEALBABYSWALL\\'] = talib.CDLCONCEALBABYSWALL(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCOUNTERATTACK\\'] = talib.CDLCOUNTERATTACK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDARKCLOUDCOVER\\'] = talib.CDLDARKCLOUDCOVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLDOJI\\'] = talib.CDLDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLDOJISTAR\\'] = talib.CDLDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDRAGONFLYDOJI\\'] = talib.CDLDRAGONFLYDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLENGULFING\\'] = talib.CDLENGULFING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLEVENINGDOJISTAR\\'] = talib.CDLEVENINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLEVENINGSTAR\\'] = talib.CDLEVENINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLGAPSIDESIDEWHITE\\'] = talib.CDLGAPSIDESIDEWHITE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLGRAVESTONEDOJI\\'] = talib.CDLGRAVESTONEDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHAMMER\\'] = talib.CDLHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHANGINGMAN\\'] = talib.CDLHANGINGMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHARAMI\\'] = talib.CDLHARAMI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHARAMICROSS\\'] = talib.CDLHARAMICROSS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIGHWAVE\\'] = talib.CDLHIGHWAVE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIKKAKE\\'] = talib.CDLHIKKAKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIKKAKEMOD\\'] = talib.CDLHIKKAKEMOD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHOMINGPIGEON\\'] = talib.CDLHOMINGPIGEON(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLIDENTICAL3CROWS\\'] = talib.CDLIDENTICAL3CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLINNECK\\'] = talib.CDLINNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLINVERTEDHAMMER\\'] = talib.CDLINVERTEDHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLKICKING\\'] = talib.CDLKICKING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLKICKINGBYLENGTH\\'] = talib.CDLKICKINGBYLENGTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLADDERBOTTOM\\'] = talib.CDLLADDERBOTTOM(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLONGLEGGEDDOJI\\'] = talib.CDLLONGLEGGEDDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLONGLINE\\'] = talib.CDLLONGLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMARUBOZU\\'] = talib.CDLMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMATCHINGLOW\\'] = talib.CDLMATCHINGLOW(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMATHOLD\\'] = talib.CDLMATHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLMORNINGDOJISTAR\\'] = talib.CDLMORNINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLMORNINGSTAR\\'] = talib.CDLMORNINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLONNECK\\'] = talib.CDLONNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLPIERCING\\'] = talib.CDLPIERCING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLRICKSHAWMAN\\'] = talib.CDLRICKSHAWMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLRISEFALL3METHODS\\'] = talib.CDLRISEFALL3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSEPARATINGLINES\\'] = talib.CDLSEPARATINGLINES(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSHOOTINGSTAR\\'] = talib.CDLSHOOTINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSHORTLINE\\'] = talib.CDLSHORTLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSPINNINGTOP\\'] = talib.CDLSPINNINGTOP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSTALLEDPATTERN\\'] = talib.CDLSTALLEDPATTERN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSTICKSANDWICH\\'] = talib.CDLSTICKSANDWICH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTAKURI\\'] = talib.CDLTAKURI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTASUKIGAP\\'] = talib.CDLTASUKIGAP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTHRUSTING\\'] = talib.CDLTHRUSTING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTRISTAR\\'] = talib.CDLTRISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLUNIQUE3RIVER\\'] = talib.CDLUNIQUE3RIVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLUPSIDEGAP2CROWS\\'] = talib.CDLUPSIDEGAP2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLXSIDEGAP3METHODS\\'] = talib.CDLXSIDEGAP3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # Price action\\n    # https://ta-lib.github.io/ta-lib-python/func_groups/momentum_indicators.html\\n    # rsi,stochRsi,cci,williamsr,mfi,roc,atr,adx\\n    df[\\'MOMENTUM\\'] = talib.MOM(df[\\'Close\\'], timeperiod=10)\\n    df[\\'DX\\'] = talib.stream_DX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'FASTK\\'], df[\\'FASTD\\'] = talib.STOCHRSI(df[\\'Close\\'], timeperiod=14, fastk_period=5, fastd_period=3,\\n                                              fastd_matype=0)\\n    df[\\'CCI\\'] = talib.CCI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'WILLR\\'] = talib.WILLR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'MFI\\'] = talib.MFI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'], timeperiod=14)\\n    df[\\'ROC\\'] = talib.ROC(df[\\'Close\\'], timeperiod=10)\\n    # df[\\'ATR\\'] = talib.ATR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'ADX\\'] = talib.ADX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'PPO\\'] = talib.PPO(df[\\'Close\\'], fastperiod=12, slowperiod=26, matype=0)\\n    df[\\'RSI\\'] = talib.RSI(df[\\'Close\\'], timeperiod=14)\\n    df[\\'STDDEV\\'] = talib.STDDEV(df[\\'Close\\'], timeperiod=14, nbdev=1)\\n    df[\\'SIN\\'] = talib.SIN(df[\\'Close\\'])\\n    df[\\'COS\\'] = talib.COS(df[\\'Close\\'])\\n    df[\\'TAN\\'] = talib.TAN(df[\\'Close\\'])\\n\\n    df = df.fillna(0)\\n\\n    return df\\n', \" raise KeyError(key) from err\\nKeyError: 'CDLENGULFING'\\n\", 'when training the profit shown keeps being the same all the time except for the Average Loss for the current code:\\n\\nimport logging\\n\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import (\\n    get_state\\n)\\nfrom .utils import (\\n    format_currency,\\n    format_position\\n)\\n\\n\\ndef monitor_memory(threshold=80):\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'close\\'):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        reward = 0\\n        is_done = False\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = 0\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n\\n        # select an action based on delta\\n        # BUY\\n        if delta > 0:\\n            action = 1\\n            reward = delta\\n            total_profit += delta\\n\\n        # SELL\\n        elif delta < 0:\\n            action = 2\\n            reward = abs(delta)\\n            total_profit += abs(delta)\\n\\n        # HOLD\\n        else:\\n            pass\\n\\n        done = (t == data_length - 1) or is_done\\n        if reward > 0:\\n            agent.remember(state, action, reward, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay2()\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    if total_profit > 0 and total_profit >= prev_agent_profit:\\n        agent.save()\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss))\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        # reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # BUY\\n        if action == 1:\\n            try:\\n                delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n            except IndexError:\\n                delta = 0\\n            # reward = delta  # max(delta, 0)\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {} | Profit: {}\".format(\\n                    format_currency(data.iloc[t][adjcp_field]),\\n                    format_position(delta)))\\n\\n        # SELL\\n        elif action == 2:\\n            try:\\n                delta = data.iloc[t][adjcp_field] - data.iloc[t + window_size][adjcp_field]\\n            except IndexError:\\n                delta = 0\\n            # reward = delta  # max(delta, 0)\\n            total_profit += delta\\n\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Profit: {}\".format(\\n                    format_currency(data.iloc[t][adjcp_field]),\\n                    format_position(delta)))\\n        # HOLD\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        # agent.memory.append((state, action, reward, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action\\n ', 'How can I monitor the  memory on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 28, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t28-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a try and except block to catch any exceptions that may\\n                                            occur during the calculation of the sigmoid function\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Adjusted the reward structure to ensure it reflects the agent\\'s actions.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added more logging to understand the actions and rewards.\\n------------------------------------------------------------------------------------------------------------------------\\nChanges Made:\\n    Action Selection: Use the agent\\'s action selection process (agent.act(state)) instead of just relying on the delta.\\n    Reward Structure: Adjusted the reward structure to ensure it reflects the agent\\'s actions.\\n    Logging: Added more logging to understand the actions and rewards.\\n\\n\"\"\"\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0, adjcp_field=\\'close\\'):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False, desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n        reward = 0\\n        is_done = False\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state)  # Use agent\\'s action selection\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            reward = delta\\n            total_profit += delta\\n        elif action == 2:  # SELL\\n            reward = -delta\\n            total_profit -= delta\\n        else:  # HOLD\\n            reward = 0\\n\\n        done = (t == data_length - 1) or is_done\\n        agent.remember(state, action, reward, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay2()\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit >= prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n            \\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss))\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state, is_eval=True)\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n\\n        if action == 1:  # BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 2:  # SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        else:  # HOLD\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        state = next_state\\n        if t == data_length - 1:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'Fix the error on the Double-DQN on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 28, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t28-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added logging statements to provide more context during model loading\\n                                            and creation.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a warning if the pretrained model path does not exist.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a Logged a message when the model is created successfully.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nKey Changes:\\n    Additional Logging: Added logging statements to provide more context during model loading and creation.\\n    Model Path Check: Added a warning if the pretrained model path does not exist.\\n    Model Creation Confirmation: Logged a message when the model is created successfully.\\n\\n\"\"\"\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nimport logging\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\"\"\"\\n    try:\\n        error = y_true - y_pred\\n        cond = keras.backend.abs(error) <= clip_delta\\n        squared_loss = 0.5 * keras.backend.square(error)\\n        quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n        return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n    except Exception as err:\\n        logging.error(\"Error in huber_loss: %s\", err)\\n        return None\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n        self.state_size = state_size\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # Model configuration\\n        self.gamma = 0.95\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n\\n        # Initialize the model\\n        self.model = self._initialize_model(pretrained)\\n\\n    def _initialize_model(self, pretrained):\\n        \"\"\"Initialize the model, loading from file if pretrained.\"\"\"\\n        try:\\n            model_path = f\"models/{self.model_name}.keras\"\\n            if pretrained:\\n                if os.path.exists(model_path):\\n                    logging.info(\"Loading pretrained model from %s\", model_path)\\n                    loaded_model = self.load()\\n                    if loaded_model is None:\\n                        raise ValueError(\"Failed to load the pretrained model.\")\\n                    self.model = loaded_model  # Assign loaded model to self.model\\n                    return self.model\\n                else:\\n                    logging.warning(\"Pretrained model path does not exist: %s\", model_path)\\n            return self._model()  # Create a new model if not loading\\n        except Exception as err:\\n            logging.error(\"Error in model initialization: %s\", err)\\n            return None\\n\\n    def _model(self):\\n        \"\"\"Creates the model.\"\"\"\\n        try:\\n            model = keras.Sequential([\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=self.action_size)\\n            ])\\n            model.compile(loss=self.loss, optimizer=self.optimizer)\\n            logging.info(\"Model created successfully.\")\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in creating model: %s\", err)\\n            return None\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory.\"\"\"\\n        try:\\n            self.memory.append((state, action, reward, done))\\n        except Exception as err:\\n            logging.error(\"Error in remember: %s\", err)\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions.\"\"\"\\n        try:\\n            if not is_eval and random.random() <= self.epsilon:\\n                return random.randrange(self.action_size)\\n\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            action_probs = self.model.predict(state, verbose=0)\\n            return np.argmax(action_probs[0])\\n        except Exception as err:\\n            logging.error(\"Error in act: %s\", err)\\n            return None\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory.\"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        try:\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            if self.strategy == \"dqn\":\\n                while len(self.memory) > 0:\\n                    state, action, reward, done = self.memory.pop()\\n                    target = reward\\n\\n                    q_values = self.model.predict(state, verbose=0)\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n                    loss += self.model.fit(\\n                        np.array(X_train), np.array(y_train),\\n                        epochs=1, verbose=0\\n                    ).history[\"loss\"][0]\\n                    \\n            # # DQN with fixed targets\\n            # elif self.strategy == \"t-dqn\":\\n            #     if self.n_iter % self.reset_every == 0:\\n            #         # reset target model weights\\n            #         self.target_model.set_weights(self.model.get_weights())\\n            #\\n            #     for state, action, reward, next_state, done in mini_batch:\\n            #         if done:\\n            #             target = reward\\n            #         else:\\n            #             # approximate deep q-learning equation with fixed targets\\n            #             target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n            #\\n            #         # estimate q-values based on current state\\n            #         q_values = self.model.predict(state)\\n            #         # update the target for current action based on discounted reward\\n            #         q_values[0][action] = target\\n            #\\n            #         X_train.append(state[0])\\n            #         y_train.append(q_values[0])\\n            #\\n            # Double DQN\\n            elif self.strategy == \"double-dqn\":\\n                if self.n_iter % self.reset_every == 0:\\n                    # reset target model weights\\n                    self.target_model.set_weights(self.model.get_weights())\\n\\n                for state, action, reward, next_state, done in mini_batch:\\n                    if done:\\n                        target = reward\\n                    else:\\n                        # approximate double deep q-learning equation\\n                        target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                            np.argmax(self.model.predict(next_state)[0])]\\n\\n                    # estimate q-values based on current state\\n                    q_values = self.model.predict(state)\\n                    # update the target for current action based on discounted reward\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n            else:\\n                raise NotImplementedError()\\n\\n            # as the training goes on we want the agent to\\n            # make less random and more optimal decisions\\n            if self.epsilon > self.epsilon_min:\\n                self.epsilon *= self.epsilon_decay\\n\\n            return loss\\n        except Exception as err:\\n            logging.error(\"Error in train_experience_replay2: %s\", err)\\n            return None\\n\\n    def save(self):\\n        \"\"\"Save the model.\"\"\"\\n        try:\\n            path = pathlib.Path()\\n            self.model.save(path / f\"models/{self.model_name}.keras\", overwrite=True)\\n            logging.info(\"Model saved successfully.\")\\n        except Exception as err:\\n            logging.error(\"Error in save: %s\", err)\\n\\n    def load(self):\\n        \"\"\"Load the model.\"\"\"\\n        try:\\n            model = tf.keras.models.load_model(f\"models/{self.model_name}.keras\", custom_objects=self.custom_objects)\\n            logging.info(\"Model loaded successfully.\")\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in load: %s\", err)\\n            return None', 'Error in train_experience_replay2: \\'Agent\\' object has no attribute \\'reset_every\\' for the following code:\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nimport logging\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\"\"\"\\n    try:\\n        error = y_true - y_pred\\n        cond = keras.backend.abs(error) <= clip_delta\\n        squared_loss = 0.5 * keras.backend.square(error)\\n        quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n        return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n    except Exception as err:\\n        logging.error(\"Error in huber_loss: %s\", err)\\n        return None\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"double-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n        self.state_size = state_size\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # Model configuration\\n        self.gamma = 0.95\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n\\n        # Initialize the models\\n        self.model = self._initialize_model(pretrained)\\n        self.target_model = self._initialize_model(pretrained)  # Initialize target model\\n        self.n_iter = 0  # Initialize iteration counter\\n\\n    def _initialize_model(self, pretrained):\\n        \"\"\"Initialize the model, loading from file if pretrained.\"\"\"\\n        try:\\n            model_path = f\"models/{self.model_name}.keras\"\\n            if pretrained:\\n                if os.path.exists(model_path):\\n                    logging.info(\"Loading pretrained model from %s\", model_path)\\n                    loaded_model = self.load()\\n                    if loaded_model is None:\\n                        raise ValueError(\"Failed to load the pretrained model.\")\\n                    return loaded_model\\n                else:\\n                    logging.warning(\"Pretrained model path does not exist: %s\", model_path)\\n            return self._model()  # Create a new model if not loading\\n        except Exception as err:\\n            logging.error(\"Error in model initialization: %s\", err)\\n            return None\\n\\n    def _model(self):\\n        \"\"\"Creates the model.\"\"\"\\n        try:\\n            model = keras.Sequential([\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=self.action_size)\\n            ])\\n            model.compile(loss=self.loss, optimizer=self.optimizer)\\n            logging.info(\"Model created successfully.\")\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in creating model: %s\", err)\\n            return None\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory.\"\"\"\\n        try:\\n            self.memory.append((state, action, reward, done))\\n        except Exception as err:\\n            logging.error(\"Error in remember: %s\", err)\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions.\"\"\"\\n        try:\\n            if not is_eval and random.random() <= self.epsilon:\\n                return random.randrange(self.action_size)\\n\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            action_probs = self.model.predict(state, verbose=0)\\n            return np.argmax(action_probs[0])\\n        except Exception as err:\\n            logging.error(\"Error in act: %s\", err)\\n            return None\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory.\"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        try:\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            mini_batch = random.sample(self.memory, min(len(self.memory), 32))  # Sample a mini-batch\\n            self.n_iter += 1  # Increment iteration counter\\n\\n            if self.strategy == \"double-dqn\":\\n                if self.n_iter % self.reset_every == 0:\\n                    # Reset target model weights\\n                    self.target_model.set_weights(self.model.get_weights())\\n\\n                for state, action, reward, next_state, done in mini_batch:\\n                    if done:\\n                        target = reward\\n                    else:\\n                        # Approximate double deep Q-learning equation\\n                        target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                            np.argmax(self.model.predict(next_state)[0])]\\n\\n                    # Estimate Q-values based on current state\\n                    q_values = self.model.predict(state)\\n                    # Update the target for current action based on discounted reward\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n            else:\\n                raise NotImplementedError(\"Strategy not implemented.\")\\n\\n            # Train the model\\n            if len(X_train) > 0:\\n                loss += self.model.fit(\\n                    np.array(X_train), np.array(y_train),\\n                    epochs=1, verbose=0\\n                ).history[\"loss\"][0]\\n\\n            # Update epsilon\\n            if self.epsilon > self.epsilon_min:\\n                self.epsilon *= self.epsilon_decay\\n\\n            return loss\\n        except Exception as err:\\n            logging.error(\"Error in train_experience_replay2: %s\", err)\\n            return None\\n\\n    def save(self):\\n        \"\"\"Save the model.\"\"\"\\n        try:\\n            path = pathlib.Path()\\n            self.model.save(path / f\"models/{self.model_name}.keras\", overwrite=True)\\n            logging.info(\"Model saved successfully.\")\\n        except Exception as err:\\n            logging.error(\"Error in save: %s\", err)\\n\\n    def load(self):\\n        \"\"\"Load the model.\"\"\"\\n        try:\\n            model = tf.keras.models.load_model(f\"models/{self.model_name}.keras\", custom_objects=self.custom_objects)\\n            logging.info(\"Model loaded successfully.\")\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in load: %s\", err)\\n            return None', 'Error in train_experience_replay2: not enough values to unpack (expected 5, got 4) for the following code\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nimport logging\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\"\"\"\\n    try:\\n        error = y_true - y_pred\\n        cond = keras.backend.abs(error) <= clip_delta\\n        squared_loss = 0.5 * keras.backend.square(error)\\n        quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n        return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n    except Exception as err:\\n        logging.error(\"Error in huber_loss: %s\", err)\\n        return None\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"double-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n        self.state_size = state_size\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # Store reset_every as an instance variable\\n        self.reset_every = reset_every\\n\\n        # Model configuration\\n        self.gamma = 0.95\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n\\n        # Initialize the models\\n        self.model = self._initialize_model(pretrained)\\n        self.target_model = self._initialize_model(pretrained)  # Initialize target model\\n        self.n_iter = 0  # Initialize iteration counter\\n\\n    def _initialize_model(self, pretrained):\\n        \"\"\"Initialize the model, loading from file if pretrained.\"\"\"\\n        try:\\n            model_path = f\"models/{self.model_name}.keras\"\\n            if pretrained:\\n                if os.path.exists(model_path):\\n                    logging.info(\"Loading pretrained model from %s\", model_path)\\n                    loaded_model = self.load()\\n                    if loaded_model is None:\\n                        raise ValueError(\"Failed to load the pretrained model.\")\\n                    return loaded_model\\n                else:\\n                    logging.warning(\"Pretrained model path does not exist: %s\", model_path)\\n            return self._model()  # Create a new model if not loading\\n        except Exception as err:\\n            logging.error(\"Error in model initialization: %s\", err)\\n            return None\\n\\n    def _model(self):\\n        \"\"\"Creates the model.\"\"\"\\n        try:\\n            model = keras.Sequential([\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=self.action_size)\\n            ])\\n            model.compile(loss=self.loss, optimizer=self.optimizer)\\n            logging.info(\"Model created successfully.\")\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in creating model: %s\", err)\\n            return None\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory.\"\"\"\\n        try:\\n            self.memory.append((state, action, reward, done))\\n        except Exception as err:\\n            logging.error(\"Error in remember: %s\", err)\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions.\"\"\"\\n        try:\\n            if not is_eval and random.random() <= self.epsilon:\\n                return random.randrange(self.action_size)\\n\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            action_probs = self.model.predict(state, verbose=0)\\n            return np.argmax(action_probs[0])\\n        except Exception as err:\\n            logging.error(\"Error in act: %s\", err)\\n            return None\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory.\"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        try:\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            mini_batch = random.sample(self.memory, min(len(self.memory), 32))  # Sample a mini-batch\\n            self.n_iter += 1  # Increment iteration counter\\n\\n            if self.strategy == \"double-dqn\":\\n                if self.n_iter % self.reset_every == 0:\\n                    # Reset target model weights\\n                    self.target_model.set_weights(self.model.get_weights())\\n\\n                for state, action, reward, next_state, done in mini_batch:\\n                    if done:\\n                        target = reward\\n                    else:\\n                        # Approximate double deep Q-learning equation\\n                        target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                            np.argmax(self.model.predict(next_state)[0])]\\n\\n                    # Estimate Q-values based on current state\\n                    q_values = self.model.predict(state)\\n                    # Update the target for current action based on discounted reward\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n            else:\\n                raise NotImplementedError(\"Strategy not implemented.\")\\n\\n            # Train the model\\n            if len(X_train) > 0:\\n                loss += self.model.fit(\\n                    np.array(X_train), np.array(y_train),\\n                    epochs=1, verbose=0\\n                ).history[\"loss\"][0]\\n\\n            # Update epsilon\\n            if self.epsilon > self.epsilon_min:\\n                self.epsilon *= self.epsilon_decay\\n\\n            return loss\\n        except Exception as err:\\n            logging.error(\"Error in train_experience_replay2: %s\", err)\\n            return None\\n\\n    def save(self):\\n        \"\"\"Save the model.\"\"\"\\n        try:\\n            path = pathlib.Path()\\n            self.model.save(path / f\"models/{self.model_name}.keras\", overwrite=True)\\n            logging.info(\"Model saved successfully.\")\\n        except Exception as err:\\n            logging.error(\"Error in save: %s\", err)\\n\\n    def load(self):\\n        \"\"\"Load the model.\"\"\"\\n        try:\\n            model = tf.keras.models.load_model(f\"models/{self.model_name}.keras\", custom_objects=self.custom_objects)\\n            logging.info(\"Model loaded successfully.\")\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in load: %s\", err)\\n            return None', 'Check for errors\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 28, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t28-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added logging statements to provide more context during model loading\\n                                            and creation.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a warning if the pretrained model path does not exist.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a Logged a message when the model is created successfully.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nKey Changes:\\n    Additional Logging: Added logging statements to provide more context during model loading and creation.\\n    Model Path Check: Added a warning if the pretrained model path does not exist.\\n    Model Creation Confirmation: Logged a message when the model is created successfully.\\n\\n\"\"\"\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nimport logging\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\"\"\"\\n    try:\\n        error = y_true - y_pred\\n        cond = keras.backend.abs(error) <= clip_delta\\n        squared_loss = 0.5 * keras.backend.square(error)\\n        quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n        return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n    except Exception as err:\\n        logging.error(\"Error in huber_loss: %s\", err)\\n        return None\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n        self.state_size = state_size\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # Model configuration\\n        self.gamma = 0.95\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n\\n        # Initialize the model\\n        self.model = self._initialize_model(pretrained)\\n\\n    def _initialize_model(self, pretrained):\\n        \"\"\"Initialize the model, loading from file if pretrained.\"\"\"\\n        try:\\n            model_path = f\"models/{self.model_name}.keras\"\\n            if pretrained:\\n                if os.path.exists(model_path):\\n                    logging.info(\"Loading pretrained model from %s\", model_path)\\n                    loaded_model = self.load()\\n                    if loaded_model is None:\\n                        raise ValueError(\"Failed to load the pretrained model.\")\\n                    self.model = loaded_model  # Assign loaded model to self.model\\n                    return self.model\\n                else:\\n                    logging.warning(\"Pretrained model path does not exist: %s\", model_path)\\n            return self._model()  # Create a new model if not loading\\n        except Exception as err:\\n            logging.error(\"Error in model initialization: %s\", err)\\n            return None\\n\\n    def _model(self):\\n        \"\"\"Creates the model.\"\"\"\\n        try:\\n            model = keras.Sequential([\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=self.action_size)\\n            ])\\n            model.compile(loss=self.loss, optimizer=self.optimizer)\\n            logging.info(\"Model created successfully.\")\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in creating model: %s\", err)\\n            return None\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory.\"\"\"\\n        try:\\n            self.memory.append((state, action, reward, done))\\n        except Exception as err:\\n            logging.error(\"Error in remember: %s\", err)\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions.\"\"\"\\n        try:\\n            if not is_eval and random.random() <= self.epsilon:\\n                return random.randrange(self.action_size)\\n\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            action_probs = self.model.predict(state, verbose=0)\\n            return np.argmax(action_probs[0])\\n        except Exception as err:\\n            logging.error(\"Error in act: %s\", err)\\n            return None\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory.\"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        try:\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            if self.strategy == \"dqn\":\\n                while len(self.memory) > 0:\\n                    state, action, reward, done = self.memory.pop()\\n                    target = reward\\n\\n                    q_values = self.model.predict(state, verbose=0)\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n                    loss += self.model.fit(\\n                        np.array(X_train), np.array(y_train),\\n                        epochs=1, verbose=0\\n                    ).history[\"loss\"][0]\\n\\n            # DQN with fixed targets\\n            elif self.strategy == \"t-dqn\":\\n                if self.n_iter % self.reset_every == 0:\\n                    # reset target model weights\\n                    self.target_model.set_weights(self.model.get_weights())\\n\\n                for state, action, reward, next_state, done in mini_batch:\\n                    if done:\\n                        target = reward\\n                    else:\\n                        # approximate deep q-learning equation with fixed targets\\n                        target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n\\n                    # estimate q-values based on current state\\n                    q_values = self.model.predict(state)\\n                    # update the target for current action based on discounted reward\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n            # # Double DQN\\n            # elif self.strategy == \"double-dqn\":\\n            #     if self.n_iter % self.reset_every == 0:\\n            #         # reset target model weights\\n            #         self.target_model.set_weights(self.model.get_weights())\\n            #\\n            #     for state, action, reward, next_state, done in mini_batch:\\n            #         if done:\\n            #             target = reward\\n            #         else:\\n            #             # approximate double deep q-learning equation\\n            #             target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n            #                 np.argmax(self.model.predict(next_state)[0])]\\n            #\\n            #         # estimate q-values based on current state\\n            #         q_values = self.model.predict(state)\\n            #         # update the target for current action based on discounted reward\\n            #         q_values[0][action] = target\\n            #\\n            #         X_train.append(state[0])\\n            #         y_train.append(q_values[0])\\n            #\\n            else:\\n                raise NotImplementedError()\\n\\n            # as the training goes on we want the agent to\\n            # make less random and more optimal decisions\\n            if self.epsilon > self.epsilon_min:\\n                self.epsilon *= self.epsilon_decay\\n\\n            return loss\\n        except Exception as err:\\n            logging.error(\"Error in train_experience_replay2: %s\", err)\\n            return None\\n\\n    def save(self):\\n        \"\"\"Save the model.\"\"\"\\n        try:\\n            path = pathlib.Path()\\n            self.model.save(path / f\"models/{self.model_name}.keras\", overwrite=True)\\n            logging.info(\"Model saved successfully.\")\\n        except Exception as err:\\n            logging.error(\"Error in save: %s\", err)\\n\\n    def load(self):\\n        \"\"\"Load the model.\"\"\"\\n        try:\\n            model = tf.keras.models.load_model(f\"models/{self.model_name}.keras\", custom_objects=self.custom_objects)\\n            logging.info(\"Model loaded successfully.\")\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in load: %s\", err)\\n            return None', 'Getting the following Error in train_experience_replay2: \\'Agent\\' object has no attribute \\'reset_every\\' on the code below\\n\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nimport logging\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\"\"\"\\n    try:\\n        error = y_true - y_pred\\n        cond = keras.backend.abs(error) <= clip_delta\\n        squared_loss = 0.5 * keras.backend.square(error)\\n        quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n        return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n    except Exception as err:\\n        logging.error(\"Error in huber_loss: %s\", err)\\n        return None\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n        self.state_size = state_size\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # Model configuration\\n        self.gamma = 0.95\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n\\n        # Initialize the models\\n        self.model = self._initialize_model(pretrained)\\n        self.target_model = self._initialize_model(pretrained)  # Initialize target model\\n        self.n_iter = 0  # Initialize iteration counter\\n\\n    def _initialize_model(self, pretrained):\\n        \"\"\"Initialize the model, loading from file if pretrained.\"\"\"\\n        try:\\n            model_path = f\"models/{self.model_name}.keras\"\\n            if pretrained:\\n                if os.path.exists(model_path):\\n                    logging.info(\"Loading pretrained model from %s\", model_path)\\n                    loaded_model = self.load()\\n                    if loaded_model is None:\\n                        raise ValueError(\"Failed to load the pretrained model.\")\\n                    return loaded_model\\n                else:\\n                    logging.warning(\"Pretrained model path does not exist: %s\", model_path)\\n            return self._model()  # Create a new model if not loading\\n        except Exception as err:\\n            logging.error(\"Error in model initialization: %s\", err)\\n            return None\\n\\n    def _model(self):\\n        \"\"\"Creates the model.\"\"\"\\n        try:\\n            model = keras.Sequential([\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=self.action_size)\\n            ])\\n            model.compile(loss=self.loss, optimizer=self.optimizer)\\n            logging.info(\"Model created successfully.\")\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in creating model: %s\", err)\\n            return None\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory.\"\"\"\\n        try:\\n            self.memory.append((state, action, reward, done))\\n        except Exception as err:\\n            logging.error(\"Error in remember: %s\", err)\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions.\"\"\"\\n        try:\\n            if not is_eval and random.random() <= self.epsilon:\\n                return random.randrange(self.action_size)\\n\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            action_probs = self.model.predict(state, verbose=0)\\n            return np.argmax(action_probs[0])\\n        except Exception as err:\\n            logging.error(\"Error in act: %s\", err)\\n            return None\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory.\"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        try:\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            mini_batch = random.sample(self.memory, min(len(self.memory), 32))  # Sample a mini-batch\\n            self.n_iter += 1  # Increment iteration counter\\n\\n            if self.strategy == \"dqn\":\\n                while len(self.memory) > 0:\\n                    state, action, reward, done = self.memory.pop()\\n                    target = reward\\n\\n                    q_values = self.model.predict(state, verbose=0)\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n                    loss += self.model.fit(\\n                        np.array(X_train), np.array(y_train),\\n                        epochs=1, verbose=0\\n                    ).history[\"loss\"][0]\\n\\n            elif self.strategy == \"t-dqn\":\\n                if self.n_iter % self.reset_every == 0:\\n                    # Reset target model weights\\n                    self.target_model.set_weights(self.model.get_weights())\\n\\n                for state, action, reward, done in mini_batch:\\n                    if done:\\n                        target = reward\\n                    else:\\n                        # Approximate deep Q-learning equation with fixed targets\\n                        target = reward + self.gamma * np.amax(self.target_model.predict(state)[0])\\n\\n                    # Estimate Q-values based on current state\\n                    q_values = self.model.predict(state)\\n                    # Update the target for current action based on discounted reward\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n            else:\\n                raise NotImplementedError(\"Strategy not implemented.\")\\n\\n            # Update epsilon\\n            if self.epsilon > self.epsilon_min:\\n                self.epsilon *= self.epsilon_decay\\n\\n            # Train the model\\n            if len(X_train) > 0:\\n                loss += self.model.fit(\\n                    np.array(X_train), np.array(y_train),\\n                    epochs=1, verbose=0\\n                ).history[\"loss\"][0]\\n\\n            return loss\\n        except Exception as err:\\n            logging.error(\"Error in train_experience_replay2: %s\", err)\\n            return None\\n\\n    def save(self):\\n        \"\"\"Save the model.\"\"\"\\n        try:\\n            path = pathlib.Path()\\n            self.model.save(path / f\"models/{self.model_name}.keras\", overwrite=True)\\n            logging.info(\"Model saved successfully.\")\\n        except Exception as err:\\n            logging.error(\"Error in save: %s\", err)\\n\\n    def load(self):\\n        \"\"\"Load the model.\"\"\"\\n        try:\\n            model = tf.keras.models.load_model(f\"models/{self.model_name}.keras\", custom_objects=self.custom_objects)\\n            logging.info(\"Model loaded successfully.\")\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in load: %s\", err)\\n            return None', 'how do I pass the strategy value when calling the main\\n\\n\"\"\"\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\"\"\"\\n\\nimport logging\\n\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    switch_k_backend_device,\\n    format_currency,\\n    format_position, show_train_result, )\\n\\n\\ndef main(train_stock, train_percent, date_field, adjcp_field, window_size, batch_size, ep_count,\\n         strategy=\"t-dqn\", model_name=\"model_debug\", debug=False):\\n    \"\"\" Trains the stock trading bot using Deep Q-Learning.\\n    Please see https://arxiv.org/abs/1312.5602 for more details.\\n\\n    Args: [python train.py --help]\\n    \"\"\"\\n    agent = Agent(window_size, strategy=strategy, pretrained=True, model_name=model_name)\\n    data = load_data(train_stock, date_field)\\n    take = int(len(data) * (train_percent / 100))\\n    train_data = data_split(data, 0, take, adjcp_field)\\n    agent_profit = 0\\n    prev_agent_profit = -1000000000\\n    episode = 0\\n    episode_count = ep_count\\n    while True:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   window_size=window_size, prev_agent_profit=agent_profit, adjcp_field=adjcp_field)\\n\\n        agent_profit = train_result[2]\\n        ave_loss = train_result[3]\\n        logging.debug(\"Current Profit at: {} Ave Loss: {} | Previous Profit: {}\".format(\\n            format_currency(agent_profit),\\n            format_currency(ave_loss),\\n            format_position(prev_agent_profit)))\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        if episode >= episode_count:\\n            break\\n        episode += 1\\n\\n    # Show Validation after finishing\\n    if agent_profit > 0:\\n        val_data = data_split(data, take, int(len(data)), adjcp_field)\\n        initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n        val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n        show_train_result(train_result, val_result, initial_offset, strategy)\\n\\n\\nif __name__ == \"__main__\":\\n    args = docopt(__doc__)\\n\\n    file_data = args[\"<file-data>\"]\\n    train_percent = args[\"<train-percent>\"]\\n\\n    date_field = args[\"--date-field\"]\\n    adjcp_field = args[\"--adjcp-field\"]\\n    window_size = int(args[\"--window-size\"])\\n    batch_size = int(args[\"--batch-size\"])\\n    ep_count = int(args[\"--episode-count\"])\\n    model_name = args[\"--model-name\"]\\n    debug = args[\"--debug\"]\\n\\n    coloredlogs.install(level=\"DEBUG\")\\n    switch_k_backend_device()\\n\\n    try:\\n        # dqn\\n        main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n             ep_count, \"dqn\", f\"{model_name}_dqn\", debug)\\n\\n        # # t-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"t-dqn\", f\"{model_name}_tdqn\", debug)\\n        #\\n        # # double-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"double-dqn\", f\"{model_name}_ddqn\", debug)\\n\\n    except KeyboardInterrupt:\\n        print(\"Aborted!\")\\n', 'Is this right \\npython train.py \"data/GOOG_2019.csv\" 95 --strategy=dqn --date-field \"Date\" --adjcp-field \"Close\" --window-size 60 --batch-size 30 --episode-count 2 --model-name model\\n', 'I get  the following:\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nwhen calling it with python train.py \"data/GOOG_2019.csv\" 95 --strategy=dqn --date-field \"Date\" --adjcp-field \"Close\" --window-size 60 --batch-size 30 --episode-count 2 --model-name model\\n', 'I keep getting the same', 'I get the below, when I run the script with:\\npython train.py \"data/GOOG_2019.csv\" 95 --strategy=dqn --date-field \"Date\" --adjcp-field \"Close\" --window-size 60 --batch-size 30 --episode-count 2 --model-name model\\n \\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nimport logging\\n\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    switch_k_backend_device,\\n    format_currency,\\n    format_position, show_train_result, )\\n\\n\\ndef main(train_stock, train_percent, date_field, adjcp_field, window_size, batch_size, ep_count,\\n         strategy=\"t-dqn\", model_name=\"model_debug\", debug=False):\\n    \"\"\" Trains the stock trading bot using Deep Q-Learning.\\n    Please see https://arxiv.org/abs/1312.5602 for more details.\\n\\n    Args: [python train.py --help]\\n    \"\"\"\\n    agent = Agent(window_size, strategy=strategy, pretrained=True, model_name=model_name)\\n    data = load_data(train_stock, date_field)\\n    take = int(len(data) * (train_percent / 100))\\n    train_data = data_split(data, 0, take, adjcp_field)\\n    agent_profit = 0\\n    prev_agent_profit = -1000000000\\n    episode = 0\\n    episode_count = ep_count\\n    while True:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   window_size=window_size, prev_agent_profit=agent_profit, adjcp_field=adjcp_field)\\n\\n        agent_profit = train_result[2]\\n        ave_loss = train_result[3]\\n        logging.debug(\"Current Profit at: {} Ave Loss: {} | Previous Profit: {}\".format(\\n            format_currency(agent_profit),\\n            format_currency(ave_loss),\\n            format_position(prev_agent_profit)))\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        if episode >= episode_count:\\n            break\\n        episode += 1\\n\\n    # Show Validation after finishing\\n    if agent_profit > 0:\\n        val_data = data_split(data, take, int(len(data)), adjcp_field)\\n        initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n        val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n        show_train_result(train_result, val_result, initial_offset, strategy)\\n\\n\\nif __name__ == \"__main__\":\\n    args = docopt(__doc__)\\n\\n    # Debugging: Print parsed arguments\\n    print(\"Parsed arguments:\", args)\\n\\n    file_data = args[\"<file-data>\"]\\n    train_percent = args[\"<train-percent>\"]\\n\\n    date_field = args[\"--date-field\"]\\n    adjcp_field = args[\"--adjcp-field\"]\\n    window_size = int(args[\"--window-size\"])\\n    batch_size = int(args[\"--batch-size\"])\\n    ep_count = int(args[\"--episode-count\"])\\n    model_name = args[\"--model-name\"]\\n    strategy = args[\"--strategy\"]\\n    debug = args[\"--debug\"]\\n\\n    coloredlogs.install(level=\"DEBUG\")\\n    switch_k_backend_device()\\n\\n    try:\\n        main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n             ep_count, strategy, model_name, debug)\\n    except KeyboardInterrupt:\\n        print(\"Aborted!\")', \"/Users/javiersanchez/PycharmProjects/trading-bot/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\\n  warnings.warn(\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\", 'Add try and exception to the following code:\\n\\nimport math\\n\\nimport numpy as np\\n\\nfrom trading_bot.utils import get_features\\n\\n\\ndef sigmoid(x):\\n    \"\"\"Performs sigmoid operation\\n    \"\"\"\\n    try:\\n        if x < 0:\\n            return 1 - 1 / (1 + math.exp(x))\\n        return 1 / (1 + math.exp(-x))\\n    except Exception as err:\\n        print(\"Error in sigmoid: \" + err)\\n\\n\\ndef get_state(data, t, n_days):\\n    \"\"\"Returns an n-day state representation ending at time t\\n    \"\"\"\\n    all_feature_name = get_features()\\n    d = t - n_days + 1\\n    all_feature = np.column_stack([data[k] for k in all_feature_name])\\n    block = all_feature[d:t + 1] if d >= 0 else np.vstack([-d * [all_feature[0]], all_feature[0:t + 1]])\\n    return block', 'Add the try, exception and check for errors on the following code:\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\n\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\\n\\n    Links: \\thttps://en.wikipedia.org/wiki/Huber_loss\\n            https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/\\n    \"\"\"\\n    error = y_true - y_pred\\n    cond = keras.backend.abs(error) <= clip_delta\\n    squared_loss = 0.5 * keras.backend.square(error)\\n    quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n    return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n\\n        # agent config\\n        self.state_size = state_size  # normalized previous days\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # model config\\n        self.model_name = model_name\\n        self.gamma = 0.95  # affinity for long term reward\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}  # important for loading the model from memory\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n        model_path = f\"models/{self.model_name}.keras\"\\n        if pretrained and os.path.exists(model_path) and self.model_name is not None:\\n            self.model = self.load()\\n        else:\\n            self.model = self._model()\\n\\n        # strategy config\\n        if self.strategy in [\"t-dqn\", \"double-dqn\"]:\\n            self.n_iter = 1\\n            self.reset_every = reset_every\\n\\n            # target network\\n            self.target_model = tf.keras.models.clone_model(self.model)\\n            self.target_model.set_weights(self.model.get_weights())\\n\\n    def _model(self):\\n        \"\"\"Creates the model\\n        \"\"\"\\n        model = keras.Sequential()\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=self.action_size))\\n\\n        model.compile(loss=self.loss, optimizer=self.optimizer)\\n        return model\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory\\n        \"\"\"\\n        self.memory.append((state, action, reward, done))\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions\\n        \"\"\"\\n        # take random action in order to diversify experience at the beginning\\n        if not is_eval and random.random() <= self.epsilon:\\n            return random.randrange(self.action_size)\\n\\n        action_probs = self.model.predict(state, verbose=0)\\n        return np.argmax(action_probs[0])\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            while len(self.memory) > 0:\\n                state, action, reward, done = self.memory.pop()\\n                # Here, target is directly set to the immediate reward\\n                target = reward\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on the immediate reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n                # update q-function parameters based on huber loss gradient\\n                loss += self.model.fit(\\n                    np.array(X_train), np.array(y_train),\\n                    epochs=1, verbose=0\\n                ).history[\"loss\"][0]\\n\\n        # DQN with fixed targets\\n        # elif self.strategy == \"t-dqn\":\\n        #     if self.n_iter % self.reset_every == 0:\\n        #         # reset target model weights\\n        #         self.target_model.set_weights(self.model.get_weights())\\n        #\\n        #     for state, action, reward, next_state, done in mini_batch:\\n        #         if done:\\n        #             target = reward\\n        #         else:\\n        #             # approximate deep q-learning equation with fixed targets\\n        #             target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n        #\\n        #         # estimate q-values based on current state\\n        #         q_values = self.model.predict(state)\\n        #         # update the target for current action based on discounted reward\\n        #         q_values[0][action] = target\\n        #\\n        #         X_train.append(state[0])\\n        #         y_train.append(q_values[0])\\n        #\\n        # # Double DQN\\n        # elif self.strategy == \"double-dqn\":\\n        #     if self.n_iter % self.reset_every == 0:\\n        #         # reset target model weights\\n        #         self.target_model.set_weights(self.model.get_weights())\\n        #\\n        #     for state, action, reward, next_state, done in mini_batch:\\n        #         if done:\\n        #             target = reward\\n        #         else:\\n        #             # approximate double deep q-learning equation\\n        #             target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n        #                 np.argmax(self.model.predict(next_state)[0])]\\n        #\\n        #         # estimate q-values based on current state\\n        #         q_values = self.model.predict(state)\\n        #         # update the target for current action based on discounted reward\\n        #         q_values[0][action] = target\\n        #\\n        #         X_train.append(state[0])\\n        #         y_train.append(q_values[0])\\n        #\\n        # else:\\n        #     raise NotImplementedError()\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def train_experience_replay(self, batch_size):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        mini_batch = random.sample(self.memory, batch_size)\\n        X_train, y_train = [], []\\n\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            for state, action, reward, done in mini_batch:\\n                # Here, target is directly set to the immediate reward\\n                target = reward\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on the immediate reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # DQN with fixed targets\\n        elif self.strategy == \"t-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation with fixed targets\\n                    target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # Double DQN\\n        elif self.strategy == \"double-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate double deep q-learning equation\\n                    target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                        np.argmax(self.model.predict(next_state)[0])]\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        else:\\n            raise NotImplementedError()\\n\\n        # update q-function parameters based on huber loss gradient\\n        loss = self.model.fit(\\n            np.array(X_train), np.array(y_train),\\n            epochs=1, verbose=0\\n        ).history[\"loss\"][0]\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def save(self):\\n        path = pathlib.Path()\\n        self.model.save(path / \"models/{}.keras\".format(self.model_name), overwrite=True)\\n\\n    def load(self):\\n        return tf.keras.models.load_model(\"models/{}.keras\".format(self.model_name), custom_objects=self.custom_objects)\\n', 'Optimize make the code more robust\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 28, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t28-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a try and except blocks to catch exceptions in various methods, \\n                                            including model creation, training, saving, and loading.\\n------------------------------------------------------------------------------------------------------------------------\\nSummary of Changes:\\n    Error Handling: Added try and except blocks to catch exceptions in various methods, including model creation,\\n    training, saving, and loading.\\n    Error Messages: Each except block prints a descriptive error message to help identify where the issue occurred.\\n    Return Values: In case of an error, methods return None or appropriate default values to indicate failure.\\n\\n\"\"\"\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\n\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\"\"\"\\n    try:\\n        error = y_true - y_pred\\n        cond = keras.backend.abs(error) <= clip_delta\\n        squared_loss = 0.5 * keras.backend.square(error)\\n        quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n        return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n    except Exception as err:\\n        print(\"Error in huber_loss: \" + str(err))\\n        return None  # Return None or a default value in case of an error\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n\\n        # agent config\\n        self.state_size = state_size  # normalized previous days\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # model config\\n        self.gamma = 0.95  # affinity for long term reward\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}  # important for loading the model from memory\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n\\n        try:\\n            model_path = f\"models/{self.model_name}.keras\"\\n            if pretrained and os.path.exists(model_path) and self.model_name is not None:\\n                self.model = self.load()\\n            else:\\n                self.model = self._model()\\n\\n            # strategy config\\n            if self.strategy in [\"t-dqn\", \"double-dqn\"]:\\n                self.n_iter = 1\\n                self.reset_every = reset_every\\n\\n                # target network\\n                self.target_model = tf.keras.models.clone_model(self.model)\\n                self.target_model.set_weights(self.model.get_weights())\\n        except Exception as err:\\n            print(\"Error in Agent initialization: \" + str(err))\\n            self.model = None  # Set model to None in case of an error\\n\\n    def _model(self):\\n        \"\"\"Creates the model\"\"\"\\n        try:\\n            model = keras.Sequential()\\n            model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n            model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n            model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n            model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n            model.add(tf.keras.layers.Dense(units=self.action_size))\\n\\n            model.compile(loss=self.loss, optimizer=self.optimizer)\\n            return model\\n        except Exception as err:\\n            print(\"Error in creating model: \" + str(err))\\n            return None\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory\"\"\"\\n        try:\\n            self.memory.append((state, action, reward, done))\\n        except Exception as err:\\n            print(\"Error in remember: \" + str(err))\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions\"\"\"\\n        try:\\n            # take random action in order to diversify experience at the beginning\\n            if not is_eval and random.random() <= self.epsilon:\\n                return random.randrange(self.action_size)\\n\\n            action_probs = self.model.predict(state, verbose=0)\\n            return np.argmax(action_probs[0])\\n        except Exception as err:\\n            print(\"Error in act: \" + str(err))\\n            return None  # Return None or a default action in case of an error\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory\"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        try:\\n            # DQN\\n            if self.strategy == \"dqn\":\\n                while len(self.memory) > 0:\\n                    state, action, reward, done = self.memory.pop()\\n                    # Here, target is directly set to the immediate reward\\n                    target = reward\\n\\n                    # estimate q-values based on current state\\n                    q_values = self.model.predict(state, verbose=0)\\n                    # update the target for current action based on the immediate reward\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n                    # update q-function parameters based on huber loss gradient\\n                    loss += self.model.fit(\\n                        np.array(X_train), np.array(y_train),\\n                        epochs=1, verbose=0\\n                    ).history[\"loss\"][0]\\n\\n            # as the training goes on we want the agent to\\n            # make less random and more optimal decisions\\n            if self.epsilon > self.epsilon_min:\\n                self.epsilon *= self.epsilon_decay\\n\\n            return loss\\n        except Exception as err:\\n            print(\"Error in train_experience_replay2: \" + str(err))\\n            return None  # Return None or a default loss in case of an error\\n\\n    def train_experience_replay(self, batch_size):\\n        \"\"\"Train on previous experiences in memory\"\"\"\\n        mini_batch = random.sample(self.memory, batch_size)\\n        X_train, y_train = [], []\\n\\n        try:\\n            # DQN\\n            if self.strategy == \"dqn\":\\n                for state, action, reward, done in mini_batch:\\n                    # Here, target is directly set to the immediate reward\\n                    target = reward\\n\\n                    # estimate q-values based on current state\\n                    q_values = self.model.predict(state, verbose=0)\\n                    # update the target for current action based on the immediate reward\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n            # DQN with fixed targets\\n            elif self.strategy == \"t-dqn\":\\n                if self.n_iter % self.reset_every == 0:\\n                    # reset target model weights\\n                    self.target_model.set_weights(self.model.get_weights())\\n\\n                for state, action, reward, next_state, done in mini_batch:\\n                    if done:\\n                        target = reward\\n                    else:\\n                        # approximate deep q-learning equation with fixed targets\\n                        target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n\\n                    # estimate q-values based on current state\\n                    q_values = self.model.predict(state)\\n                    # update the target for current action based on discounted reward\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n            # Double DQN\\n            elif self.strategy == \"double-dqn\":\\n                if self.n_iter % self.reset_every == 0:\\n                    # reset target model weights\\n                    self.target_model.set_weights(self.model.get_weights())\\n\\n                for state, action, reward, next_state, done in mini_batch:\\n                    if done:\\n                        target = reward\\n                    else:\\n                        # approximate double deep q-learning equation\\n                        target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                            np.argmax(self.model.predict(next_state)[0])]\\n\\n                    # estimate q-values based on current state\\n                    q_values = self.model.predict(state)\\n                    # update the target for current action based on discounted reward\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n            else:\\n                raise NotImplementedError()\\n\\n            # update q-function parameters based on huber loss gradient\\n            loss = self.model.fit(\\n                np.array(X_train), np.array(y_train),\\n                epochs=1, verbose=0\\n            ).history[\"loss\"][0]\\n\\n            # as the training goes on we want the agent to\\n            # make less random and more optimal decisions\\n            if self.epsilon > self.epsilon_min:\\n                self.epsilon *= self.epsilon_decay\\n\\n            return loss\\n        except Exception as err:\\n            print(\"Error in train_experience_replay: \" + str(err))\\n            return None  # Return None or a default loss in case of an error\\n\\n    def save(self):\\n        try:\\n            path = pathlib.Path()\\n            self.model.save(path / \"models/{}.keras\".format(self.model_name), overwrite=True)\\n        except Exception as err:\\n            print(\"Error in save: \" + str(err))\\n\\n    def load(self):\\n        try:\\n            return tf.keras.models.load_model(\"models/{}.keras\".format(self.model_name), custom_objects=self.custom_objects)\\n        except Exception as err:\\n            print(\"Error in load: \" + str(err))\\n            return None  # Return None in case of an error', 'the following code is generating the following Error in train_experience_replay2: \\'NoneType\\' object has no attribute \\'predict\\'\\n\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 28, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t28-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a try and except blocks to catch exceptions in various methods,\\n                                            including model creation, training, saving, and loading.\\n------------------------------------------------------------------------------------------------------------------------\\nSummary of Changes:\\n    Error Handling: Added try and except blocks to catch exceptions in various methods, including model creation,\\n    training, saving, and loading.\\n    Error Messages: Each except block prints a descriptive error message to help identify where the issue occurred.\\n    Return Values: In case of an error, methods return None or appropriate default values to indicate failure.\\n\\n\"\"\"\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\n\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\"\"\"\\n    try:\\n        error = y_true - y_pred\\n        cond = keras.backend.abs(error) <= clip_delta\\n        squared_loss = 0.5 * keras.backend.square(error)\\n        quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n        return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n    except Exception as err:\\n        print(\"Error in huber_loss: \" + str(err))\\n        return None  # Return None or a default value in case of an error\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n\\n        # agent config\\n        self.state_size = state_size  # normalized previous days\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # model config\\n        self.gamma = 0.95  # affinity for long term reward\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}  # important for loading the model from memory\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n\\n        try:\\n            model_path = f\"models/{self.model_name}.keras\"\\n            if pretrained and os.path.exists(model_path) and self.model_name is not None:\\n                self.model = self.load()\\n            else:\\n                self.model = self._model()\\n\\n            # strategy config\\n            if self.strategy in [\"t-dqn\", \"double-dqn\"]:\\n                self.n_iter = 1\\n                self.reset_every = reset_every\\n\\n                # target network\\n                self.target_model = tf.keras.models.clone_model(self.model)\\n                self.target_model.set_weights(self.model.get_weights())\\n        except Exception as err:\\n            print(\"Error in Agent initialization: \" + str(err))\\n            self.model = None  # Set model to None in case of an error\\n\\n    def _model(self):\\n        \"\"\"Creates the model\"\"\"\\n        try:\\n            model = keras.Sequential()\\n            model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n            model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n            model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n            model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n            model.add(tf.keras.layers.Dense(units=self.action_size))\\n\\n            model.compile(loss=self.loss, optimizer=self.optimizer)\\n            return model\\n        except Exception as err:\\n            print(\"Error in creating model: \" + str(err))\\n            return None\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory\"\"\"\\n        try:\\n            self.memory.append((state, action, reward, done))\\n        except Exception as err:\\n            print(\"Error in remember: \" + str(err))\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions\"\"\"\\n        try:\\n            # take random action in order to diversify experience at the beginning\\n            if not is_eval and random.random() <= self.epsilon:\\n                return random.randrange(self.action_size)\\n\\n            action_probs = self.model.predict(state, verbose=0)\\n            return np.argmax(action_probs[0])\\n        except Exception as err:\\n            print(\"Error in act: \" + str(err))\\n            return None  # Return None or a default action in case of an error\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory\"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        try:\\n            # DQN\\n            if self.strategy == \"dqn\":\\n                while len(self.memory) > 0:\\n                    state, action, reward, done = self.memory.pop()\\n                    # Here, target is directly set to the immediate reward\\n                    target = reward\\n\\n                    # estimate q-values based on current state\\n                    q_values = self.model.predict(state, verbose=0)\\n                    # update the target for current action based on the immediate reward\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n                    # update q-function parameters based on huber loss gradient\\n                    loss += self.model.fit(\\n                        np.array(X_train), np.array(y_train),\\n                        epochs=1, verbose=0\\n                    ).history[\"loss\"][0]\\n\\n            # as the training goes on we want the agent to\\n            # make less random and more optimal decisions\\n            if self.epsilon > self.epsilon_min:\\n                self.epsilon *= self.epsilon_decay\\n\\n            return loss\\n        except Exception as err:\\n            print(\"Error in train_experience_replay2: \" + str(err))\\n            return None  # Return None or a default loss in case of an error\\n\\n    def train_experience_replay(self, batch_size):\\n        \"\"\"Train on previous experiences in memory\"\"\"\\n        mini_batch = random.sample(self.memory, batch_size)\\n        X_train, y_train = [], []\\n\\n        try:\\n            # DQN\\n            if self.strategy == \"dqn\":\\n                for state, action, reward, done in mini_batch:\\n                    # Here, target is directly set to the immediate reward\\n                    target = reward\\n\\n                    # estimate q-values based on current state\\n                    q_values = self.model.predict(state, verbose=0)\\n                    # update the target for current action based on the immediate reward\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n            # DQN with fixed targets\\n            elif self.strategy == \"t-dqn\":\\n                if self.n_iter % self.reset_every == 0:\\n                    # reset target model weights\\n                    self.target_model.set_weights(self.model.get_weights())\\n\\n                for state, action, reward, next_state, done in mini_batch:\\n                    if done:\\n                        target = reward\\n                    else:\\n                        # approximate deep q-learning equation with fixed targets\\n                        target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n\\n                    # estimate q-values based on current state\\n                    q_values = self.model.predict(state)\\n                    # update the target for current action based on discounted reward\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n            # Double DQN\\n            elif self.strategy == \"double-dqn\":\\n                if self.n_iter % self.reset_every == 0:\\n                    # reset target model weights\\n                    self.target_model.set_weights(self.model.get_weights())\\n\\n                for state, action, reward, next_state, done in mini_batch:\\n                    if done:\\n                        target = reward\\n                    else:\\n                        # approximate double deep q-learning equation\\n                        target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                            np.argmax(self.model.predict(next_state)[0])]\\n\\n                    # estimate q-values based on current state\\n                    q_values = self.model.predict(state)\\n                    # update the target for current action based on discounted reward\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n            else:\\n                raise NotImplementedError()\\n\\n            # update q-function parameters based on huber loss gradient\\n            loss = self.model.fit(\\n                np.array(X_train), np.array(y_train),\\n                epochs=1, verbose=0\\n            ).history[\"loss\"][0]\\n\\n            # as the training goes on we want the agent to\\n            # make less random and more optimal decisions\\n            if self.epsilon > self.epsilon_min:\\n                self.epsilon *= self.epsilon_decay\\n\\n            return loss\\n        except Exception as err:\\n            print(\"Error in train_experience_replay: \" + str(err))\\n            return None  # Return None or a default loss in case of an error\\n\\n    def save(self):\\n        try:\\n            path = pathlib.Path()\\n            self.model.save(path / \"models/{}.keras\".format(self.model_name), overwrite=True)\\n        except Exception as err:\\n            print(\"Error in save: \" + str(err))\\n\\n    def load(self):\\n        try:\\n            return tf.keras.models.load_model(\"models/{}.keras\".format(self.model_name), custom_objects=self.custom_objects)\\n        except Exception as err:\\n            print(\"Error in load: \" + str(err))\\n            return None  # Return None in case of an error', 'Error in train_experience_replay2: Model is not initialized.\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 28, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t28-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a try and except blocks to catch exceptions in various methods,\\n                                            including model creation, training, saving, and loading.\\n------------------------------------------------------------------------------------------------------------------------\\nSummary of Changes:\\n    Error Handling: Added try and except blocks to catch exceptions in various methods, including model creation,\\n    training, saving, and loading.\\n    Error Messages: Each except block prints a descriptive error message to help identify where the issue occurred.\\n    Return Values: In case of an error, methods return None or appropriate default values to indicate failure.\\n\\n\"\"\"\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nimport logging\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\"\"\"\\n    try:\\n        error = y_true - y_pred\\n        cond = keras.backend.abs(error) <= clip_delta\\n        squared_loss = 0.5 * keras.backend.square(error)\\n        quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n        return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n    except Exception as err:\\n        logging.error(\"Error in huber_loss: %s\", err)\\n        return None\\n\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n        self.state_size = state_size\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # Model configuration\\n        self.gamma = 0.95\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n\\n        self.model = self._initialize_model(pretrained)\\n\\n    def _initialize_model(self, pretrained):\\n        \"\"\"Initialize the model, loading from file if pretrained.\"\"\"\\n        try:\\n            model_path = f\"models/{self.model_name}.keras\"\\n            if pretrained and os.path.exists(model_path):\\n                model = self.load()\\n                if model is None:\\n                    raise ValueError(\"Failed to load the pretrained model.\")\\n                return model\\n            return self._model()\\n        except Exception as err:\\n            logging.error(\"Error in model initialization: %s\", err)\\n            return None\\n\\n    def _model(self):\\n        \"\"\"Creates the model.\"\"\"\\n        try:\\n            model = keras.Sequential([\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=self.action_size)\\n            ])\\n            model.compile(loss=self.loss, optimizer=self.optimizer)\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in creating model: %s\", err)\\n            return None\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory.\"\"\"\\n        try:\\n            self.memory.append((state, action, reward, done))\\n        except Exception as err:\\n            logging.error(\"Error in remember: %s\", err)\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions.\"\"\"\\n        try:\\n            if not is_eval and random.random() <= self.epsilon:\\n                return random.randrange(self.action_size)\\n\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            action_probs = self.model.predict(state, verbose=0)\\n            return np.argmax(action_probs[0])\\n        except Exception as err:\\n            logging.error(\"Error in act: %s\", err)\\n            return None\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory.\"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        try:\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            if self.strategy == \"dqn\":\\n                while len(self.memory) > 0:\\n                    state, action, reward, done = self.memory.pop()\\n                    target = reward\\n\\n                    q_values = self.model.predict(state, verbose=0)\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n                    loss += self.model.fit(\\n                        np.array(X_train), np.array(y_train),\\n                        epochs=1, verbose=0\\n                    ).history[\"loss\"][0]\\n\\n            if self.epsilon > self.epsilon_min:\\n                self.epsilon *= self.epsilon_decay\\n\\n            return loss\\n        except Exception as err:\\n            logging.error(\"Error in train_experience_replay2: %s\", err)\\n            return None\\n\\n    def save(self):\\n        \"\"\"Save the model.\"\"\"\\n        try:\\n            path = pathlib.Path()\\n            self.model.save(path / f\"models/{self.model_name}.keras\", overwrite=True)\\n            logging.info(\"Model saved successfully.\")\\n        except Exception as err:\\n            logging.error(\"Error in save: %s\", err)\\n\\n    def load(self):\\n        \"\"\"Load the model.\"\"\"\\n        try:\\n            model = tf.keras.models.load_model(f\"models/{self.model_name}.keras\", custom_objects=self.custom_objects)\\n            logging.info(\"Model loaded successfully.\")\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in load: %s\", err)\\n            return None', 'add the following to the code, If model already exist continue with training\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 28, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t28-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added logging statements to provide more context during model loading\\n                                            and creation.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a warning if the pretrained model path does not exist.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a Logged a message when the model is created successfully.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nKey Changes:\\n    Additional Logging: Added logging statements to provide more context during model loading and creation.\\n    Model Path Check: Added a warning if the pretrained model path does not exist.\\n    Model Creation Confirmation: Logged a message when the model is created successfully.\\n\\n\"\"\"\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nimport logging\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\"\"\"\\n    try:\\n        error = y_true - y_pred\\n        cond = keras.backend.abs(error) <= clip_delta\\n        squared_loss = 0.5 * keras.backend.square(error)\\n        quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n        return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n    except Exception as err:\\n        logging.error(\"Error in huber_loss: %s\", err)\\n        return None\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n        self.state_size = state_size\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # Model configuration\\n        self.gamma = 0.95\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n\\n        self.model = self._initialize_model(pretrained)\\n\\n    def _initialize_model(self, pretrained):\\n        \"\"\"Initialize the model, loading from file if pretrained.\"\"\"\\n        try:\\n            model_path = f\"models/{self.model_name}.keras\"\\n            if pretrained:\\n                if os.path.exists(model_path):\\n                    logging.info(\"Loading pretrained model from %s\", model_path)\\n                    model = self.load()\\n                    if model is None:\\n                        raise ValueError(\"Failed to load the pretrained model.\")\\n                    return model\\n                else:\\n                    logging.warning(\"Pretrained model path does not exist: %s\", model_path)\\n            return self._model()\\n        except Exception as err:\\n            logging.error(\"Error in model initialization: %s\", err)\\n            return None\\n\\n    def _model(self):\\n        \"\"\"Creates the model.\"\"\"\\n        try:\\n            model = keras.Sequential([\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=self.action_size)\\n            ])\\n            model.compile(loss=self.loss, optimizer=self.optimizer)\\n            logging.info(\"Model created successfully.\")\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in creating model: %s\", err)\\n            return None\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory.\"\"\"\\n        try:\\n            self.memory.append((state, action, reward, done))\\n        except Exception as err:\\n            logging.error(\"Error in remember: %s\", err)\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions.\"\"\"\\n        try:\\n            if not is_eval and random.random() <= self.epsilon:\\n                return random.randrange(self.action_size)\\n\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            action_probs = self.model.predict(state, verbose=0)\\n            return np.argmax(action_probs[0])\\n        except Exception as err:\\n            logging.error(\"Error in act: %s\", err)\\n            return None\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory.\"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        try:\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            if self.strategy == \"dqn\":\\n                while len(self.memory) > 0:\\n                    state, action, reward, done = self.memory.pop()\\n                    target = reward\\n\\n                    q_values = self.model.predict(state, verbose=0)\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n                    loss += self.model.fit(\\n                        np.array(X_train), np.array(y_train),\\n                        epochs=1, verbose=0\\n                    ).history[\"loss\"][0]\\n\\n            if self.epsilon > self.epsilon_min:\\n                self.epsilon *= self.epsilon_decay\\n\\n            return loss\\n        except Exception as err:\\n            logging.error(\"Error in train_experience_replay2: %s\", err)\\n            return None\\n\\n    def save(self):\\n        \"\"\"Save the model.\"\"\"\\n        try:\\n            path = pathlib.Path()\\n            self.model.save(path / f\"models/{self.model_name}.keras\", overwrite=True)\\n            logging.info(\"Model saved successfully.\")\\n        except Exception as err:\\n            logging.error(\"Error in save: %s\", err)\\n\\n    def load(self):\\n        \"\"\"Load the model.\"\"\"\\n        try:\\n            model = tf.keras.models.load_model(f\"models/{self.model_name}.keras\", custom_objects=self.custom_objects)\\n            logging.info(\"Model loaded successfully.\")\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in load: %s\", err)\\n            return None', 'Add the following to the following code:\\nif the model already exists, it continues with training instead of starting from scratch. if the model is loaded successfully, you can skip the model creation step and proceed directly to training.\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 28, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t28-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added logging statements to provide more context during model loading\\n                                            and creation.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a warning if the pretrained model path does not exist.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a Logged a message when the model is created successfully.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nKey Changes:\\n    Additional Logging: Added logging statements to provide more context during model loading and creation.\\n    Model Path Check: Added a warning if the pretrained model path does not exist.\\n    Model Creation Confirmation: Logged a message when the model is created successfully.\\n\\n\"\"\"\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nimport logging\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\"\"\"\\n    try:\\n        error = y_true - y_pred\\n        cond = keras.backend.abs(error) <= clip_delta\\n        squared_loss = 0.5 * keras.backend.square(error)\\n        quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n        return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n    except Exception as err:\\n        logging.error(\"Error in huber_loss: %s\", err)\\n        return None\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n        self.state_size = state_size\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # Model configuration\\n        self.gamma = 0.95\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n\\n        self.model = self._initialize_model(pretrained)\\n\\n    def _initialize_model(self, pretrained):\\n        \"\"\"Initialize the model, loading from file if pretrained.\"\"\"\\n        try:\\n            model_path = f\"models/{self.model_name}.keras\"\\n            if pretrained:\\n                if os.path.exists(model_path):\\n                    logging.info(\"Loading pretrained model from %s\", model_path)\\n                    model = self.load()\\n                    if model is None:\\n                        raise ValueError(\"Failed to load the pretrained model.\")\\n                    return model\\n                else:\\n                    logging.warning(\"Pretrained model path does not exist: %s\", model_path)\\n            return self._model()\\n        except Exception as err:\\n            logging.error(\"Error in model initialization: %s\", err)\\n            return None\\n\\n    def _model(self):\\n        \"\"\"Creates the model.\"\"\"\\n        try:\\n            model = keras.Sequential([\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=self.action_size)\\n            ])\\n            model.compile(loss=self.loss, optimizer=self.optimizer)\\n            logging.info(\"Model created successfully.\")\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in creating model: %s\", err)\\n            return None\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory.\"\"\"\\n        try:\\n            self.memory.append((state, action, reward, done))\\n        except Exception as err:\\n            logging.error(\"Error in remember: %s\", err)\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions.\"\"\"\\n        try:\\n            if not is_eval and random.random() <= self.epsilon:\\n                return random.randrange(self.action_size)\\n\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            action_probs = self.model.predict(state, verbose=0)\\n            return np.argmax(action_probs[0])\\n        except Exception as err:\\n            logging.error(\"Error in act: %s\", err)\\n            return None\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory.\"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        try:\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            if self.strategy == \"dqn\":\\n                while len(self.memory) > 0:\\n                    state, action, reward, done = self.memory.pop()\\n                    target = reward\\n\\n                    q_values = self.model.predict(state, verbose=0)\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n                    loss += self.model.fit(\\n                        np.array(X_train), np.array(y_train),\\n                        epochs=1, verbose=0\\n                    ).history[\"loss\"][0]\\n\\n            if self.epsilon > self.epsilon_min:\\n                self.epsilon *= self.epsilon_decay\\n\\n            return loss\\n        except Exception as err:\\n            logging.error(\"Error in train_experience_replay2: %s\", err)\\n            return None\\n\\n    def save(self):\\n        \"\"\"Save the model.\"\"\"\\n        try:\\n            path = pathlib.Path()\\n            self.model.save(path / f\"models/{self.model_name}.keras\", overwrite=True)\\n            logging.info(\"Model saved successfully.\")\\n        except Exception as err:\\n            logging.error(\"Error in save: %s\", err)\\n\\n    def load(self):\\n        \"\"\"Load the model.\"\"\"\\n        try:\\n            model = tf.keras.models.load_model(f\"models/{self.model_name}.keras\", custom_objects=self.custom_objects)\\n            logging.info(\"Model loaded successfully.\")\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in load: %s\", err)\\n            return None', 'I get the following Error in train_experience_replay2: Model is not initialized when model exist\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 28, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t28-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added logging statements to provide more context during model loading\\n                                            and creation.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a warning if the pretrained model path does not exist.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a Logged a message when the model is created successfully.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nKey Changes:\\n    Additional Logging: Added logging statements to provide more context during model loading and creation.\\n    Model Path Check: Added a warning if the pretrained model path does not exist.\\n    Model Creation Confirmation: Logged a message when the model is created successfully.\\n\\n\"\"\"\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nimport logging\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\"\"\"\\n    try:\\n        error = y_true - y_pred\\n        cond = keras.backend.abs(error) <= clip_delta\\n        squared_loss = 0.5 * keras.backend.square(error)\\n        quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n        return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n    except Exception as err:\\n        logging.error(\"Error in huber_loss: %s\", err)\\n        return None\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n        self.state_size = state_size\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # Model configuration\\n        self.gamma = 0.95\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n\\n        # Initialize the model\\n        self.model = self._initialize_model(pretrained)\\n\\n    def _initialize_model(self, pretrained):\\n        \"\"\"Initialize the model, loading from file if pretrained.\"\"\"\\n        try:\\n            model_path = f\"models/{self.model_name}.keras\"\\n            if pretrained:\\n                if os.path.exists(model_path):\\n                    logging.info(\"Loading pretrained model from %s\", model_path)\\n                    model = self.load()\\n                    if model is None:\\n                        raise ValueError(\"Failed to load the pretrained model.\")\\n                    return model  # Return the loaded model\\n                else:\\n                    logging.warning(\"Pretrained model path does not exist: %s\", model_path)\\n            return self._model()  # Create a new model if not loading\\n        except Exception as err:\\n            logging.error(\"Error in model initialization: %s\", err)\\n            return None\\n\\n    def _model(self):\\n        \"\"\"Creates the model.\"\"\"\\n        try:\\n            model = keras.Sequential([\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=256, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=128, activation=\"relu\"),\\n                tf.keras.layers.Dense(units=self.action_size)\\n            ])\\n            model.compile(loss=self.loss, optimizer=self.optimizer)\\n            logging.info(\"Model created successfully.\")\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in creating model: %s\", err)\\n            return None\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory.\"\"\"\\n        try:\\n            self.memory.append((state, action, reward, done))\\n        except Exception as err:\\n            logging.error(\"Error in remember: %s\", err)\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions.\"\"\"\\n        try:\\n            if not is_eval and random.random() <= self.epsilon:\\n                return random.randrange(self.action_size)\\n\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            action_probs = self.model.predict(state, verbose=0)\\n            return np.argmax(action_probs[0])\\n        except Exception as err:\\n            logging.error(\"Error in act: %s\", err)\\n            return None\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory.\"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        try:\\n            if self.model is None:\\n                raise ValueError(\"Model is not initialized.\")\\n\\n            if self.strategy == \"dqn\":\\n                while len(self.memory) > 0:\\n                    state, action, reward, done = self.memory.pop()\\n                    target = reward\\n\\n                    q_values = self.model.predict(state, verbose=0)\\n                    q_values[0][action] = target\\n\\n                    X_train.append(state[0])\\n                    y_train.append(q_values[0])\\n\\n                    loss += self.model.fit(\\n                        np.array(X_train), np.array(y_train),\\n                        epochs=1, verbose=0\\n                    ).history[\"loss\"][0]\\n\\n            if self.epsilon > self.epsilon_min:\\n                self.epsilon *= self.epsilon_decay\\n\\n            return loss\\n        except Exception as err:\\n            logging.error(\"Error in train_experience_replay2: %s\", err)\\n            return None\\n\\n    def save(self):\\n        \"\"\"Save the model.\"\"\"\\n        try:\\n            path = pathlib.Path()\\n            self.model.save(path / f\"models/{self.model_name}.keras\", overwrite=True)\\n            logging.info(\"Model saved successfully.\")\\n        except Exception as err:\\n            logging.error(\"Error in save: %s\", err)\\n\\n    def load(self):\\n        \"\"\"Load the model.\"\"\"\\n        try:\\n            model = tf.keras.models.load_model(f\"models/{self.model_name}.keras\", custom_objects=self.custom_objects)\\n            logging.info(\"Model loaded successfully.\")\\n            return model\\n        except Exception as err:\\n            logging.error(\"Error in load: %s\", err)\\n            return None', 'If previous_agent_profit = 12,000 the first time, and previous_agent_profit = 6,000 the second time, and previous_agent_profit = 9,000 the third time, I loose the previous_agent_profit = 12,000 in code below:\\n\\n\"\"\"\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\"\"\"\\n\\nimport logging\\n\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    switch_k_backend_device,\\n    format_currency,\\n    format_position, show_train_result, )\\n\\n\\ndef main(train_stock, train_percent, date_field, adjcp_field, window_size, batch_size, ep_count,\\n         strategy=\"t-dqn\", model_name=\"model_debug\", debug=False):\\n    \"\"\" Trains the stock trading bot using Deep Q-Learning.\\n    Please see https://arxiv.org/abs/1312.5602 for more details.\\n\\n    Args: [python train.py --help]\\n    \"\"\"\\n    agent = Agent(window_size, strategy=strategy, pretrained=True, model_name=model_name)\\n    data = load_data(train_stock, date_field)\\n    take = int(len(data) * (train_percent / 100))\\n    train_data = data_split(data, 0, take, adjcp_field)\\n    agent_profit = 0\\n    prev_agent_profit = -1000000000\\n    episode = 0\\n    episode_count = ep_count\\n    while True:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   window_size=window_size, prev_agent_profit=agent_profit, adjcp_field=adjcp_field)\\n\\n        agent_profit = train_result[2]\\n        ave_loss = train_result[3]\\n        logging.debug(\"Current Profit at: {} Ave Loss: {} | Previous Profit: {}\".format(\\n            format_currency(agent_profit),\\n            format_currency(ave_loss),\\n            format_position(prev_agent_profit)))\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        if episode >= episode_count:\\n            break\\n        episode += 1\\n\\n    # Show Validation after finishing\\n    if agent_profit > 0:\\n        val_data = data_split(data, take, int(len(data)), adjcp_field)\\n        initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n        val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n        show_train_result(train_result, val_result, initial_offset, strategy)\\n\\n\\nif __name__ == \"__main__\":\\n    args = docopt(__doc__)\\n\\n    file_data = args[\"<file-data>\"]\\n    train_percent = args[\"<train-percent>\"]\\n\\n    date_field = args[\"--date-field\"]\\n    adjcp_field = args[\"--adjcp-field\"]\\n    window_size = int(args[\"--window-size\"])\\n    batch_size = int(args[\"--batch-size\"])\\n    ep_count = int(args[\"--episode-count\"])\\n    model_name = args[\"--model-name\"]\\n    debug = args[\"--debug\"]\\n\\n    coloredlogs.install(level=\"DEBUG\")\\n    switch_k_backend_device()\\n\\n    try:\\n        # dqn\\n        main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n             ep_count, \"dqn\", f\"{model_name}_dqn\", debug)\\n\\n        # # t-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"t-dqn\", f\"{model_name}_tdqn\", debug)\\n        #\\n        # # double-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"double-dqn\", f\"{model_name}_ddqn\", debug)\\n\\n    except KeyboardInterrupt:\\n        print(\"Aborted!\")\\n', 'Check for any errors, and optimizations on the following code:\\n\\nimport os\\nfrom pathlib import Path\\n\\nimport pandas as pd\\nfrom flask import Flask, jsonify, request\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.ops import get_state\\nfrom trading_bot.utils import switch_k_backend_device, add_indicators\\n\\napp = Flask(__name__)\\napp.config[\\'WTF_CSRF_CHECK_DEFAULT\\'] = False\\napp.config[\\'WTF_CSRF_ENABLED\\'] = False\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    global agent\\n    rq = request.json\\n\\n    # Extract the data\\n    data = rq.get(\\'Data\\', [])\\n\\n    # Check for the presence of the apiKey\\n    # api_key = rq.get(\\'apiKey\\', \\'\\')\\n\\n    # if not api_key or api_key != \"apiKey\":\\n    #     return jsonify({\"error\": \"Invalid or missing API key\"}), 401\\n\\n    # Convert the data to a DataFrame\\n    df = pd.DataFrame(data, columns=[\\'Time\\', \\'Open\\', \\'High\\', \\'Low\\', \\'Close\\', \\'Volume\\'])\\n    df = add_indicators(df)\\n\\n    # Convert the \\'date\\' column to datetime\\n    # df[\\'date\\'] = pd.to_datetime(df[\\'date\\'], format=\\'%m/%d/%Y %H:%M\\')\\n    t = len(df) - 1\\n    state = get_state(df, t, window_size)\\n    output = agent.act(state, True)\\n    if output == 1:\\n        return jsonify({\\'Res\\': \\'buy\\'}), 200\\n    if output == 2:\\n        return jsonify({\\'Res\\': \\'sell\\'}), 200\\n\\n    return jsonify({\\'Res\\': \\'hold\\'}), 200\\n\\n\\nif __name__ == \"__main__\":\\n    predict_data = None\\n    directory_path = os.path.realpath(__file__)\\n    folder = Path(directory_path)\\n    window_size = 60\\n    env = None\\n    env_test = None\\n    switch_k_backend_device()\\n\\n    agent = Agent(window_size, pretrained=True, model_name=\\'model_dqn\\')\\n    app.run(host=\"0.0.0.0\", debug=False)\\n', \"Check for errors\\n\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n\\n\\n# Generate initial DataFrame with 5 columns and 4000 rows\\nnp.random.seed(0) # For reproducibility\\ndata = np.random.randint(1, 100, size=(4000, 5))\\ndf_initial = pd.DataFrame(data, columns=[f'Column{i+1}' for i in range(5)])\\n\\n# Prepare data for training\\nX = df_initial.values[:-5] # Use all rows except the last 5 for features\\ny = df_initial.values[5:] # Use rows 5 to 4000 for target\\n\\n# Train a linear regression model for each column\\nmodels = []\\nfor i in range(5):\\n    model = LinearRegression()\\n    model.fit(X, y[:, i])\\n    models.append(model)\\n\\n# Predict the next 5 rows\\nlast_rows = df_initial.values[-5:] # Last 5 rows to predict the next values\\npredictions = np.array([model.predict(last_rows) for model in models]).T\\n\\n# Create a DataFrame for the predicted columns\\ndf_predictions = pd.DataFrame(predictions, columns=[f'Predicted_Column{i+1}' for i in range(5)])\\n\\n# Display the predicted DataFrame\\nprint(df_predictions)\", 'If Current Profit at: 12,000 the first time, and Current Profit at: 6,000 the second time, and Current Profit at: 9,000 the third time, the model get\\'s overwritten by the third one instead of holding the largest Current Profit in the code below:\\n\\n\"\"\"\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\\nKey Changes:\\n    Introduced max_agent_profit: This variable keeps track of the maximum profit observed across all episodes.\\n    Updated prev_agent_profit: This variable is updated within each episode to reflect the profit of the current\\n    episode.\\n    Printed max_agent_profit: At the end of the training, the maximum profit observed is printed.\\n\\n\"\"\"\\n\\nimport logging\\n\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    switch_k_backend_device,\\n    format_currency,\\n    format_position, show_train_result, )\\n\\n\\ndef main(train_stock, train_percent, date_field, adjcp_field, window_size, batch_size, ep_count,\\n         strategy=\"t-dqn\", model_name=\"model_debug\", debug=False):\\n    \"\"\" Trains the stock trading bot using Deep Q-Learning. \"\"\"\\n    agent = Agent(window_size, strategy=strategy, pretrained=True, model_name=model_name)\\n    data = load_data(train_stock, date_field)\\n    take = int(len(data) * (train_percent / 100))\\n    train_data = data_split(data, 0, take, adjcp_field)\\n    agent_profit = 0\\n    prev_agent_profit = -1000000000\\n    max_agent_profit = -1000000000  # New variable to track the maximum profit\\n    episode = 0\\n    episode_count = ep_count\\n    while True:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   window_size=window_size, prev_agent_profit=max_agent_profit, adjcp_field=adjcp_field)\\n\\n        agent_profit = train_result[2]\\n        ave_loss = train_result[3]\\n        logging.debug(\"Current Profit at: {} Ave Loss: {} | Previous Profit: {}\".format(\\n            format_currency(agent_profit),\\n            format_currency(ave_loss),\\n            format_position(prev_agent_profit)))\\n\\n        # Update the maximum profit observed so far\\n        if agent_profit > max_agent_profit:\\n            max_agent_profit = agent_profit\\n\\n        # Update the previous profit for the next episode\\n        prev_agent_profit = agent_profit\\n\\n        if episode >= episode_count:\\n            break\\n        episode += 1\\n\\n    # Show Validation after finishing\\n    if agent_profit > 0:\\n        val_data = data_split(data, take, int(len(data)), adjcp_field)\\n        initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n        val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n        show_train_result(train_result, val_result, initial_offset, strategy)\\n\\n    # Print the maximum profit observed\\n    print(\"Maximum Profit Observed: {}\".format(format_currency(max_agent_profit)))\\n\\n\\nif __name__ == \"__main__\":\\n    args = docopt(__doc__)\\n\\n    file_data = args[\"<file-data>\"]\\n    train_percent = args[\"<train-percent>\"]\\n\\n    date_field = args[\"--date-field\"]\\n    adjcp_field = args[\"--adjcp-field\"]\\n    window_size = int(args[\"--window-size\"])\\n    batch_size = int(args[\"--batch-size\"])\\n    ep_count = int(args[\"--episode-count\"])\\n    model_name = args[\"--model-name\"]\\n    # strategy = args[\"--strategy\"]\\n    debug = args[\"--debug\"]\\n\\n    coloredlogs.install(level=\"DEBUG\")\\n    switch_k_backend_device()\\n\\n    try:\\n        # dqn\\n        main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n             ep_count, \"dqn\", f\"{model_name}_dqn\", debug)\\n\\n        # # t-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"t-dqn\", f\"{model_name}_tdqn\", debug)\\n        #\\n        # # double-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"double-dqn\", f\"{model_name}_ddqn\", debug)\\n\\n    except KeyboardInterrupt:\\n        print(\"Aborted!\")', 'what is the difference between the dqn, t-dqn, and the double-dqn', 'which one is better for Stock market training prediction', 'If previous_agent_profit = 12,000 the first time, and previous_agent_profit = 6,000 the second time, and previous_agent_profit = 9,000 the third time, I loose the previous_agent_profit = 12,000 in code below:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 28, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t28-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a try and except block to catch any exceptions that may\\n                                            occur during the calculation of the sigmoid function\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Adjusted the reward structure to ensure it reflects the agent\\'s actions.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added more logging to understand the actions and rewards.\\n------------------------------------------------------------------------------------------------------------------------\\nChanges Made:\\n    Action Selection: Use the agent\\'s action selection process (agent.act(state)) instead of just relying on the delta.\\n    Reward Structure: Adjusted the reward structure to ensure it reflects the agent\\'s actions.\\n    Logging: Added more logging to understand the actions and rewards.\\n\\n\"\"\"\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0, adjcp_field=\\'close\\'):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False, desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n        reward = 0\\n        is_done = False\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state)  # Use agent\\'s action selection\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            reward = delta\\n            total_profit += delta\\n        elif action == 2:  # SELL\\n            reward = -delta\\n            total_profit -= delta\\n        else:  # HOLD\\n            reward = 0\\n\\n        done = (t == data_length - 1) or is_done\\n        agent.remember(state, action, reward, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay2()\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit >= prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved successfully after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss))\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state, is_eval=True)\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n\\n        if action == 1:  # BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 2:  # SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        else:  # HOLD\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        state = next_state\\n        if t == data_length - 1:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'Check the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 28, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t28-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a try and except block to catch any exceptions that may\\n                                            occur during the calculation of the sigmoid function\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Adjusted the reward structure to ensure it reflects the agent\\'s actions.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added more logging to understand the actions and rewards.\\n[JSanchez]\\t02-Sep-2024:\\tRev. 1.02\\t    After each episode, update prev_agent_profit to retain the highest profit\\n                                            observed so far using prev_agent_profit = max(prev_agent_profit, total_profit).\\n[JSanchez]\\t02-Sep-2024:\\tRev. 1.02\\t    Ensure that prev_agent_profit is returned from the train_model function so\\n                                            that it can be used in subsequent episodes.\\n------------------------------------------------------------------------------------------------------------------------\\nChanges Made:\\n    Action Selection: Use the agent\\'s action selection process (agent.act(state)) instead of just relying on the delta.\\n    Reward Structure: Adjusted the reward structure to ensure it reflects the agent\\'s actions.\\n    Logging: Added more logging to understand the actions and rewards.\\n    Update prev_agent_profit: After each episode, update prev_agent_profit to retain the highest profit observed so\\n    far using prev_agent_profit = max(prev_agent_profit, total_profit).\\n    Return prev_agent_profit: Ensure that prev_agent_profit is returned from the train_model function so that it can\\n    be used in subsequent episodes.\\n\\n\"\"\"\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0, adjcp_field=\\'close\\'):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False, desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n        reward = 0\\n        is_done = False\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state)  # Use agent\\'s action selection\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            reward = delta\\n            total_profit += delta\\n        elif action == 2:  # SELL\\n            reward = -delta\\n            total_profit -= delta\\n        else:  # HOLD\\n            reward = 0\\n\\n        done = (t == data_length - 1) or is_done\\n        agent.remember(state, action, reward, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay2()\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit >= prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved successfully after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Update prev_agent_profit to retain the highest profit observed\\n    prev_agent_profit = max(prev_agent_profit, total_profit)\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss)), prev_agent_profit\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state, is_eval=True)\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n\\n        if action == 1:  # BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 2:  # SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        else:  # HOLD\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        state = next_state\\n        if t == data_length - 1:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'make parameters like threshold in monitor_memory, ep_count, batch_size, and window_size configurable through function arguments or a configuration file. This will enhance flexibility.\\nEnsure that the return values from train_model are used effectively in the calling function.\\ninclude additional performance metrics, such as average profit per trade or win/loss ratio, to evaluate the agent\\'s performance more comprehensively.\\nadd docstrings to your functions to describe their purpose, parameters, and return values.\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 28, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t28-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a try and except block to catch any exceptions that may\\n                                            occur during the calculation of the sigmoid function\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Adjusted the reward structure to ensure it reflects the agent\\'s actions.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added more logging to understand the actions and rewards.\\n[JSanchez]\\t02-Sep-2024:\\tRev. 1.02\\t    After each episode, update prev_agent_profit to retain the highest profit\\n                                            observed so far using prev_agent_profit = max(prev_agent_profit, total_profit).\\n[JSanchez]\\t02-Sep-2024:\\tRev. 1.02\\t    Ensure that prev_agent_profit is returned from the train_model function so\\n                                            that it can be used in subsequent episodes.\\n------------------------------------------------------------------------------------------------------------------------\\nChanges Made:\\n    Action Selection: Use the agent\\'s action selection process (agent.act(state)) instead of just relying on the delta.\\n    Reward Structure: Adjusted the reward structure to ensure it reflects the agent\\'s actions.\\n    Logging: Added more logging to understand the actions and rewards.\\n    Update prev_agent_profit: After each episode, update prev_agent_profit to retain the highest profit observed so\\n    far using prev_agent_profit = max(prev_agent_profit, total_profit).\\n    Return prev_agent_profit: Ensure that prev_agent_profit is returned from the train_model function so that it can\\n    be used in subsequent episodes.\\n\\n\"\"\"\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0, adjcp_field=\\'close\\'):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False, desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n        reward = 0\\n        is_done = False\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state)  # Use agent\\'s action selection\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            reward = delta\\n            total_profit += delta\\n        elif action == 2:  # SELL\\n            reward = -delta\\n            total_profit -= delta\\n        else:  # HOLD\\n            reward = 0\\n\\n        done = (t == data_length - 1) or is_done\\n        agent.remember(state, action, reward, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay2()\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit >= prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved successfully after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Update prev_agent_profit to retain the highest profit observed\\n    prev_agent_profit = max(prev_agent_profit, total_profit)\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss)), prev_agent_profit\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state, is_eval=True)\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n\\n        if action == 1:  # BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 2:  # SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        else:  # HOLD\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        state = next_state\\n        if t == data_length - 1:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'Add to the following code when market reversal to sell when on a buy, and buy when on a sell\\n\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"\\n    Monitor system memory usage and print a warning if it exceeds the threshold.\\n\\n    Parameters:\\n    threshold (int): Memory usage threshold percentage.\\n\\n    Returns:\\n    bool: True if memory usage exceeds the threshold, False otherwise.\\n    \"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0, adjcp_field=\\'close\\'):\\n    \"\"\"\\n    Train the trading agent over a specified number of episodes.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    episode (int): The current episode number.\\n    data (DataFrame): The historical stock data.\\n    ep_count (int): Total number of episodes for training.\\n    batch_size (int): Size of the mini-batch for experience replay.\\n    window_size (int): The size of the window for state representation.\\n    prev_agent_profit (float): The highest profit observed so far.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (episode, ep_count, total_profit, avg_loss, prev_agent_profit)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n    trades = 0\\n    wins = 0\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False, desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n        reward = 0\\n        is_done = False\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state)  # Use agent\\'s action selection\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            reward = delta\\n            total_profit += delta\\n            trades += 1\\n            if delta > 0:\\n                wins += 1\\n        elif action == 2:  # SELL\\n            reward = -delta\\n            total_profit -= delta\\n            trades += 1\\n            if delta < 0:\\n                wins += 1\\n        else:  # HOLD\\n            reward = 0\\n\\n        done = (t == data_length - 1) or is_done\\n        agent.remember(state, action, reward, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay2()\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit >= prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved successfully after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Update prev_agent_profit to retain the highest profit observed\\n    prev_agent_profit = max(prev_agent_profit, total_profit)\\n\\n    avg_profit_per_trade = total_profit / trades if trades > 0 else 0\\n    win_loss_ratio = wins / trades if trades > 0 else 0\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss)), prev_agent_profit, avg_profit_per_trade, win_loss_ratio\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    \"\"\"\\n    Evaluate the trading agent on historical stock data.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window_size (int): The size of the window for state representation.\\n    debug (bool): Flag to enable/disable debug logging.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (total_profit, history)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state, is_eval=True)\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n\\n        if action == 1:  # BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 2:  # SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        else:  # HOLD\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        state = next_state\\n        if t == data_length - 1:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    \"\"\"\\n    Predict the next action using the trained trading agent.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window (int): The size of the window for state representation.\\n\\n    Returns:\\n    int: The predicted action (0: HOLD, 1: BUY, 2: SELL).\\n    \"\"\"\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'Check the following for errors:\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\n\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\\n\\n    Links: \\thttps://en.wikipedia.org/wiki/Huber_loss\\n            https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/\\n    \"\"\"\\n    error = y_true - y_pred\\n    cond = keras.backend.abs(error) <= clip_delta\\n    squared_loss = 0.5 * keras.backend.square(error)\\n    quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n    return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n\\n        # agent config\\n        self.state_size = state_size  # normalized previous days\\n        self.action_size = 3  # [sit, buy, sell, close buy, close sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # model config\\n        self.model_name = model_name\\n        self.gamma = 0.95  # affinity for long term reward\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}  # important for loading the model from memory\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n        model_path = f\"models/{self.model_name}.keras\"\\n        if pretrained and os.path.exists(model_path) and self.model_name is not None:\\n            self.model = self.load()\\n        else:\\n            self.model = self._model()\\n\\n        # strategy config\\n        if self.strategy in [\"t-dqn\", \"double-dqn\"]:\\n            self.n_iter = 1\\n            self.reset_every = reset_every\\n\\n            # target network\\n            self.target_model = tf.keras.models.clone_model(self.model)\\n            self.target_model.set_weights(self.model.get_weights())\\n\\n    def _model(self):\\n        \"\"\"Creates the model\\n        \"\"\"\\n        model = keras.Sequential()\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=self.action_size))\\n\\n        model.compile(loss=self.loss, optimizer=self.optimizer)\\n        return model\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory\\n        \"\"\"\\n        self.memory.append((state, action, reward, done))\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions\\n        \"\"\"\\n        # take random action in order to diversify experience at the beginning\\n        if not is_eval and random.random() <= self.epsilon:\\n            return random.randrange(self.action_size)\\n\\n        action_probs = self.model.predict(state, verbose=0)\\n        return np.argmax(action_probs[0])\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            while len(self.memory) > 0:\\n                state, action, reward, done = self.memory.pop()\\n                # Here, target is directly set to the immediate reward\\n                target = reward\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on the immediate reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n                # update q-function parameters based on huber loss gradient\\n                loss += self.model.fit(\\n                    np.array(X_train), np.array(y_train),\\n                    epochs=1, verbose=0\\n                ).history[\"loss\"][0]\\n\\n        # DQN with fixed targets\\n        elif self.strategy == \"t-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation with fixed targets\\n                    target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # Double DQN\\n        elif self.strategy == \"double-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate double deep q-learning equation\\n                    target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                        np.argmax(self.model.predict(next_state)[0])]\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        else:\\n            raise NotImplementedError()\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def train_experience_replay(self, batch_size):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        mini_batch = random.sample(self.memory, batch_size)\\n        X_train, y_train = [], []\\n\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            for state, action, reward, done in mini_batch:\\n                # Here, target is directly set to the immediate reward\\n                target = reward\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on the immediate reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # DQN with fixed targets\\n        elif self.strategy == \"t-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation with fixed targets\\n                    target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # Double DQN\\n        elif self.strategy == \"double-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate double deep q-learning equation\\n                    target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                        np.argmax(self.model.predict(next_state)[0])]\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        else:\\n            raise NotImplementedError()\\n\\n        # update q-function parameters based on huber loss gradient\\n        loss = self.model.fit(\\n            np.array(X_train), np.array(y_train),\\n            epochs=1, verbose=0\\n        ).history[\"loss\"][0]\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def save(self):\\n        path = pathlib.Path()\\n        self.model.save(path / \"models/{}.keras\".format(self.model_name), overwrite=True)\\n\\n    def load(self):\\n        return tf.keras.models.load_model(\"models/{}.keras\".format(self.model_name), custom_objects=self.custom_objects)\\n\\n\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"\\n    Monitor system memory usage and print a warning if it exceeds the threshold.\\n\\n    Parameters:\\n    threshold (int): Memory usage threshold percentage.\\n\\n    Returns:\\n    bool: True if memory usage exceeds the threshold, False otherwise.\\n    \"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0, adjcp_field=\\'close\\'):\\n    \"\"\"\\n    Train the trading agent over a specified number of episodes.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    episode (int): The current episode number.\\n    data (DataFrame): The historical stock data.\\n    ep_count (int): Total number of episodes for training.\\n    batch_size (int): Size of the mini-batch for experience replay.\\n    window_size (int): The size of the window for state representation.\\n    prev_agent_profit (float): The highest profit observed so far.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (episode, ep_count, total_profit, avg_loss, prev_agent_profit)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n    trades = 0\\n    wins = 0\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False, desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n        reward = 0\\n        is_done = False\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state)  # Use agent\\'s action selection\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            reward = delta\\n            total_profit += delta\\n            trades += 1\\n            if delta > 0:\\n                wins += 1\\n        elif action == 2:  # SELL\\n            reward = -delta\\n            total_profit -= delta\\n            trades += 1\\n            if delta < 0:\\n                wins += 1\\n        else:  # HOLD\\n            reward = 0\\n\\n        done = (t == data_length - 1) or is_done\\n        agent.remember(state, action, reward, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay2()\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit >= prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved successfully after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Update prev_agent_profit to retain the highest profit observed\\n    prev_agent_profit = max(prev_agent_profit, total_profit)\\n\\n    avg_profit_per_trade = total_profit / trades if trades > 0 else 0\\n    win_loss_ratio = wins / trades if trades > 0 else 0\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss)), prev_agent_profit, avg_profit_per_trade, win_loss_ratio\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    \"\"\"\\n    Evaluate the trading agent on historical stock data.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window_size (int): The size of the window for state representation.\\n    debug (bool): Flag to enable/disable debug logging.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (total_profit, history)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state, is_eval=True)\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n\\n        if action == 1:  # BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 2:  # SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        else:  # HOLD\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        state = next_state\\n        if t == data_length - 1:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    \"\"\"\\n    Predict the next action using the trained trading agent.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window (int): The size of the window for state representation.\\n\\n    Returns:\\n    int: The predicted action (0: HOLD, 1: BUY, 2: SELL).\\n    \"\"\"\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action\\n\\n', '  File \"/Users/javiersanchez/PycharmProjects/trading-bot/trading_bot/methods.py\", line 218, in train_model\\n    loss = agent.train_experience_replay2()\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/javiersanchez/PycharmProjects/trading-bot/trading_bot/agent.py\", line 541, in train_experience_replay2\\n    for state, action, reward, next_state, done in mini_batch:\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nValueError: not enough values to unpack (expected 5, got 4)\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\n\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\\n\\n    Links: \\thttps://en.wikipedia.org/wiki/Huber_loss\\n            https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/\\n    \"\"\"\\n    error = y_true - y_pred\\n    cond = keras.backend.abs(error) <= clip_delta\\n    squared_loss = 0.5 * keras.backend.square(error)\\n    quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n    return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n        self.state_size = state_size  # normalized previous days\\n        self.action_size = 3  # [sit, buy, sell] - adjust if needed\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # model config\\n        self.gamma = 0.95  # affinity for long term reward\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}  # important for loading the model from memory\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n        model_path = f\"models/{self.model_name}.keras\"\\n        if pretrained and os.path.exists(model_path) and self.model_name is not None:\\n            self.model = self.load()\\n        else:\\n            self.model = self._model()\\n\\n        # strategy config\\n        if self.strategy in [\"t-dqn\", \"double-dqn\"]:\\n            self.n_iter = 1\\n            self.reset_every = reset_every\\n            self.target_model = tf.keras.models.clone_model(self.model)\\n            self.target_model.set_weights(self.model.get_weights())\\n\\n    def _model(self):\\n        \"\"\"Creates the model\"\"\"\\n        model = keras.Sequential()\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=self.action_size))\\n\\n        model.compile(loss=self.loss, optimizer=self.optimizer)\\n        return model\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory\"\"\"\\n        self.memory.append((state, action, reward, done))\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions\"\"\"\\n        if not is_eval and random.random() <= self.epsilon:\\n            return random.randrange(self.action_size)\\n\\n        action_probs = self.model.predict(state, verbose=0)\\n        return np.argmax(action_probs[0])\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory\"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        mini_batch = random.sample(self.memory, 32)  # Sample a mini-batch\\n\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            while len(self.memory) > 0:\\n                state, action, reward, done = self.memory.pop()\\n                target = reward\\n                q_values = self.model.predict(state, verbose=0)\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n                loss += self.model.fit(\\n                    np.array(X_train), np.array(y_train),\\n                    epochs=1, verbose=0\\n                ).history[\"loss\"][0]\\n\\n        # DQN with fixed targets\\n        elif self.strategy == \"t-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                target = reward if done else reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n                q_values = self.model.predict(state)\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # Double DQN\\n        elif self.strategy == \"double-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                target = reward if done else reward + self.gamma * self.target_model.predict(next_state)[0][\\n                    np.argmax(self.model.predict(next_state)[0])]\\n                q_values = self.model.predict(state)\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        else:\\n            raise NotImplementedError()\\n\\n        # Update the model\\n        loss += self.model.fit(\\n            np.array(X_train), np.array(y_train),\\n            epochs=1, verbose=0\\n        ).history[\"loss\"][0]\\n\\n        # Epsilon decay\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def save(self):\\n        \"\"\"Save the model to disk\"\"\"\\n        path = pathlib.Path(\"models\")\\n        path.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\\n        self.model.save(path / f\"{self.model_name}.keras\", overwrite=True)\\n\\n    def load(self):\\n        \"\"\"Load the model from disk\"\"\"\\n        return tf.keras.models.load_model(f\"models/{self.model_name}.keras\", custom_objects=self.custom_objects)\\n\\n\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"\\n    Monitor system memory usage and print a warning if it exceeds the threshold.\\n\\n    Parameters:\\n    threshold (int): Memory usage threshold percentage.\\n\\n    Returns:\\n    bool: True if memory usage exceeds the threshold, False otherwise.\\n    \"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0, adjcp_field=\\'close\\'):\\n    \"\"\"\\n    Train the trading agent over a specified number of episodes.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    episode (int): The current episode number.\\n    data (DataFrame): The historical stock data.\\n    ep_count (int): Total number of episodes for training.\\n    batch_size (int): Size of the mini-batch for experience replay.\\n    window_size (int): The size of the window for state representation.\\n    prev_agent_profit (float): The highest profit observed so far.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (episode, ep_count, total_profit, avg_loss, prev_agent_profit)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n    trades = 0\\n    wins = 0\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False, desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n        reward = 0\\n        is_done = False\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state)  # Use agent\\'s action selection\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            reward = delta\\n            total_profit += delta\\n            trades += 1\\n            if delta > 0:\\n                wins += 1\\n        elif action == 2:  # SELL\\n            reward = -delta\\n            total_profit -= delta\\n            trades += 1\\n            if delta < 0:\\n                wins += 1\\n        else:  # HOLD\\n            reward = 0\\n\\n        done = (t == data_length - 1) or is_done\\n        agent.remember(state, action, reward, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay2()\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit >= prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved successfully after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Update prev_agent_profit to retain the highest profit observed\\n    prev_agent_profit = max(prev_agent_profit, total_profit)\\n\\n    avg_profit_per_trade = total_profit / trades if trades > 0 else 0\\n    win_loss_ratio = wins / trades if trades > 0 else 0\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss)), prev_agent_profit, avg_profit_per_trade, win_loss_ratio\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    \"\"\"\\n    Evaluate the trading agent on historical stock data.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window_size (int): The size of the window for state representation.\\n    debug (bool): Flag to enable/disable debug logging.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (total_profit, history)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state, is_eval=True)\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n\\n        if action == 1:  # BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 2:  # SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        else:  # HOLD\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        state = next_state\\n        if t == data_length - 1:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    \"\"\"\\n    Predict the next action using the trained trading agent.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window (int): The size of the window for state representation.\\n\\n    Returns:\\n    int: The predicted action (0: HOLD, 1: BUY, 2: SELL).\\n    \"\"\"\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action\\n', 'Document the following code parameters and features:\\n\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0, adjcp_field=\\'close\\'):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False, desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n        reward = 0\\n        is_done = False\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state)  # Use agent\\'s action selection\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            reward = delta\\n            total_profit += delta\\n        elif action == 2:  # SELL\\n            reward = -delta\\n            total_profit -= delta\\n        else:  # HOLD\\n            reward = 0\\n\\n        done = (t == data_length - 1) or is_done\\n        agent.remember(state, action, reward, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay2()\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit >= prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved successfully after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Update prev_agent_profit to retain the highest profit observed\\n    prev_agent_profit = max(prev_agent_profit, total_profit)\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss)), prev_agent_profit\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state, is_eval=True)\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n\\n        if action == 1:  # BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 2:  # SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        else:  # HOLD\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        state = next_state\\n        if t == data_length - 1:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action\\n', 'on the actual code', 'add docstrings to your functions to describe their purpose, parameters, and return values.\\n\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0, adjcp_field=\\'close\\'):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False, desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n        reward = 0\\n        is_done = False\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state)  # Use agent\\'s action selection\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            reward = delta\\n            total_profit += delta\\n        elif action == 2:  # SELL\\n            reward = -delta\\n            total_profit -= delta\\n        else:  # HOLD\\n            reward = 0\\n\\n        done = (t == data_length - 1) or is_done\\n        agent.remember(state, action, reward, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay2()\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit >= prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved successfully after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Update prev_agent_profit to retain the highest profit observed\\n    prev_agent_profit = max(prev_agent_profit, total_profit)\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss)), prev_agent_profit\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state, is_eval=True)\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n\\n        if action == 1:  # BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 2:  # SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        else:  # HOLD\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        state = next_state\\n        if t == data_length - 1:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action\\n\\n\\n\\n', 'add docstrings to your functions to describe their purpose, parameters, and return values.\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\n\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\\n\\n    Links: \\thttps://en.wikipedia.org/wiki/Huber_loss\\n            https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/\\n    \"\"\"\\n    error = y_true - y_pred\\n    cond = keras.backend.abs(error) <= clip_delta\\n    squared_loss = 0.5 * keras.backend.square(error)\\n    quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n    return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n\\n        # agent config\\n        self.state_size = state_size  # normalized previous days\\n        self.action_size = 3  # [sit, buy, sell, close buy, close sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # model config\\n        self.model_name = model_name\\n        self.gamma = 0.95  # affinity for long term reward\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}  # important for loading the model from memory\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n        model_path = f\"models/{self.model_name}.keras\"\\n        if pretrained and os.path.exists(model_path) and self.model_name is not None:\\n            self.model = self.load()\\n        else:\\n            self.model = self._model()\\n\\n        # strategy config\\n        if self.strategy in [\"t-dqn\", \"double-dqn\"]:\\n            self.n_iter = 1\\n            self.reset_every = reset_every\\n\\n            # target network\\n            self.target_model = tf.keras.models.clone_model(self.model)\\n            self.target_model.set_weights(self.model.get_weights())\\n\\n    def _model(self):\\n        \"\"\"Creates the model\\n        \"\"\"\\n        model = keras.Sequential()\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=self.action_size))\\n\\n        model.compile(loss=self.loss, optimizer=self.optimizer)\\n        return model\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory\\n        \"\"\"\\n        self.memory.append((state, action, reward, done))\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions\\n        \"\"\"\\n        # take random action in order to diversify experience at the beginning\\n        if not is_eval and random.random() <= self.epsilon:\\n            return random.randrange(self.action_size)\\n\\n        action_probs = self.model.predict(state, verbose=0)\\n        return np.argmax(action_probs[0])\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            while len(self.memory) > 0:\\n                state, action, reward, done = self.memory.pop()\\n                # Here, target is directly set to the immediate reward\\n                target = reward\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on the immediate reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n                # update q-function parameters based on huber loss gradient\\n                loss += self.model.fit(\\n                    np.array(X_train), np.array(y_train),\\n                    epochs=1, verbose=0\\n                ).history[\"loss\"][0]\\n\\n        # # DQN with fixed targets\\n        # elif self.strategy == \"t-dqn\":\\n        #     if self.n_iter % self.reset_every == 0:\\n        #         # reset target model weights\\n        #         self.target_model.set_weights(self.model.get_weights())\\n        #\\n        #     for state, action, reward, next_state, done in mini_batch:\\n        #         if done:\\n        #             target = reward\\n        #         else:\\n        #             # approximate deep q-learning equation with fixed targets\\n        #             target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n        #\\n        #         # estimate q-values based on current state\\n        #         q_values = self.model.predict(state)\\n        #         # update the target for current action based on discounted reward\\n        #         q_values[0][action] = target\\n        #\\n        #         X_train.append(state[0])\\n        #         y_train.append(q_values[0])\\n\\n        # # Double DQN\\n        # elif self.strategy == \"double-dqn\":\\n        #     if self.n_iter % self.reset_every == 0:\\n        #         # reset target model weights\\n        #         self.target_model.set_weights(self.model.get_weights())\\n        #\\n        #     for state, action, reward, next_state, done in mini_batch:\\n        #         if done:\\n        #             target = reward\\n        #         else:\\n        #             # approximate double deep q-learning equation\\n        #             target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n        #                 np.argmax(self.model.predict(next_state)[0])]\\n        #\\n        #         # estimate q-values based on current state\\n        #         q_values = self.model.predict(state)\\n        #         # update the target for current action based on discounted reward\\n        #         q_values[0][action] = target\\n        #\\n        #         X_train.append(state[0])\\n        #         y_train.append(q_values[0])\\n        #\\n        # else:\\n        #     raise NotImplementedError()\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def train_experience_replay(self, batch_size):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        mini_batch = random.sample(self.memory, batch_size)\\n        X_train, y_train = [], []\\n\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            for state, action, reward, done in mini_batch:\\n                # Here, target is directly set to the immediate reward\\n                target = reward\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on the immediate reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # DQN with fixed targets\\n        elif self.strategy == \"t-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation with fixed targets\\n                    target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # Double DQN\\n        elif self.strategy == \"double-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate double deep q-learning equation\\n                    target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                        np.argmax(self.model.predict(next_state)[0])]\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        else:\\n            raise NotImplementedError()\\n\\n        # update q-function parameters based on huber loss gradient\\n        loss = self.model.fit(\\n            np.array(X_train), np.array(y_train),\\n            epochs=1, verbose=0\\n        ).history[\"loss\"][0]\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def save(self):\\n        path = pathlib.Path()\\n        self.model.save(path / \"models/{}.keras\".format(self.model_name), overwrite=True)\\n\\n    def load(self):\\n        return tf.keras.models.load_model(\"models/{}.keras\".format(self.model_name), custom_objects=self.custom_objects)', 'Clean and check optimize the following code:\\n\\nimport logging\\nimport os\\n\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\n# Formats Position\\nformat_position = lambda price: (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n# Formats Currency\\nformat_currency = lambda price: \\'${0:.2f}\\'.format(abs(price))\\n\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\"Displays training results\"\"\"\\n    try:\\n        if val_position == initial_offset or val_position == 0.0:\\n            logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: USELESS  Train Loss: {:.4f}\\'\\n                         .format(strategy, result[0], result[1], format_position(result[2]), result[3]))\\n            return False\\n        else:\\n            logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: {}  Train Loss: {:.4f}\\'\\n                         .format(strategy, result[0], result[1], format_position(result[2]),\\n                                 format_position(val_position), result[3]))\\n            return True\\n    except Exception as e:\\n        logging.error(f\"Error in show_train_result: {e}\")\\n        return False\\n\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\"Displays eval results\"\"\"\\n    try:\\n        if profit == initial_offset or profit == 0.0:\\n            logging.info(\\'{}: USELESS\\\\n\\'.format(model_name))\\n        else:\\n            logging.info(\\'{}: {}\\\\n\\'.format(model_name, format_position(profit)))\\n    except Exception as e:\\n        logging.error(f\"Error in show_eval_result: {e}\")\\n\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\"Reads stock data from csv file\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        if date_field != \"Date\":\\n            df[\\'Date\\'] = df[date_field]\\n        df[\\'adjcp\\'] = df[adjcp_field]\\n        df = df.sort_values([date_field], ascending=True)\\n        df = add_indicators(df)\\n        return df\\n    except Exception as e:\\n        logging.error(f\"Error in load_data: {e}\")\\n        return pd.DataFrame()\\n\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\"Reads stock data from csv file\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        df = df.sort_values([date_field])\\n        return list(df[adjcp_field])\\n    except Exception as e:\\n        logging.error(f\"Error in get_stock_data: {e}\")\\n        return []\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\"Splits the dataset into training or testing using date\"\"\"\\n    try:\\n        data = df[(df.index >= start) & (df.index < end)]\\n        data.reset_index(drop=True, inplace=True)\\n        return data\\n    except Exception as e:\\n        logging.error(f\"Error in data_split: {e}\")\\n        return pd.DataFrame()\\n\\ndef switch_k_backend_device():\\n    \"\"\"Switches `keras` backend from GPU to CPU if required.\"\"\"\\n    try:\\n        if keras.backend == \"tensorflow\":\\n            logging.debug(\"Switching to TensorFlow for CPU\")\\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n    except Exception as e:\\n        logging.error(f\"Error in switch_k_backend_device: {e}\")\\n\\n\\ndef get_features():\\n    all_feature_name = [\\n        # \\'CDL2CROWS\\',\\n        # \\'CDL3BLACKCROWS\\',\\n        # \\'CDL3INSIDE\\',\\n        # \\'CDL3LINESTRIKE\\',\\n        # \\'CDL3OUTSIDE\\',\\n        # \\'CDL3STARSINSOUTH\\',\\n        # \\'CDL3WHITESOLDIERS\\',\\n        # \\'CDLABANDONEDBABY\\',\\n        # \\'CDLADVANCEBLOCK\\',\\n        # \\'CDLBELTHOLD\\',\\n        # \\'CDLBREAKAWAY\\',\\n        # \\'CDLCLOSINGMARUBOZU\\',\\n        # \\'CDLCONCEALBABYSWALL\\',\\n        # \\'CDLCOUNTERATTACK\\',\\n        \\'CDLDARKCLOUDCOVER\\',\\n        # \\'CDLDOJI\\',\\n        # \\'CDLDOJISTAR\\',\\n        \\'CDLDRAGONFLYDOJI\\',\\n        \\'CDLENGULFING\\',\\n        # \\'CDLEVENINGDOJISTAR\\',\\n        \\'CDLEVENINGSTAR\\',\\n        # \\'CDLGAPSIDESIDEWHITE\\',\\n        \\'CDLGRAVESTONEDOJI\\',\\n        \\'CDLHAMMER\\',\\n        \\'CDLHANGINGMAN\\',\\n        # \\'CDLHARAMI\\',\\n        # \\'CDLHARAMICROSS\\',\\n        # \\'CDLHIGHWAVE\\',\\n        # \\'CDLHIKKAKE\\',\\n        # \\'CDLHIKKAKEMOD\\',\\n        # \\'CDLHOMINGPIGEON\\',\\n        # \\'CDLIDENTICAL3CROWS\\',\\n        # \\'CDLINNECK\\',\\n        # \\'CDLINVERTEDHAMMER\\',\\n        # \\'CDLKICKING\\',\\n        # \\'CDLKICKINGBYLENGTH\\',\\n        # \\'CDLLADDERBOTTOM\\',\\n        # \\'CDLLONGLEGGEDDOJI\\',\\n        # \\'CDLLONGLINE\\',\\n        # \\'CDLMARUBOZU\\',\\n        # \\'CDLMATCHINGLOW\\',\\n        # \\'CDLMATHOLD\\',\\n        # \\'CDLMORNINGDOJISTAR\\',\\n        \\'CDLMORNINGSTAR\\',\\n        # \\'CDLONNECK\\',\\n        # \\'CDLPIERCING\\',\\n        # \\'CDLRICKSHAWMAN\\',\\n        # \\'CDLRISEFALL3METHODS\\',\\n        # \\'CDLSEPARATINGLINES\\',\\n        # \\'CDLSHOOTINGSTAR\\',\\n        # \\'CDLSHORTLINE\\',\\n        # \\'CDLSPINNINGTOP\\',\\n        # \\'CDLSTALLEDPATTERN\\',\\n        # \\'CDLSTICKSANDWICH\\',\\n        # \\'CDLTAKURI\\',\\n        # \\'CDLTASUKIGAP\\',\\n        # \\'CDLTHRUSTING\\',\\n        # \\'CDLTRISTAR\\',\\n        # \\'CDLUNIQUE3RIVER\\',\\n        # \\'CDLUPSIDEGAP2CROWS\\',\\n        # \\'CDLXSIDEGAP3METHODS\\',\\n        \\'MOMENTUM\\',\\n        # \\'DX\\',\\n        \\'FASTK\\',\\n        \\'FASTD\\',\\n        # \\'DX\\',\\n        # \\'CCI\\',\\n        \\'WILLR\\',\\n        \\'MFI\\',\\n        # \\'ROC\\',\\n        \\'ADX\\',\\n        \\'PPO\\',\\n        \\'RSI\\'\\n        # \\'STDDEV\\'\\n        # \\'SIN\\',\\n        # \\'COS\\',\\n        # \\'TAN\\'\\n    ]\\n    return all_feature_name\\n\\n\\ndef add_indicators(df):\\n    # Pattern Recognition Functions\\n    # df[\\'CDL2CROWS\\'] = talib.CDL2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3BLACKCROWS\\'] = talib.CDL3BLACKCROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3INSIDE\\'] = talib.CDL3INSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3LINESTRIKE\\'] = talib.CDL3LINESTRIKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3OUTSIDE\\'] = talib.CDL3OUTSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3STARSINSOUTH\\'] = talib.CDL3STARSINSOUTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3WHITESOLDIERS\\'] = talib.CDL3WHITESOLDIERS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLABANDONEDBABY\\'] = talib.CDLABANDONEDBABY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLADVANCEBLOCK\\'] = talib.CDLADVANCEBLOCK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLBELTHOLD\\'] = talib.CDLBELTHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLBREAKAWAY\\'] = talib.CDLBREAKAWAY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCLOSINGMARUBOZU\\'] = talib.CDLCLOSINGMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCONCEALBABYSWALL\\'] = talib.CDLCONCEALBABYSWALL(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCOUNTERATTACK\\'] = talib.CDLCOUNTERATTACK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDARKCLOUDCOVER\\'] = talib.CDLDARKCLOUDCOVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLDOJI\\'] = talib.CDLDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLDOJISTAR\\'] = talib.CDLDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDRAGONFLYDOJI\\'] = talib.CDLDRAGONFLYDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLENGULFING\\'] = talib.CDLENGULFING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLEVENINGDOJISTAR\\'] = talib.CDLEVENINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLEVENINGSTAR\\'] = talib.CDLEVENINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLGAPSIDESIDEWHITE\\'] = talib.CDLGAPSIDESIDEWHITE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLGRAVESTONEDOJI\\'] = talib.CDLGRAVESTONEDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHAMMER\\'] = talib.CDLHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHANGINGMAN\\'] = talib.CDLHANGINGMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHARAMI\\'] = talib.CDLHARAMI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHARAMICROSS\\'] = talib.CDLHARAMICROSS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIGHWAVE\\'] = talib.CDLHIGHWAVE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIKKAKE\\'] = talib.CDLHIKKAKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIKKAKEMOD\\'] = talib.CDLHIKKAKEMOD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHOMINGPIGEON\\'] = talib.CDLHOMINGPIGEON(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLIDENTICAL3CROWS\\'] = talib.CDLIDENTICAL3CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLINNECK\\'] = talib.CDLINNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLINVERTEDHAMMER\\'] = talib.CDLINVERTEDHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLKICKING\\'] = talib.CDLKICKING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLKICKINGBYLENGTH\\'] = talib.CDLKICKINGBYLENGTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLADDERBOTTOM\\'] = talib.CDLLADDERBOTTOM(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLONGLEGGEDDOJI\\'] = talib.CDLLONGLEGGEDDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLONGLINE\\'] = talib.CDLLONGLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMARUBOZU\\'] = talib.CDLMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMATCHINGLOW\\'] = talib.CDLMATCHINGLOW(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMATHOLD\\'] = talib.CDLMATHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLMORNINGDOJISTAR\\'] = talib.CDLMORNINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLMORNINGSTAR\\'] = talib.CDLMORNINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLONNECK\\'] = talib.CDLONNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLPIERCING\\'] = talib.CDLPIERCING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLRICKSHAWMAN\\'] = talib.CDLRICKSHAWMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLRISEFALL3METHODS\\'] = talib.CDLRISEFALL3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSEPARATINGLINES\\'] = talib.CDLSEPARATINGLINES(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSHOOTINGSTAR\\'] = talib.CDLSHOOTINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSHORTLINE\\'] = talib.CDLSHORTLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSPINNINGTOP\\'] = talib.CDLSPINNINGTOP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSTALLEDPATTERN\\'] = talib.CDLSTALLEDPATTERN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSTICKSANDWICH\\'] = talib.CDLSTICKSANDWICH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTAKURI\\'] = talib.CDLTAKURI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTASUKIGAP\\'] = talib.CDLTASUKIGAP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTHRUSTING\\'] = talib.CDLTHRUSTING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTRISTAR\\'] = talib.CDLTRISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLUNIQUE3RIVER\\'] = talib.CDLUNIQUE3RIVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLUPSIDEGAP2CROWS\\'] = talib.CDLUPSIDEGAP2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLXSIDEGAP3METHODS\\'] = talib.CDLXSIDEGAP3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # Price action\\n    # https://ta-lib.github.io/ta-lib-python/func_groups/momentum_indicators.html\\n    # rsi,stochRsi,cci,williamsr,mfi,roc,atr,adx\\n    df[\\'MOMENTUM\\'] = talib.MOM(df[\\'Close\\'], timeperiod=10)\\n    # df[\\'DX\\'] = talib.stream_DX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'FASTK\\'], df[\\'FASTD\\'] = talib.STOCHRSI(df[\\'Close\\'], timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)\\n    # df[\\'CCI\\'] = talib.CCI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'WILLR\\'] = talib.WILLR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'MFI\\'] = talib.MFI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'], timeperiod=14)\\n    # df[\\'ROC\\'] = talib.ROC(df[\\'Close\\'], timeperiod=10)\\n    # df[\\'ATR\\'] = talib.ATR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'ADX\\'] = talib.ADX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'PPO\\'] = talib.PPO(df[\\'Close\\'], fastperiod=12, slowperiod=26, matype=0)\\n    df[\\'RSI\\'] = talib.RSI(df[\\'Close\\'], timeperiod=14)\\n    # df[\\'STDDEV\\'] = talib.STDDEV(df[\\'Close\\'], timeperiod=14, nbdev=1)\\n    # df[\\'SIN\\'] = talib.SIN(df[\\'Close\\'])\\n    # df[\\'COS\\'] = talib.COS(df[\\'Close\\'])\\n    # df[\\'TAN\\'] = talib.TAN(df[\\'Close\\'])\\n\\n    df = df.fillna(0)\\n\\n    return df\\n', 'Optimize make more robust the code below:\\n\\nimport logging\\nimport os\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\n# Formats Position\\ndef format_position(price):\\n    \"\"\"Formats the position as a string with a dollar sign.\"\"\"\\n    return (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n# Formats Currency\\ndef format_currency(price):\\n    \"\"\"Formats the currency as a string with a dollar sign.\"\"\"\\n    return \\'${0:.2f}\\'.format(abs(price))\\n\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\"Displays training results.\"\"\"\\n    try:\\n        if val_position in {initial_offset, 0.0}:\\n            logging.info(f\\'{strategy}: Episode {result[0]}/{result[1]} - Train Position: {format_position(result[2])}  Val Position: USELESS  Train Loss: {result[3]:.4f}\\')\\n            return False\\n        else:\\n            logging.info(f\\'{strategy}: Episode {result[0]}/{result[1]} - Train Position: {format_position(result[2])}  Val Position: {format_position(val_position)}  Train Loss: {result[3]:.4f}\\')\\n            return True\\n    except Exception as e:\\n        logging.error(f\"Error in show_train_result: {e}\")\\n        return False\\n\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\"Displays evaluation results.\"\"\"\\n    try:\\n        if profit in {initial_offset, 0.0}:\\n            logging.info(f\\'{model_name}: USELESS\\\\n\\')\\n        else:\\n            logging.info(f\\'{model_name}: {format_position(profit)}\\\\n\\')\\n    except Exception as e:\\n        logging.error(f\"Error in show_eval_result: {e}\")\\n\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\"Reads stock data from a CSV file.\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        if date_field != \"Date\":\\n            df[\\'Date\\'] = df[date_field]\\n        df[\\'adjcp\\'] = df[adjcp_field]\\n        df.sort_values([date_field], ascending=True, inplace=True)\\n        return add_indicators(df)\\n    except Exception as e:\\n        logging.error(f\"Error in load_data: {e}\")\\n        return pd.DataFrame()\\n\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\"Reads stock data from a CSV file and returns adjusted close prices.\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        df.sort_values([date_field], inplace=True)\\n        return list(df[adjcp_field])\\n    except Exception as e:\\n        logging.error(f\"Error in get_stock_data: {e}\")\\n        return []\\n\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\"Splits the dataset into training or testing using date.\"\"\"\\n    try:\\n        data = df[(df.index >= start) & (df.index < end)]\\n        data.reset_index(drop=True, inplace=True)\\n        return data\\n    except Exception as e:\\n        logging.error(f\"Error in data_split: {e}\")\\n        return pd.DataFrame()\\n\\n\\ndef switch_k_backend_device():\\n    \"\"\"Switches `keras` backend from GPU to CPU if required.\"\"\"\\n    try:\\n        if keras.backend.backend() == \"tensorflow\":\\n            logging.debug(\"Switching to TensorFlow for CPU\")\\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n    except Exception as e:\\n        logging.error(f\"Error in switch_k_backend_device: {e}\")\\n\\n\\ndef get_features():\\n    \"\"\"Returns a list of feature names for technical indicators.\"\"\"\\n    return [\\n        \\'CDLDARKCLOUDCOVER\\',\\n        \\'CDLDRAGONFLYDOJI\\',\\n        \\'CDLENGULFING\\',\\n        \\'CDLEVENINGSTAR\\',\\n        \\'CDLGRAVESTONEDOJI\\',\\n        \\'CDLHAMMER\\',\\n        \\'CDLHANGINGMAN\\',\\n        \\'CDLMORNINGSTAR\\',\\n        \\'MOMENTUM\\',\\n        \\'FASTK\\',\\n        \\'FASTD\\',\\n        \\'WILLR\\',\\n        \\'MFI\\',\\n        \\'ADX\\',\\n        \\'PPO\\',\\n        \\'RSI\\'\\n    ]\\n\\n\\ndef add_indicators(df):\\n    \"\"\"Adds technical indicators to the DataFrame.\"\"\"\\n    df[\\'CDLDARKCLOUDCOVER\\'] = talib.CDLDARKCLOUDCOVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLDRAGONFLYDOJI\\'] = talib.CDLDRAGONFLYDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLENGULFING\\'] = talib.CDLENGULFING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLEVENINGSTAR\\'] = talib.CDLEVENINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLGRAVESTONEDOJI\\'] = talib.CDLGRAVESTONEDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHAMMER\\'] = talib.CDLHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHANGINGMAN\\'] = talib.CDLHANGINGMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLMORNINGSTAR\\'] = talib.CDLMORNINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'MOMENTUM\\'] = talib.MOM(df[\\'Close\\'], timeperiod=10)\\n    df[\\'FASTK\\'], df[\\'FASTD\\'] = talib.STOCHRSI(df[\\'Close\\'], timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)\\n    df[\\'WILLR\\'] = talib.WILLR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'MFI\\'] = talib.MFI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'], timeperiod=14)\\n    df[\\'ADX\\'] = talib.ADX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'PPO\\'] = talib.PPO(df[\\'Close\\'], fastperiod=12, slowperiod=26, matype=0)\\n    df[\\'RSI\\'] = talib.RSI(df[\\'Close\\'], timeperiod=14)\\n\\n    return df.fillna(0)', 'Traceback (most recent call last):\\n  File \"/Users/javiersanchez/PycharmProjects/trading-bot/train.py\", line 110, in <module>\\n    main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n  File \"/Users/javiersanchez/PycharmProjects/trading-bot/train.py\", line 51, in main\\n    train_data = data_split(data, 0, take, adjcp_field)\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nTypeError: data_split() takes 3 positional arguments but 4 were given', 'on the for name, func in indicators.items(): the following indicators are missing: FASTK, FASTD ADX, PPO, RSI\\n\\nimport logging\\nimport os\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\ndef format_position(price):\\n    \"\"\"Formats the position as a string with a dollar sign.\"\"\"\\n    return f\"{\\'-$\\' if price < 0 else \\'+$\\'}{abs(price):.2f}\"\\n\\ndef format_currency(price):\\n    \"\"\"Formats the currency as a string with a dollar sign.\"\"\"\\n    return f\"${abs(price):.2f}\"\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\"Displays training results.\"\"\"\\n    try:\\n        val_position_str = \"USELESS\" if val_position in {initial_offset, 0.0} else format_position(val_position)\\n        logging.info(f\\'{strategy}: Episode {result[0]}/{result[1]} - Train Position: {format_position(result[2])}  Val Position: {val_position_str}  Train Loss: {result[3]:.4f}\\')\\n        return val_position_str != \"USELESS\"\\n    except Exception as e:\\n        logging.error(f\"Error in show_train_result: {e}\")\\n        return False\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\"Displays evaluation results.\"\"\"\\n    try:\\n        profit_str = \"USELESS\" if profit in {initial_offset, 0.0} else format_position(profit)\\n        logging.info(f\\'{model_name}: {profit_str}\\\\n\\')\\n    except Exception as e:\\n        logging.error(f\"Error in show_eval_result: {e}\")\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\"Reads stock data from a CSV file.\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        if date_field != \"Date\":\\n            df[\\'Date\\'] = df[date_field]\\n        df[\\'adjcp\\'] = df[adjcp_field]\\n        df.sort_values(by=date_field, ascending=True, inplace=True)\\n        return add_indicators(df)\\n    except FileNotFoundError:\\n        logging.error(f\"File not found: {stock_file}\")\\n        return pd.DataFrame()\\n    except Exception as e:\\n        logging.error(f\"Error in load_data: {e}\")\\n        return pd.DataFrame()\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\"Reads stock data from a CSV file and returns adjusted close prices.\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        df.sort_values(by=date_field, inplace=True)\\n        return df[adjcp_field].tolist()\\n    except FileNotFoundError:\\n        logging.error(f\"File not found: {stock_file}\")\\n        return []\\n    except Exception as e:\\n        logging.error(f\"Error in get_stock_data: {e}\")\\n        return []\\n\\ndef data_split(df, start, end):\\n    \"\"\"Splits the dataset into training or testing using date.\"\"\"\\n    try:\\n        data = df[(df.index >= start) & (df.index < end)].reset_index(drop=True)\\n        return data\\n    except Exception as e:\\n        logging.error(f\"Error in data_split: {e}\")\\n        return pd.DataFrame()\\n\\ndef switch_k_backend_device():\\n    \"\"\"Switches `keras` backend from GPU to CPU if required.\"\"\"\\n    try:\\n        if keras.backend.backend() == \"tensorflow\":\\n            logging.debug(\"Switching to TensorFlow for CPU\")\\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n    except Exception as e:\\n        logging.error(f\"Error in switch_k_backend_device: {e}\")\\n\\ndef get_features():\\n    \"\"\"Returns a list of feature names for technical indicators.\"\"\"\\n    return [\\n        \\'CDLDARKCLOUDCOVER\\',\\n        \\'CDLDRAGONFLYDOJI\\',\\n        \\'CDLENGULFING\\',\\n        \\'CDLEVENINGSTAR\\',\\n        \\'CDLGRAVESTONEDOJI\\',\\n        \\'CDLHAMMER\\',\\n        \\'CDLHANGINGMAN\\',\\n        \\'CDLMORNINGSTAR\\',\\n        \\'MOMENTUM\\',\\n        \\'FASTK\\',\\n        \\'FASTD\\',\\n        \\'WILLR\\',\\n        \\'MFI\\',\\n        \\'ADX\\',\\n        \\'PPO\\',\\n        \\'RSI\\'\\n    ]\\n\\ndef add_indicators(df):\\n    \"\"\"Adds technical indicators to the DataFrame.\"\"\"\\n    indicators = {\\n        \\'CDLDARKCLOUDCOVER\\': talib.CDLDARKCLOUDCOVER,\\n        \\'CDLDRAGONFLYDOJI\\': talib.CDLDRAGONFLYDOJI,\\n        \\'CDLENGULFING\\': talib.CDLENGULFING,\\n        \\'CDLEVENINGSTAR\\': talib.CDLEVENINGSTAR,\\n        \\'CDLGRAVESTONEDOJI\\': talib.CDLGRAVESTONEDOJI,\\n        \\'CDLHAMMER\\': talib.CDLHAMMER,\\n        \\'CDLHANGINGMAN\\': talib.CDLHANGINGMAN,\\n        \\'CDLMORNINGSTAR\\': talib.CDLMORNINGSTAR,\\n        \\'MOMENTUM\\': lambda close: talib.MOM(close, timeperiod=10),\\n        \\'FASTK\\': lambda close: talib.STOCHRSI(close, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)[0],\\n        \\'FASTD\\': lambda close: talib.STOCHRSI(close, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)[1],\\n        \\'WILLR\\': talib.WILLR,\\n        \\'MFI\\': lambda high, low, close, volume: talib.MFI(high, low, close, volume, timeperiod=14),\\n        \\'ADX\\': talib.ADX,\\n        \\'PPO\\': lambda close: talib.PPO(close, fastperiod=12, slowperiod=26, matype=0),\\n        \\'RSI\\': lambda close: talib.RSI(close, timeperiod=14)\\n    }\\n\\n    for name, func in indicators.items():\\n        if name == \\'MFI\\':\\n            df[name] = func(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'])\\n        else:\\n            df[name] = func(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n\\n    return df.fillna(0)', 'Fix errors\\n\\nimport logging\\nimport os\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\n# Formats Position\\ndef format_position(price):\\n    \"\"\"Formats the position as a string with a dollar sign.\"\"\"\\n    return (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n# Formats Currency\\ndef format_currency(price):\\n    \"\"\"Formats the currency as a string with a dollar sign.\"\"\"\\n    return \\'${0:.2f}\\'.format(abs(price))\\n\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\"Displays training results.\"\"\"\\n    try:\\n        if val_position in {initial_offset, 0.0}:\\n            logging.info(f\\'{strategy}: Episode {result[0]}/{result[1]} - Train Position: {format_position(result[2])}  Val Position: USELESS  Train Loss: {result[3]:.4f}\\')\\n            return False\\n        else:\\n            logging.info(f\\'{strategy}: Episode {result[0]}/{result[1]} - Train Position: {format_position(result[2])}  Val Position: {format_position(val_position)}  Train Loss: {result[3]:.4f}\\')\\n            return True\\n    except Exception as e:\\n        logging.error(f\"Error in show_train_result: {e}\")\\n        return False\\n\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\"Displays evaluation results.\"\"\"\\n    try:\\n        if profit in {initial_offset, 0.0}:\\n            logging.info(f\\'{model_name}: USELESS\\\\n\\')\\n        else:\\n            logging.info(f\\'{model_name}: {format_position(profit)}\\\\n\\')\\n    except Exception as e:\\n        logging.error(f\"Error in show_eval_result: {e}\")\\n\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\"Reads stock data from a CSV file.\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        if date_field != \"Date\":\\n            df[\\'Date\\'] = df[date_field]\\n        df[\\'adjcp\\'] = df[adjcp_field]\\n        df.sort_values([date_field], ascending=True, inplace=True)\\n        return add_indicators(df)\\n    except Exception as e:\\n        logging.error(f\"Error in load_data: {e}\")\\n        return pd.DataFrame()\\n\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\"Reads stock data from a CSV file and returns adjusted close prices.\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        df.sort_values([date_field], inplace=True)\\n        return list(df[adjcp_field])\\n    except Exception as e:\\n        logging.error(f\"Error in get_stock_data: {e}\")\\n        return []\\n\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\"Splits the dataset into training or testing using date.\"\"\"\\n    try:\\n        data = df[(df.index >= start) & (df.index < end)]\\n        data.reset_index(drop=True, inplace=True)\\n        return data\\n    except Exception as e:\\n        logging.error(f\"Error in data_split: {e}\")\\n        return pd.DataFrame()\\n\\n\\ndef switch_k_backend_device():\\n    \"\"\"Switches `keras` backend from GPU to CPU if required.\"\"\"\\n    try:\\n        if keras.backend.backend() == \"tensorflow\":\\n            logging.debug(\"Switching to TensorFlow for CPU\")\\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n    except Exception as e:\\n        logging.error(f\"Error in switch_k_backend_device: {e}\")\\n\\n\\ndef get_features():\\n    \"\"\"Returns a list of feature names for technical indicators.\"\"\"\\n    return [\\n        \\'CDLDARKCLOUDCOVER\\',\\n        \\'CDLDRAGONFLYDOJI\\',\\n        \\'CDLENGULFING\\',\\n        \\'CDLEVENINGSTAR\\',\\n        \\'CDLGRAVESTONEDOJI\\',\\n        \\'CDLHAMMER\\',\\n        \\'CDLHANGINGMAN\\',\\n        \\'CDLMORNINGSTAR\\',\\n        \\'MOMENTUM\\',\\n        \\'FASTK\\',\\n        \\'FASTD\\',\\n        \\'WILLR\\',\\n        \\'MFI\\',\\n        \\'ADX\\',\\n        \\'PPO\\',\\n        \\'RSI\\'\\n    ]\\n\\n\\ndef add_indicators(df):\\n    \"\"\"Adds technical indicators to the DataFrame.\"\"\"\\n    indicators = {\\n        \\'CDLDARKCLOUDCOVER\\'] = talib.CDLDARKCLOUDCOVER,\\n        \\'CDLDRAGONFLYDOJI\\'] = talib.CDLDRAGONFLYDOJI,\\n        \\'CDLENGULFING\\'] = talib.CDLENGULFING,\\n        \\'CDLEVENINGSTAR\\'] = talib.CDLEVENINGSTAR,\\n        \\'CDLGRAVESTONEDOJI\\'] = talib.CDLGRAVESTONEDOJI,\\n        \\'CDLHAMMER\\'] = talib.CDLHAMMER,\\n        \\'CDLHANGINGMAN\\'] = talib.CDLHANGINGMAN,\\n        \\'CDLMORNINGSTAR\\'] = talib.CDLMORNINGSTAR,\\n        \\'RSI\\': lambda close: talib.RSI(close, timeperiod=14),\\n        \\'MOMENTUM\\': lambda close: talib.MOM(close, timeperiod=10),\\n        \\'CCI\\': lambda high, low, close: talib.CCI(high, low, close, timeperiod=14),\\n        \\'MFI\\': lambda high, low, close, volume: talib.MFI(high, low, close, volume, timeperiod=14),\\n        \\'ROC\\': lambda close: talib.ROC(close, timeperiod=10),\\n        \\'ADX\\': lambda high, low, close: talib.ADX(high, low, close, timeperiod=14),\\n        \\'PPO\\': lambda close: talib.PPO(close, fastperiod=5, slowperiod=15, matype=8)\\n\\n    }\\n    \\n    for indicator, func in indicators.items():\\n        if\\n            indicator in [\\'CDLABANDONEDBABY\\', \\'CDLDARKCLOUDCOVER\\', \\'CDLEVENINGDOJISTAR\\', \\'CDLEVENINGSTAR\\', \\'CDLMATHOLD\\',\\n                        \\'CDLMORNINGDOJISTAR\\', \\'CDLMORNINGSTAR\\']:\\n            df[indicator] = func(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n        elif indicator in [\\'RSI\\', \\'MOMENTUM\\', \\'ROC\\', \\'PPO\\']:\\n            df[indicator] = func(df[\\'Close\\'])\\n        elif indicator == \\'MFI\\':\\n            df[indicator] = func(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'])\\n        elif indicator in [\\'CCI\\', \\'ADX\\']:\\n            df[indicator] = func(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n        else:\\n            df[indicator] = func(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    \\n    # Fill NaN values with 0\\n    df.fillna(0, inplace=True)\\n    return df\\n', 'ERROR Error in load_data: CDLDRAGONFLYDOJI() got an unexpected keyword argument \\'penetration\\'\\nError in get_state: \\'CDLDARKCLOUDCOVER\\'\\n\\nimport logging\\nimport os\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\n# Formats Position\\ndef format_position(price):\\n    \"\"\"Formats the position as a string with a dollar sign.\"\"\"\\n    return (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n# Formats Currency\\ndef format_currency(price):\\n    \"\"\"Formats the currency as a string with a dollar sign.\"\"\"\\n    return \\'${0:.2f}\\'.format(abs(price))\\n\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\"Displays training results.\"\"\"\\n    try:\\n        if val_position in {initial_offset, 0.0}:\\n            logging.info(f\\'{strategy}: Episode {result[0]}/{result[1]} - Train Position: {format_position(result[2])}  Val Position: USELESS  Train Loss: {result[3]:.4f}\\')\\n            return False\\n        else:\\n            logging.info(f\\'{strategy}: Episode {result[0]}/{result[1]} - Train Position: {format_position(result[2])}  Val Position: {format_position(val_position)}  Train Loss: {result[3]:.4f}\\')\\n            return True\\n    except Exception as e:\\n        logging.error(f\"Error in show_train_result: {e}\")\\n        return False\\n\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\"Displays evaluation results.\"\"\"\\n    try:\\n        if profit in {initial_offset, 0.0}:\\n            logging.info(f\\'{model_name}: USELESS\\\\n\\')\\n        else:\\n            logging.info(f\\'{model_name}: {format_position(profit)}\\\\n\\')\\n    except Exception as e:\\n        logging.error(f\"Error in show_eval_result: {e}\")\\n\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\"Reads stock data from a CSV file.\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        if date_field != \"Date\":\\n            df[\\'Date\\'] = df[date_field]\\n        df[\\'adjcp\\'] = df[adjcp_field]\\n        df.sort_values([date_field], ascending=True, inplace=True)\\n        return add_indicators(df)\\n    except Exception as e:\\n        logging.error(f\"Error in load_data: {e}\")\\n        return pd.DataFrame()\\n\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\"Reads stock data from a CSV file and returns adjusted close prices.\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        df.sort_values([date_field], inplace=True)\\n        return list(df[adjcp_field])\\n    except Exception as e:\\n        logging.error(f\"Error in get_stock_data: {e}\")\\n        return []\\n\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\"Splits the dataset into training or testing using date.\"\"\"\\n    try:\\n        data = df[(df.index >= start) & (df.index < end)]\\n        data.reset_index(drop=True, inplace=True)\\n        return data\\n    except Exception as e:\\n        logging.error(f\"Error in data_split: {e}\")\\n        return pd.DataFrame()\\n\\n\\ndef switch_k_backend_device():\\n    \"\"\"Switches `keras` backend from GPU to CPU if required.\"\"\"\\n    try:\\n        if keras.backend.backend() == \"tensorflow\":\\n            logging.debug(\"Switching to TensorFlow for CPU\")\\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n    except Exception as e:\\n        logging.error(f\"Error in switch_k_backend_device: {e}\")\\n\\n\\ndef get_features():\\n    \"\"\"Returns a list of feature names for technical indicators.\"\"\"\\n    return [\\n        \\'CDLDARKCLOUDCOVER\\',\\n        \\'CDLDRAGONFLYDOJI\\',\\n        \\'CDLENGULFING\\',\\n        \\'CDLEVENINGSTAR\\',\\n        \\'CDLGRAVESTONEDOJI\\',\\n        \\'CDLHAMMER\\',\\n        \\'CDLHANGINGMAN\\',\\n        \\'CDLMORNINGSTAR\\',\\n        \\'MOMENTUM\\',\\n        \\'FASTK\\',\\n        \\'FASTD\\',\\n        \\'CCI\\',\\n        \\'WILLR\\',\\n        \\'MFI\\',\\n        \\'ROC\\',\\n        \\'ADX\\',\\n        \\'PPO\\',\\n        \\'RSI\\'\\n    ]\\n\\n\\ndef add_indicators(df):\\n    \"\"\"Adds technical indicators to the DataFrame.\"\"\"\\n    indicators = {\\n        \\'CDLDARKCLOUDCOVER\\': talib.CDLDARKCLOUDCOVER,\\n        \\'CDLDRAGONFLYDOJI\\': talib.CDLDRAGONFLYDOJI,\\n        \\'CDLENGULFING\\': talib.CDLENGULFING,\\n        \\'CDLEVENINGSTAR\\': talib.CDLEVENINGSTAR,\\n        \\'CDLGRAVESTONEDOJI\\': talib.CDLGRAVESTONEDOJI,\\n        \\'CDLHAMMER\\': talib.CDLHAMMER,\\n        \\'CDLHANGINGMAN\\': talib.CDLHANGINGMAN,\\n        \\'CDLMORNINGSTAR\\': talib.CDLMORNINGSTAR,\\n        \\'RSI\\': lambda close: talib.RSI(close, timeperiod=14),\\n        \\'MOMENTUM\\': lambda close: talib.MOM(close, timeperiod=10),\\n        \\'FASTK\\': lambda close: talib.STOCHRSI(close, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0),\\n        \\'FASTD\\': lambda close: talib.STOCHRSI(close, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0),\\n        \\'CCI\\': lambda high, low, close: talib.CCI(high, low, close, timeperiod=14),\\n        \\'WILLR\\': lambda high, low, close: talib.WILLR(high, low, close, timeperiod=14),\\n        \\'MFI\\': lambda high, low, close, volume: talib.MFI(high, low, close, volume, timeperiod=14),\\n        \\'ROC\\': lambda close: talib.ROC(close, timeperiod=10),\\n        \\'ADX\\': lambda high, low, close: talib.ADX(high, low, close, timeperiod=14),\\n        \\'PPO\\': lambda close: talib.PPO(close, fastperiod=5, slowperiod=15, matype=8)\\n    }\\n\\n    for indicator, func in indicators.items():\\n        if indicator in [\\'CDLDARKCLOUDCOVER\\', \\'CDLDRAGONFLYDOJI\\']:\\n            df[indicator] = func(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n        elif indicator in [\\'CDLENGULFING\\', \\'CDLEVENINGSTAR\\', \\'CDLGRAVESTONEDOJI\\',\\n                         \\'CDLHAMMER\\', \\'CDLHANGINGMAN\\', \\'CDLMORNINGSTAR\\']:\\n            df[indicator] = func(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n        elif indicator in [\\'RSI\\', \\'MOMENTUM\\', \\'ROC\\', \\'PPO\\',\\'FASTK\\', \\'FASTD\\']:\\n            df[indicator] = func(df[\\'Close\\'])\\n        elif indicator == \\'MFI\\':\\n            df[indicator] = func(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'])\\n        elif indicator in [\\'CCI\\', \\'ADX\\', \\'WILLR\\']:\\n            df[indicator] = func(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n        else:\\n            df[indicator] = func(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n\\n    # Fill NaN values with 0\\n    df.fillna(0, inplace=True)\\n    return df', 'Fix error\\n\\nimport logging\\nimport os\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\n# Formats Position\\ndef format_position(price):\\n    \"\"\"Formats the position as a string with a dollar sign.\"\"\"\\n    return (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n# Formats Currency\\ndef format_currency(price):\\n    \"\"\"Formats the currency as a string with a dollar sign.\"\"\"\\n    return \\'${0:.2f}\\'.format(abs(price))\\n\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\"Displays training results.\"\"\"\\n    try:\\n        if val_position in {initial_offset, 0.0}:\\n            logging.info(f\\'{strategy}: Episode {result[0]}/{result[1]} - Train Position: {format_position(result[2])}  Val Position: USELESS  Train Loss: {result[3]:.4f}\\')\\n            return False\\n        else:\\n            logging.info(f\\'{strategy}: Episode {result[0]}/{result[1]} - Train Position: {format_position(result[2])}  Val Position: {format_position(val_position)}  Train Loss: {result[3]:.4f}\\')\\n            return True\\n    except Exception as e:\\n        logging.error(f\"Error in show_train_result: {e}\")\\n        return False\\n\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\"Displays evaluation results.\"\"\"\\n    try:\\n        if profit in {initial_offset, 0.0}:\\n            logging.info(f\\'{model_name}: USELESS\\\\n\\')\\n        else:\\n            logging.info(f\\'{model_name}: {format_position(profit)}\\\\n\\')\\n    except Exception as e:\\n        logging.error(f\"Error in show_eval_result: {e}\")\\n\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\"Reads stock data from a CSV file.\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        if date_field != \"Date\":\\n            df[\\'Date\\'] = df[date_field]\\n        df[\\'adjcp\\'] = df[adjcp_field]\\n        df.sort_values([date_field], ascending=True, inplace=True)\\n        return add_indicators(df)\\n    except Exception as e:\\n        logging.error(f\"Error in load_data: {e}\")\\n        return pd.DataFrame()\\n\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\"Reads stock data from a CSV file and returns adjusted close prices.\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        df.sort_values([date_field], inplace=True)\\n        return list(df[adjcp_field])\\n    except Exception as e:\\n        logging.error(f\"Error in get_stock_data: {e}\")\\n        return []\\n\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\"Splits the dataset into training or testing using date.\"\"\"\\n    try:\\n        data = df[(df.index >= start) & (df.index < end)]\\n        data.reset_index(drop=True, inplace=True)\\n        return data\\n    except Exception as e:\\n        logging.error(f\"Error in data_split: {e}\")\\n        return pd.DataFrame()\\n\\n\\ndef switch_k_backend_device():\\n    \"\"\"Switches `keras` backend from GPU to CPU if required.\"\"\"\\n    try:\\n        if keras.backend.backend() == \"tensorflow\":\\n            logging.debug(\"Switching to TensorFlow for CPU\")\\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n    except Exception as e:\\n        logging.error(f\"Error in switch_k_backend_device: {e}\")\\n\\n\\ndef get_features():\\n    \"\"\"Returns a list of feature names for technical indicators.\"\"\"\\n    return [\\n        \\'CDLDARKCLOUDCOVER\\',\\n        \\'CDLDRAGONFLYDOJI\\',\\n        \\'CDLENGULFING\\',\\n        \\'CDLEVENINGSTAR\\',\\n        \\'CDLGRAVESTONEDOJI\\',\\n        \\'CDLHAMMER\\',\\n        \\'CDLHANGINGMAN\\',\\n        \\'CDLMORNINGSTAR\\',\\n        \\'MOMENTUM\\',\\n        \\'FASTK\\',\\n        \\'FASTD\\',\\n        \\'CCI\\',\\n        \\'WILLR\\',\\n        \\'MFI\\',\\n        \\'ROC\\',\\n        \\'ADX\\',\\n        \\'PPO\\',\\n        \\'RSI\\'\\n    ]\\n\\n\\ndef add_indicators(df):\\n    \"\"\"Adds technical indicators to the DataFrame.\"\"\"\\n    indicators = {\\n        \\'CDLDARKCLOUDCOVER\\': talib.CDLDARKCLOUDCOVER,\\n        \\'CDLDRAGONFLYDOJI\\': talib.CDLDRAGONFLYDOJI,\\n        \\'CDLENGULFING\\': talib.CDLENGULFING,\\n        \\'CDLEVENINGSTAR\\': talib.CDLEVENINGSTAR,\\n        \\'CDLGRAVESTONEDOJI\\': talib.CDLGRAVESTONEDOJI,\\n        \\'CDLHAMMER\\': talib.CDLHAMMER,\\n        \\'CDLHANGINGMAN\\': talib.CDLHANGINGMAN,\\n        \\'CDLMORNINGSTAR\\': talib.CDLMORNINGSTAR,\\n        \\'RSI\\': lambda close: talib.RSI(close, timeperiod=14),\\n        \\'MOMENTUM\\': lambda close: talib.MOM(close, timeperiod=10),\\n        \\'FASTD\\',\\'FASTK\\': lambda close: talib.STOCHRSI(close, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)[0],\\n        # \\'FASTD\\': lambda close: talib.STOCHRSI(close, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)[1],\\n        \\'CCI\\': lambda high, low, close: talib.CCI(high, low, close, timeperiod=14),\\n        \\'WILLR\\': lambda high, low, close: talib.WILLR(high, low, close, timeperiod=14),\\n        \\'MFI\\': lambda high, low, close, volume: talib.MFI(high, low, close, volume, timeperiod=14),\\n        \\'ROC\\': lambda close: talib.ROC(close, timeperiod=10),\\n        \\'ADX\\': lambda high, low, close: talib.ADX(high, low, close, timeperiod=14),\\n        \\'PPO\\': lambda close: talib.PPO(close, fastperiod=5, slowperiod=15, matype=8)\\n    }\\n\\n    for indicator, func in indicators.items():\\n        if indicator in [\\'CDLDARKCLOUDCOVER\\', \\'CDLEVENINGSTAR\\']:\\n            df[indicator] = func(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n        elif indicator in [\\'CDLDRAGONFLYDOJI\\', \\'CDLENGULFING\\', \\'CDLGRAVESTONEDOJI\\',\\n                         \\'CDLHAMMER\\', \\'CDLHANGINGMAN\\', \\'CDLMORNINGSTAR\\']:\\n            df[indicator] = func(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n        elif indicator in [\\'RSI\\', \\'MOMENTUM\\', \\'ROC\\', \\'PPO\\', \\'FASTK\\', \\'FASTD\\']:\\n            df[indicator] = func(df[\\'Close\\'])\\n        elif indicator == \\'MFI\\':\\n            df[indicator] = func(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'])\\n        elif indicator in [\\'CCI\\', \\'ADX\\', \\'WILLR\\']:\\n            df[indicator] = func(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n        else:\\n            df[indicator] = func(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n\\n    # Fill NaN values with 0\\n    df.fillna(0, inplace=True)\\n    return df']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5873/35267 [04:20<22:33, 21.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 127942 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Failed ['The rewards/Delta is not correctly calculated for the following code:\\n\\nimport gc\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"Monitor memory usage and log if it exceeds the threshold.\"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        logging.warning(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n    \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n    current_price = data.iloc[t][adjcp_field]\\n    reward = 0\\n    delta = 0\\n\\n    if action == 1:  # BUY\\n        agent.inventory.append(current_price)  # Add the current price to inventory\\n        logging.debug(f\"BUY: Current Price: {current_price}, Inventory: {agent.inventory}\")\\n\\n        # Calculate delta if there are at least 2 items in inventory\\n        if len(agent.inventory) > 1:\\n            bought_price = agent.inventory[-2]  # Use the second last item as the bought price\\n            delta = ((current_price - bought_price) * 4 * 12.5) - trans_cost\\n            reward = max(delta, 0)  # Ensure reward is not negative\\n            logging.debug(f\"BUY: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.debug(\"BUY: Inventory has less than 2 items, no delta calculated.\")\\n\\n    elif action == 2:  # SELL\\n        if len(agent.inventory) > 1:\\n            bought_price = agent.inventory[-2]  # Use the second last item as the bought price\\n            delta = ((bought_price + current_price) * 4 * 12.5) - trans_cost\\n            reward = max(delta, 0)  # Ensure reward is not negative\\n            logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.warning(\"SELL: Inventory is empty, no trade executed.\")\\n\\n    return reward, delta\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    action_map = {\\n        1: \"BUY\",\\n        2: \"SELL\",\\n        0: \"HOLD\"  # Assuming 0 is the HOLD action\\n    }\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # Validate action\\n        if action not in action_map:\\n            logging.warning(f\"Unexpected action {action} at time {t}. Defaulting to HOLD.\")\\n            action = 0  # Default to HOLD if action is invalid\\n\\n        # Execute trade and get reward and delta\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        # Log the action and reward\\n        logging.info(f\"Action: {action_map[action]}, Reward: {reward}, Delta: {delta}\")\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n            logging.info(f\"Episode {episode}, Step {t}, Loss: {loss:.4f}\")\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    action_map = {\\n        1: \"BUY\",\\n        2: \"SELL\",\\n        0: \"HOLD\"  # Assuming 0 is the HOLD action\\n    }\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # Validate action\\n        if action not in action_map:\\n            logging.warning(f\"Unexpected action {action} at time {t}. Defaulting to HOLD.\")\\n            action = 0  # Default to HOLD if action is invalid\\n\\n        # Execute trade and log history\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        # Log the action taken\\n        history.append((data.iloc[t][adjcp_field], action_map[action]))\\n\\n        # Log the action and reward\\n        logging.info(f\"Action: {action_map[action]}, Reward: {reward}, Delta: {delta}\")\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'give me the correct code', \"Shouldn't the rewards be when we are making money for both the buy and the sell\", 'Check the following for errors\\n\\nimport gc\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"Monitor memory usage and log if it exceeds the threshold.\"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        logging.warning(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n    \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n    current_price = data.iloc[t][adjcp_field]\\n    reward = 0\\n    delta = 0\\n\\n    if action == 1:  # BUY\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n        reward = delta\\n        total_profit += delta\\n\\n        # agent.inventory.append(current_price)  # Add the current price to inventory\\n        # logging.debug(f\"BUY: Current Price: {current_price}, Inventory: {agent.inventory}\")\\n        # \\n        # # Calculate delta if there are at least 2 items in inventory\\n        # if len(agent.inventory) > 1:\\n        #     bought_price = agent.inventory[-2]  # Use the second last item as the bought price\\n        #     delta = ((current_price - bought_price) * 4 * 12.5) - trans_cost\\n        #     reward = max(delta, 0)  # Ensure reward is not negative\\n        #     logging.debug(f\"BUY: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        # else:\\n        #     logging.debug(\"BUY: Inventory has less than 2 items, no delta calculated.\")\\n        # # Reward for buying can be set to a small positive value to encourage buying\\n        # reward += 0.1\\n\\n    elif action == 2:  # SELL\\n        try:\\n            delta = data.iloc[t][adjcp_field] - data.iloc[t + window_size][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n        reward = delta\\n        total_profit += delta\\n\\n        # if len(agent.inventory) > 0:\\n        #     bought_price = agent.inventory.pop(0)  # Use the first item as the bought price and remove it from inventory\\n        #     delta = ((current_price - bought_price) * 4 * 12.5) - trans_cost\\n        #     reward = max(delta, 0)  # Ensure reward is not negative\\n        #     logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        # else:\\n        #     logging.warning(\"SELL: Inventory is empty, no trade executed.\")\\n        # # Reward for selling can be set to a small positive value to encourage selling\\n        # reward += 0.1\\n\\n    return reward, delta\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    action_map = {\\n        1: \"BUY\",\\n        2: \"SELL\",\\n        0: \"HOLD\"  # Assuming 0 is the HOLD action\\n    }\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # Validate action\\n        if action not in action_map:\\n            logging.warning(f\"Unexpected action {action} at time {t}. Defaulting to HOLD.\")\\n            action = 0  # Default to HOLD if action is invalid\\n\\n        # Execute trade and get reward and delta\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        # Log the action and reward\\n        logging.info(f\"Action: {action_map[action]}, Reward: {reward}, Delta: {delta}\")\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n            logging.info(f\"Episode {episode}, Step {t}, Loss: {loss:.4f}\")\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    action_map = {\\n        1: \"BUY\",\\n        2: \"SELL\",\\n        0: \"HOLD\"  # Assuming 0 is the HOLD action\\n    }\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # Validate action\\n        if action not in action_map:\\n            logging.warning(f\"Unexpected action {action} at time {t}. Defaulting to HOLD.\")\\n            action = 0  # Default to HOLD if action is invalid\\n\\n        # Execute trade and log history\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        # Log the action taken\\n        history.append((data.iloc[t][adjcp_field], action_map[action]))\\n\\n        # Log the action and reward\\n        logging.info(f\"Action: {action_map[action]}, Reward: {reward}, Delta: {delta}\")\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'How do I call the indicator_utils.py from the following code in order to build the indicators that is setup on it in order for it to work when the OHLCV is pass from Ninjatrader app:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nfrom modules.indicator_utils import convert_to_float, add_indicators  # Import the new module\\n\\napp = Flask(__name__)\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    data = request.get_json()\\n\\n    trend = data[\\'trend\\']\\n    indicators = data[\\'indicators\\']\\n\\n    t = int(data[\\'t\\'])\\n    window_size = int(data[\\'window_size\\'])\\n\\n    state = get_state(trend, indicators, t, window_size)\\n    decision = model.predict(state)\\n    action = np.argmax(decision[0])\\n\\n    response = {\\'action\\': int(action)}\\n    return jsonify(response)\\n\\n\\nif __name__==\\'__main__\\':\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n    app.run(host=\"0.0.0.0\", debug=False, port=5020)', 'could not convert string to float date', 'ValueError: could not convert string to float: \\'Date\\' on the following code:\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nfrom modules.indicator_utils import convert_to_float, add_indicators  # Import the new module\\n\\napp = Flask(__name__)\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    data = request.get_json()\\n\\n    # Extract OHLCV data\\n    ohlcv = data[\\'indicators\\']\\n    t = int(data[\\'t\\'])\\n    window_size = int(data[\\'window_size\\'])\\n\\n    # Convert OHLCV data to float\\n    ohlcv = convert_to_float(ohlcv)\\n\\n    # Add indicators to the OHLCV data\\n    indicators = add_indicators(ohlcv)\\n\\n    # Extract trend data (e.g., closing prices)\\n    trend = ohlcv[\\'Close\\']\\n\\n    state = get_state(trend, indicators, t, window_size)\\n    decision = model.predict(state)\\n    action = np.argmax(decision[0])\\n\\n    response = {\\'action\\': int(action)}\\n    return jsonify(response)\\n\\nif __name__==\\'__main__\\':\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n    app.run(host=\"0.0.0.0\", debug=False, port=5010)', 'IndexError: list index out of range \\n\\nimport gc\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"Monitor memory usage and log if it exceeds the threshold.\"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        logging.warning(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n    \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n    current_price = data.iloc[t][adjcp_field]\\n    reward = 0\\n    delta = 0\\n\\n    if action == 1:  # BUY\\n        agent.inventory.append(current_price)  # Add the current price to inventory\\n        logging.debug(f\"BUY: Current Price: {current_price}, Inventory: {agent.inventory}\")\\n\\n        # Calculate delta if there are at least 2 items in inventory\\n        if len(agent.inventory) > 1:\\n            bought_price = agent.inventory[-2]  # Use the second last item as the bought price\\n            delta = ((current_price - bought_price) * 4 * 12.5) - trans_cost\\n            reward = max(delta, 0)  # Ensure reward is not negative\\n            logging.debug(f\"BUY: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.debug(\"BUY: Inventory has less than 2 items, no delta calculated.\")\\n\\n    elif action == 2:  # SELL\\n        if len(agent.inventory) > 0:\\n            bought_price = agent.inventory[-2]  # Use the second last item as the bought price\\n            delta = ((bought_price - current_price) * 4 * 12.5) - trans_cost\\n            reward = max(delta, 0)  # Ensure reward is not negative\\n            logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.warning(\"SELL: Inventory is empty, no trade executed.\")\\n\\n    return reward, delta\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    action_map = {\\n        1: \"BUY\",\\n        2: \"SELL\",\\n        0: \"HOLD\"  # Assuming 0 is the HOLD action\\n    }\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # Validate action\\n        if action not in action_map:\\n            logging.warning(f\"Unexpected action {action} at time {t}. Defaulting to HOLD.\")\\n            action = 0  # Default to HOLD if action is invalid\\n\\n        # Execute trade and get reward and delta\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n            logging.info(f\"Episode {episode}, Step {t}, Loss: {loss:.4f}\")\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    action_map = {\\n        1: \"BUY\",\\n        2: \"SELL\",\\n        0: \"HOLD\"  # Assuming 0 is the HOLD action\\n    }\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # Validate action\\n        if action not in action_map:\\n            logging.warning(f\"Unexpected action {action} at time {t}. Defaulting to HOLD.\")\\n            action = 0  # Default to HOLD if action is invalid\\n\\n        # Execute trade and log history\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        # Log the action taken\\n        history.append((data.iloc[t][adjcp_field], action_map[action]))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'How can I assemble the following:\\n\\nColumns in DataFrame after adding indicators: Index([\\'Date\\', \\'Open\\', \\'High\\', \\'Low\\', \\'Close\\', \\'Volume\\', \\'momentum\\', \\'adx\\', \\'ppo\\'],\\n      dtype=\\'object\\')\\n\\nfrom the \\'data = request.get_json()\\' on the following code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nfrom modules.indicator_utils import convert_to_float, add_indicators  # Import the new module\\n\\napp = Flask(__name__)\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    data = request.get_json()\\n\\n    trend = data[\\'trend\\']\\n    # indicators = data[\\'indicators\\']\\n    # Define the list of indicators to be used\\n    selected_indicators = add_indicators(data)\\n\\n    # Extract the selected indicators\\n    indicators = {indicator: data[indicator].tolist() for indicator in selected_indicators}\\n\\n    t = int(data[\\'t\\'])\\n    window_size = int(data[\\'window_size\\'])\\n\\n    state = get_state(trend, indicators, t, window_size)\\n    decision = model.predict(state)\\n    action = np.argmax(decision[0])\\n\\n    response = {\\'action\\': int(action)}\\n    return jsonify(response)\\n\\n\\nif __name__==\\'__main__\\':\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n    app.run(host=\"0.0.0.0\", debug=False, port=5010)', 'Fix the following error \\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nfrom modules.indicator_utils import convert_to_float, add_indicators  # Import the new module\\n\\napp = Flask(__name__)\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    data = request.get_json()\\n\\n    # Extract OHLCV data\\n    ohlcv_data = data[\\'indicators\\']  # Assuming the OHLCV data is passed under the key \\'ohlcv\\'\\n\\n    # Convert OHLCV data to DataFrame\\n    import pandas as pd\\n    ohlcv_df = pd.DataFrame(ohlcv_data)\\n\\n    # Add indicators to the OHLCV DataFrame\\n    indicators_df = add_indicators(ohlcv_df)\\n\\n    # Combine the original OHLCV data with the indicators\\n    # Extract the selected indicators\\n    indicators = {indicator: indicators_df[indicator].tolist() for indicator in indicators_df}\\n    # combined_df = ohlcv_df.join(indicators_df)\\n\\n    # Ensure the DataFrame has the required columns\\n    required_columns = [\\'Date\\', \\'Open\\', \\'High\\', \\'Low\\', \\'Close\\', \\'Volume\\', \\'momentum\\', \\'adx\\', \\'ppo\\']\\n    combined_df = combined_df[required_columns]\\n\\n    # Extract trend data (e.g., closing prices)\\n    trend = combined_df[\\'Close\\'].tolist()\\n\\n    t = int(data[\\'t\\'])\\n    window_size = int(data[\\'window_size\\'])\\n\\n    # Prepare indicators for the get_state function\\n    indicators = {indicator: combined_df[indicator].tolist() for indicator in [\\'momentum\\', \\'adx\\', \\'ppo\\']}\\n\\n    state = get_state(trend, indicators, t, window_size)\\n    decision = model.predict(state)\\n    action = np.argmax(decision[0])\\n\\n    response = {\\'action\\': int(action)}\\n    return jsonify(response)\\n\\n# def predict():\\n#     data = request.get_json()\\n#\\n#     trend = data[\\'trend\\']\\n#     # indicators = data[\\'indicators\\']\\n#     # Define the list of indicators to be used\\n#     selected_indicators = add_indicators(data)\\n#\\n#     # Extract the selected indicators\\n#     indicators = {indicator: data[indicator].tolist() for indicator in selected_indicators}\\n#\\n#     t = int(data[\\'t\\'])\\n#     window_size = int(data[\\'window_size\\'])\\n#\\n#     state = get_state(trend, indicators, t, window_size)\\n#     decision = model.predict(state)\\n#     action = np.argmax(decision[0])\\n#\\n#     response = {\\'action\\': int(action)}\\n#     return jsonify(response)\\n\\n\\nif __name__==\\'__main__\\':\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n    app.run(host=\"0.0.0.0\", debug=False, port=5010)', 'AttributeError: \\'builtin_function_or_method\\' object has no attribute \\'is_unique\\'\\n10.10.2.95 - - [19/Aug/2024 21:10:29] \"POST /predict HTTP/1.1\" 500 -\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nimport pandas as pd\\nfrom modules.indicator_utils import convert_to_float, add_indicators  # Import the new module\\n\\n\\napp = Flask(__name__)\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    data = request.get_json()\\n\\n    # Extract OHLCV data\\n    ohlcv_data = data[\\'indicators\\']  # Assuming the OHLCV data is passed under the key \\'ohlcv\\'\\n\\n    # Convert OHLCV data to DataFrame\\n    ohlcv_df = pd.DataFrame(ohlcv_data)\\n\\n    # Add indicators to the OHLCV DataFrame\\n    indicators_df = add_indicators(ohlcv_df)\\n\\n    # Combine the original OHLCV data with the indicators\\n    combined_df = ohlcv_df.join(indicators_df)\\n\\n    # Ensure the DataFrame has the required columns\\n    required_columns = [\\'Date\\', \\'Open\\', \\'High\\', \\'Low\\', \\'Close\\', \\'Volume\\', \\'momentum\\', \\'adx\\', \\'ppo\\']\\n    combined_df = combined_df[required_columns]\\n\\n    # Extract trend data (e.g., closing prices)\\n    trend = combined_df[\\'Close\\'].tolist()\\n\\n    t = int(data[\\'t\\'])\\n    window_size = int(data[\\'window_size\\'])\\n\\n    # Prepare indicators for the get_state function\\n    indicators = {indicator: combined_df[indicator].tolist() for indicator in [\\'momentum\\', \\'adx\\', \\'ppo\\']}\\n\\n    state = get_state(trend, indicators, t, window_size)\\n    decision = model.predict(state)\\n    action = np.argmax(decision[0])\\n\\n    response = {\\'action\\': int(action)}\\n    return jsonify(response)\\n\\nif __name__==\\'__main__\\':\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n    app.run(host=\"0.0.0.0\", debug=False, port=5010)\\n', 'AttributeError: \\'list\\' object has no attribute \\'head\\'\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nimport pandas as pd\\nfrom modules.indicator_utils import convert_to_float, add_indicators  # Import the new module\\n\\n\\napp = Flask(__name__)\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    data = request.get_json()\\n\\n    # Extract OHLCV data\\n    ohlcv_data = data[\\'indicators\\']  # Assuming the OHLCV data is passed under the key \\'indicators\\'\\n\\n    # Convert OHLCV data to DataFrame\\n    ohlcv_df = pd.DataFrame(ohlcv_data)\\n\\n    # Debug: Check the structure of the DataFrame\\n    print(\"OHLCV DataFrame:\")\\n    print(ohlcv_df.head())\\n\\n    # Add indicators to the OHLCV DataFrame\\n    indicators_df = add_indicators(ohlcv_df)\\n\\n    # Debug: Check the structure of the indicators DataFrame\\n    print(\"Indicators DataFrame:\")\\n    print(indicators_df.head())\\n\\n    # Combine the original OHLCV data with the indicators\\n    combined_df = ohlcv_df.join(indicators_df)\\n\\n    # Ensure the DataFrame has the required columns\\n    required_columns = [\\'Date\\', \\'Open\\', \\'High\\', \\'Low\\', \\'Close\\', \\'Volume\\', \\'momentum\\', \\'adx\\', \\'ppo\\']\\n\\n    # Check if all required columns are present\\n    missing_columns = [col for col in required_columns if col not in combined_df.columns]\\n    if missing_columns:\\n        return jsonify({\\'error\\': f\\'Missing columns in DataFrame: {missing_columns}\\'}), 400\\n\\n    combined_df = combined_df[required_columns]\\n\\n    # Extract trend data (e.g., closing prices)\\n    trend = combined_df[\\'Close\\'].tolist()\\n\\n    t = int(data[\\'t\\'])\\n    window_size = int(data[\\'window_size\\'])\\n\\n    # Prepare indicators for the get_state function\\n    indicators = {indicator: combined_df[indicator].tolist() for indicator in [\\'momentum\\', \\'adx\\', \\'ppo\\']}\\n\\n    state = get_state(trend, indicators, t, window_size)\\n    decision = model.predict(state)\\n    action = np.argmax(decision[0])\\n\\n    response = {\\'action\\': int(action)}\\n    return jsonify(response)\\n\\nif __name__==\\'__main__\\':\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n    app.run(host=\"0.0.0.0\", debug=False, port=5010)\\n', 'What does the following code do:\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions\\n        \"\"\"\\n        # take random action in order to diversify experience at the beginning\\n        if not is_eval and random.random() <= self.epsilon:\\n            return random.randrange(self.action_size)\\n\\n        action_probs = self.model.predict(state, verbose=0)\\n        return np.argmax(action_probs[0])\\n', 'How do I create one that is not random', 'can you provide me with one that doesn\\'t randomly generated number if is less than or equal to self.epsilon, a random action is chosen for the following code:\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions\\n        \"\"\"\\n        # take random action in order to diversify experience at the beginning\\n        if not is_eval and random.random() <= self.epsilon:\\n            return random.randrange(self.action_size)\\n\\n        action_probs = self.model.predict(state, verbose=0)\\n        return np.argmax(action_probs[0])', 'ValueError: Weights for model \\'sequential\\' have not yet been created. Weights are created when the model is first called on inputs or `build()` is called with an `input_shape`.\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t07-Aug-2024:\\tRev. 1.00\\t    First release.\\n\\nSummary of Changes:\\n    Constants: Introduced constants for action size and memory size.\\n    Model Creation: Simplified the model creation process using a loop.\\n    Reduced Redundancy: Consolidated the training logic to minimize code duplication.\\n    Improved Documentation: Enhanced docstrings for clarity.\\n    Utility Methods: Added utility methods for better organization.\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\"\"\"\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\n\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\\n\\n    Links: \\thttps://en.wikipedia.org/wiki/Huber_loss\\n            https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/\\n    \"\"\"\\n    error = y_true - y_pred\\n    cond = keras.backend.abs(error) <= clip_delta\\n    squared_loss = 0.5 * keras.backend.square(error)\\n    quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n    return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n\\n        # agent config\\n        self.state_size = state_size  # normalized previous days\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # model config\\n        self.model_name = model_name\\n        self.gamma = 0.95  # affinity for long term reward\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}  # important for loading the model from memory\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n        model_path = f\"models/{self.model_name}.keras\"\\n        if pretrained and os.path.exists(model_path) and self.model_name is not None:\\n            self.model = self.load()\\n        else:\\n            self.model = self._model()\\n\\n        # strategy config\\n        if self.strategy in [\"t-dqn\", \"double-dqn\"]:\\n            self.n_iter = 1\\n            self.reset_every = reset_every\\n\\n            # target network\\n            self.target_model = tf.keras.models.clone_model(self.model)\\n            self.target_model.set_weights(self.model.get_weights())\\n\\n    def _model(self):\\n        \"\"\"Creates the model\\n        \"\"\"\\n        model = keras.Sequential()\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=self.action_size))\\n\\n        model.compile(loss=self.loss, optimizer=self.optimizer)\\n        return model\\n\\n    def remember(self, state, action, reward, next_state, done):\\n        \"\"\"Adds relevant data to memory\\n        \"\"\"\\n        self.memory.append((state, action, reward, next_state, done))\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions\\n        \"\"\"\\n        # # take random action in order to diversify experience at the beginning\\n        # if not is_eval and random.random() <= self.epsilon:\\n        #     return random.randrange(self.action_size)\\n\\n        action_probs = self.model.predict(state, verbose=0)\\n        return np.argmax(action_probs[0])\\n\\n    def train_experience_replay(self, batch_size):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        mini_batch = random.sample(self.memory, batch_size)\\n        X_train, y_train = [], []\\n\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation\\n                    target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # DQN with fixed targets\\n        elif self.strategy == \"t-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation with fixed targets\\n                    target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # Double DQN\\n        elif self.strategy == \"double-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate double deep q-learning equation\\n                    target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                        np.argmax(self.model.predict(next_state)[0])]\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        else:\\n            raise NotImplementedError()\\n\\n        # update q-function parameters based on huber loss gradient\\n        loss = self.model.fit(\\n            np.array(X_train), np.array(y_train),\\n            epochs=1, verbose=0\\n        ).history[\"loss\"][0]\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def save(self):\\n        path = pathlib.Path()\\n        self.model.save(path / \"models/{}.keras\".format(self.model_name), overwrite=True)\\n\\n    def load(self):\\n        return tf.keras.models.load_model(\"models/{}.keras\".format(self.model_name), custom_objects=self.custom_objects)\\n', 'ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 600), found shape=(None, 68)\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t07-Aug-2024:\\tRev. 1.00\\t    First release.\\n\\nSummary of Changes:\\n    Constants: Introduced constants for action size and memory size.\\n    Model Creation: Simplified the model creation process using a loop.\\n    Reduced Redundancy: Consolidated the training logic to minimize code duplication.\\n    Improved Documentation: Enhanced docstrings for clarity.\\n    Utility Methods: Added utility methods for better organization.\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\"\"\"\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\n\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\\n\\n    Links: \\thttps://en.wikipedia.org/wiki/Huber_loss\\n            https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/\\n    \"\"\"\\n    error = y_true - y_pred\\n    cond = keras.backend.abs(error) <= clip_delta\\n    squared_loss = 0.5 * keras.backend.square(error)\\n    quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n    return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n\\n        # agent config\\n        self.state_size = state_size  # normalized previous days\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # model config\\n        self.model_name = model_name\\n        self.gamma = 0.95  # affinity for long term reward\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}  # important for loading the model from memory\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n        model_path = f\"models/{self.model_name}.keras\"\\n        if pretrained and os.path.exists(model_path) and self.model_name is not None:\\n            self.model = self.load()\\n        else:\\n            self.model = self._model()\\n            # Call the model on a dummy input to initialize weights\\n            self.model(np.zeros((1, self.state_size)))  # Assuming state_size is the input dimension\\n\\n        # strategy config\\n        if self.strategy in [\"t-dqn\", \"double-dqn\"]:\\n            self.n_iter = 1\\n            self.reset_every = reset_every\\n\\n            # target network\\n            self.target_model = tf.keras.models.clone_model(self.model)\\n            self.target_model.set_weights(self.model.get_weights())\\n\\n    def _model(self):\\n        \"\"\"Creates the model\"\"\"\\n        model = keras.Sequential()\\n        model.add(tf.keras.layers.Input(shape=(self.state_size,)))  # Specify input shape\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=self.action_size))\\n\\n        model.compile(loss=self.loss, optimizer=self.optimizer)\\n        return model\\n\\n    def remember(self, state, action, reward, next_state, done):\\n        \"\"\"Adds relevant data to memory\\n        \"\"\"\\n        self.memory.append((state, action, reward, next_state, done))\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions\\n        \"\"\"\\n        # # take random action in order to diversify experience at the beginning\\n        # if not is_eval and random.random() <= self.epsilon:\\n        #     return random.randrange(self.action_size)\\n\\n        action_probs = self.model.predict(state, verbose=0)\\n        return np.argmax(action_probs[0])\\n\\n    def train_experience_replay(self, batch_size):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        mini_batch = random.sample(self.memory, batch_size)\\n        X_train, y_train = [], []\\n\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation\\n                    target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # DQN with fixed targets\\n        elif self.strategy == \"t-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation with fixed targets\\n                    target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # Double DQN\\n        elif self.strategy == \"double-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate double deep q-learning equation\\n                    target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                        np.argmax(self.model.predict(next_state)[0])]\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        else:\\n            raise NotImplementedError()\\n\\n        # update q-function parameters based on huber loss gradient\\n        loss = self.model.fit(\\n            np.array(X_train), np.array(y_train),\\n            epochs=1, verbose=0\\n        ).history[\"loss\"][0]\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def save(self):\\n        path = pathlib.Path()\\n        self.model.save(path / \"models/{}.keras\".format(self.model_name), overwrite=True)\\n\\n    def load(self):\\n        return tf.keras.models.load_model(\"models/{}.keras\".format(self.model_name), custom_objects=self.custom_objects)\\n', 'ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 120), found shape=(None, 68)\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t07-Aug-2024:\\tRev. 1.00\\t    First release.\\n\\nSummary of Changes:\\n    Constants: Introduced constants for action size and memory size.\\n    Model Creation: Simplified the model creation process using a loop.\\n    Reduced Redundancy: Consolidated the training logic to minimize code duplication.\\n    Improved Documentation: Enhanced docstrings for clarity.\\n    Utility Methods: Added utility methods for better organization.\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\"\"\"\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\n\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\\n\\n    Links: \\thttps://en.wikipedia.org/wiki/Huber_loss\\n            https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/\\n    \"\"\"\\n    error = y_true - y_pred\\n    cond = keras.backend.abs(error) <= clip_delta\\n    squared_loss = 0.5 * keras.backend.square(error)\\n    quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n    return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n\\n        # agent config\\n        self.state_size = state_size  # normalized previous days\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # model config\\n        self.model_name = model_name\\n        self.gamma = 0.95  # affinity for long term reward\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}  # important for loading the model from memory\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n        model_path = f\"models/{self.model_name}.keras\"\\n        if pretrained and os.path.exists(model_path) and self.model_name is not None:\\n            self.model = self.load()\\n        else:\\n            self.model = self._model()\\n            # Call the model on a dummy input to initialize weights\\n            self.model(np.zeros((1, self.state_size)))  # Assuming state_size is the input dimension\\n\\n        # strategy config\\n        if self.strategy in [\"t-dqn\", \"double-dqn\"]:\\n            self.n_iter = 1\\n            self.reset_every = reset_every\\n\\n            # target network\\n            self.target_model = tf.keras.models.clone_model(self.model)\\n            self.target_model.set_weights(self.model.get_weights())\\n\\n    def _model(self):\\n        \"\"\"Creates the model\"\"\"\\n        model = keras.Sequential()\\n        model.add(tf.keras.layers.Input(shape=(self.state_size,)))  # Specify input shape\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=self.action_size))\\n\\n        model.compile(loss=self.loss, optimizer=self.optimizer)\\n        return model\\n\\n    def remember(self, state, action, reward, next_state, done):\\n        \"\"\"Adds relevant data to memory\\n        \"\"\"\\n        self.memory.append((state, action, reward, next_state, done))\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions\\n        \"\"\"\\n        # # take random action in order to diversify experience at the beginning\\n        # if not is_eval and random.random() <= self.epsilon:\\n        #     return random.randrange(self.action_size)\\n\\n        action_probs = self.model.predict(state, verbose=0)\\n        return np.argmax(action_probs[0])\\n\\n    def train_experience_replay(self, batch_size):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        mini_batch = random.sample(self.memory, batch_size)\\n        X_train, y_train = [], []\\n\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation\\n                    target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # DQN with fixed targets\\n        elif self.strategy == \"t-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation with fixed targets\\n                    target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # Double DQN\\n        elif self.strategy == \"double-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate double deep q-learning equation\\n                    target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                        np.argmax(self.model.predict(next_state)[0])]\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        else:\\n            raise NotImplementedError()\\n\\n        # update q-function parameters based on huber loss gradient\\n        loss = self.model.fit(\\n            np.array(X_train), np.array(y_train),\\n            epochs=1, verbose=0\\n        ).history[\"loss\"][0]\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def save(self):\\n        path = pathlib.Path()\\n        self.model.save(path / \"models/{}.keras\".format(self.model_name), overwrite=True)\\n\\n    def load(self):\\n        return tf.keras.models.load_model(\"models/{}.keras\".format(self.model_name), custom_objects=self.custom_objects)\\n', 'Which is the most optimal way to make the following code to use a more sophisticate effective patterns\\n\\nimport logging\\nimport os\\n\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\n# Formats Position\\nformat_position = lambda price: (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n# Formats Currency\\nformat_currency = lambda price: \\'${0:.2f}\\'.format(abs(price))\\n\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\" Displays training results\\n    \"\"\"\\n    if val_position == initial_offset or val_position == 0.0:\\n        logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: USELESS  Train Loss: {:.4f}\\'\\n                     .format(strategy, result[0], result[1], format_position(result[2]), result[3]))\\n        return False\\n    else:\\n        logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: {}  Train Loss: {:.4f})\\'\\n                     .format(strategy, result[0], result[1], format_position(result[2]), format_position(val_position),\\n                             result[3], ))\\n        return True\\n\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\" Displays eval results\\n    \"\"\"\\n    if profit == initial_offset or profit == 0.0:\\n        logging.info(\\'{}: USELESS\\\\n\\'.format(model_name))\\n    else:\\n        logging.info(\\'{}: {}\\\\n\\'.format(model_name, format_position(profit)))\\n\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\"Reads stock data from csv file\\n    \"\"\"\\n\\n    df = pd.read_csv(stock_file)\\n    if date_field != \"Date\":\\n        df[\\'Date\\'] = df[date_field]\\n\\n    # df = df.sort_values([date_field], ascending=False)\\n    # df[\\'diff\\'] = df[adjcp_field].diff(periods=5)\\n    df[\\'adjcp\\'] = df[adjcp_field]\\n    df = df.sort_values([date_field], ascending=True)\\n    # Add Indicators and Pattern Recognition Functions\\n    df = add_indicators(df)\\n    return df\\n\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\"Reads stock data from csv file\\n    \"\"\"\\n    df = pd.read_csv(stock_file)\\n    df = df.sort_values([date_field])\\n    return list(df[adjcp_field])\\n\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\"\\n    split the dataset into training or testing using date\\n    :param df: (df) pandas dataframe\\n    :param start:\\n    :param end:\\n    :return: (df) pandas dataframe\\n    \"\"\"\\n    data = df[(df.index >= start) & (df.index < end)]\\n    data.reset_index()\\n    return data\\n\\n\\ndef switch_k_backend_device():\\n    \"\"\" Switches `keras` backend from GPU to CPU if required.\\n\\n    Faster computation on CPU (if using tensorflow-gpu).\\n    \"\"\"\\n    if keras.backend == \"tensorflow\":\\n        logging.debug(\"switching to TensorFlow for CPU\")\\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n\\n\\ndef get_features():\\n    all_feature_name = [\\'CDL2CROWS\\',\\n                        \\'CDL3BLACKCROWS\\',\\n                        \\'CDL3INSIDE\\',\\n                        \\'CDL3LINESTRIKE\\',\\n                        \\'CDL3OUTSIDE\\',\\n                        \\'CDL3STARSINSOUTH\\',\\n                        \\'CDL3WHITESOLDIERS\\',\\n                        \\'CDLABANDONEDBABY\\',\\n                        \\'CDLADVANCEBLOCK\\',\\n                        \\'CDLBELTHOLD\\',\\n                        \\'CDLBREAKAWAY\\',\\n                        \\'CDLCLOSINGMARUBOZU\\',\\n                        \\'CDLCONCEALBABYSWALL\\',\\n                        \\'CDLCOUNTERATTACK\\',\\n                        \\'CDLDARKCLOUDCOVER\\',\\n                        \\'CDLDOJI\\',\\n                        \\'CDLDOJISTAR\\',\\n                        \\'CDLDRAGONFLYDOJI\\',\\n                        \\'CDLENGULFING\\',\\n                        \\'CDLEVENINGDOJISTAR\\',\\n                        \\'CDLEVENINGSTAR\\',\\n                        \\'CDLGAPSIDESIDEWHITE\\',\\n                        \\'CDLGRAVESTONEDOJI\\',\\n                        \\'CDLHAMMER\\',\\n                        \\'CDLHANGINGMAN\\',\\n                        \\'CDLHARAMI\\',\\n                        \\'CDLHARAMICROSS\\',\\n                        \\'CDLHIGHWAVE\\',\\n                        \\'CDLHIKKAKE\\',\\n                        \\'CDLHIKKAKEMOD\\',\\n                        \\'CDLHOMINGPIGEON\\',\\n                        \\'CDLIDENTICAL3CROWS\\',\\n                        \\'CDLINNECK\\',\\n                        \\'CDLINVERTEDHAMMER\\',\\n                        \\'CDLKICKING\\',\\n                        \\'CDLKICKINGBYLENGTH\\',\\n                        \\'CDLLADDERBOTTOM\\',\\n                        \\'CDLLONGLEGGEDDOJI\\',\\n                        \\'CDLLONGLINE\\',\\n                        \\'CDLMARUBOZU\\',\\n                        \\'CDLMATCHINGLOW\\',\\n                        \\'CDLMATHOLD\\',\\n                        \\'CDLMORNINGDOJISTAR\\',\\n                        \\'CDLMORNINGSTAR\\',\\n                        \\'CDLONNECK\\',\\n                        \\'CDLPIERCING\\',\\n                        \\'CDLRICKSHAWMAN\\',\\n                        \\'CDLRISEFALL3METHODS\\',\\n                        \\'CDLSEPARATINGLINES\\',\\n                        \\'CDLSHOOTINGSTAR\\',\\n                        \\'CDLSHORTLINE\\',\\n                        \\'CDLSPINNINGTOP\\',\\n                        \\'CDLSTALLEDPATTERN\\',\\n                        \\'CDLSTICKSANDWICH\\',\\n                        \\'CDLTAKURI\\',\\n                        \\'CDLTASUKIGAP\\',\\n                        \\'CDLTHRUSTING\\',\\n                        \\'CDLTRISTAR\\',\\n                        \\'CDLUNIQUE3RIVER\\',\\n                        \\'CDLUPSIDEGAP2CROWS\\',\\n                        \\'CDLXSIDEGAP3METHODS\\',\\n                        \\'MOMENTUM\\',\\n                        # \\'DX\\',\\n                        # \\'CCI\\',\\n                        \\'WILLR\\',\\n                        \\'MFI\\',\\n                        \\'ROC\\',\\n                        \\'ATR\\',\\n                        \\'ADX\\',\\n                        \\'PPO\\',\\n                        ]\\n    return all_feature_name\\n\\n\\ndef add_indicators(df):\\n    # Pattern Recognition Functions\\n    df[\\'CDL2CROWS\\'] = talib.CDL2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3BLACKCROWS\\'] = talib.CDL3BLACKCROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3INSIDE\\'] = talib.CDL3INSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3LINESTRIKE\\'] = talib.CDL3LINESTRIKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3OUTSIDE\\'] = talib.CDL3OUTSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3STARSINSOUTH\\'] = talib.CDL3STARSINSOUTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3WHITESOLDIERS\\'] = talib.CDL3WHITESOLDIERS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLABANDONEDBABY\\'] = talib.CDLABANDONEDBABY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLADVANCEBLOCK\\'] = talib.CDLADVANCEBLOCK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLBELTHOLD\\'] = talib.CDLBELTHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLBREAKAWAY\\'] = talib.CDLBREAKAWAY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLCLOSINGMARUBOZU\\'] = talib.CDLCLOSINGMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLCONCEALBABYSWALL\\'] = talib.CDLCONCEALBABYSWALL(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLCOUNTERATTACK\\'] = talib.CDLCOUNTERATTACK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDARKCLOUDCOVER\\'] = talib.CDLDARKCLOUDCOVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLDOJI\\'] = talib.CDLDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDOJISTAR\\'] = talib.CDLDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDRAGONFLYDOJI\\'] = talib.CDLDRAGONFLYDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLENGULFING\\'] = talib.CDLENGULFING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLEVENINGDOJISTAR\\'] = talib.CDLEVENINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLEVENINGSTAR\\'] = talib.CDLEVENINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLGAPSIDESIDEWHITE\\'] = talib.CDLGAPSIDESIDEWHITE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLGRAVESTONEDOJI\\'] = talib.CDLGRAVESTONEDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHAMMER\\'] = talib.CDLHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHANGINGMAN\\'] = talib.CDLHANGINGMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHARAMI\\'] = talib.CDLHARAMI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHARAMICROSS\\'] = talib.CDLHARAMICROSS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHIGHWAVE\\'] = talib.CDLHIGHWAVE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHIKKAKE\\'] = talib.CDLHIKKAKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHIKKAKEMOD\\'] = talib.CDLHIKKAKEMOD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHOMINGPIGEON\\'] = talib.CDLHOMINGPIGEON(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLIDENTICAL3CROWS\\'] = talib.CDLIDENTICAL3CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLINNECK\\'] = talib.CDLINNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLINVERTEDHAMMER\\'] = talib.CDLINVERTEDHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLKICKING\\'] = talib.CDLKICKING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLKICKINGBYLENGTH\\'] = talib.CDLKICKINGBYLENGTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLLADDERBOTTOM\\'] = talib.CDLLADDERBOTTOM(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLLONGLEGGEDDOJI\\'] = talib.CDLLONGLEGGEDDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLLONGLINE\\'] = talib.CDLLONGLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLMARUBOZU\\'] = talib.CDLMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLMATCHINGLOW\\'] = talib.CDLMATCHINGLOW(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLMATHOLD\\'] = talib.CDLMATHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLMORNINGDOJISTAR\\'] = talib.CDLMORNINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLMORNINGSTAR\\'] = talib.CDLMORNINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLONNECK\\'] = talib.CDLONNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLPIERCING\\'] = talib.CDLPIERCING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLRICKSHAWMAN\\'] = talib.CDLRICKSHAWMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLRISEFALL3METHODS\\'] = talib.CDLRISEFALL3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSEPARATINGLINES\\'] = talib.CDLSEPARATINGLINES(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSHOOTINGSTAR\\'] = talib.CDLSHOOTINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSHORTLINE\\'] = talib.CDLSHORTLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSPINNINGTOP\\'] = talib.CDLSPINNINGTOP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSTALLEDPATTERN\\'] = talib.CDLSTALLEDPATTERN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSTICKSANDWICH\\'] = talib.CDLSTICKSANDWICH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLTAKURI\\'] = talib.CDLTAKURI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLTASUKIGAP\\'] = talib.CDLTASUKIGAP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLTHRUSTING\\'] = talib.CDLTHRUSTING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLTRISTAR\\'] = talib.CDLTRISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLUNIQUE3RIVER\\'] = talib.CDLUNIQUE3RIVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLUPSIDEGAP2CROWS\\'] = talib.CDLUPSIDEGAP2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLXSIDEGAP3METHODS\\'] = talib.CDLXSIDEGAP3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # Price action\\n    # https://ta-lib.github.io/ta-lib-python/func_groups/momentum_indicators.html\\n    # rsi,stochRsi,cci,williamsr,mfi,roc,atr,adx\\n    # df[\\'RSI\\'] = talib.RSI(df[\\'Close\\'], timeperiod=14)\\n    df[\\'MOMENTUM\\'] = talib.MOM(df[\\'Close\\'], timeperiod=10)\\n    # df[\\'DX\\'] = talib.stream_DX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    # df[\\'STOCHRSI\\'] = talib.STOCHRSI(df[\\'Close\\'], timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)\\n    # df[\\'CCI\\'] = talib.CCI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'WILLR\\'] = talib.WILLR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'MFI\\'] = talib.MFI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'], timeperiod=14)\\n    df[\\'ROC\\'] = talib.ROC(df[\\'Close\\'], timeperiod=10)\\n    df[\\'ATR\\'] = talib.ATR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'ADX\\'] = talib.ADX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'PPO\\'] = talib.PPO(df[\\'Close\\'], fastperiod=12, slowperiod=26, matype=0)\\n\\n    df = df.fillna(0)\\n\\n    return df\\n', 'Which is the most optimal way to make the following code:\\n\\nimport logging\\nimport os\\n\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\n# Formats Position\\nformat_position = lambda price: (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n# Formats Currency\\nformat_currency = lambda price: \\'${0:.2f}\\'.format(abs(price))\\n\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\" Displays training results\\n    \"\"\"\\n    if val_position == initial_offset or val_position == 0.0:\\n        logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: USELESS  Train Loss: {:.4f}\\'\\n                     .format(strategy, result[0], result[1], format_position(result[2]), result[3]))\\n        return False\\n    else:\\n        logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: {}  Train Loss: {:.4f})\\'\\n                     .format(strategy, result[0], result[1], format_position(result[2]), format_position(val_position),\\n                             result[3], ))\\n        return True\\n\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\" Displays eval results\\n    \"\"\"\\n    if profit == initial_offset or profit == 0.0:\\n        logging.info(\\'{}: USELESS\\\\n\\'.format(model_name))\\n    else:\\n        logging.info(\\'{}: {}\\\\n\\'.format(model_name, format_position(profit)))\\n\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\"Reads stock data from csv file\\n    \"\"\"\\n\\n    df = pd.read_csv(stock_file)\\n    if date_field != \"Date\":\\n        df[\\'Date\\'] = df[date_field]\\n\\n    # df = df.sort_values([date_field], ascending=False)\\n    # df[\\'diff\\'] = df[adjcp_field].diff(periods=5)\\n    df[\\'adjcp\\'] = df[adjcp_field]\\n    df = df.sort_values([date_field], ascending=True)\\n    # Add Indicators and Pattern Recognition Functions\\n    df = add_indicators(df)\\n    return df\\n\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\"Reads stock data from csv file\\n    \"\"\"\\n    df = pd.read_csv(stock_file)\\n    df = df.sort_values([date_field])\\n    return list(df[adjcp_field])\\n\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\"\\n    split the dataset into training or testing using date\\n    :param df: (df) pandas dataframe\\n    :param start:\\n    :param end:\\n    :return: (df) pandas dataframe\\n    \"\"\"\\n    data = df[(df.index >= start) & (df.index < end)]\\n    data.reset_index()\\n    return data\\n\\n\\ndef switch_k_backend_device():\\n    \"\"\" Switches `keras` backend from GPU to CPU if required.\\n\\n    Faster computation on CPU (if using tensorflow-gpu).\\n    \"\"\"\\n    if keras.backend == \"tensorflow\":\\n        logging.debug(\"switching to TensorFlow for CPU\")\\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n\\n\\ndef get_features():\\n    all_feature_name = [\\'CDL2CROWS\\',\\n                        \\'CDL3BLACKCROWS\\',\\n                        \\'CDL3INSIDE\\',\\n                        \\'CDL3LINESTRIKE\\',\\n                        \\'CDL3OUTSIDE\\',\\n                        \\'CDL3STARSINSOUTH\\',\\n                        \\'CDL3WHITESOLDIERS\\',\\n                        \\'CDLABANDONEDBABY\\',\\n                        \\'CDLADVANCEBLOCK\\',\\n                        \\'CDLBELTHOLD\\',\\n                        \\'CDLBREAKAWAY\\',\\n                        \\'CDLCLOSINGMARUBOZU\\',\\n                        \\'CDLCONCEALBABYSWALL\\',\\n                        \\'CDLCOUNTERATTACK\\',\\n                        \\'CDLDARKCLOUDCOVER\\',\\n                        \\'CDLDOJI\\',\\n                        \\'CDLDOJISTAR\\',\\n                        \\'CDLDRAGONFLYDOJI\\',\\n                        \\'CDLENGULFING\\',\\n                        \\'CDLEVENINGDOJISTAR\\',\\n                        \\'CDLEVENINGSTAR\\',\\n                        \\'CDLGAPSIDESIDEWHITE\\',\\n                        \\'CDLGRAVESTONEDOJI\\',\\n                        \\'CDLHAMMER\\',\\n                        \\'CDLHANGINGMAN\\',\\n                        \\'CDLHARAMI\\',\\n                        \\'CDLHARAMICROSS\\',\\n                        \\'CDLHIGHWAVE\\',\\n                        \\'CDLHIKKAKE\\',\\n                        \\'CDLHIKKAKEMOD\\',\\n                        \\'CDLHOMINGPIGEON\\',\\n                        \\'CDLIDENTICAL3CROWS\\',\\n                        \\'CDLINNECK\\',\\n                        \\'CDLINVERTEDHAMMER\\',\\n                        \\'CDLKICKING\\',\\n                        \\'CDLKICKINGBYLENGTH\\',\\n                        \\'CDLLADDERBOTTOM\\',\\n                        \\'CDLLONGLEGGEDDOJI\\',\\n                        \\'CDLLONGLINE\\',\\n                        \\'CDLMARUBOZU\\',\\n                        \\'CDLMATCHINGLOW\\',\\n                        \\'CDLMATHOLD\\',\\n                        \\'CDLMORNINGDOJISTAR\\',\\n                        \\'CDLMORNINGSTAR\\',\\n                        \\'CDLONNECK\\',\\n                        \\'CDLPIERCING\\',\\n                        \\'CDLRICKSHAWMAN\\',\\n                        \\'CDLRISEFALL3METHODS\\',\\n                        \\'CDLSEPARATINGLINES\\',\\n                        \\'CDLSHOOTINGSTAR\\',\\n                        \\'CDLSHORTLINE\\',\\n                        \\'CDLSPINNINGTOP\\',\\n                        \\'CDLSTALLEDPATTERN\\',\\n                        \\'CDLSTICKSANDWICH\\',\\n                        \\'CDLTAKURI\\',\\n                        \\'CDLTASUKIGAP\\',\\n                        \\'CDLTHRUSTING\\',\\n                        \\'CDLTRISTAR\\',\\n                        \\'CDLUNIQUE3RIVER\\',\\n                        \\'CDLUPSIDEGAP2CROWS\\',\\n                        \\'CDLXSIDEGAP3METHODS\\',\\n                        \\'MOMENTUM\\',\\n                        # \\'DX\\',\\n                        # \\'CCI\\',\\n                        \\'WILLR\\',\\n                        \\'MFI\\',\\n                        \\'ROC\\',\\n                        \\'ATR\\',\\n                        \\'ADX\\',\\n                        \\'PPO\\',\\n                        ]\\n    return all_feature_name\\n\\n\\ndef add_indicators(df):\\n    # Pattern Recognition Functions\\n    df[\\'CDL2CROWS\\'] = talib.CDL2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3BLACKCROWS\\'] = talib.CDL3BLACKCROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3INSIDE\\'] = talib.CDL3INSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3LINESTRIKE\\'] = talib.CDL3LINESTRIKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3OUTSIDE\\'] = talib.CDL3OUTSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3STARSINSOUTH\\'] = talib.CDL3STARSINSOUTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3WHITESOLDIERS\\'] = talib.CDL3WHITESOLDIERS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLABANDONEDBABY\\'] = talib.CDLABANDONEDBABY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLADVANCEBLOCK\\'] = talib.CDLADVANCEBLOCK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLBELTHOLD\\'] = talib.CDLBELTHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLBREAKAWAY\\'] = talib.CDLBREAKAWAY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLCLOSINGMARUBOZU\\'] = talib.CDLCLOSINGMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLCONCEALBABYSWALL\\'] = talib.CDLCONCEALBABYSWALL(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLCOUNTERATTACK\\'] = talib.CDLCOUNTERATTACK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDARKCLOUDCOVER\\'] = talib.CDLDARKCLOUDCOVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLDOJI\\'] = talib.CDLDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDOJISTAR\\'] = talib.CDLDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDRAGONFLYDOJI\\'] = talib.CDLDRAGONFLYDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLENGULFING\\'] = talib.CDLENGULFING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLEVENINGDOJISTAR\\'] = talib.CDLEVENINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLEVENINGSTAR\\'] = talib.CDLEVENINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLGAPSIDESIDEWHITE\\'] = talib.CDLGAPSIDESIDEWHITE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLGRAVESTONEDOJI\\'] = talib.CDLGRAVESTONEDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHAMMER\\'] = talib.CDLHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHANGINGMAN\\'] = talib.CDLHANGINGMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHARAMI\\'] = talib.CDLHARAMI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHARAMICROSS\\'] = talib.CDLHARAMICROSS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHIGHWAVE\\'] = talib.CDLHIGHWAVE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHIKKAKE\\'] = talib.CDLHIKKAKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHIKKAKEMOD\\'] = talib.CDLHIKKAKEMOD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHOMINGPIGEON\\'] = talib.CDLHOMINGPIGEON(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLIDENTICAL3CROWS\\'] = talib.CDLIDENTICAL3CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLINNECK\\'] = talib.CDLINNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLINVERTEDHAMMER\\'] = talib.CDLINVERTEDHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLKICKING\\'] = talib.CDLKICKING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLKICKINGBYLENGTH\\'] = talib.CDLKICKINGBYLENGTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLLADDERBOTTOM\\'] = talib.CDLLADDERBOTTOM(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLLONGLEGGEDDOJI\\'] = talib.CDLLONGLEGGEDDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLLONGLINE\\'] = talib.CDLLONGLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLMARUBOZU\\'] = talib.CDLMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLMATCHINGLOW\\'] = talib.CDLMATCHINGLOW(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLMATHOLD\\'] = talib.CDLMATHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLMORNINGDOJISTAR\\'] = talib.CDLMORNINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLMORNINGSTAR\\'] = talib.CDLMORNINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLONNECK\\'] = talib.CDLONNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLPIERCING\\'] = talib.CDLPIERCING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLRICKSHAWMAN\\'] = talib.CDLRICKSHAWMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLRISEFALL3METHODS\\'] = talib.CDLRISEFALL3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSEPARATINGLINES\\'] = talib.CDLSEPARATINGLINES(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSHOOTINGSTAR\\'] = talib.CDLSHOOTINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSHORTLINE\\'] = talib.CDLSHORTLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSPINNINGTOP\\'] = talib.CDLSPINNINGTOP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSTALLEDPATTERN\\'] = talib.CDLSTALLEDPATTERN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSTICKSANDWICH\\'] = talib.CDLSTICKSANDWICH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLTAKURI\\'] = talib.CDLTAKURI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLTASUKIGAP\\'] = talib.CDLTASUKIGAP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLTHRUSTING\\'] = talib.CDLTHRUSTING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLTRISTAR\\'] = talib.CDLTRISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLUNIQUE3RIVER\\'] = talib.CDLUNIQUE3RIVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLUPSIDEGAP2CROWS\\'] = talib.CDLUPSIDEGAP2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLXSIDEGAP3METHODS\\'] = talib.CDLXSIDEGAP3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # Price action\\n    # https://ta-lib.github.io/ta-lib-python/func_groups/momentum_indicators.html\\n    # rsi,stochRsi,cci,williamsr,mfi,roc,atr,adx\\n    # df[\\'RSI\\'] = talib.RSI(df[\\'Close\\'], timeperiod=14)\\n    df[\\'MOMENTUM\\'] = talib.MOM(df[\\'Close\\'], timeperiod=10)\\n    # df[\\'DX\\'] = talib.stream_DX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    # df[\\'STOCHRSI\\'] = talib.STOCHRSI(df[\\'Close\\'], timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)\\n    # df[\\'CCI\\'] = talib.CCI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'WILLR\\'] = talib.WILLR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'MFI\\'] = talib.MFI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'], timeperiod=14)\\n    df[\\'ROC\\'] = talib.ROC(df[\\'Close\\'], timeperiod=10)\\n    df[\\'ATR\\'] = talib.ATR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'ADX\\'] = talib.ADX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'PPO\\'] = talib.PPO(df[\\'Close\\'], fastperiod=12, slowperiod=26, matype=0)\\n\\n    df = df.fillna(0)\\n\\n    return df\\n', 'Cannot find reference \\'keras\\' in \\'__init__.py | __init__.py\\'\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t07-Aug-2024:\\tRev. 1.00\\t    First release.\\n\\nSummary of Changes:\\n    Constants: Introduced constants for action size and memory size.\\n    Model Creation: Simplified the model creation process using a loop.\\n    Reduced Redundancy: Consolidated the training logic to minimize code duplication.\\n    Improved Documentation: Enhanced docstrings for clarity.\\n    Utility Methods: Added utility methods for better organization.\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\"\"\"\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\n\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\\n\\n    Links: \\thttps://en.wikipedia.org/wiki/Huber_loss\\n            https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/\\n    \"\"\"\\n    error = y_true - y_pred\\n    cond = keras.backend.abs(error) <= clip_delta\\n    squared_loss = 0.5 * keras.backend.square(error)\\n    quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n    return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n\\n        # agent config\\n        self.state_size = state_size  # normalized previous days\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # model config\\n        self.model_name = model_name\\n        self.gamma = 0.95  # affinity for long term reward\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}  # important for loading the model from memory\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n        model_path = f\"models/{self.model_name}.keras\"\\n        if pretrained and os.path.exists(model_path) and self.model_name is not None:\\n            self.model = self.load()\\n        else:\\n            self.model = self._model()\\n\\n        # strategy config\\n        if self.strategy in [\"t-dqn\", \"double-dqn\"]:\\n            self.n_iter = 1\\n            self.reset_every = reset_every\\n\\n            # target network\\n            self.target_model = tf.keras.models.clone_model(self.model)\\n            self.target_model.set_weights(self.model.get_weights())\\n\\n    def _model(self):\\n        \"\"\"Creates the model\\n        \"\"\"\\n        model = keras.Sequential()\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=self.action_size))\\n\\n        model.compile(loss=self.loss, optimizer=self.optimizer)\\n        return model\\n\\n    def remember(self, state, action, reward, next_state, done):\\n        \"\"\"Adds relevant data to memory\\n        \"\"\"\\n        self.memory.append((state, action, reward, next_state, done))\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions\\n        \"\"\"\\n        # # take random action in order to diversify experience at the beginning\\n        # if not is_eval and random.random() <= self.epsilon:\\n        #     return random.randrange(self.action_size)\\n\\n        action_probs = self.model.predict(state, verbose=0)\\n        return np.argmax(action_probs[0])\\n\\n    def train_experience_replay(self, batch_size):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        mini_batch = random.sample(self.memory, batch_size)\\n        X_train, y_train = [], []\\n\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation\\n                    target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # DQN with fixed targets\\n        elif self.strategy == \"t-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation with fixed targets\\n                    target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # Double DQN\\n        elif self.strategy == \"double-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate double deep q-learning equation\\n                    target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                        np.argmax(self.model.predict(next_state)[0])]\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        else:\\n            raise NotImplementedError()\\n\\n        # update q-function parameters based on huber loss gradient\\n        loss = self.model.fit(\\n            np.array(X_train), np.array(y_train),\\n            epochs=1, verbose=0\\n        ).history[\"loss\"][0]\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def save(self):\\n        path = pathlib.Path()\\n        self.model.save(path / \"models/{}.keras\".format(self.model_name), overwrite=True)\\n\\n    def load(self):\\n        return tf.keras.models.load_model(\"models/{}.keras\".format(self.model_name), custom_objects=self.custom_objects)\\n', 'what is the output for the block if window_size = 30 for the following:\\n\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]', 'what is the output for the block if window_size = 30 and t = 29 for the following:\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]', 'Can we use Dropout layers on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n            self.experience_replay = []  # Initialize experience replay buffer\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n            holding_penalty = 0  # Penalty for holding positions too long\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Penalty for holding positions too long\\n                if position != 0:\\n                    holding_penalty += 1\\n                    if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                        earned_money -= 5  # Penalty for holding too long\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                # Store experience in replay buffer\\n                self.experience_replay.append((state, action, earned_money, next_state))\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return (earned_money / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n', 'Can we use Dropout layers on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(123)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract and scale the selected indicators\\n    scalers = {}\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        scaler = StandardScaler()\\n        indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n        scalers[indicator] = scaler  # Save the fitted scaler\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n    # Define the step-by-step prediction function\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                print(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                print(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract and scale the selected indicators for testing\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = scalers[indicator].transform(df2[indicator].values.reshape(-1, 1)).flatten()\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()', 'can add it to the code', 'Can we use Dropout layers on the following code, and if so add it to the code\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n            self.experience_replay = []  # Initialize experience replay buffer\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n            holding_penalty = 0  # Penalty for holding positions too long\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Penalty for holding positions too long\\n                if position != 0:\\n                    holding_penalty += 1\\n                    if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                        earned_money -= 5  # Penalty for holding too long\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                # Store experience in replay buffer\\n                self.experience_replay.append((state, action, earned_money, next_state))\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return (earned_money / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n', 'Can we use Dropout layers on the following code, and if so add it to the code\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\nimport numpy as np\\n\\n\\nclass Model:\\n    def __init__(self, input_size: int, layer_size: int, output_size: int):\\n        \"\"\"\\n        Initializes the model with random weights.\\n\\n        Parameters:\\n        - input_size: Number of input features.\\n        - layer_size: Number of neurons in the hidden layer.\\n        - output_size: Number of output classes.\\n        \"\"\"\\n        self.weights = [\\n            np.random.normal(0, 0.1, (input_size, layer_size)),  # Input to hidden layer weights\\n            np.random.normal(0, 0.1, (layer_size, output_size)),  # Hidden to output layer weights\\n        ]\\n        self.biases = [\\n            np.random.normal(0, 0.1, (1, layer_size)),  # Bias for hidden layer\\n            np.random.normal(0, 0.1, (1, output_size)),  # Bias for output layer\\n        ]\\n\\n    def predict(self, inputs: np.ndarray) -> np.ndarray:\\n        \"\"\"\\n        Makes a prediction based on the input data.\\n\\n        Parameters:\\n        - inputs: Input data for prediction.\\n\\n        Returns:\\n        - Decision: Output predictions.\\n        \"\"\"\\n        feed = np.dot(inputs, self.weights[0]) + self.biases[0]\\n        decision = np.dot(feed, self.weights[1]) + self.biases[1]\\n        return decision\\n\\n    def get_weights(self) -> list:\\n        \"\"\"Returns the current weights of the model.\"\"\"\\n        return self.weights\\n\\n    def set_weights(self, weights: list):\\n        \"\"\"\\n        Sets the weights of the model.\\n\\n        Parameters:\\n        - weights: New weights to set for the model.\\n        \"\"\"\\n        if len(weights) != len(self.weights):\\n            raise ValueError(\"Weights must have the same length as the current weights.\")\\n\\n        for i in range(len(weights)):\\n            if weights[i].shape != self.weights[i].shape:\\n                raise ValueError(\\n                    f\"Weight matrix {i} must have shape {self.weights[i].shape}, but got {weights[i].shape}.\")\\n\\n        self.weights = weights\\n\\n', 'What is wrong with the following get_tick_reward, I keep loosing trades\\n\\n  def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n            holding_penalty = 0  # Penalty for holding positions too long\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Penalty for holding positions too long\\n                if position != 0:\\n                    holding_penalty += 1\\n                    if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                        earned_money -= 5  # Penalty for holding too long\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                # Store experience in replay buffer\\n                self.experience_replay.append((state, action, earned_money, next_state))\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return (earned_money / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n', 'What is wrong with following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\n\\nReward Function:\\n\\nSharpe Ratio: The reward function now evaluates the performance of trades using the Sharpe Ratio.\\nPenalty for Holding: Introduced a penalty for holding positions too long without action.\\n\"\"\"\\n\\nimport numpy as np\\n\\nclass Deep_Evolution_Strategy():\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def adaptive_sigma(self, rewards):\\n        \"\"\"Adjust sigma based on the mean and standard deviation of rewards.\"\"\"\\n        mean_reward = np.mean(rewards)\\n        std_reward = np.std(rewards)\\n        if std_reward > 0:\\n            self.sigma *= 1 + (mean_reward / std_reward)  # Adjust sigma based on performance\\n\\n    def train(self, epoch=200, print_every=1, update_frequency=5, patience=10):\\n        previous_mean_reward = -np.inf\\n        no_improvement_count = 0\\n        all_rewards = []  # To store rewards from all epochs\\n\\n        for i in range(epoch):\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = [np.random.randn(*w.shape) + np.random.uniform(-0.01, 0.01, w.shape) for w in self.weights]\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            self.adaptive_sigma(rewards)  # Adjust sigma based on rewards\\n\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] += (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T, rewards).T\\n\\n            # Store the average reward for this epoch\\n            avg_reward = np.mean(rewards)\\n            all_rewards.append(avg_reward)\\n\\n            if (i + 1) % print_every == 0:\\n                print(f\\'Epoch {i + 1}/{epoch}, Average Reward: {avg_reward}\\')\\n\\n                if avg_reward < previous_mean_reward:\\n                    no_improvement_count += 1\\n                else:\\n                    no_improvement_count = 0\\n                    previous_mean_reward = avg_reward\\n\\n                if no_improvement_count >= patience:\\n                    print(\"Early stopping...\")\\n                    break\\n\\n        return all_rewards  # Return all rewards collected during training\\n', 'How do I add it to the code', 'What is wrong with the following codes:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(6)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[14:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Log the number of NaNs in each column after conversion\\n    logging.info(f\\'NaN values after conversion:\\\\n{df1.isna().sum()}\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            # converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.to_numeric(df1[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n\\n    # Extract and scale the selected indicators\\n    scalers = {}\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        scaler = StandardScaler()\\n        indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n        scalers[indicator] = scaler  # Save the fitted scaler\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n    # Define the step-by-step prediction function\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                logging.info(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                logging.info(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                              indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n    logging.info(f\\'Training completed. Total gains: {total_gains}, Total investment: {invest}%\\')\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract and scale the selected indicators for testing\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = scalers[indicator].transform(df2[indicator].values.reshape(-1, 1)).flatten()\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n    logging.info(f\\'Testing completed. Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n    logging.info(f\\'Loaded model testing completed. Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:    Rev date:      Revision:      Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]  15-Jun-2024:   Rev. 1.00      First release.\\n[JSanchez]  17-Jun-2024:   Rev. 1.01      Added the following rsi, cci, and adx indicator.\\n[JSanchez]  21-Jun-2024:   Rev. 1.02      Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]  23-Aug-2024:   Rev. 1.03      Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n            self.experience_replay = []  # Initialize experience replay buffer\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            holding_penalty = 0  # Penalty for holding positions too long\\n            returns = []  # Store returns for Sharpe Ratio calculation\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Penalty for holding positions too long\\n                if position != 0:\\n                    holding_penalty += 1\\n                    if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                        earned_money -= 5  # Penalty for holding too long\\n\\n                # Store the return for Sharpe Ratio calculation\\n                returns.append(earned_money)\\n\\n                # Store experience in replay buffer\\n                self.experience_replay.append((state, action, earned_money, next_state))\\n\\n                state = next_state\\n\\n            # Calculate Sharpe Ratio\\n            mean_return = np.mean(returns)\\n            std_return = np.std(returns)\\n            risk_free_rate = 0.01  # Example risk-free rate\\n            sharpe_ratio = (mean_return - risk_free_rate) / (std_return + 1e-7)\\n\\n            logging.debug(f\\'Sharpe Ratio: {sharpe_ratio}\\')\\n            return sharpe_ratio\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added logging to track the execution flow and errors\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added ltry-except blocks around file operations and data processing to\\n                                            catch specific exceptions like FileNotFoundError and\\n                                            pd.errors.EmptyDataError\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added Informative logging messages to indicate successful operations\\n                                            and errors.\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added to ensure that the scalers are fitted with the re-train data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nLogging Configuration: Set up logging to track the execution flow and errors. The log messages include timestamps\\nand severity levels.\\n\\nTry-Except Blocks: Added try-except blocks around file operations and data processing to catch specific exceptions\\nlike FileNotFoundError and pd.errors.EmptyDataError. A general exception handler is also included to catch any\\nother unexpected errors.\\n\\nLogging Messages: Informative logging messages are added to indicate successful operations and errors.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers: The code now saves the fitted scalers instead of new instances, preventing the \"not fitted\"\\nerror when you attempt to use them.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\n\\ndef run_retrading_simulation(model):\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    try:\\n        # Load and preprocess data\\n        df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n        logging.info(\"Data loaded successfully.\")\\n\\n        # Calculate the number of rows to train\\n        rows_count = len(df1)\\n        rows_toTrain = int(rows_count * 0.10)  # Use last 10% of the data for training\\n        df1 = df1.iloc[-rows_toTrain:].copy()  # Select the last 10%\\n\\n        # Ensure \\'Date\\' column exists\\n        if \\'Date\\' not in df1.columns:\\n            df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n        # Log the number of NaNs in each column after conversion\\n        logging.info(f\\'NaN values after conversion:\\\\n{df1.isna().sum()}\\')\\n\\n        # Convert columns to float, excluding the \\'Date\\' column\\n        def convert_to_float(lst):\\n            return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan\\n                    for i in lst]\\n\\n        # Apply conversion and handle length mismatches\\n        for col in df1.columns:\\n            if col != \\'Date\\':\\n                # converted_values = convert_to_float(df1[col].tolist())\\n                df1[col] = pd.to_numeric(df1[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n        logging.info(\"Data preprocessing completed successfully.\")\\n\\n        # Extract and scale the selected indicators\\n        scalers = {}\\n        indicators = {}\\n        for indicator in selected_indicators:\\n            scaler = StandardScaler()\\n            indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n            scalers[indicator] = scaler  # Save the fitted scaler\\n\\n        close = df1[\\'Close\\'].values.flatten()\\n\\n        # Save the scalers for future use\\n        joblib.dump(scalers, \\'output/scalers.pkl\\')\\n        logging.info(\"Scalers saved successfully.\")\\n\\n        # Initialize agent\\n        window_size, skip, initial_money = 30, 1, 10000\\n        agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                                  indicators=indicators)\\n\\n        # Train the agent\\n        agent.fit(iterations=500, checkpoint=10)\\n        logging.info(\"Agent training completed successfully.\")\\n\\n        # Perform predictions\\n        states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n        # Save the updated model\\n        joblib.dump(model, \\'output/model.pkl\\')\\n        logging.info(\"Model saved successfully.\")\\n\\n        # Plotting results\\n        plt.figure(figsize=(15, 5))\\n        plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n        plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n    except pd.errors.EmptyDataError:\\n        logging.error(\"No data found in the CSV file.\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        run_retrading_simulation(model)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\n\\nReward Function:\\n\\nSharpe Ratio: The reward function now evaluates the performance of trades using the Sharpe Ratio.\\nPenalty for Holding: Introduced a penalty for holding positions too long without action.\\n\"\"\"\\n\\nimport numpy as np\\n\\nclass Deep_Evolution_Strategy():\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def adaptive_sigma(self, rewards):\\n        \"\"\"Adjust sigma based on the mean and standard deviation of rewards.\"\"\"\\n        mean_reward = np.mean(rewards)\\n        std_reward = np.std(rewards)\\n        if std_reward > 0:\\n            self.sigma *= 1 + (mean_reward / std_reward)  # Adjust sigma based on performance\\n\\n    def train(self, epoch=200, print_every=1, update_frequency=5, patience=10):\\n        previous_mean_reward = -np.inf\\n        no_improvement_count = 0\\n        all_rewards = []  # To store rewards from all epochs\\n\\n        for i in range(epoch):\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = [np.random.randn(*w.shape) + np.random.uniform(-0.01, 0.01, w.shape) for w in self.weights]\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            self.adaptive_sigma(rewards)  # Adjust sigma based on rewards\\n\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] += (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T, rewards).T\\n\\n            # Store the average reward for this epoch\\n            avg_reward = np.mean(rewards)\\n            all_rewards.append(avg_reward)\\n\\n            if (i + 1) % print_every == 0:\\n                print(f\\'Epoch {i + 1}/{epoch}, Average Reward: {avg_reward}\\')\\n\\n                if avg_reward < previous_mean_reward:\\n                    no_improvement_count += 1\\n                else:\\n                    no_improvement_count = 0\\n                    previous_mean_reward = avg_reward\\n\\n                if no_improvement_count >= patience:\\n                    print(\"Early stopping...\")\\n                    break\\n\\n        return all_rewards  # Return all rewards collected during training\\n\\n\\n\"\"\"\\n---------------------------', 'What is wrong with the following:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:    Rev date:      Revision:      Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]  15-Jun-2024:   Rev. 1.00      First release.\\n[JSanchez]  17-Jun-2024:   Rev. 1.01      Added the following rsi, cci, and adx indicator.\\n[JSanchez]  21-Jun-2024:   Rev. 1.02      Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]  23-Aug-2024:   Rev. 1.03      Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n            self.experience_replay = []  # Initialize experience replay buffer\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            holding_penalty = 0  # Penalty for holding positions too long\\n            returns = []  # Store returns for Sharpe Ratio calculation\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Penalty for holding positions too long\\n                if position != 0:\\n                    holding_penalty += 1\\n                    if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                        earned_money -= 5  # Penalty for holding too long\\n\\n                # Store the return for Sharpe Ratio calculation\\n                returns.append(earned_money)\\n\\n                # Store experience in replay buffer\\n                self.experience_replay.append((state, action, earned_money, next_state))\\n\\n                state = next_state\\n\\n            # Calculate Sharpe Ratio\\n            mean_return = np.mean(returns)\\n            std_return = np.std(returns)\\n            risk_free_rate = 0.01  # Example risk-free rate\\n            sharpe_ratio = (mean_return - risk_free_rate) / (std_return + 1e-7)\\n\\n            logging.debug(f\\'Sharpe Ratio: {sharpe_ratio}\\')\\n            return sharpe_ratio\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise', 'What is wrong with the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\n\\nReward Function:\\n\\nSharpe Ratio: The reward function now evaluates the performance of trades using the Sharpe Ratio.\\nPenalty for Holding: Introduced a penalty for holding positions too long without action.\\n\"\"\"\\n\\nimport numpy as np\\n\\nclass Deep_Evolution_Strategy():\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def adaptive_sigma(self, rewards):\\n        \"\"\"Adjust sigma based on the mean and standard deviation of rewards.\"\"\"\\n        mean_reward = np.mean(rewards)\\n        std_reward = np.std(rewards)\\n        if std_reward > 0:\\n            self.sigma *= 1 + (mean_reward / std_reward)  # Adjust sigma based on performance\\n\\n    def train(self, epoch=200, print_every=1, update_frequency=5, patience=10):\\n        previous_mean_reward = -np.inf\\n        no_improvement_count = 0\\n        all_rewards = []  # To store rewards from all epochs\\n\\n        for i in range(epoch):\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = [np.random.randn(*w.shape) + np.random.uniform(-0.01, 0.01, w.shape) for w in self.weights]\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            self.adaptive_sigma(rewards)  # Adjust sigma based on rewards\\n\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] += (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T, rewards).T\\n\\n            # Store the average reward for this epoch\\n            avg_reward = np.mean(rewards)\\n            all_rewards.append(avg_reward)\\n\\n            if (i + 1) % print_every == 0:\\n                print(f\\'Epoch {i + 1}/{epoch}, Average Reward: {avg_reward}\\')\\n\\n                if avg_reward < previous_mean_reward:\\n                    no_improvement_count += 1\\n                else:\\n                    no_improvement_count = 0\\n                    previous_mean_reward = avg_reward\\n\\n                if no_improvement_count >= patience:\\n                    print(\"Early stopping...\")\\n                    break\\n\\n        return all_rewards  # Return all rewards collected during training\\n\\n', 'What is wrong with the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t27-Jun-2024:\\tRev. 1.02\\t    Added to the predict method, a mask is created using a binomial\\n                                            distribution to randomly drop neurons based on the specified\\n                                            dropout rate.\\n[JSanchez]\\t27-Aug-2024:\\tRev. 1.03\\t    Added a dropout_rate parameter to the __init__ method\\n------------------------------------------------------------------------------------------------------------------------\\n\\nDropout Rate: Added a dropout_rate parameter to the __init__ method to specify the fraction of neurons to drop.\\nDropout Implementation: In the predict method, a mask is created using a binomial distribution to randomly drop\\nneurons based on the specified dropout rate.\\n\\n\"\"\"\\n\\nimport numpy as np\\n\\n\\nclass Model:\\n    def __init__(self, input_size: int, layer_size: int, output_size: int, dropout_rate: float = 0.5):\\n        \"\"\"\\n        Initializes the model with random weights.\\n\\n        Parameters:\\n        - input_size: Number of input features.\\n        - layer_size: Number of neurons in the hidden layer.\\n        - output_size: Number of output classes.\\n        - dropout_rate: Fraction of neurons to drop during prediction.\\n        \"\"\"\\n        self.weights = [\\n            np.random.normal(0, 0.1, (input_size, layer_size)),  # Input to hidden layer weights\\n            np.random.normal(0, 0.1, (layer_size, output_size)),  # Hidden to output layer weights\\n        ]\\n        self.biases = [\\n            np.random.normal(0, 0.1, (1, layer_size)),  # Bias for hidden layer\\n            np.random.normal(0, 0.1, (1, output_size)),  # Bias for output layer\\n        ]\\n        self.dropout_rate = dropout_rate  # Set the dropout rate\\n\\n    def predict(self, inputs: np.ndarray) -> np.ndarray:\\n        \"\"\"\\n        Makes a prediction based on the input data.\\n\\n        Parameters:\\n        - inputs: Input data for prediction.\\n\\n        Returns:\\n        - Decision: Output predictions.\\n        \"\"\"\\n        feed = np.dot(inputs, self.weights[0]) + self.biases[0]\\n\\n        # Apply Dropout to the hidden layer\\n        if self.dropout_rate > 0:\\n            mask = np.random.binomial(1, 1 - self.dropout_rate, size=feed.shape)  # Create a mask\\n            feed *= mask  # Apply the mask to the feed\\n\\n        decision = np.dot(feed, self.weights[1]) + self.biases[1]\\n        return decision\\n\\n    def get_weights(self) -> list:\\n        \"\"\"Returns the current weights of the model.\"\"\"\\n        return self.weights\\n\\n    def set_weights(self, weights: list):\\n        \"\"\"\\n        Sets the weights of the model.\\n\\n        Parameters:\\n        - weights: New weights to set for the model.\\n        \"\"\"\\n        if len(weights) != len(self.weights):\\n            raise ValueError(\"Weights must have the same length as the current weights.\")\\n\\n        for i in range(len(weights)):\\n            if weights[i].shape != self.weights[i].shape:\\n                raise ValueError(\\n                    f\"Weight matrix {i} must have shape {self.weights[i].shape}, but got {weights[i].shape}.\")\\n\\n        self.weights = weights\\n\\n', 'What is wrong with the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added logging to track the execution flow and errors\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added ltry-except blocks around file operations and data processing to\\n                                            catch specific exceptions like FileNotFoundError and\\n                                            pd.errors.EmptyDataError\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added Informative logging messages to indicate successful operations\\n                                            and errors.\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added to ensure that the scalers are fitted with the re-train data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nLogging Configuration: Set up logging to track the execution flow and errors. The log messages include timestamps\\nand severity levels.\\n\\nTry-Except Blocks: Added try-except blocks around file operations and data processing to catch specific exceptions\\nlike FileNotFoundError and pd.errors.EmptyDataError. A general exception handler is also included to catch any\\nother unexpected errors.\\n\\nLogging Messages: Informative logging messages are added to indicate successful operations and errors.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers: The code now saves the fitted scalers instead of new instances, preventing the \"not fitted\"\\nerror when you attempt to use them.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\n\\ndef run_retrading_simulation(model):\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    try:\\n        # Load and preprocess data\\n        df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n        logging.info(\"Data loaded successfully.\")\\n\\n        # Calculate the number of rows to train\\n        rows_count = len(df1)\\n        rows_toTrain = int(rows_count * 0.10)  # Use last 10% of the data for training\\n        df1 = df1.iloc[-rows_toTrain:].copy()  # Select the last 10%\\n\\n        # Ensure \\'Date\\' column exists\\n        if \\'Date\\' not in df1.columns:\\n            df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n        # Log the number of NaNs in each column after conversion\\n        logging.info(f\\'NaN values after conversion:\\\\n{df1.isna().sum()}\\')\\n\\n        # Convert columns to float, excluding the \\'Date\\' column\\n        def convert_to_float(lst):\\n            return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan\\n                    for i in lst]\\n\\n        # Apply conversion and handle length mismatches\\n        for col in df1.columns:\\n            if col != \\'Date\\':\\n                # converted_values = convert_to_float(df1[col].tolist())\\n                df1[col] = pd.to_numeric(df1[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n        logging.info(\"Data preprocessing completed successfully.\")\\n\\n        # Extract and scale the selected indicators\\n        scalers = {}\\n        indicators = {}\\n        for indicator in selected_indicators:\\n            scaler = StandardScaler()\\n            indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n            scalers[indicator] = scaler  # Save the fitted scaler\\n\\n        close = df1[\\'Close\\'].values.flatten()\\n\\n        # Save the scalers for future use\\n        joblib.dump(scalers, \\'output/scalers.pkl\\')\\n        logging.info(\"Scalers saved successfully.\")\\n\\n        # Initialize agent\\n        window_size, skip, initial_money = 30, 1, 10000\\n        agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                                  indicators=indicators)\\n\\n        # Train the agent\\n        agent.fit(iterations=500, checkpoint=10)\\n        logging.info(\"Agent training completed successfully.\")\\n\\n        # Perform predictions\\n        states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n        # Save the updated model\\n        joblib.dump(model, \\'output/model.pkl\\')\\n        logging.info(\"Model saved successfully.\")\\n\\n        # Plotting results\\n        plt.figure(figsize=(15, 5))\\n        plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n        plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n    except pd.errors.EmptyDataError:\\n        logging.error(\"No data found in the CSV file.\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        run_retrading_simulation(model)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")\\n', 'What is wrong with the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(6)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[14:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Log the number of NaNs in each column after conversion\\n    logging.info(f\\'NaN values after conversion:\\\\n{df1.isna().sum()}\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            # converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.to_numeric(df1[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n\\n    # Extract and scale the selected indicators\\n    scalers = {}\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        scaler = StandardScaler()\\n        indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n        scalers[indicator] = scaler  # Save the fitted scaler\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n    # Define the step-by-step prediction function\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                logging.info(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                logging.info(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                              indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n    logging.info(f\\'Training completed. Total gains: {total_gains}, Total investment: {invest}%\\')\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract and scale the selected indicators for testing\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = scalers[indicator].transform(df2[indicator].values.reshape(-1, 1)).flatten()\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n    logging.info(f\\'Testing completed. Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n    logging.info(f\\'Loaded model testing completed. Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()', 'if I perform an scalers on the RSI, and ADX indicator will it prevent the get_tick_reward and the buy_tick nodule for not working correctly\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:    Rev date:      Revision:      Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]  15-Jun-2024:   Rev. 1.00      First release.\\n[JSanchez]  17-Jun-2024:   Rev. 1.01      Added the following rsi, cci, and adx indicator.\\n[JSanchez]  21-Jun-2024:   Rev. 1.02      Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]  23-Aug-2024:   Rev. 1.03      Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n            self.experience_replay = []  # Initialize experience replay buffer\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            holding_penalty = 0  # Penalty for holding positions too long\\n            returns = []  # Store returns for Sharpe Ratio calculation\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Penalty for holding positions too long\\n                if position != 0:\\n                    holding_penalty += 1\\n                    if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                        earned_money -= 5  # Penalty for holding too long\\n\\n                # Store the return for Sharpe Ratio calculation\\n                returns.append(earned_money)\\n\\n                # Store experience in replay buffer\\n                self.experience_replay.append((state, action, earned_money, next_state))\\n\\n                state = next_state\\n\\n            # Calculate Sharpe Ratio\\n            mean_return = np.mean(returns)\\n            std_return = np.std(returns)\\n            risk_free_rate = 0.01  # Example risk-free rate\\n            sharpe_ratio = (mean_return - risk_free_rate) / (std_return + 1e-7)\\n\\n            logging.debug(f\\'Sharpe Ratio: {sharpe_ratio}\\')\\n            return sharpe_ratio\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise', 'ERROR - An error occurred: __init__() missing 1 required positional argument: \\'scalers\\'\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:    Rev date:      Revision:      Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]  15-Jun-2024:   Rev. 1.00      First release.\\n[JSanchez]  17-Jun-2024:   Rev. 1.01      Added the following rsi, cci, and adx indicator.\\n[JSanchez]  21-Jun-2024:   Rev. 1.02      Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]  23-Aug-2024:   Rev. 1.03      Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators, scalers):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.scalers = scalers  # Save the scalers for later use\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n            self.experience_replay = []  # Initialize experience replay buffer\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            holding_penalty = 0  # Penalty for holding positions too long\\n            returns = []  # Store returns for Sharpe Ratio calculation\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < self.scalers[\\'rsi\\'].transform([[30]])[0][0]:  # Adjusted for scaled RSI\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > self.scalers[\\'adx\\'].transform([[25]])[0][0]:  # Adjusted for scaled ADX\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Penalty for holding positions too long\\n                if position != 0:\\n                    holding_penalty += 1\\n                    if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                        earned_money -= 5  # Penalty for holding too long\\n\\n                # Store the return for Sharpe Ratio calculation\\n                returns.append(earned_money)\\n\\n                # Store experience in replay buffer\\n                self.experience_replay.append((state, action, earned_money, next_state))\\n\\n                state = next_state\\n\\n            # Calculate Sharpe Ratio\\n            mean_return = np.mean(returns)\\n            std_return = np.std(returns)\\n            risk_free_rate = 0.01  # Example risk-free rate\\n            sharpe_ratio = (mean_return - risk_free_rate) / (std_return + 1e-7)\\n\\n            logging.debug(f\\'Sharpe Ratio: {sharpe_ratio}\\')\\n            return sharpe_ratio\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > self.scalers[\\'rsi\\'].transform([[70]])[0][0] and position == 1:  # Adjusted for scaled RSI\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise', 'ERROR - An error occurred: __init__() missing 1 required positional argument: \\'scalers\\'\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:    Rev date:      Revision:      Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]  15-Jun-2024:   Rev. 1.00      First release.\\n[JSanchez]  17-Jun-2024:   Rev. 1.01      Added the following rsi, cci, and adx indicator.\\n[JSanchez]  21-Jun-2024:   Rev. 1.02      Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]  23-Aug-2024:   Rev. 1.03      Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators, scalers):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.scalers = scalers  # Save the scalers for later use\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n            self.experience_replay = []  # Initialize experience replay buffer\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            holding_penalty = 0  # Penalty for holding positions too long\\n            returns = []  # Store returns for Sharpe Ratio calculation\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < self.scalers[\\'rsi\\'].transform([[30]])[0][0]:  # Adjusted for scaled RSI\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > self.scalers[\\'adx\\'].transform([[25]])[0][0]:  # Adjusted for scaled ADX\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Penalty for holding positions too long\\n                if position != 0:\\n                    holding_penalty += 1\\n                    if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                        earned_money -= 5  # Penalty for holding too long\\n\\n                # Store the return for Sharpe Ratio calculation\\n                returns.append(earned_money)\\n\\n                # Store experience in replay buffer\\n                self.experience_replay.append((state, action, earned_money, next_state))\\n\\n                state = next_state\\n\\n            # Calculate Sharpe Ratio\\n            mean_return = np.mean(returns)\\n            std_return = np.std(returns)\\n            risk_free_rate = 0.01  # Example risk-free rate\\n            sharpe_ratio = (mean_return - risk_free_rate) / (std_return + 1e-7)\\n\\n            logging.debug(f\\'Sharpe Ratio: {sharpe_ratio}\\')\\n            return sharpe_ratio\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > self.scalers[\\'rsi\\'].transform([[70]])[0][0] and position == 1:  # Adjusted for scaled RSI\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise', 'My total gains are always on a negative, check the following codes:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(2023)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    try:\\n        df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n        logging.info(\"Data loaded successfully.\")\\n\\n        # Check if the DataFrame is empty\\n        if df1.empty:\\n            logging.error(\"The loaded DataFrame is empty.\")\\n            return\\n\\n        rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n        # Copy the records to df1 for training\\n        df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n        # Ensure \\'Date\\' column exists\\n        if \\'Date\\' not in df1.columns:\\n            logging.warning(\"Date column not found. Creating a default date range.\")\\n            df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n        # Convert columns to float, excluding the \\'Date\\' column\\n        for col in df1.columns:\\n            if col != \\'Date\\':\\n                df1[col] = pd.to_numeric(df1[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n        # Handle NaNs\\n        if df1.isna().sum().sum() > 0:\\n            logging.warning(f\\'NaN values found in DataFrame. Filling NaNs with 0.\\')\\n            df1.fillna(0, inplace=True)\\n\\n        logging.info(\"Data preprocessing completed successfully.\")\\n\\n        # Extract and scale the selected indicators\\n        scalers = {}\\n        indicators = {}\\n        for indicator in selected_indicators:\\n            if indicator in df1.columns:\\n                scaler = StandardScaler()\\n                indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n                scalers[indicator] = scaler  # Save the fitted scaler\\n            else:\\n                logging.warning(f\\'Indicator {indicator} not found in DataFrame.\\')\\n\\n        close = df1[\\'Close\\'].values.flatten()\\n\\n        # Save the scalers for future use\\n        joblib.dump(scalers, \\'output/scalers.pkl\\')\\n        logging.info(\"Scalers saved successfully.\")\\n\\n        # Define the step-by-step prediction function\\n        def step_by_step_prediction(agent, trend):\\n            position, bought_price, earned_money = 0, 0, 0\\n\\n            for t in range(0, len(trend) - 1, agent.skip):\\n                action = agent.act(agent.get_state(t))\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position, bought_price = 1, trend[t] + 0.25\\n                    logging.info(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position, bought_price = -1, trend[t] - 0.25\\n                    logging.info(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n        # Initialize agent\\n        window_size, skip, initial_money = 30, 1, 10000\\n        model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n        agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                                  indicators=indicators, scalers=scalers)  # Pass scalers here\\n\\n        # Train the agent\\n        agent.fit(iterations=500, checkpoint=10)\\n\\n        # Perform predictions\\n        states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n        logging.info(f\\'Training completed. Total gains: {total_gains}, Total investment: {invest}%\\')\\n\\n        # Save the model\\n        joblib.dump(model, \\'output/model.pkl\\')\\n        logging.info(\"Model saved successfully.\")\\n\\n        # Plotting results\\n        plt.figure(figsize=(15, 5))\\n        plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n        plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n        # Testing\\n        df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n        df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n        # Convert test data\\n        for col in df2.columns:\\n            if col != \\'Date\\':\\n                df2[col] = pd.to_numeric(df2[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n        # Handle NaNs in test data\\n        if df2.isna().sum().sum() > 0:\\n            logging.warning(f\\'NaN values found in test DataFrame. Filling NaNs with 0.\\')\\n            df2.fillna(0, inplace=True)\\n\\n        # Extract and scale the selected indicators for testing\\n        test_indicators = {}\\n        for indicator in selected_indicators:\\n            if indicator in df2.columns:\\n                test_indicators[indicator] = scalers[indicator].transform(df2[indicator].values.reshape(-1, 1)).flatten()\\n            else:\\n                logging.warning(f\\'Indicator {indicator} not found in test DataFrame.\\')\\n\\n        close_test = df2[\\'Close\\'].values.flatten()\\n\\n        # Initialize test agent\\n        agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators, scalers=scalers)  # Pass scalers here\\n        states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n        logging.info(f\\'Testing completed. Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n\\n        # Aligning the indices for testing data\\n        test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n        test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n        test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n        test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n        test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n        test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n        # Calculate metrics\\n        ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n        # Plotting test results\\n        plt.figure(figsize=(15, 5))\\n        plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n        plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n        plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n        # Load and test the saved model\\n        loaded_model = joblib.load(\\'output/model.pkl\\')\\n        loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators, scalers=scalers)  # Pass scalers here\\n        loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n        logging.info(f\\'Loaded model testing completed. Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n\\n        # Plot the loaded model results\\n        plt.figure(figsize=(15, 5))\\n        plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n        plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n        plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n        # Perform step-by-step predictions\\n        step_by_step_prediction(agentTest, close_test)\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n    except pd.errors.EmptyDataError:\\n        logging.error(\"No data found in the CSV file.\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:    Rev date:      Revision:      Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]  15-Jun-2024:   Rev. 1.00      First release.\\n[JSanchez]  17-Jun-2024:   Rev. 1.01      Added the following rsi, cci, and adx indicator.\\n[JSanchez]  21-Jun-2024:   Rev. 1.02      Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]  23-Aug-2024:   Rev. 1.03      Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators, scalers):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.scalers = scalers  # Save the scalers for later use\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n            self.experience_replay = []  # Initialize experience replay buffer\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            holding_penalty = 0  # Penalty for holding positions too long\\n            returns = []  # Store returns for Sharpe Ratio calculation\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < self.scalers[\\'rsi\\'].transform([[30]])[0][0]:  # Adjusted for scaled RSI\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > self.scalers[\\'adx\\'].transform([[25]])[0][0]:  # Adjusted for scaled ADX\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Penalty for holding positions too long\\n                if position != 0:\\n                    holding_penalty += 1\\n                    if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                        earned_money -= 5  # Penalty for holding too long\\n\\n                # Store the return for Sharpe Ratio calculation\\n                returns.append(earned_money)\\n\\n                # Store experience in replay buffer\\n                self.experience_replay.append((state, action, earned_money, next_state))\\n\\n                state = next_state\\n\\n            # Calculate Sharpe Ratio\\n            mean_return = np.mean(returns)\\n            std_return = np.std(returns)\\n            risk_free_rate = 0.01  # Example risk-free rate\\n            sharpe_ratio = (mean_return - risk_free_rate) / (std_return + 1e-7)\\n\\n            logging.debug(f\\'Sharpe Ratio: {sharpe_ratio}\\')\\n            return sharpe_ratio\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > self.scalers[\\'rsi\\'].transform([[70]])[0][0] and position == 1:  # Adjusted for scaled RSI\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\n\\nReward Function:\\n\\nSharpe Ratio: The reward function now evaluates the performance of trades using the Sharpe Ratio.\\nPenalty for Holding: Introduced a penalty for holding positions too long without action.\\n\"\"\"\\n\\nimport numpy as np\\n\\nclass Deep_Evolution_Strategy():\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            # Ensure weights do not become excessively large or small\\n            new_weight = weights[index] + jittered\\n            weights_population.append(new_weight)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def adaptive_sigma(self, rewards):\\n        \"\"\"Adjust sigma based on the mean and standard deviation of rewards.\"\"\"\\n        mean_reward = np.mean(rewards)\\n        std_reward = np.std(rewards)\\n        if std_reward > 0:\\n            self.sigma *= 1 + (mean_reward / std_reward)  # Adjust sigma based on performance\\n            # Cap sigma to prevent it from becoming too large\\n            self.sigma = min(self.sigma, 1.0)  # Example cap\\n\\n    def train(self, epoch=200, print_every=1, update_frequency=5, patience=10):\\n        previous_mean_reward = -np.inf\\n        no_improvement_count = 0\\n        all_rewards = []  # To store rewards from all epochs\\n\\n        for i in range(epoch):\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = [np.random.randn(*w.shape) + np.random.uniform(-0.01, 0.01, w.shape) for w in self.weights]\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            # Normalize rewards\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            self.adaptive_sigma(rewards)  # Adjust sigma based on rewards\\n\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] += (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T, rewards).T\\n\\n            # Store the average reward for this epoch\\n            avg_reward = np.mean(rewards)\\n            all_rewards.append(avg_reward)\\n\\n            if (i + 1) % print_every == 0:\\n                print(f\\'Epoch {i + 1}/{epoch}, Average Reward: {avg_reward}\\')\\n\\n                if avg_reward < previous_mean_reward:\\n                    no_improvement_count += 1\\n                else:\\n                    no_improvement_count = 0\\n                    previous_mean_reward = avg_reward\\n\\n                if no_improvement_count >= patience:\\n                    print(\"Early stopping...\")\\n                    break\\n\\n        return all_rewards  # Return all rewards collected during training\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t27-Jun-2024:\\tRev. 1.02\\t    Added to the predict method, a mask is created using a binomial\\n                                            distribution to randomly drop neurons based on the specified\\n                                            dropout rate.\\n[JSanchez]\\t27-Aug-2024:\\tRev. 1.03\\t    Added a dropout_rate parameter to the __init__ method\\n------------------------------------------------------------------------------------------------------------------------\\n\\nDropout Rate: Added a dropout_rate parameter to the __init__ method to specify the fraction of neurons to drop.\\nDropout Implementation: In the predict method, a mask is created using a binomial distribution to randomly drop\\nneurons based on the specified dropout rate.\\n\\n\"\"\"\\n\\nimport numpy as np\\n\\nclass Model:\\n    def __init__(self, input_size: int, layer_size: int, output_size: int, dropout_rate: float = 0.5):\\n        \"\"\"\\n        Initializes the model with random weights.\\n\\n        Parameters:\\n        - input_size: Number of input features.\\n        - layer_size: Number of neurons in the hidden layer.\\n        - output_size: Number of output classes.\\n        - dropout_rate: Fraction of neurons to drop during prediction.\\n        \"\"\"\\n        self.weights = [\\n            np.random.normal(0, 0.1, (input_size, layer_size)),  # Input to hidden layer weights\\n            np.random.normal(0, 0.1, (layer_size, output_size)),  # Hidden to output layer weights\\n        ]\\n        self.biases = [\\n            np.random.normal(0, 0.1, (1, layer_size)),  # Bias for hidden layer\\n            np.random.normal(0, 0.1, (1, output_size)),  # Bias for output layer\\n        ]\\n        self.dropout_rate = dropout_rate  # Set the dropout rate\\n\\n    def predict(self, inputs: np.ndarray) -> np.ndarray:\\n        \"\"\"\\n        Makes a prediction based on the input data.\\n\\n        Parameters:\\n        - inputs: Input data for prediction.\\n\\n        Returns:\\n        - Decision: Output predictions.\\n        \"\"\"\\n        feed = np.dot(inputs, self.weights[0]) + self.biases[0]\\n\\n        # Apply Dropout to the hidden layer\\n        if self.dropout_rate > 0:\\n            mask = np.random.binomial(1, 1 - self.dropout_rate, size=feed.shape)  # Create a mask\\n            feed *= mask / (1 - self.dropout_rate)  # Scale the output\\n\\n        decision = np.dot(feed, self.weights[1]) + self.biases[1]\\n        return decision\\n\\n    def get_weights(self) -> list:\\n        \"\"\"Returns the current weights of the model.\"\"\"\\n        return self.weights\\n\\n    def set_weights(self, weights: list):\\n        \"\"\"\\n        Sets the weights of the model.\\n\\n        Parameters:\\n        - weights: New weights to set for the model.\\n        \"\"\"\\n        if len(weights) != len(self.weights):\\n            raise ValueError(\"Weights must have the same length as the current weights.\")\\n\\n        for i in range(len(weights)):\\n            if weights[i].shape != self.weights[i].shape:\\n                raise ValueError(\\n                    f\"Weight matrix {i} must have shape {self.weights[i].shape}, but got {weights[i].shape}.\")\\n\\n        self.weights = weights', 'How do I Experiment with different hyperparameters to find the optimal settings for your model, and where do change it', 'Add the kalman filter to the following code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\n# from modules.indicator_utils import convert_to_float, add_indicators  # Import the new module\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(6)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    try:\\n        df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n        logging.info(\"Data loaded successfully.\")\\n\\n        # Check if the DataFrame is empty\\n        if df1.empty:\\n            logging.error(\"The loaded DataFrame is empty.\")\\n            return\\n\\n        rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n        # Copy the records to df1 for training\\n        df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n        # Ensure \\'Date\\' column exists\\n        if \\'Date\\' not in df1.columns:\\n            logging.warning(\"Date column not found. Creating a default date range.\")\\n            df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n        # Convert columns to float, excluding the \\'Date\\' column\\n        for col in df1.columns:\\n            if col != \\'Date\\':\\n                df1[col] = pd.to_numeric(df1[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n        # Handle NaNs\\n        if df1.isna().sum().sum() > 0:\\n            logging.warning(f\\'NaN values found in DataFrame. Filling NaNs with 0.\\')\\n            df1.fillna(0, inplace=True)\\n\\n        logging.info(\"Data preprocessing completed successfully.\")\\n        # # Load and preprocess data\\n        # df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n        # rows_count = (len(df1) + 1)  # get the number of rows on the csv file\\n        # calc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\n        # rows_toTrain = int(rows_count - calc_perc)\\n        #\\n        # # Copy the numbers of records to df1 for training\\n        # df1 = df1.iloc[1:rows_toTrain, :].copy()\\n        #\\n        # # Check if \\'Date\\' column exists, otherwise create it\\n        # if \\'Date\\' not in df1.columns:\\n        #     df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n        #\\n        # # Convert columns to float, excluding the \\'Date\\' column\\n        # for col in df1.columns:\\n        #     if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        #         df1[col] = convert_to_float(df1[col].tolist())\\n        #\\n        # Define the list of indicators to be used\\n        # selected_indicators = add_indicators(df1)\\n\\n        # Verify that the indicators are added to the DataFrame\\n        print(\"Columns in DataFrame after adding indicators:\", df1.columns)\\n\\n        # Extract the selected indicators\\n        indicators = {indicator: df1[indicator].tolist() for indicator in selected_indicators}\\n        close = df1[\\'Close\\'].tolist()\\n\\n        def step_by_step_prediction(agent, trend):\\n            state = agent.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            earned_money = 0\\n\\n            for t in range(0, len(trend) - 1, agent.skip):\\n                action = agent.act(state)\\n                next_state = agent.get_state(t + 1)\\n\\n                if action == 1 and (position == 0 or position == -1):\\n                    if position == -1:\\n                        earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = trend[t]\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):\\n                    if position == 1:\\n                        earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = trend[t]\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n                state = next_state\\n\\n        # Update agent initialization with the new indicators\\n        window_size = 30\\n        skip = 1\\n        initial_money = 6000\\n\\n        model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n        agent = agent_funct.Agent(\\n            model=model,\\n            window_size=window_size,\\n            trend=close,\\n            skip=skip,\\n            initial_money=initial_money,\\n            indicators=indicators\\n        )\\n\\n        # logging.info(\\'Starting training...\\')\\n        agent.fit(iterations=500, checkpoint=10)\\n\\n        # logging.info(\\'Training complete. Starting prediction...\\')\\n        states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n        # Save the model\\n        joblib.dump(model, \\'output/model.pkl\\')\\n\\n        fig = plt.figure(figsize=(15, 5))\\n        plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n        plt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n        # Testing\\n        df2 = pd.read_csv(\\'data/dataFile.csv\\')\\n        df2 = df2.iloc[rows_toTrain + 1:, :].copy()\\n        # Save DataFrame to CSV file\\n        df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n\\n        # Convert test data\\n        for col in df2.columns:\\n            if col != \\'Date\\':\\n                df2[col] = pd.to_numeric(df2[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n        # # Define the list of indicators to be used\\n        # selected_indicators = add_indicators(df2)\\n\\n        # Verify that the indicators are added to the DataFrame\\n        print(\"Columns in DataFrame after adding indicators:\", df2.columns)\\n\\n        # Extract the selected indicators for testing\\n        test_indicators = {indicator: df2[indicator].tolist() for indicator in selected_indicators}\\n        close_test = df2[\\'Close\\'].tolist()\\n\\n        agentTest = agent_funct.Agent(\\n            model=model,\\n            window_size=window_size,\\n            trend=close_test,\\n            skip=skip,\\n            initial_money=initial_money,\\n            indicators=test_indicators\\n        )\\n\\n        states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n        # Aligning the indices for testing data\\n        test = pd.DataFrame()\\n        test[\\'Close\\'] = close_test\\n        test[\\'Signal\\'] = 0\\n        test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n        test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n        test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n        test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n        test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n        # Calculate metrics\\n        ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n        fig = plt.figure(figsize=(15, 5))\\n        plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n        plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n        plt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n        # Save the test results\\n        # joblib.dump(agentTest, \\'agent_test.pkl\\')\\n\\n        # Load and test the saved model\\n        loaded_model = joblib.load(\\'output/model.pkl\\')\\n        loaded_agent = agent_funct.Agent(\\n            model=loaded_model,\\n            window_size=window_size,\\n            trend=close_test,\\n            skip=skip,\\n            initial_money=initial_money,\\n            indicators=test_indicators\\n        )\\n        loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n        # Plot the loaded model results\\n        fig1 = plt.figure(figsize=(15, 5))\\n        plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n        plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n        plt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n        # Perform step-by-step predictions\\n        step_by_step_prediction(agentTest, close_test)\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n    except pd.errors.EmptyDataError:\\n        logging.error(\"No data found in the CSV file.\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()', 'Add the Kalman filter to the indicators, df1[\\'Close\\'], and df2[\\'Close\\'] on the following code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\n# from modules.indicator_utils import convert_to_float, add_indicators  # Import the new module\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(6)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    try:\\n        df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n        logging.info(\"Data loaded successfully.\")\\n\\n        # Check if the DataFrame is empty\\n        if df1.empty:\\n            logging.error(\"The loaded DataFrame is empty.\")\\n            return\\n\\n        rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n        # Copy the records to df1 for training\\n        df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n        # Ensure \\'Date\\' column exists\\n        if \\'Date\\' not in df1.columns:\\n            logging.warning(\"Date column not found. Creating a default date range.\")\\n            df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n        # Convert columns to float, excluding the \\'Date\\' column\\n        for col in df1.columns:\\n            if col != \\'Date\\':\\n                df1[col] = pd.to_numeric(df1[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n        # Handle NaNs\\n        if df1.isna().sum().sum() > 0:\\n            logging.warning(f\\'NaN values found in DataFrame. Filling NaNs with 0.\\')\\n            df1.fillna(0, inplace=True)\\n\\n        logging.info(\"Data preprocessing completed successfully.\")\\n        # # Load and preprocess data\\n        # df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n        # rows_count = (len(df1) + 1)  # get the number of rows on the csv file\\n        # calc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\n        # rows_toTrain = int(rows_count - calc_perc)\\n        #\\n        # # Copy the numbers of records to df1 for training\\n        # df1 = df1.iloc[1:rows_toTrain, :].copy()\\n        #\\n        # # Check if \\'Date\\' column exists, otherwise create it\\n        # if \\'Date\\' not in df1.columns:\\n        #     df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n        #\\n        # # Convert columns to float, excluding the \\'Date\\' column\\n        # for col in df1.columns:\\n        #     if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        #         df1[col] = convert_to_float(df1[col].tolist())\\n        #\\n        # Define the list of indicators to be used\\n        # selected_indicators = add_indicators(df1)\\n\\n        # Verify that the indicators are added to the DataFrame\\n        print(\"Columns in DataFrame after adding indicators:\", df1.columns)\\n\\n        # Extract the selected indicators\\n        indicators = {indicator: df1[indicator].tolist() for indicator in selected_indicators}\\n        close = df1[\\'Close\\'].tolist()\\n\\n        def step_by_step_prediction(agent, trend):\\n            state = agent.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            earned_money = 0\\n\\n            for t in range(0, len(trend) - 1, agent.skip):\\n                action = agent.act(state)\\n                next_state = agent.get_state(t + 1)\\n\\n                if action == 1 and (position == 0 or position == -1):\\n                    if position == -1:\\n                        earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = trend[t]\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):\\n                    if position == 1:\\n                        earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = trend[t]\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n                state = next_state\\n\\n        # Update agent initialization with the new indicators\\n        window_size = 30\\n        skip = 1\\n        initial_money = 6000\\n\\n        model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n        agent = agent_funct.Agent(\\n            model=model,\\n            window_size=window_size,\\n            trend=close,\\n            skip=skip,\\n            initial_money=initial_money,\\n            indicators=indicators\\n        )\\n\\n        # logging.info(\\'Starting training...\\')\\n        agent.fit(iterations=500, checkpoint=10)\\n\\n        # logging.info(\\'Training complete. Starting prediction...\\')\\n        states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n        # Save the model\\n        joblib.dump(model, \\'output/model.pkl\\')\\n\\n        fig = plt.figure(figsize=(15, 5))\\n        plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n        plt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n        # Testing\\n        df2 = pd.read_csv(\\'data/dataFile.csv\\')\\n        df2 = df2.iloc[rows_toTrain + 1:, :].copy()\\n        # Save DataFrame to CSV file\\n        df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n\\n        # Convert test data\\n        for col in df2.columns:\\n            if col != \\'Date\\':\\n                df2[col] = pd.to_numeric(df2[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n        # # Define the list of indicators to be used\\n        # selected_indicators = add_indicators(df2)\\n\\n        # Verify that the indicators are added to the DataFrame\\n        print(\"Columns in DataFrame after adding indicators:\", df2.columns)\\n\\n        # Extract the selected indicators for testing\\n        test_indicators = {indicator: df2[indicator].tolist() for indicator in selected_indicators}\\n        close_test = df2[\\'Close\\'].tolist()\\n\\n        agentTest = agent_funct.Agent(\\n            model=model,\\n            window_size=window_size,\\n            trend=close_test,\\n            skip=skip,\\n            initial_money=initial_money,\\n            indicators=test_indicators\\n        )\\n\\n        states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n        # Aligning the indices for testing data\\n        test = pd.DataFrame()\\n        test[\\'Close\\'] = close_test\\n        test[\\'Signal\\'] = 0\\n        test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n        test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n        test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n        test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n        test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n        # Calculate metrics\\n        ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n        fig = plt.figure(figsize=(15, 5))\\n        plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n        plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n        plt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n        # Save the test results\\n        # joblib.dump(agentTest, \\'agent_test.pkl\\')\\n\\n        # Load and test the saved model\\n        loaded_model = joblib.load(\\'output/model.pkl\\')\\n        loaded_agent = agent_funct.Agent(\\n            model=loaded_model,\\n            window_size=window_size,\\n            trend=close_test,\\n            skip=skip,\\n            initial_money=initial_money,\\n            indicators=test_indicators\\n        )\\n        loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n        # Plot the loaded model results\\n        fig1 = plt.figure(figsize=(15, 5))\\n        plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n        plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n        plt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n        # Perform step-by-step predictions\\n        step_by_step_prediction(agentTest, close_test)\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n    except pd.errors.EmptyDataError:\\n        logging.error(\"No data found in the CSV file.\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()', 'Would it help on the training of Deep Reinforcement Learning', 'would I need it only on the Closing price instead of on the indicators?', 'Add the Kalman filter to the indicators and the trend on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.02\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t18-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t28-Jun-2024:\\tRev. 1.02\\t    Fixed the way indicators are accessed from the incoming JSON data.\\n[JSanchez]\\t28-Jun-2024:\\tRev. 1.02\\t    Improved error handling to check for missing keys in the request data.\\n[JSanchez]\\t28-Jun-2024:\\tRev. 1.02\\t    Added a check to ensure the model is loaded before processing predictions.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nKey Changes Made:\\n\\n    Model Loading: The model is loaded globally, and a check is added to ensure it is loaded before making predictions.\\n    Indicators Handling: Fixed the way indicators are accessed from the incoming JSON data.\\n    Error Handling: Improved error handling to check for missing keys in the request data.\\n    Model Check: Added a check to ensure the model is loaded before processing predictions.\\n\\n\"\"\"\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nimport logging\\n\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\napp = Flask(__name__)\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d: t + 1] if d >= 0 else -d * [trend[0]] + trend[0: t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    if model is None:\\n        return jsonify({\\'error\\': \\'Model not loaded\\'}), 500\\n\\n    try:\\n        data = request.get_json()\\n\\n        # Check for required keys\\n        if \\'trend\\' not in data or \\'indicators\\' not in data or \\'t\\' not in data or \\'window_size\\' not in data:\\n            return jsonify({\\'error\\': \\'Missing required data\\'}), 400\\n\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        # Ensure indicators are correctly formatted\\n        indicators = {indicator: indicators[indicator] for indicator in indicators}  # Use the correct format\\n\\n        t = int(data[\\'t\\'])\\n        window_size = int(data[\\'window_size\\'])\\n\\n        state = get_state(trend, indicators, t, window_size)\\n        decision = model.predict(state)\\n        action = np.argmax(decision[0])\\n\\n        response = {\\'action\\': int(action)}\\n        return jsonify(response)\\n\\n    except KeyError as e:\\n        logging.error(f\"Key error: {e}\")\\n        return jsonify({\\'error\\': \\'Key error\\'}), 400\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n        return jsonify({\\'error\\': \\'An error occurred\\'}), 500\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        app.run(host=\"0.0.0.0\", debug=False, port=5010)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")', 'Add the Kalman filter to the Trend on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.02\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t18-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t28-Jun-2024:\\tRev. 1.02\\t    Fixed the way indicators are accessed from the incoming JSON data.\\n[JSanchez]\\t28-Jun-2024:\\tRev. 1.02\\t    Improved error handling to check for missing keys in the request data.\\n[JSanchez]\\t28-Jun-2024:\\tRev. 1.02\\t    Added a check to ensure the model is loaded before processing predictions.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nKey Changes Made:\\n\\n    Model Loading: The model is loaded globally, and a check is added to ensure it is loaded before making predictions.\\n    Indicators Handling: Fixed the way indicators are accessed from the incoming JSON data.\\n    Error Handling: Improved error handling to check for missing keys in the request data.\\n    Model Check: Added a check to ensure the model is loaded before processing predictions.\\n\\n\"\"\"\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nimport logging\\n\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\napp = Flask(__name__)\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d: t + 1] if d >= 0 else -d * [trend[0]] + trend[0: t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    if model is None:\\n        return jsonify({\\'error\\': \\'Model not loaded\\'}), 500\\n\\n    try:\\n        data = request.get_json()\\n\\n        # Check for required keys\\n        if \\'trend\\' not in data or \\'indicators\\' not in data or \\'t\\' not in data or \\'window_size\\' not in data:\\n            return jsonify({\\'error\\': \\'Missing required data\\'}), 400\\n\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        # Ensure indicators are correctly formatted\\n        indicators = {indicator: indicators[indicator] for indicator in indicators}  # Use the correct format\\n\\n        t = int(data[\\'t\\'])\\n        window_size = int(data[\\'window_size\\'])\\n\\n        state = get_state(trend, indicators, t, window_size)\\n        decision = model.predict(state)\\n        action = np.argmax(decision[0])\\n\\n        response = {\\'action\\': int(action)}\\n        return jsonify(response)\\n\\n    except KeyError as e:\\n        logging.error(f\"Key error: {e}\")\\n        return jsonify({\\'error\\': \\'Key error\\'}), 400\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n        return jsonify({\\'error\\': \\'An error occurred\\'}), 500\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        app.run(host=\"0.0.0.0\", debug=False, port=5010)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")', 'Add the Kalman filter to the df1[\\'Close\\'] on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added logging to track the execution flow and errors\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added ltry-except blocks around file operations and data processing to\\n                                            catch specific exceptions like FileNotFoundError and\\n                                            pd.errors.EmptyDataError\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added Informative logging messages to indicate successful operations\\n                                            and errors.\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added to ensure that the scalers are fitted with the re-train data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nLogging Configuration: Set up logging to track the execution flow and errors. The log messages include timestamps\\nand severity levels.\\n\\nTry-Except Blocks: Added try-except blocks around file operations and data processing to catch specific exceptions\\nlike FileNotFoundError and pd.errors.EmptyDataError. A general exception handler is also included to catch any\\nother unexpected errors.\\n\\nLogging Messages: Informative logging messages are added to indicate successful operations and errors.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers: The code now saves the fitted scalers instead of new instances, preventing the \"not fitted\"\\nerror when you attempt to use them.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\n# from sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef run_retrading_simulation(model):\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    try:\\n        # Load and preprocess data\\n        df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n        logging.info(\"Data loaded successfully.\")\\n\\n        # Check if the DataFrame is empty\\n        if df1.empty:\\n            logging.error(\"The loaded DataFrame is empty.\")\\n            return\\n\\n        # Calculate the number of rows to train\\n        rows_count = len(df1)\\n        rows_toTrain = int(rows_count * 0.10)  # Use last 10% of the data for training\\n        df1 = df1.iloc[-rows_toTrain:].copy()  # Select the last 10%\\n\\n        # Ensure \\'Date\\' column exists\\n        if \\'Date\\' not in df1.columns:\\n            logging.warning(\"Date column not found. Creating a default date range.\")\\n            df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n        # Convert columns to float, excluding the \\'Date\\' column\\n        for col in df1.columns:\\n            if col != \\'Date\\':\\n                df1[col] = pd.to_numeric(df1[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n        # Handle NaNs\\n        if df1.isna().sum().sum() > 0:\\n            logging.warning(f\\'NaN values found in DataFrame. Filling NaNs with 0.\\')\\n            df1.fillna(0, inplace=True)\\n\\n        logging.info(\"Data preprocessing completed successfully.\")\\n\\n        # Extract the selected indicators\\n        indicators = {indicator: df1[indicator].tolist() for indicator in selected_indicators}\\n        close = df1[\\'Close\\'].tolist()\\n\\n        # Initialize agent\\n        window_size, skip, initial_money = 30, 1, 10000\\n        agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                                  indicators=indicators)\\n\\n        # Train the agent\\n        agent.fit(iterations=500, checkpoint=10)\\n        logging.info(\"Agent training completed successfully.\")\\n\\n        # Perform predictions\\n        states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n        # Save the updated model\\n        joblib.dump(model, \\'output/model.pkl\\')\\n        logging.info(\"Model saved successfully.\")\\n\\n        # Plotting results\\n        plt.figure(figsize=(15, 5))\\n        plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n        plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n    except pd.errors.EmptyDataError:\\n        logging.error(\"No data found in the CSV file.\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        run_retrading_simulation(model)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\") ', 'ERROR - An error occurred: index 30 is out of bounds for axis 0 with size 30\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 28, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t28-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.01\\t    Added the kalman_filter function is defined to apply the Kalman Filter\\n                                            to the data.\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.01\\t    Added the Kalman Filter is applied to the trend data before processing\\n                                            it for predictions.\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.01\\t    Added the Kalman Filter is also applied to the indicators to ensure they\\n                                            are smoothed before being used in the model.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nKey Changes Made\\n    Kalman Filter Function: A kalman_filter function is defined to apply the Kalman Filter to the data.\\n    Application of Kalman Filter: The Kalman Filter is applied to the trend data before processing it for predictions.\\n    Indicators Processing: The Kalman Filter is also applied to the indicators to ensure they are smoothed before being\\n    used in the model.\\n\\n\"\"\"\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nimport logging\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\napp = Flask(__name__)\\n\\n# Kalman Filter implementation\\ndef kalman_filter(data):\\n    n_iter = len(data)\\n    sz = (n_iter,)  # size of array\\n    q = 1e-5  # process variance\\n    r = 0.1**2  # measurement variance\\n    xhat = np.zeros(sz)  # a posteriori estimate of x\\n    P = np.zeros(sz)  # a posteriori error estimate\\n    xhat[0] = data[0]  # initial state\\n    P[0] = 1.0  # initial error estimate\\n\\n    for k in range(1, n_iter):\\n        # Prediction update\\n        xhatminus = xhat[k-1]  # predicted state\\n        Pminus = P[k-1] + q  # predicted error estimate\\n\\n        # Measurement update\\n        K = Pminus / (Pminus + r)  # Kalman gain\\n        xhat[k] = xhatminus + K * (data[k] - xhatminus)  # updated state\\n        P[k] = (1 - K) * Pminus  # updated error estimate\\n\\n    return xhat\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d: t + 1] if d >= 0 else -d * [trend[0]] + trend[0: t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    if model is None:\\n        return jsonify({\\'error\\': \\'Model not loaded\\'}), 500\\n\\n    try:\\n        data = request.get_json()\\n\\n        # Check for required keys\\n        if \\'trend\\' not in data or \\'indicators\\' not in data or \\'t\\' not in data or \\'window_size\\' not in data:\\n            return jsonify({\\'error\\': \\'Missing required data\\'}), 400\\n\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        # Apply Kalman Filter to the trend\\n        trend = kalman_filter(trend)\\n\\n        # Ensure indicators are correctly formatted\\n        indicators = {indicator: indicators[indicator] for indicator in indicators}  # Use the correct format\\n\\n        # Apply Kalman Filter to indicators\\n        indicators = {indicator: kalman_filter(indicators[indicator]) for indicator in indicators}\\n\\n        t = int(data[\\'t\\'])\\n        window_size = int(data[\\'window_size\\'])\\n\\n        state = get_state(trend, indicators, t, window_size)\\n        decision = model.predict(state)\\n        action = np.argmax(decision[0])\\n\\n        response = {\\'action\\': int(action)}\\n        return jsonify(response)\\n\\n    except KeyError as e:\\n        logging.error(f\"Key error: {e}\")\\n        return jsonify({\\'error\\': \\'Key error\\'}), 400\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n        return jsonify({\\'error\\': \\'An error occurred\\'}), 500\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        app.run(host=\"0.0.0.0\", debug=False, port=5010)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")\\n', 'Check error\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nimport logging\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\napp = Flask(__name__)\\n\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    try:\\n        data = request.get_json()\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        indicators = {indicator: df1[indicator].tolist() for indicator in selected_indicators}\\n\\n        # Now indicators are scaled and flattened\\n        t = int(data[\\'t\\'])\\n        window_size = int(data[\\'window_size\\'])\\n\\n        state = get_state(trend, indicators, t, window_size)\\n        decision = model.predict(state)\\n        action = np.argmax(decision[0])\\n\\n        response = {\\'action\\': int(action)}\\n        return jsonify(response)\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n        return jsonify({\\'error\\': \\'File not found\\'}), 500\\n    except KeyError as e:\\n        logging.error(f\"Key error: {e}\")\\n        return jsonify({\\'error\\': \\'Key error\\'}), 400\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n        return jsonify({\\'error\\': \\'An error occurred\\'}), 500\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        app.run(host=\"0.0.0.0\", debug=False, port=5010)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")', 'Fix the errors\\n\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nimport logging\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\napp = Flask(__name__)\\n\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    try:\\n        data = request.get_json()\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        indicators = {indicator: data[\\'indicators\\'].tolist() for indicator in indicators}\\n\\n        # Now indicators are scaled and flattened\\n        t = int(data[\\'t\\'])\\n        window_size = int(data[\\'window_size\\'])\\n\\n        state = get_state(trend, indicators, t, window_size)\\n        decision = model.predict(state)\\n        action = np.argmax(decision[0])\\n\\n        response = {\\'action\\': int(action)}\\n        return jsonify(response)\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n        return jsonify({\\'error\\': \\'File not found\\'}), 500\\n    except KeyError as e:\\n        logging.error(f\"Key error: {e}\")\\n        return jsonify({\\'error\\': \\'Key error\\'}), 400\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n        return jsonify({\\'error\\': \\'An error occurred\\'}), 500\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        app.run(host=\"0.0.0.0\", debug=False, port=5010)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")', 'On training the code does not let go of a sell or buy on time when the market turns around\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 29, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.01\\t    Added try-except blocks in critical areas to catch and log errors.\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.01\\t    Created a method map_action_to_string to improve readability and\\n                                            maintainability.\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.01\\t    Introduced a method update_position to handle position updates based on\\n                                            actions.\\n------------------------------------------------------------------------------------------------------------------------\\n\\n\\nKey Changes Made\\n    Exception Handling: Added try-except blocks in critical areas to catch and log errors.\\n    Action Mapping: Created a method map_action_to_string to improve readability and maintainability.\\n    Position Management: Introduced a method update_position to handle position updates based on actions.\\n    State Recording: Created a method record_action_states to encapsulate the logic for recording states based on\\n    actions.\\n    Final Position Handling: Separated the logic for finalizing positions into a method finalize_position for clarity.\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent:\\n    POPULATION_SIZE = 15\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        self.model = model\\n        self.window_size = window_size\\n        self.trend = trend\\n        self.skip = skip\\n        self.initial_money = initial_money\\n        self.indicators = indicators\\n        self.es = deep.Deep_Evolution_Strategy(\\n            self.model.get_weights(),\\n            self.get_tick_reward,\\n            self.POPULATION_SIZE,\\n            self.SIGMA,\\n            self.LEARNING_RATE,\\n        )\\n\\n    def act(self, sequence, position, training=False):\\n        decision = self.model.predict(np.array(sequence))\\n        # Hold,Long,Short,Close Long,Close Short,Stand By\\n        if position == 0:\\n            mask = [0, 1, 1, 0, 0, 1]  # Allow Long, Short, StandBy\\n        elif position == 1:\\n            mask = [1, 0, 1, 1, 0, 0]  # Allow Hold, Close Long\\n        elif position == -1:\\n            mask = [1, 1, 0, 0, 1, 0]  # Allow Hold, Close Short\\n        masked_decision = np.multiply(decision[0], mask)\\n        return np.argmax(masked_decision)\\n\\n    def get_state(self, t):\\n        window_size = self.window_size + 1\\n        d = t - window_size + 1\\n        block = self.trend[d: t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0: t + 1]\\n        res = []\\n        for i in range(window_size - 1):\\n            res.append(block[i + 1] - block[i])\\n        indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n        state = np.array([res + indicators_state])\\n        return state\\n\\n    def predict(self):\\n        actions = []\\n        position = 0\\n        for t in range(len(self.trend) - self.window_size):\\n            state = self.get_state(t)\\n            action_index = self.act(state, position)\\n            if action_index == 0:\\n                actions.append(\\'Hold\\')\\n            elif action_index == 1:\\n                actions.append(\\'Buy\\')\\n                position = 1\\n            elif action_index == 2:\\n                actions.append(\\'Sell\\')\\n                position = -1\\n            elif action_index == 3:\\n                actions.append(\\'Close Long\\')\\n                position = 0\\n            elif action_index == 4:\\n                actions.append(\\'Close Short\\')\\n                position = 0\\n            elif action_index == 5:\\n                actions.append(\\'StandBy\\')\\n        return actions\\n\\n    def get_tick_reward(self, weights):\\n        initial_money = self.initial_money\\n        earned_money = initial_money\\n        self.model.weights = weights\\n        state = self.get_state(0)\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and position == 0:  # Long\\n                position = 1\\n                bought_price = self.trend[t]\\n\\n            elif action == 3 and position == 1:  # Close Long\\n                earned_money += (self.trend[t] - bought_price) * 4 * 12.5 - 5.8\\n                position = 0\\n\\n            elif action == 2 and position == 0:  # Short\\n                position = -1\\n                bought_price = self.trend[t]\\n\\n            elif action == 4 and position == -1:  # Close Short\\n                earned_money += (bought_price - self.trend[t]) * 4 * 12.5 - 5.8\\n                position = 0\\n\\n            state = next_state\\n\\n        # Close any open position at the end\\n        if position == 1:  # Close Long\\n            earned_money += (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n\\n        elif position == -1:  # Close Short\\n            earned_money += (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n\\n        return ((earned_money - initial_money) / initial_money) * 100\\n\\n    def fit(self, iterations, checkpoint):\\n        self.es.train(iterations, print_every=checkpoint)\\n\\n    def buy_tick(self):\\n        initial_money = self.initial_money\\n        state = self.get_state(0)\\n        states_long = []\\n        states_short = []\\n        states_hold = []\\n        states_close_long = []\\n        states_close_short = []\\n        states_stand_by = []\\n\\n        earned_money = 0\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and position == 0:  # Long\\n                position = 1\\n                bought_price = self.trend[t]\\n                states_long.append(t)\\n                print(f\\'day {t}: buy 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 3 and position == 1:  # Close Long\\n                earned_money += (self.trend[t] - bought_price) * 4 * 12.5 - 5.8\\n                position = 0\\n                states_close_long.append(t)\\n                print(f\\'day {t}: close long at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 2 and position == 0:  # Short\\n                position = -1\\n                bought_price = self.trend[t]\\n                states_short.append(t)\\n                print(f\\'day {t}: sell 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 4 and position == -1:  # Close Short\\n                earned_money += (bought_price - self.trend[t]) * 4 * 12.5 - 5.8\\n                position = 0\\n                states_close_short.append(t)\\n                print(f\\'day {t}: close short at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 0 and position != 0:  # Hold\\n                states_hold.append(t)\\n                print(f\\'day {t}: hold at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 5 and position == 0:  # StandBy\\n                states_stand_by.append(t)\\n                print(f\\'day {t}: standby at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            state = next_state\\n\\n        # Close any open position at the end\\n        if position == 1:  # Close Long\\n            earned_money += (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n            states_close_long.append(len(self.trend) - 1)\\n            print(f\\'final day: close long at price {self.trend[-1]}, total balance {earned_money}\\')\\n\\n        elif position == -1:  # Close Short\\n            earned_money += (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n            states_close_short.append(len(self.trend) - 1)\\n            print(f\\'final day: close short at price {self.trend[-1]}, total balance {earned_money}\\')\\n\\n        roi = (earned_money / initial_money) * 100\\n        total_gains = earned_money\\n        print(f\\'Total Gains: {total_gains}, ROI: {roi}%\\')\\n        return states_long, states_short, states_hold, states_close_long, states_close_short, states_stand_by, total_gains, roi\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n        max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n        return ticks_earned, max_drawdown, start_idx, end_idx\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        peak = cumulative_profit[0]\\n        max_drawdown = 0\\n        start_idx, end_idx = 0, 0\\n\\n        for i in range(1, len(cumulative_profit)):\\n            if cumulative_profit[i] > peak:\\n                peak = cumulative_profit[i]\\n            drawdown = peak - cumulative_profit[i]\\n            if drawdown > max_drawdown:\\n                max_drawdown = drawdown\\n                start_idx, end_idx = peak, cumulative_profit[i]\\n        return max_drawdown, start_idx, end_idx\\n', 'On the Market Reversal Handling, can we make it to act quicker', 'Add a logic to handle market reversals more effectively by ensuring positions are closed on timely matter when necessary\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 29, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.01\\t    Added try-except blocks in critical areas to catch and log errors.\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.01\\t    Created a method map_action_to_string to improve readability and\\n                                            maintainability.\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.01\\t    Introduced a method update_position to handle position updates based on\\n                                            actions.\\n------------------------------------------------------------------------------------------------------------------------\\n\\n\\nKey Changes Made\\n    Exception Handling: Added try-except blocks in critical areas to catch and log errors.\\n    Action Mapping: Created a method map_action_to_string to improve readability and maintainability.\\n    Position Management: Introduced a method update_position to handle position updates based on actions.\\n    State Recording: Created a method record_action_states to encapsulate the logic for recording states based on\\n    actions.\\n    Final Position Handling: Separated the logic for finalizing positions into a method finalize_position for clarity.\\n    Market Reversal Handling: Added logic to handle market reversals more effectively by ensuring positions are closed\\n    when necessary.\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent:\\n    POPULATION_SIZE = 15\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        self.model = model\\n        self.window_size = window_size\\n        self.trend = trend\\n        self.skip = skip\\n        self.initial_money = initial_money\\n        self.indicators = indicators\\n        self.es = deep.Deep_Evolution_Strategy(\\n            self.model.get_weights(),\\n            self.get_tick_reward,\\n            self.POPULATION_SIZE,\\n            self.SIGMA,\\n            self.LEARNING_RATE,\\n        )\\n\\n    def act(self, sequence, position, training=False):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            # Hold, Long, Short, Close Long, Close Short, Stand By\\n            if position == 0:\\n                mask = [0, 1, 1, 0, 0, 1]  # Allow Long, Short, StandBy\\n            elif position == 1:\\n                mask = [1, 0, 1, 1, 0, 0]  # Allow Hold, Close Long\\n            elif position == -1:\\n                mask = [1, 1, 0, 0, 1, 0]  # Allow Hold, Close Short\\n            masked_decision = np.multiply(decision[0], mask)\\n            return np.argmax(masked_decision)\\n        except Exception as e:\\n            logging.error(f\"Error in action decision: {e}\")\\n            return 0  # Default to Hold if there\\'s an error\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n            block = self.trend[d: t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0: t + 1]\\n            res = [block[i + 1] - block[i] for i in range(window_size - 1)]\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n            return state\\n        except IndexError as e:\\n            logging.error(f\"Index error in get_state: {e}\")\\n            return np.zeros((1, len(self.indicators) + self.window_size - 1))  # Return a zero state\\n        except Exception as e:\\n            logging.error(f\"Unexpected error in get_state: {e}\")\\n            return np.zeros((1, len(self.indicators) + self.window_size - 1))  # Return a zero state\\n\\n    def predict(self):\\n        actions = []\\n        position = 0\\n        for t in range(len(self.trend) - self.window_size):\\n            state = self.get_state(t)\\n            action_index = self.act(state, position)\\n            actions.append(self.map_action_to_string(action_index))\\n            position = self.update_position(action_index, position)\\n        return actions\\n\\n    def map_action_to_string(self, action_index):\\n        action_map = {\\n            0: \\'Hold\\',\\n            1: \\'Buy\\',\\n            2: \\'Sell\\',\\n            3: \\'Close Long\\',\\n            4: \\'Close Short\\',\\n            5: \\'StandBy\\'\\n        }\\n        return action_map.get(action_index, \\'Hold\\')\\n\\n    def update_position(self, action_index, current_position):\\n        try:\\n            if action_index == 1:  # Buy\\n                return 1\\n            elif action_index == 2:  # Sell\\n                return -1\\n            elif action_index in [3, 4]:  # Close Long or Close Short\\n                return 0\\n            return current_position  # Hold or StandBy\\n        except Exception as e:\\n            logging.error(f\"Error updating position: {e}\")\\n            return current_position  # Return current position in case of error\\n\\n    def get_tick_reward(self, weights):\\n        initial_money = self.initial_money\\n        earned_money = initial_money\\n        self.model.weights = weights\\n        state = self.get_state(0)\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and position == 0:  # Long\\n                position = 1\\n                bought_price = self.trend[t]\\n\\n            elif action == 3 and position == 1:  # Close Long\\n                earned_money += (self.trend[t] - bought_price) * 4 * 12.5 - 5.8\\n                position = 0\\n\\n            elif action == 2 and position == 0:  # Short\\n                position = -1\\n                bought_price = self.trend[t]\\n\\n            elif action == 4 and position == -1:  # Close Short\\n                earned_money += (bought_price - self.trend[t]) * 4 * 12.5 - 5.8\\n                position = 0\\n\\n            state = next_state\\n\\n        # Close any open position at the end\\n        if position == 1:  # Close Long\\n            earned_money += (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n\\n        elif position == -1:  # Close Short\\n            earned_money += (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n\\n        return ((earned_money - initial_money) / initial_money) * 100\\n\\n    def fit(self, iterations, checkpoint):\\n        self.es.train(iterations, print_every=checkpoint)\\n\\n    def buy_tick(self):\\n        initial_money = self.initial_money\\n        state = self.get_state(0)\\n        states_long = []\\n        states_short = []\\n        states_hold = []\\n        states_close_long = []\\n        states_close_short = []\\n        states_stand_by = []\\n\\n        earned_money = initial_money\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and position == 0:  # Long\\n                position = 1\\n                bought_price = self.trend[t]\\n                states_long.append(t)\\n                print(f\\'day {t}: buy 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 3 and position == 1:  # Close Long\\n                earned_money += (self.trend[t] - bought_price) * 4 * 12.5 - 5.8\\n                position = 0\\n                states_close_long.append(t)\\n                print(f\\'day {t}: close long at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 2 and position == 0:  # Short\\n                position = -1\\n                bought_price = self.trend[t]\\n                states_short.append(t)\\n                print(f\\'day {t}: sell 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 4 and position == -1:  # Close Short\\n                earned_money += (bought_price - self.trend[t]) * 4 * 12.5 - 5.8\\n                position = 0\\n                states_close_short.append(t)\\n                print(f\\'day {t}: close short at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 0 and position != 0:  # Hold\\n                states_hold.append(t)\\n                print(f\\'day {t}: hold at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 5 and position == 0:  # StandBy\\n                states_stand_by.append(t)\\n                print(f\\'day {t}: standby at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            state = next_state\\n\\n        # Close any open position at the end\\n        if position == 1:  # Close Long\\n            earned_money += (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n            states_close_long.append(len(self.trend) - 1)\\n            print(f\\'final day: close long at price {self.trend[-1]}, total balance {earned_money}\\')\\n\\n        elif position == -1:  # Close Short\\n            earned_money += (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n            states_close_short.append(len(self.trend) - 1)\\n            print(f\\'final day: close short at price {self.trend[-1]}, total balance {earned_money}\\')\\n\\n        roi = (earned_money / initial_money) * 100\\n        total_gains = earned_money - initial_money\\n        print(f\\'Total Gains: {total_gains}, ROI: {roi}%\\')\\n        return states_long, states_short, states_hold, states_close_long, states_close_short, states_stand_by, total_gains, roi\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error calculating metrics: {e}\")\\n            return 0, 0, 0, 0  # Return default values in case of error\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit[i] > peak:\\n                    peak = cumulative_profit[i]\\n                drawdown = peak - cumulative_profit[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error calculating max drawdown: {e}\")\\n            return 0, 0, 0  # Return default values in case of error', 'Add a logic to handle market reversals more effectively by ensuring positions are closed on timely matter when necessary\\n\\n\\nimport numpy as np\\nimport logging\\nimport time\\n\\nclass Deep_Evolution_Strategy:\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        try:\\n            weights_population = []\\n            for index, i in enumerate(population):\\n                jittered = self.sigma * i\\n                weights_population.append(weights[index] + jittered)\\n            return weights_population\\n        except Exception as e:\\n            logging.error(f\"Error in getting weights from population: {e}\")\\n            return weights  # Return original weights in case of error\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def adaptive_sigma(self, rewards):\\n        \"\"\"Adjust sigma based on the mean and standard deviation of rewards.\"\"\"\\n        mean_reward = np.mean(rewards)\\n        std_reward = np.std(rewards)\\n        if std_reward > 0:\\n            self.sigma *= 1 + (mean_reward / std_reward)  # Adjust sigma based on performance\\n            # Cap sigma to prevent it from becoming too large\\n            self.sigma = min(self.sigma, 1.0)  # Example cap\\n\\n    def train(self, epoch=100, print_every=1):\\n        lasttime = time.time()\\n        for i in range(epoch):\\n            logging.debug(f\\'Starting epoch {i + 1}/{epoch}\\')\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = []\\n                for w in self.weights:\\n                    x.append(np.random.randn(*w.shape))\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            self.adaptive_sigma(rewards)  # Adjust sigma based on rewards\\n\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] = w + (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T,\\n                                                                                                              rewards).T\\n            if (i + 1) % print_every == 0:\\n                logging.info(f\\'iter {i + 1}. reward: {self.reward_function(self.weights)}\\')\\n        logging.info(\\'time taken to train: %s seconds\\', time.time() - lasttime)\\n\\n', 'What will be the more effective way to detect market reversal and once is detected Close Long or the Short position if you currently on one of them, on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 29, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.01\\t    Added try-except blocks in critical areas to catch and log errors.\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.01\\t    Created a method map_action_to_string to improve readability and\\n                                            maintainability.\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.01\\t    Introduced a method update_position to handle position updates based on\\n                                            actions.\\n------------------------------------------------------------------------------------------------------------------------\\n\\n\\nKey Changes Made\\n    Exception Handling: Added try-except blocks in critical areas to catch and log errors.\\n    Action Mapping: Created a method map_action_to_string to improve readability and maintainability.\\n    Position Management: Introduced a method update_position to handle position updates based on actions.\\n    State Recording: Created a method record_action_states to encapsulate the logic for recording states based on\\n    actions.\\n    Final Position Handling: Separated the logic for finalizing positions into a method finalize_position for clarity.\\n    Market Reversal Handling: Added logic to handle market reversals more effectively by ensuring positions are closed\\n    when necessary.\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent:\\n    POPULATION_SIZE = 15\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        self.model = model\\n        self.window_size = window_size\\n        self.trend = trend\\n        self.skip = skip\\n        self.initial_money = initial_money\\n        self.indicators = indicators\\n        self.es = deep.Deep_Evolution_Strategy(\\n            self.model.get_weights(),\\n            self.get_tick_reward,\\n            self.POPULATION_SIZE,\\n            self.SIGMA,\\n            self.LEARNING_RATE,\\n        )\\n\\n    def act(self, sequence, position, training=False):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            # Hold, Long, Short, Close Long, Close Short, Stand By\\n            if position == 0:\\n                mask = [0, 1, 1, 0, 0, 1]  # Allow Long, Short, StandBy\\n            elif position == 1:\\n                mask = [1, 0, 1, 1, 0, 0]  # Allow Hold, Close Long\\n            elif position == -1:\\n                mask = [1, 1, 0, 0, 1, 0]  # Allow Hold, Close Short\\n            masked_decision = np.multiply(decision[0], mask)\\n            return np.argmax(masked_decision)\\n        except Exception as e:\\n            logging.error(f\"Error in action decision: {e}\")\\n            return 0  # Default to Hold if there\\'s an error\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n            block = self.trend[d: t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0: t + 1]\\n            res = [block[i + 1] - block[i] for i in range(window_size - 1)]\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n            return state\\n        except IndexError as e:\\n            logging.error(f\"Index error in get_state: {e}\")\\n            return np.zeros((1, len(self.indicators) + self.window_size - 1))  # Return a zero state\\n        except Exception as e:\\n            logging.error(f\"Unexpected error in get_state: {e}\")\\n            return np.zeros((1, len(self.indicators) + self.window_size - 1))  # Return a zero state\\n\\n    def predict(self):\\n        actions = []\\n        position = 0\\n        for t in range(len(self.trend) - self.window_size):\\n            state = self.get_state(t)\\n            action_index = self.act(state, position)\\n            actions.append(self.map_action_to_string(action_index))\\n            position = self.update_position(action_index, position)\\n        return actions\\n\\n    def map_action_to_string(self, action_index):\\n        action_map = {\\n            0: \\'Hold\\',\\n            1: \\'Buy\\',\\n            2: \\'Sell\\',\\n            3: \\'Close Long\\',\\n            4: \\'Close Short\\',\\n            5: \\'StandBy\\'\\n        }\\n        return action_map.get(action_index, \\'Hold\\')\\n\\n    def update_position(self, action_index, current_position):\\n        try:\\n            if action_index == 1:  # Buy\\n                return 1\\n            elif action_index == 2:  # Sell\\n                return -1\\n            elif action_index in [3, 4]:  # Close Long or Close Short\\n                return 0\\n            return current_position  # Hold or StandBy\\n        except Exception as e:\\n            logging.error(f\"Error updating position: {e}\")\\n            return current_position  # Return current position in case of error\\n\\n    def get_tick_reward(self, weights):\\n        initial_money = self.initial_money\\n        earned_money = initial_money\\n        self.model.weights = weights\\n        state = self.get_state(0)\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and position == 0:  # Long\\n                position = 1\\n                bought_price = self.trend[t]\\n\\n            elif action == 3 and position == 1:  # Close Long\\n                earned_money += (self.trend[t] - bought_price) * 4 * 12.5 - 5.8\\n                position = 0\\n\\n            elif action == 2 and position == 0:  # Short\\n                position = -1\\n                bought_price = self.trend[t]\\n\\n            elif action == 4 and position == -1:  # Close Short\\n                earned_money += (bought_price - self.trend[t]) * 4 * 12.5 - 5.8\\n                position = 0\\n\\n            state = next_state\\n\\n        # Close any open position at the end\\n        if position == 1:  # Close Long\\n            earned_money += (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n\\n        elif position == -1:  # Close Short\\n            earned_money += (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n\\n        return ((earned_money - initial_money) / initial_money) * 100\\n\\n    def fit(self, iterations, checkpoint):\\n        self.es.train(iterations, print_every=checkpoint)\\n\\n    def buy_tick(self):\\n        initial_money = self.initial_money\\n        state = self.get_state(0)\\n        states_long = []\\n        states_short = []\\n        states_hold = []\\n        states_close_long = []\\n        states_close_short = []\\n        states_stand_by = []\\n\\n        earned_money = initial_money\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and position == 0:  # Long\\n                position = 1\\n                bought_price = self.trend[t]\\n                states_long.append(t)\\n                print(f\\'day {t}: buy 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 3 and position == 1:  # Close Long\\n                earned_money += (self.trend[t] - bought_price) * 4 * 12.5 - 5.8\\n                position = 0\\n                states_close_long.append(t)\\n                print(f\\'day {t}: close long at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 2 and position == 0:  # Short\\n                position = -1\\n                bought_price = self.trend[t]\\n                states_short.append(t)\\n                print(f\\'day {t}: sell 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 4 and position == -1:  # Close Short\\n                earned_money += (bought_price - self.trend[t]) * 4 * 12.5 - 5.8\\n                position = 0\\n                states_close_short.append(t)\\n                print(f\\'day {t}: close short at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 0 and position != 0:  # Hold\\n                states_hold.append(t)\\n                print(f\\'day {t}: hold at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 5 and position == 0:  # StandBy\\n                states_stand_by.append(t)\\n                print(f\\'day {t}: standby at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            state = next_state\\n\\n        # Close any open position at the end\\n        if position == 1:  # Close Long\\n            earned_money += (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n            states_close_long.append(len(self.trend) - 1)\\n            print(f\\'final day: close long at price {self.trend[-1]}, total balance {earned_money}\\')\\n\\n        elif position == -1:  # Close Short\\n            earned_money += (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n            states_close_short.append(len(self.trend) - 1)\\n            print(f\\'final day: close short at price {self.trend[-1]}, total balance {earned_money}\\')\\n\\n        roi = (earned_money / initial_money) * 100\\n        total_gains = earned_money - initial_money\\n        print(f\\'Total Gains: {total_gains}, ROI: {roi}%\\')\\n        return states_long, states_short, states_hold, states_close_long, states_close_short, states_stand_by, total_gains, roi\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error calculating metrics: {e}\")\\n            return 0, 0, 0, 0  # Return default values in case of error\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit[i] > peak:\\n                    peak = cumulative_profit[i]\\n                drawdown = peak - cumulative_profit[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error calculating max drawdown: {e}\")\\n            return 0, 0, 0  # Return default values in case of error', \"these are the indicators i use to train the deep reinforcement learning \\nIndex(['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'rsi', 'cci', 'adx',\\n       'momentum', 'ppoValue', 'ppoSmoothed', 'QQESmoothed', 'QQEFast',\\n       'QQESlow', 'QQEDiff1', 'QQEDiff2', 'QQEDiff3'],\\n      dtype='object')\\nthe test doesn't give me good results, what should I use for a better results\", \"these are the indicators i use to train the deep reinforcement learning on a 6-Range Index(['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'rsi', 'cci', 'adx', 'momentum', 'ppoValue', 'ppoSmoothed', 'QQESmoothed', 'QQEFast', 'QQESlow', 'QQEDiff1', 'QQEDiff2', 'QQEDiff3'], dtype='object') the test doesn't give me good results, what should I use for a better results\", \"I can't use the bollinger what else can i use\", \"What is wrong with the following code:\\n\\nclass Agent:\\n    POPULATION_SIZE = 15\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        self.model = model\\n        self.window_size = window_size\\n        self.trend = trend\\n        self.skip = skip\\n        self.initial_money = initial_money\\n        self.indicators = indicators\\n        self.es = Deep_Evolution_Strategy(\\n            self.model.get_weights(),\\n            self.get_tick_reward,\\n            self.POPULATION_SIZE,\\n            self.SIGMA,\\n            self.LEARNING_RATE,\\n        )\\n\\n    def act(self, sequence, position, training=False):\\n        decision = self.model.predict(np.array(sequence))\\n        # Hold,Long,Short,Close Long,Close Short,Stand By\\n        if position == 0:\\n            mask = [0, 1, 1, 0, 0, 1]  # Allow Long, Short, StandBy\\n        elif position == 1:\\n            mask = [1, 0, 1, 1, 0, 0]  # Allow Hold, Close Long\\n        elif position == -1:\\n            mask = [1, 1, 0, 0, 1, 0]  # Allow Hold, Close Short\\n        masked_decision = np.multiply(decision[0], mask)\\n        return np.argmax(masked_decision)\\n\\n    def get_state(self, t):\\n        window_size = self.window_size + 1\\n        d = t - window_size + 1\\n        block = self.trend[d: t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0: t + 1]\\n        res = []\\n        for i in range(window_size - 1):\\n            res.append(block[i + 1] - block[i])\\n        indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n        state = np.array([res + indicators_state])\\n        return state\\n\\n    def predict(self):\\n        actions = []\\n        position = 0\\n        for t in range(len(self.trend) - self.window_size):\\n            state = self.get_state(t)\\n            action_index = self.act(state, position)\\n            if action_index == 0:\\n                actions.append('Hold')\\n            elif action_index == 1:\\n                actions.append('Buy')\\n                position = 1\\n            elif action_index == 2:\\n                actions.append('Sell')\\n                position = -1\\n            elif action_index == 3:\\n                actions.append('Close Long')\\n                position = 0\\n            elif action_index == 4:\\n                actions.append('Close Short')\\n                position = 0\\n            elif action_index == 5:\\n                actions.append('StandBy')\\n        return actions\\n\\n    def get_tick_reward(self, weights):\\n        initial_money = self.initial_money\\n        earned_money = initial_money\\n        self.model.weights = weights\\n        state = self.get_state(0)\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and position == 0:  # Long\\n                position = 1\\n                bought_price = self.trend[t]\\n\\n            elif action == 3 and position == 1:  # Close Long\\n                earned_money += (self.trend[t] - bought_price) * 4 * 12.5 - 5.8\\n                position = 0\\n\\n            elif action == 2 and position == 0:  # Short\\n                position = -1\\n                bought_price = self.trend[t]\\n\\n            elif action == 4 and position == -1:  # Close Short\\n                earned_money += (bought_price - self.trend[t]) * 4 * 12.5 - 5.8\\n                position = 0\\n\\n            state = next_state\\n\\n        # Close any open position at the end\\n        if position == 1:  # Close Long\\n            earned_money += (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n\\n        elif position == -1:  # Close Short\\n            earned_money += (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n\\n        return ((earned_money - initial_money) / initial_money) * 100\\n\\n    def fit(self, iterations, checkpoint):\\n        self.es.train(iterations, print_every=checkpoint)\\n\\n    def buy_tick(self):\\n        initial_money = self.initial_money\\n        state = self.get_state(0)\\n        states_long = []\\n        states_short = []\\n        states_hold = []\\n        states_close_long = []\\n        states_close_short = []\\n        states_stand_by = []\\n\\n        earned_money = 0\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and position == 0:  # Long\\n                position = 1\\n                bought_price = self.trend[t]\\n                states_long.append(t)\\n                print(f'day {t}: buy 1 contract at price {self.trend[t]}, total balance {earned_money}')\\n\\n            elif action == 3 and position == 1:  # Close Long\\n                earned_money += (self.trend[t] - bought_price) * 4 * 12.5 - 5.8\\n                position = 0\\n                states_close_long.append(t)\\n                print(f'day {t}: close long at price {self.trend[t]}, total balance {earned_money}')\\n\\n            elif action == 2 and position == 0:  # Short\\n                position = -1\\n                bought_price = self.trend[t]\\n                states_short.append(t)\\n                print(f'day {t}: sell 1 contract at price {self.trend[t]}, total balance {earned_money}')\\n\\n            elif action == 4 and position == -1:  # Close Short\\n                earned_money += (bought_price - self.trend[t]) * 4 * 12.5 - 5.8\\n                position = 0\\n                states_close_short.append(t)\\n                print(f'day {t}: close short at price {self.trend[t]}, total balance {earned_money}')\\n\\n            elif action == 0 and position != 0:  # Hold\\n                states_hold.append(t)\\n                print(f'day {t}: hold at price {self.trend[t]}, total balance {earned_money}')\\n\\n            elif action == 5 and position == 0:  # StandBy\\n                states_stand_by.append(t)\\n                print(f'day {t}: standby at price {self.trend[t]}, total balance {earned_money}')\\n\\n            state = next_state\\n\\n        # Close any open position at the end\\n        if position == 1:  # Close Long\\n            earned_money += (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n            states_close_long.append(len(self.trend) - 1)\\n            print(f'final day: close long at price {self.trend[-1]}, total balance {earned_money}')\\n\\n        elif position == -1:  # Close Short\\n            earned_money += (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n            states_close_short.append(len(self.trend) - 1)\\n            print(f'final day: close short at price {self.trend[-1]}, total balance {earned_money}')\\n\\n        roi = (earned_money / initial_money) * 100\\n        total_gains = earned_money\\n        print(f'Total Gains: {total_gains}, ROI: {roi}%')\\n        return states_long, states_short, states_hold, states_close_long, states_close_short, states_stand_by, total_gains, roi\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n        max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n        return ticks_earned, max_drawdown, start_idx, end_idx\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        peak = cumulative_profit[0]\\n        max_drawdown = 0\\n        start_idx, end_idx = 0, 0\\n\\n        for i in range(1, len(cumulative_profit)):\\n            if cumulative_profit[i] > peak:\\n                peak = cumulative_profit[i]\\n            drawdown = peak - cumulative_profit[i]\\n            if drawdown > max_drawdown:\\n                max_drawdown = drawdown\\n                start_idx, end_idx = peak, cumulative_profit[i]\\n        return max_drawdown, start_idx, end_idx\", 'Check the following code:\\n\\nimport numpy as np\\nimport pandas as pd\\nimport time\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport joblib\\nimport logging\\nfrom matplotlib.backends.backend_pdf import PdfPages\\n\\nsns.set()\\n\\n# Setup logging\\nlogging.basicConfig(level=logging.DEBUG)\\n\\n# Set random seed for reproducibility\\nnp.random.seed(6)\\nsns.set()\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n]\\n\\n\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nlogging.info(\"Data loaded successfully.\")\\n\\nrows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n# Copy the records to df1 for training\\ndf1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n# Check if \\'Date\\' column exists, otherwise create it\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\ndef convert_to_float(lst):\\n    return [float(i) for i in lst]\\n\\n# Ensure \\'Date\\' column exists\\nif \\'Date\\' not in df1.columns:\\n    logging.warning(\"Date column not found. Creating a default date range.\")\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\nfor col in df1.columns:\\n    if col != \\'Date\\':\\n        df1[col] = pd.to_numeric(df1[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n# Handle NaNs\\nif df1.isna().sum().sum() > 0:\\n    logging.warning(f\\'NaN values found in DataFrame. Filling NaNs with 0.\\')\\n    df1.fillna(0, inplace=True)\\n\\nlogging.info(\"Data preprocessing completed successfully.\")\\n\\n# Verify that the indicators are added to the DataFrame\\nprint(\"Columns in DataFrame after adding indicators:\", df1.columns)\\n\\n# Extract the selected indicators\\nindicators = {indicator: df1[indicator].tolist() for indicator in selected_indicators}\\nclose = df1[\\'Close\\'].tolist()\\n\\nclass Deep_Evolution_Strategy:\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def train(self, epoch=100, print_every=1):\\n        lasttime = time.time()\\n        for i in range(epoch):\\n            logging.debug(f\\'Starting epoch {i + 1}/{epoch}\\')\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = []\\n                for w in self.weights:\\n                    x.append(np.random.randn(*w.shape))\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] = w + (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T,\\n                                                                                                              rewards).T\\n            if (i + 1) % print_every == 0:\\n                logging.info(f\\'iter {i + 1}. reward: {self.reward_function(self.weights)}\\')\\n        logging.info(\\'time taken to train: %s seconds\\', time.time() - lasttime)\\n\\n\\nclass Model:\\n    def __init__(self, input_size, layer_size, output_size):\\n        self.weights = [\\n            np.random.randn(input_size, layer_size),\\n            np.random.randn(layer_size, output_size),\\n            np.random.randn(1, layer_size),\\n        ]\\n\\n    def predict(self, inputs):\\n        feed = np.dot(inputs, self.weights[0]) + self.weights[-1]\\n        decision = np.dot(feed, self.weights[1])\\n        return decision\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def set_weights(self, weights):\\n        self.weights = weights\\n\\n\\nclass Agent:\\n    POPULATION_SIZE = 15\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        self.model = model\\n        self.window_size = window_size\\n        self.trend = trend\\n        self.skip = skip\\n        self.initial_money = initial_money\\n        self.indicators = indicators\\n        self.es = Deep_Evolution_Strategy(\\n            self.model.get_weights(),\\n            self.get_tick_reward,\\n            self.POPULATION_SIZE,\\n            self.SIGMA,\\n            self.LEARNING_RATE,\\n        )\\n\\n    def act(self, sequence, position, training=False):\\n        decision = self.model.predict(np.array(sequence))\\n        # Hold,Long,Short,Close Long,Close Short,Stand By\\n        if position == 0:\\n            mask = [0, 1, 1, 0, 0, 1]  # Allow Long, Short, StandBy\\n        elif position == 1:\\n            mask = [1, 0, 1, 1, 0, 0]  # Allow Hold, Close Long\\n        elif position == -1:\\n            mask = [1, 1, 0, 0, 1, 0]  # Allow Hold, Close Short\\n        masked_decision = np.multiply(decision[0], mask)\\n        return np.argmax(masked_decision)\\n\\n\\n    def get_state(self, t):\\n        if t < 0 or t >= len(self.trend):\\n            raise IndexError(\"Index t is out of bounds for trend array.\")\\n\\n        window_size = self.window_size + 1\\n        d = t - window_size + 1\\n        block = self.trend[d: t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0: t + 1]\\n        res = []\\n        for i in range(window_size - 1):\\n            res.append(block[i + 1] - block[i])\\n\\n        indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n        state = np.array([res + indicators_state])\\n        return state\\n\\n    def predict(self):\\n        actions = []\\n        position = 0\\n        for t in range(len(self.trend) - self.window_size):\\n            state = self.get_state(t)\\n            action_index = self.act(state, position)\\n            if action_index == 0:\\n                actions.append(\\'Hold\\')\\n            elif action_index == 1:\\n                actions.append(\\'Buy\\')\\n                position = 1\\n            elif action_index == 2:\\n                actions.append(\\'Sell\\')\\n                position = -1\\n            elif action_index == 3:\\n                actions.append(\\'Close Long\\')\\n                position = 0\\n            elif action_index == 4:\\n                actions.append(\\'Close Short\\')\\n                position = 0\\n            elif action_index == 5:\\n                actions.append(\\'StandBy\\')\\n        return actions\\n\\n    def get_tick_reward(self, weights):\\n        initial_money = self.initial_money\\n        earned_money = initial_money\\n        self.model.weights = weights\\n        state = self.get_state(0)\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and position == 0:  # Long\\n                position = 1\\n                bought_price = self.trend[t]\\n\\n            elif action == 3 and position == 1:  # Close Long\\n                earned_money += (self.trend[t] - bought_price) * 4 * 12.5 - 5.8\\n                position = 0\\n\\n            elif action == 2 and position == 0:  # Short\\n                position = -1\\n                bought_price = self.trend[t]\\n\\n            elif action == 4 and position == -1:  # Close Short\\n                earned_money += (bought_price - self.trend[t]) * 4 * 12.5 - 5.8\\n                position = 0\\n\\n            state = next_state\\n\\n        # Close any open position at the end\\n        if position == 1:  # Close Long\\n            earned_money += (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n\\n        elif position == -1:  # Close Short\\n            earned_money += (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n\\n        return ((earned_money - initial_money) / initial_money) * 100\\n\\n    def fit(self, iterations, checkpoint):\\n        self.es.train(iterations, print_every=checkpoint)\\n\\n    def buy_tick(self):\\n        initial_money = self.initial_money\\n        state = self.get_state(0)\\n        states_long = []\\n        states_short = []\\n        states_hold = []\\n        states_close_long = []\\n        states_close_short = []\\n        states_stand_by = []\\n\\n        earned_money = 0\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and position == 0:  # Long\\n                position = 1\\n                bought_price = self.trend[t]\\n                states_long.append(t)\\n                print(f\\'day {t}: buy 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 3 and position == 1:  # Close Long\\n                earned_money += (self.trend[t] - bought_price) * 4 * 12.5 - 5.8\\n                position = 0\\n                states_close_long.append(t)\\n                print(f\\'day {t}: close long at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 2 and position == 0:  # Short\\n                position = -1\\n                bought_price = self.trend[t]\\n                states_short.append(t)\\n                print(f\\'day {t}: sell 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 4 and position == -1:  # Close Short\\n                earned_money += (bought_price - self.trend[t]) * 4 * 12.5 - 5.8\\n                position = 0\\n                states_close_short.append(t)\\n                print(f\\'day {t}: close short at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 0 and position != 0:  # Hold\\n                states_hold.append(t)\\n                print(f\\'day {t}: hold at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 5 and position == 0:  # StandBy\\n                states_stand_by.append(t)\\n                print(f\\'day {t}: standby at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            state = next_state\\n\\n        # Close any open position at the end\\n        if position == 1:  # Close Long\\n            earned_money += (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n            states_close_long.append(len(self.trend) - 1)\\n            print(f\\'final day: close long at price {self.trend[-1]}, total balance {earned_money}\\')\\n\\n        elif position == -1:  # Close Short\\n            earned_money += (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n            states_close_short.append(len(self.trend) - 1)\\n            print(f\\'final day: close short at price {self.trend[-1]}, total balance {earned_money}\\')\\n\\n        roi = (earned_money / initial_money) * 100\\n        total_gains = earned_money\\n        print(f\\'Total Gains: {total_gains}, ROI: {roi}%\\')\\n        return states_long, states_short, states_hold, states_close_long, states_close_short, states_stand_by, total_gains, roi\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n        max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n        return ticks_earned, max_drawdown, start_idx, end_idx\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        peak = cumulative_profit[0]\\n        max_drawdown = 0\\n        start_idx, end_idx = 0, 0\\n\\n        for i in range(1, len(cumulative_profit)):\\n            if cumulative_profit[i] > peak:\\n                peak = cumulative_profit[i]\\n            drawdown = peak - cumulative_profit[i]\\n            if drawdown > max_drawdown:\\n                max_drawdown = drawdown\\n                start_idx, end_idx = peak, cumulative_profit[i]\\n        return max_drawdown, start_idx, end_idx\\n\\n\\ndef step_by_step_prediction(agent, trend, window_size):\\n    initial_money = agent.initial_money\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = initial_money\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state, position)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and position == 0:  # Long\\n            position = 1\\n            bought_price = trend[t]\\n            print(f\\'day {t}: buy 1 contract at price {trend[t]}, total balance {earned_money}\\')\\n\\n        elif action == 3 and position == 1:  # Close Long\\n            earned_money += (trend[t] - bought_price) * 4 * 12.5 - 5.8\\n            position = 0\\n            print(f\\'day {t}: close long at price {trend[t]}, total balance {earned_money}\\')\\n\\n        elif action == 2 and position == 0:  # Short\\n            position = -1\\n            bought_price = trend[t]\\n            print(f\\'day {t}: sell 1 contract at price {trend[t]}, total balance {earned_money}\\')\\n\\n        elif action == 4 and position == -1:  # Close Short\\n            earned_money += (bought_price - trend[t]) * 4 * 12.5 - 5.8\\n            position = 0\\n            print(f\\'day {t}: close short at price {trend[t]}, total balance {earned_money}\\')\\n\\n        elif action == 0 and position != 0:  # Hold\\n            print(f\\'day {t}: hold at price {trend[t]}, total balance {earned_money}\\')\\n\\n        elif action == 5 and position == 0:  # StandBy\\n            print(f\\'day {t}: standby at price {trend[t]}, total balance {earned_money}\\')\\n\\n        state = next_state\\n\\n    # Close any open position at the end\\n    if position == 1:  # Close Long\\n        earned_money += (trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n        print(f\\'final day: close long at price {trend[-1]}, total balance {earned_money}\\')\\n\\n    elif position == -1:  # Close Short\\n        earned_money += (bought_price - trend[-1]) * 4 * 12.5 - 5.8\\n        print(f\\'final day: close short at price {trend[-1]}, total balance {earned_money}\\')\\n\\n    roi = (earned_money / initial_money) * 100\\n    total_gains = earned_money\\n    print(f\\'Total Gains: {total_gains}, ROI: {roi}%\\')\\n    return states_long, states_short, states_hold, states_close_long, states_close_short, states_stand_by, total_gains, roi\\n\\n\\n# Update agent initialization with MACD and signal line\\nwindow_size = 30\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=6)\\nagent = Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\nlogging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\nlogging.info(\\'Training complete. Starting prediction...\\')\\nstates_long, states_short, states_hold, states_close_long, states_close_short, states_stand_by, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', label=\\'buying signal\\', markevery=states_long)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', label=\\'selling signal\\', markevery=states_short)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain + 1:, :].copy()\\n\\nfor col in df2.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df2[col] = convert_to_float(df2[col].tolist())\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: df2[indicator].tolist() for indicator in selected_indicators}\\nclose_test = df2[\\'Close\\'].tolist()\\n\\nagentTest = Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_longT, states_shortT, states_holdT, states_close_longT, states_close_shortT, states_stand_byT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_longT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_shortT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', label=\\'buying signal\\', markevery=states_longT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', label=\\'selling signal\\', markevery=states_shortT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test, window_size)\\n\\n# # Save results to PDF\\n# pdf_filename = \\'report.pdf\\'\\n# pdf_pages = PdfPages(pdf_filename)\\n\\n# # Plotting the training set results\\n# fig2 = plt.figure(figsize=(15, 5))\\n# plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n# plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', label=\\'buying signal\\', markevery=states_long)\\n# plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', label=\\'selling signal\\', markevery=states_short)\\n# plt.title(f\\'Training Set: Total gains {total_gains}, Total investment {invest}%\\')\\n# plt.legend()\\n# plt.show()\\n#\\n# # Plotting the test set results\\n# fig3 = plt.figure(figsize=(15, 5))\\n# plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n# plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', label=\\'buying signal\\', markevery=states_longT)\\n# plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', label=\\'selling signal\\', markevery=states_shortT)\\n# plt.title(f\\'Test Set: Total gains {total_gainsT}, ROI {roiT}%\\')\\n# plt.legend()\\n# plt.show()\\n\\n', 'On the Deep Reinforcement Learning training I get $9,450.00 when I train, but on the 10% test of the data that the DRL hasn\\'t seen I loose $900.00 \\nHow do I fix it\\n\\nimport numpy as np\\nimport pandas as pd\\nimport time\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport joblib\\nimport logging\\nfrom matplotlib.backends.backend_pdf import PdfPages\\n\\nsns.set()\\n\\n# Setup logging\\nlogging.basicConfig(level=logging.DEBUG)\\n\\n# Kalman Filter implementation\\ndef kalman_filter(data):\\n    n_iter = len(data)\\n    sz = (n_iter,)  # size of array\\n    q = 1e-5  # process variance\\n    r = 0.1**2  # measurement variance\\n    xhat = np.zeros(sz)  # a posteri estimate of x\\n    P = np.zeros(sz)  # a posteri error estimate\\n    xhat[0] = data[0]  # initial state\\n    P[0] = 1.0  # initial error estimate\\n\\n    for k in range(1, n_iter):\\n        # Prediction update\\n        xhatminus = xhat[k-1]  # predicted state\\n        Pminus = P[k-1] + q  # predicted error estimate\\n\\n        # Measurement update\\n        K = Pminus / (Pminus + r)  # Kalman gain\\n        xhat[k] = xhatminus + K * (data[k] - xhatminus)  # updated state\\n        P[k] = (1 - K) * Pminus  # updated error estimate\\n\\n    return xhat\\n\\n\\n# Set random seed for reproducibility\\nnp.random.seed(42)\\nsns.set()\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    \\'Stoch_D\\', \\'Stoch_K\\', \\'obv\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n]\\n\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nlogging.info(\"Data loaded successfully.\")\\n\\nrows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n# Copy the records to df1 for training\\ndf1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n# Ensure \\'Date\\' column exists\\nif \\'Date\\' not in df1.columns:\\n    logging.warning(\"Date column not found. Creating a default date range.\")\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\nfor col in df1.columns:\\n    if col != \\'Date\\':\\n        df1[col] = pd.to_numeric(df1[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n# Handle NaNs\\nif df1.isna().sum().sum() > 0:\\n    logging.warning(f\\'NaN values found in DataFrame. Filling NaNs with 0.\\')\\n    df1.fillna(0, inplace=True)\\n\\nlogging.info(\"Data preprocessing completed successfully.\")\\n\\n# for indicator in selected_indicators:\\n#     if indicator in df1.columns:\\n#         df1[indicator] = kalman_filter(df1[indicator].values)\\n\\n# Extract the selected indicators\\nindicators = {indicator: df1[indicator].tolist() for indicator in selected_indicators}\\nclose = df1[\\'Close\\'].tolist()\\n\\n\\nclass Deep_Evolution_Strategy:\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def adaptive_sigma(self, rewards):\\n        \"\"\"Adjust sigma based on the mean and standard deviation of rewards.\"\"\"\\n        mean_reward = np.mean(rewards)\\n        std_reward = np.std(rewards)\\n        if std_reward > 0:\\n            self.sigma *= 1 + (mean_reward / std_reward)  # Adjust sigma based on performance\\n            # Cap sigma to prevent it from becoming too large\\n            self.sigma = min(self.sigma, 1.0)  # Example cap\\n\\n    def train(self, epoch=100, print_every=1):\\n        lasttime = time.time()\\n        for i in range(epoch):\\n            logging.debug(f\\'Starting epoch {i + 1}/{epoch}\\')\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = []\\n                for w in self.weights:\\n                    x.append(np.random.randn(*w.shape))\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            # Normalize rewards\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            self.adaptive_sigma(rewards)  # Adjust sigma based on rewards\\n\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] = w + (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T,\\n                                                                                                              rewards).T\\n            if (i + 1) % print_every == 0:\\n                logging.info(f\\'iter {i + 1}. reward: {self.reward_function(self.weights)}\\')\\n        logging.info(\\'time taken to train: %s seconds\\', time.time() - lasttime)\\n\\nclass Model:\\n    # def __init__(self, input_size, layer_size, output_size):\\n    #     self.weights = [\\n    #         np.random.randn(input_size, layer_size),\\n    #         np.random.randn(layer_size, output_size),\\n    #         np.random.randn(1, layer_size),\\n    #     ]\\n    #\\n    # def predict(self, inputs):\\n    #     feed = np.dot(inputs, self.weights[0]) + self.weights[-1]\\n    #     decision = np.dot(feed, self.weights[1])\\n    #     return decision\\n    #\\n    # def get_weights(self):\\n    #     return self.weights\\n\\n    # def set_weights(self, weights):\\n    #     self.weights = weights\\n    def __init__(self, input_size: int, layer_size: int, output_size: int, dropout_rate: float = 0.5):\\n        \"\"\"\\n        Initializes the model with random weights.\\n\\n        Parameters:\\n        - input_size: Number of input features.\\n        - layer_size: Number of neurons in the hidden layer.\\n        - output_size: Number of output classes.\\n        - dropout_rate: Fraction of neurons to drop during prediction.\\n        \"\"\"\\n        self.weights = [\\n            np.random.normal(0, 0.1, (input_size, layer_size)),  # Input to hidden layer weights\\n            np.random.normal(0, 0.1, (layer_size, output_size)),  # Hidden to output layer weights\\n        ]\\n        self.biases = [\\n            np.random.normal(0, 0.1, (1, layer_size)),  # Bias for hidden layer\\n            np.random.normal(0, 0.1, (1, output_size)),  # Bias for output layer\\n        ]\\n        self.dropout_rate = dropout_rate  # Set the dropout rate\\n\\n    def predict(self, inputs: np.ndarray) -> np.ndarray:\\n        \"\"\"\\n        Makes a prediction based on the input data.\\n\\n        Parameters:\\n        - inputs: Input data for prediction.\\n\\n        Returns:\\n        - Decision: Output predictions.\\n        \"\"\"\\n        feed = np.dot(inputs, self.weights[0]) + self.biases[0]\\n\\n        # Apply Dropout to the hidden layer\\n        if self.dropout_rate > 0:\\n            mask = np.random.binomial(1, 1 - self.dropout_rate, size=feed.shape)  # Create a mask\\n            feed *= mask / (1 - self.dropout_rate)  # Scale the output\\n\\n        decision = np.dot(feed, self.weights[1]) + self.biases[1]\\n        return decision\\n\\n    def get_weights(self) -> list:\\n        \"\"\"Returns the current weights of the model.\"\"\"\\n        return self.weights\\n\\n    def set_weights(self, weights: list):\\n        \"\"\"\\n        Sets the weights of the model.\\n\\n        Parameters:\\n        - weights: New weights to set for the model.\\n        \"\"\"\\n        if len(weights) != len(self.weights):\\n            raise ValueError(\"Weights must have the same length as the current weights.\")\\n\\n        for i in range(len(weights)):\\n            if weights[i].shape != self.weights[i].shape:\\n                raise ValueError(\\n                    f\"Weight matrix {i} must have shape {self.weights[i].shape}, but got {weights[i].shape}.\")\\n\\n        self.weights = weights\\n\\nclass Agent:\\n    POPULATION_SIZE = 15\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        self.model = model\\n        self.window_size = window_size\\n        self.trend = trend\\n        self.skip = skip\\n        self.initial_money = initial_money\\n        self.indicators = indicators\\n        self.es = Deep_Evolution_Strategy(\\n            self.model.get_weights(),\\n            self.get_tick_reward,\\n            self.POPULATION_SIZE,\\n            self.SIGMA,\\n            self.LEARNING_RATE,\\n        )\\n\\n    def act(self, sequence, position, training=False):\\n        decision = self.model.predict(np.array(sequence))\\n        # Hold,Long,Short,Close Long,Close Short,Stand By\\n        if position == 0:\\n            mask = [0, 1, 1, 0, 0, 1]  # Allow Long, Short, StandBy\\n        elif position == 1:\\n            mask = [1, 0, 1, 1, 0, 0]  # Allow Hold, Close Long\\n        elif position == -1:\\n            mask = [1, 1, 0, 0, 1, 0]  # Allow Hold, Close Short\\n        masked_decision = np.multiply(decision[0], mask)\\n        return np.argmax(masked_decision)\\n\\n    def get_state(self, t):\\n        window_size = self.window_size + 1\\n        d = t - window_size + 1\\n        block = self.trend[d: t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0: t + 1]\\n        res = []\\n        for i in range(window_size - 1):\\n            res.append(block[i + 1] - block[i])\\n        indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n        state = np.array([res + indicators_state])\\n        return state\\n\\n    def predict(self):\\n        actions = []\\n        position = 0\\n        for t in range(len(self.trend) - self.window_size):\\n            state = self.get_state(t)\\n            action_index = self.act(state, position)\\n            if action_index == 0:\\n                actions.append(\\'Hold\\')\\n            elif action_index == 1:\\n                actions.append(\\'Buy\\')\\n                position = 1\\n            elif action_index == 2:\\n                actions.append(\\'Sell\\')\\n                position = -1\\n            elif action_index == 3:\\n                actions.append(\\'Close Long\\')\\n                position = 0\\n            elif action_index == 4:\\n                actions.append(\\'Close Short\\')\\n                position = 0\\n            elif action_index == 5:\\n                actions.append(\\'StandBy\\')\\n        return actions\\n\\n    def get_tick_reward(self, weights):\\n        initial_money = self.initial_money\\n        earned_money = initial_money\\n        self.model.weights = weights\\n        state = self.get_state(0)\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and position == 0:  # Long\\n                position = 1\\n                bought_price = self.trend[t]\\n\\n            elif action == 3 and position == 1:  # Close Long\\n                earned_money += (self.trend[t] - bought_price) * 4 * 12.5 - 5.8\\n                position = 0\\n\\n            elif action == 2 and position == 0:  # Short\\n                position = -1\\n                bought_price = self.trend[t]\\n\\n            elif action == 4 and position == -1:  # Close Short\\n                earned_money += (bought_price - self.trend[t]) * 4 * 12.5 - 5.8\\n                position = 0\\n\\n            state = next_state\\n\\n        # Close any open position at the end\\n        if position == 1:  # Close Long\\n            earned_money += (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n\\n        elif position == -1:  # Close Short\\n            earned_money += (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n\\n        return ((earned_money - initial_money) / initial_money) * 100\\n\\n    def fit(self, iterations, checkpoint):\\n        self.es.train(iterations, print_every=checkpoint)\\n\\n    def buy_tick(self):\\n        initial_money = self.initial_money\\n        state = self.get_state(0)\\n        states_long = []\\n        states_short = []\\n        states_hold = []\\n        states_close_long = []\\n        states_close_short = []\\n        states_stand_by = []\\n\\n        earned_money = 0\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and position == 0:  # Long\\n                position = 1\\n                bought_price = self.trend[t]\\n                states_long.append(t)\\n                print(f\\'day {t}: buy 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 3 and position == 1:  # Close Long\\n                earned_money += (self.trend[t] - bought_price) * 4 * 12.5 - 5.8\\n                position = 0\\n                states_close_long.append(t)\\n                print(f\\'day {t}: close long at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 2 and position == 0:  # Short\\n                position = -1\\n                bought_price = self.trend[t]\\n                states_short.append(t)\\n                print(f\\'day {t}: sell 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 4 and position == -1:  # Close Short\\n                earned_money += (bought_price - self.trend[t]) * 4 * 12.5 - 5.8\\n                position = 0\\n                states_close_short.append(t)\\n                print(f\\'day {t}: close short at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 0 and position != 0:  # Hold\\n                states_hold.append(t)\\n                print(f\\'day {t}: hold at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 5 and position == 0:  # StandBy\\n                states_stand_by.append(t)\\n                print(f\\'day {t}: standby at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            state = next_state\\n\\n        # Close any open position at the end\\n        if position == 1:  # Close Long\\n            earned_money += (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n            states_close_long.append(len(self.trend) - 1)\\n            print(f\\'final day: close long at price {self.trend[-1]}, total balance {earned_money}\\')\\n\\n        elif position == -1:  # Close Short\\n            earned_money += (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n            states_close_short.append(len(self.trend) - 1)\\n            print(f\\'final day: close short at price {self.trend[-1]}, total balance {earned_money}\\')\\n\\n        roi = (earned_money / initial_money) * 100\\n        total_gains = earned_money\\n        print(f\\'Total Gains: {total_gains}, ROI: {roi}%\\')\\n        return states_long, states_short, states_hold, states_close_long, states_close_short, states_stand_by, total_gains, roi\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n        max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n        return ticks_earned, max_drawdown, start_idx, end_idx\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        peak = cumulative_profit[0]\\n        max_drawdown = 0\\n        start_idx, end_idx = 0, 0\\n\\n        for i in range(1, len(cumulative_profit)):\\n            if cumulative_profit[i] > peak:\\n                peak = cumulative_profit[i]\\n            drawdown = peak - cumulative_profit[i]\\n            if drawdown > max_drawdown:\\n                max_drawdown = drawdown\\n                start_idx, end_idx = peak, cumulative_profit[i]\\n        return max_drawdown, start_idx, end_idx\\n\\ndef step_by_step_prediction(agent, trend, window_size):\\n    initial_money = agent.initial_money\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = initial_money\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state, position)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and position == 0:  # Long\\n            position = 1\\n            bought_price = trend[t]\\n            print(f\\'day {t}: buy 1 contract at price {trend[t]}, total balance {earned_money}\\')\\n\\n        elif action == 3 and position == 1:  # Close Long\\n            earned_money += (trend[t] - bought_price) * 4 * 12.5 - 5.8\\n            position = 0\\n            print(f\\'day {t}: close long at price {trend[t]}, total balance {earned_money}\\')\\n\\n        elif action == 2 and position == 0:  # Short\\n            position = -1\\n            bought_price = trend[t]\\n            print(f\\'day {t}: sell 1 contract at price {trend[t]}, total balance {earned_money}\\')\\n\\n        elif action == 4 and position == -1:  # Close Short\\n            earned_money += (bought_price - trend[t]) * 4 * 12.5 - 5.8\\n            position = 0\\n            print(f\\'day {t}: close short at price {trend[t]}, total balance {earned_money}\\')\\n\\n        elif action == 0 and position != 0:  # Hold\\n            print(f\\'day {t}: hold at price {trend[t]}, total balance {earned_money}\\')\\n\\n        elif action == 5 and position == 0:  # StandBy\\n            print(f\\'day {t}: standby at price {trend[t]}, total balance {earned_money}\\')\\n\\n        state = next_state\\n\\n    # Close any open position at the end\\n    if position == 1:  # Close Long\\n        earned_money += (trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n        print(f\\'final day: close long at price {trend[-1]}, total balance {earned_money}\\')\\n\\n    elif position == -1:  # Close Short\\n        earned_money += (bought_price - trend[-1]) * 4 * 12.5 - 5.8\\n        print(f\\'final day: close short at price {trend[-1]}, total balance {earned_money}\\')\\n\\n    roi = (earned_money / initial_money) * 100\\n    total_gains = earned_money\\n    print(f\\'Total Gains: {total_gains}, ROI: {roi}%\\')\\n    return states_long, states_short, states_hold, states_close_long, states_close_short, states_stand_by, total_gains, roi\\n\\nwindow_size = 30\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=6)\\nagent = Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\nlogging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\nlogging.info(\\'Training complete. Starting prediction...\\')\\nstates_long, states_short, states_hold, states_close_long, states_close_short, states_stand_by, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'model.pkl\\')\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', label=\\'buying signal\\', markevery=states_long)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', label=\\'selling signal\\', markevery=states_short)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Load the trained model\\nmodel = joblib.load(\\'model.pkl\\')\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain + 1:, :].copy()\\n\\n# Convert test data\\nfor col in df2.columns:\\n    if col != \\'Date\\':\\n        df2[col] = pd.to_numeric(df2[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n# for indicator in selected_indicators:\\n#     if indicator in df2.columns:\\n#         df2[indicator] = kalman_filter(df2[indicator].values)\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: df2[indicator].tolist() for indicator in selected_indicators}\\nclose_test = df2[\\'Close\\'].tolist()\\n\\nagentTest = Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_longT, states_shortT, states_holdT, states_close_longT, states_close_shortT, states_stand_byT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_longT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_shortT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', label=\\'buying signal\\', markevery=states_longT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'pink\\', label=\\'selling signal\\', markevery=states_close_longT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', label=\\'selling signal\\', markevery=states_shortT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'pink\\', label=\\'selling signal\\', markevery=states_close_shortT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test, window_size)\\n\\n\\n ', 'How do I use a more complex architectures like LSTM or GRU, which are better suited for time-series data. On the following Code:\\n\\nimport numpy as np\\nimport pandas as pd\\nimport time\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport joblib\\nimport logging\\nfrom sklearn.model_selection import KFold\\nfrom sklearn.metrics import mean_squared_error\\n\\nsns.set()\\n\\n# Setup logging\\nlogging.basicConfig(level=logging.DEBUG)\\n\\n# Kalman Filter implementation\\ndef kalman_filter(data):\\n    n_iter = len(data)\\n    sz = (n_iter,)  # size of array\\n    q = 1e-5  # process variance\\n    r = 0.1**2  # measurement variance\\n    xhat = np.zeros(sz)  # a posteri estimate of x\\n    P = np.zeros(sz)  # a posteri error estimate\\n    xhat[0] = data[0]  # initial state\\n    P[0] = 1.0  # initial error estimate\\n\\n    for k in range(1, n_iter):\\n        # Prediction update\\n        xhatminus = xhat[k-1]  # predicted state\\n        Pminus = P[k-1] + q  # predicted error estimate\\n\\n        # Measurement update\\n        K = Pminus / (Pminus + r)  # Kalman gain\\n        xhat[k] = xhatminus + K * (data[k] - xhatminus)  # updated state\\n        P[k] = (1 - K) * Pminus  # updated error estimate\\n\\n    return xhat\\n\\n\\n# Set random seed for reproducibility\\nnp.random.seed(42)\\nsns.set()\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    \\'Stoch_D\\', \\'momentum\\', \\'obv\\', \\'ppoValue\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n]\\n\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nlogging.info(\"Data loaded successfully.\")\\n\\nrows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n# Copy the records to df1 for training\\ndf1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n# Ensure \\'Date\\' column exists\\nif \\'Date\\' not in df1.columns:\\n    logging.warning(\"Date column not found. Creating a default date range.\")\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\nfor col in df1.columns:\\n    if col != \\'Date\\':\\n        df1[col] = pd.to_numeric(df1[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n# Handle NaNs\\nif df1.isna().sum().sum() > 0:\\n    logging.warning(f\\'NaN values found in DataFrame. Filling NaNs with 0.\\')\\n    df1.fillna(0, inplace=True)\\n\\nlogging.info(\"Data preprocessing completed successfully.\")\\n\\n# for indicator in selected_indicators:\\n#     if indicator in df1.columns:\\n#         df1[indicator] = kalman_filter(df1[indicator].values)\\n\\n# Extract the selected indicators\\nindicators = {indicator: df1[indicator].tolist() for indicator in selected_indicators}\\nclose = df1[\\'Close\\'].tolist()\\n\\n\\nclass Deep_Evolution_Strategy:\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def adaptive_sigma(self, rewards):\\n        \"\"\"Adjust sigma based on the mean and standard deviation of rewards.\"\"\"\\n        mean_reward = np.mean(rewards)\\n        std_reward = np.std(rewards)\\n        if std_reward > 0:\\n            self.sigma *= 1 + (mean_reward / std_reward)  # Adjust sigma based on performance\\n            # Cap sigma to prevent it from becoming too large\\n            self.sigma = min(self.sigma, 1.0)  # Example cap\\n\\n    def train(self, epoch=100, print_every=1):\\n        lasttime = time.time()\\n        for i in range(epoch):\\n            logging.debug(f\\'Starting epoch {i + 1}/{epoch}\\')\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = []\\n                for w in self.weights:\\n                    x.append(np.random.randn(*w.shape))\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            # Normalize rewards\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            self.adaptive_sigma(rewards)  # Adjust sigma based on rewards\\n\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] = w + (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T,\\n                                                                                                              rewards).T\\n            if (i + 1) % print_every == 0:\\n                logging.info(f\\'iter {i + 1}. reward: {self.reward_function(self.weights)}\\')\\n        logging.info(\\'time taken to train: %s seconds\\', time.time() - lasttime)\\n\\n\\nclass Model:\\n    def __init__(self, input_size: int, layer_size: int, output_size: int, dropout_rate: float = 0.5, l2_reg: float = 0.01):\\n        \"\"\"\\n        Initializes the model with random weights.\\n\\n        Parameters:\\n        - input_size: Number of input features.\\n        - layer_size: Number of neurons in the hidden layer.\\n        - output_size: Number of output classes.\\n        - dropout_rate: Fraction of neurons to drop during prediction.\\n        - l2_reg: L2 regularization factor.\\n        \"\"\"\\n        self.weights = [\\n            np.random.normal(0, 0.1, (input_size, layer_size)),  # Input to hidden layer weights\\n            np.random.normal(0, 0.1, (layer_size, output_size)),  # Hidden to output layer weights\\n        ]\\n        self.biases = [\\n            np.random.normal(0, 0.1, (1, layer_size)),  # Bias for hidden layer\\n            np.random.normal(0, 0.1, (1, output_size)),  # Bias for output layer\\n        ]\\n        self.dropout_rate = dropout_rate  # Set the dropout rate\\n        self.l2_reg = l2_reg  # Set the L2 regularization factor\\n\\n    def predict(self, inputs: np.ndarray) -> np.ndarray:\\n        \"\"\"\\n        Makes a prediction based on the input data.\\n\\n        Parameters:\\n        - inputs: Input data for prediction.\\n\\n        Returns:\\n        - Decision: Output predictions.\\n        \"\"\"\\n        feed = np.dot(inputs, self.weights[0]) + self.biases[0]\\n\\n        # Apply Dropout to the hidden layer\\n        if self.dropout_rate > 0:\\n            mask = np.random.binomial(1, 1 - self.dropout_rate, size=feed.shape)  # Create a mask\\n            feed *= mask / (1 - self.dropout_rate)  # Scale the output\\n\\n        decision = np.dot(feed, self.weights[1]) + self.biases[1]\\n\\n        # Apply L2 regularization\\n        l2_loss = self.l2_reg * (np.sum(np.square(self.weights[0])) + np.sum(np.square(self.weights[1])))\\n        decision += l2_loss\\n\\n        return decision\\n\\n    def get_weights(self) -> list:\\n        \"\"\"Returns the current weights of the model.\"\"\"\\n        return self.weights\\n\\n    def set_weights(self, weights: list):\\n        \"\"\"\\n        Sets the weights of the model.\\n\\n        Parameters:\\n        - weights: New weights to set for the model.\\n        \"\"\"\\n        if len(weights) != len(self.weights):\\n            raise ValueError(\"Weights must have the same length as the current weights.\")\\n\\n        for i in range(len(weights)):\\n            if weights[i].shape != self.weights[i].shape:\\n                raise ValueError(\\n                    f\"Weight matrix {i} must have shape {self.weights[i].shape}, but got {weights[i].shape}.\")\\n\\n        self.weights = weights\\n\\n\\nclass Agent:\\n    POPULATION_SIZE = 15\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        self.model = model\\n        self.window_size = window_size\\n        self.trend = trend\\n        self.skip = skip\\n        self.initial_money = initial_money\\n        self.indicators = indicators\\n        self.es = Deep_Evolution_Strategy(\\n            self.model.get_weights(),\\n            self.get_tick_reward,\\n            self.POPULATION_SIZE,\\n            self.SIGMA,\\n            self.LEARNING_RATE,\\n        )\\n\\n    def act(self, sequence, position, training=False):\\n        decision = self.model.predict(np.array(sequence))\\n        # Hold,Long,Short,Close Long,Close Short,Stand By\\n        if position == 0:\\n            mask = [0, 1, 1, 0, 0, 1]  # Allow Long, Short, StandBy\\n        elif position == 1:\\n            mask = [1, 0, 1, 1, 0, 0]  # Allow Hold, Close Long\\n        elif position == -1:\\n            mask = [1, 1, 0, 0, 1, 0]  # Allow Hold, Close Short\\n        masked_decision = np.multiply(decision[0], mask)\\n        return np.argmax(masked_decision)\\n\\n    def get_state(self, t):\\n        window_size = self.window_size + 1\\n        d = t - window_size + 1\\n        block = self.trend[d: t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0: t + 1]\\n        res = []\\n        for i in range(window_size - 1):\\n            res.append(block[i + 1] - block[i])\\n        indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n        state = np.array([res + indicators_state])\\n        return state\\n\\n    def predict(self):\\n        actions = []\\n        position = 0\\n        for t in range(len(self.trend) - self.window_size):\\n            state = self.get_state(t)\\n            action_index = self.act(state, position)\\n            if action_index == 0:\\n                actions.append(\\'Hold\\')\\n            elif action_index == 1:\\n                actions.append(\\'Buy\\')\\n                position = 1\\n            elif action_index == 2:\\n                actions.append(\\'Sell\\')\\n                position = -1\\n            elif action_index == 3:\\n                actions.append(\\'Close Long\\')\\n                position = 0\\n            elif action_index == 4:\\n                actions.append(\\'Close Short\\')\\n                position = 0\\n            elif action_index == 5:\\n                actions.append(\\'StandBy\\')\\n        return actions\\n\\n    def get_tick_reward(self, weights):\\n        initial_money = self.initial_money\\n        earned_money = initial_money\\n        self.model.weights = weights\\n        state = self.get_state(0)\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and position == 0:  # Long\\n                position = 1\\n                bought_price = self.trend[t]\\n\\n            elif action == 3 and position == 1:  # Close Long\\n                earned_money += (self.trend[t] - bought_price) * 4 * 12.5 - 5.8\\n                position = 0\\n\\n            elif action == 2 and position == 0:  # Short\\n                position = -1\\n                bought_price = self.trend[t]\\n\\n            elif action == 4 and position == -1:  # Close Short\\n                earned_money += (bought_price - self.trend[t]) * 4 * 12.5 - 5.8\\n                position = 0\\n\\n            state = next_state\\n\\n        # Close any open position at the end\\n        if position == 1:  # Close Long\\n            earned_money += (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n\\n        elif position == -1:  # Close Short\\n            earned_money += (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n\\n        return ((earned_money - initial_money) / initial_money) * 100\\n\\n    def fit(self, iterations, checkpoint):\\n        self.es.train(iterations, print_every=checkpoint)\\n\\n    def buy_tick(self):\\n        initial_money = self.initial_money\\n        state = self.get_state(0)\\n        states_long = []\\n        states_short = []\\n        states_hold = []\\n        states_close_long = []\\n        states_close_short = []\\n        states_stand_by = []\\n\\n        earned_money = 0\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and position == 0:  # Long\\n                position = 1\\n                bought_price = self.trend[t]\\n                states_long.append(t)\\n                print(f\\'day {t}: buy 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 3 and position == 1:  # Close Long\\n                earned_money += (self.trend[t] - bought_price) * 4 * 12.5 - 5.8\\n                position = 0\\n                states_close_long.append(t)\\n                print(f\\'day {t}: close long at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 2 and position == 0:  # Short\\n                position = -1\\n                bought_price = self.trend[t]\\n                states_short.append(t)\\n                print(f\\'day {t}: sell 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 4 and position == -1:  # Close Short\\n                earned_money += (bought_price - self.trend[t]) * 4 * 12.5 - 5.8\\n                position = 0\\n                states_close_short.append(t)\\n                print(f\\'day {t}: close short at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 0 and position != 0:  # Hold\\n                states_hold.append(t)\\n                print(f\\'day {t}: hold at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 5 and position == 0:  # StandBy\\n                states_stand_by.append(t)\\n                print(f\\'day {t}: standby at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            state = next_state\\n\\n        # Close any open position at the end\\n        if position == 1:  # Close Long\\n            earned_money += (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n            states_close_long.append(len(self.trend) - 1)\\n            print(f\\'final day: close long at price {self.trend[-1]}, total balance {earned_money}\\')\\n\\n        elif position == -1:  # Close Short\\n            earned_money += (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n            states_close_short.append(len(self.trend) - 1)\\n            print(f\\'final day: close short at price {self.trend[-1]}, total balance {earned_money}\\')\\n\\n        roi = (earned_money / initial_money) * 100\\n        total_gains = earned_money\\n        print(f\\'Total Gains: {total_gains}, ROI: {roi}%\\')\\n        return states_long, states_short, states_hold, states_close_long, states_close_short, states_stand_by, total_gains, roi\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n        max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n        return ticks_earned, max_drawdown, start_idx, end_idx\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        peak = cumulative_profit[0]\\n        max_drawdown = 0\\n        start_idx, end_idx = 0, 0\\n\\ndef step_by_step_prediction(agent, trend, window_size):\\n    initial_money = agent.initial_money\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = initial_money\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state, position)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and position == 0:  # Long\\n            position = 1\\n            bought_price = trend[t]\\n            print(f\\'day {t}: buy 1 contract at price {trend[t]}, total balance {earned_money}\\')\\n\\n        elif action == 3 and position == 1:  # Close Long\\n            earned_money += (trend[t] - bought_price) * 4 * 12.5 - 5.8\\n            position = 0\\n            print(f\\'day {t}: close long at price {trend[t]}, total balance {earned_money}\\')\\n\\n        elif action == 2 and position == 0:  # Short\\n            position = -1\\n            bought_price = trend[t]\\n            print(f\\'day {t}: sell 1 contract at price {trend[t]}, total balance {earned_money}\\')\\n\\n        elif action == 4 and position == -1:  # Close Short\\n            earned_money += (bought_price - trend[t]) * 4 * 12.5 - 5.8\\n            position = 0\\n            print(f\\'day {t}: close short at price {trend[t]}, total balance {earned_money}\\')\\n\\n        elif action == 0 and position != 0:  # Hold\\n            print(f\\'day {t}: hold at price {trend[t]}, total balance {earned_money}\\')\\n\\n        elif action == 5 and position == 0:  # StandBy\\n            print(f\\'day {t}: standby at price {trend[t]}, total balance {earned_money}\\')\\n\\n        state = next_state\\n\\n    # Close any open position at the end\\n    if position == 1:  # Close Long\\n        earned_money += (trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n        print(f\\'final day: close long at price {trend[-1]}, total balance {earned_money}\\')\\n\\n    elif position == -1:  # Close Short\\n        earned_money += (bought_price - trend[-1]) * 4 * 12.5 - 5.8\\n        print(f\\'final day: close short at price {trend[-1]}, total balance {earned_money}\\')\\n\\n    roi = (earned_money / initial_money) * 100\\n    total_gains = earned_money\\n    print(f\\'Total Gains: {total_gains}, ROI: {roi}%\\')\\n    return states_long, states_short, states_hold, states_close_long, states_close_short, states_stand_by, total_gains, roi\\n\\nwindow_size = 30\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=6)\\nagent = Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\nlogging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\nlogging.info(\\'Training complete. Starting prediction...\\')\\nstates_long, states_short, states_hold, states_close_long, states_close_short, states_stand_by, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'model.pkl\\')\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', label=\\'buying signal\\', markevery=states_long)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', label=\\'selling signal\\', markevery=states_short)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Load the trained model\\nmodel = joblib.load(\\'model.pkl\\')\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain + 1:, :].copy()\\n\\n# Convert test data\\nfor col in df2.columns:\\n    if col != \\'Date\\':\\n        df2[col] = pd.to_numeric(df2[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n# for indicator in selected_indicators:\\n#     if indicator in df2.columns:\\n#         df2[indicator] = kalman_filter(df2[indicator].values)\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: df2[indicator].tolist() for indicator in selected_indicators}\\nclose_test = df2[\\'Close\\'].tolist()\\n\\nagentTest = Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_longT, states_shortT, states_holdT, states_close_longT, states_close_shortT, states_stand_byT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_longT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_shortT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', label=\\'buying signal\\', markevery=states_longT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'pink\\', label=\\'selling signal\\', markevery=states_close_longT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', label=\\'selling signal\\', markevery=states_shortT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'pink\\', label=\\'selling signal\\', markevery=states_close_shortT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test, window_size)\\n\\n\\n', 'Create the entire code', 'Fix, optimize, add exception errors,  and add the Kalman Filter to df1[\\'Close\\'], df2[\\'Close\\'], and the indicators for the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport pandas as pd\\nimport time\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport joblib\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(6)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    logging.info(\"Data loaded successfully.\")\\n\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n    # Check if \\'Date\\' column exists, otherwise create it\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) for i in lst]\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        logging.warning(\"Date column not found. Creating a default date range.\")\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            df1[col] = pd.to_numeric(df1[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n    # Handle NaNs\\n    if df1.isna().sum().sum() > 0:\\n        logging.warning(f\\'NaN values found in DataFrame. Filling NaNs with 0.\\')\\n        df1.fillna(0, inplace=True)\\n\\n    logging.info(\"Data preprocessing completed successfully.\")\\n\\n    # # Verify that the indicators are added to the DataFrame\\n    # print(\"Columns in DataFrame after adding indicators:\", df1.columns)\\n\\n    # Extract the selected indicators\\n    indicators = {indicator: df1[indicator].tolist() for indicator in selected_indicators}\\n    close = df1[\\'Close\\'].tolist()\\n\\n    # Define the step-by-step prediction function\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                logging.info(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                logging.info(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Update agent initialization with MACD and signal line\\n    window_size = 30\\n    skip = 1\\n    initial_money = 10000\\n\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=6)\\n    agent = agent_funct.Agent(\\n        model=model,\\n        window_size=window_size,\\n        trend=close,\\n        skip=skip,\\n        initial_money=initial_money,\\n        indicators=indicators\\n    )\\n\\n    logging.info(\\'Starting training...\\')\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    logging.info(\\'Training complete. Starting prediction...\\')\\n    states_long, states_short, states_hold, states_close_long, states_close_short, states_stand_by, total_gains, invest = agent.buy_tick()\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    fig = plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', label=\\'buying signal\\', markevery=states_long)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', label=\\'selling signal\\', markevery=states_short)\\n    plt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\')\\n    df2 = df2.iloc[rows_toTrain + 1:, :].copy()\\n\\n    for col in df2.columns:\\n        if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n            df2[col] = convert_to_float(df2[col].tolist())\\n\\n    # Extract the selected indicators for testing\\n    test_indicators = {indicator: df2[indicator].tolist() for indicator in selected_indicators}\\n    close_test = df2[\\'Close\\'].tolist()\\n\\n    agentTest = agent_funct.Agent(\\n        model=model,\\n        window_size=window_size,\\n        trend=close_test,\\n        skip=skip,\\n        initial_money=initial_money,\\n        indicators=test_indicators\\n    )\\n\\n    states_longT, states_shortT, states_holdT, states_close_longT, states_close_shortT, states_stand_byT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame()\\n    test[\\'Close\\'] = close_test\\n    test[\\'Signal\\'] = 0\\n    test.loc[test.index.intersection(states_longT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_shortT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    fig1 = plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', label=\\'buying signal\\', markevery=states_longT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', label=\\'selling signal\\', markevery=states_shortT)\\n    plt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\nrun_trading_simulation()\\n', 'Fix the following code, and add exception rules\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        self.model = model\\n        self.window_size = window_size\\n        self.trend = trend\\n        self.skip = skip\\n        self.initial_money = initial_money\\n        self.indicators = indicators\\n        self.es = deep.Deep_Evolution_Strategy(\\n            self.model.get_weights(),\\n            self.get_tick_reward,\\n            self.POPULATION_SIZE,\\n            self.SIGMA,\\n            self.LEARNING_RATE,\\n        )\\n\\n    def act(self, sequence, position, training=False):\\n        decision = self.model.predict(np.array(sequence))\\n        # Hold,Long,Short,Close Long,Close Short,Stand By\\n        if position == 0:\\n            mask = [0, 1, 1, 0, 0, 1]  # Allow Long, Short, StandBy\\n        elif position == 1:\\n            mask = [1, 0, 1, 1, 0, 0]  # Allow Hold, Close Long\\n        elif position == -1:\\n            mask = [1, 1, 0, 0, 1, 0]  # Allow Hold, Close Short\\n        masked_decision = np.multiply(decision[0], mask)\\n        return np.argmax(masked_decision)\\n\\n    def get_state(self, t):\\n        window_size = self.window_size + 1\\n        d = t - window_size + 1\\n        block = self.trend[d: t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0: t + 1]\\n        res = []\\n        for i in range(window_size - 1):\\n            res.append(block[i + 1] - block[i])\\n        indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n        state = np.array([res + indicators_state])\\n        return state\\n\\n    def predict(self):\\n        actions = []\\n        position = 0\\n        for t in range(len(self.trend) - self.window_size):\\n            state = self.get_state(t)\\n            action_index = self.act(state, position)\\n            if action_index == 0:\\n                actions.append(\\'Hold\\')\\n            elif action_index == 1:\\n                actions.append(\\'Buy\\')\\n                position = 1\\n            elif action_index == 2:\\n                actions.append(\\'Sell\\')\\n                position = -1\\n            elif action_index == 3:\\n                actions.append(\\'Close Long\\')\\n                position = 0\\n            elif action_index == 4:\\n                actions.append(\\'Close Short\\')\\n                position = 0\\n            elif action_index == 5:\\n                actions.append(\\'StandBy\\')\\n        return actions\\n\\n    def get_tick_reward(self, weights):\\n        initial_money = self.initial_money\\n        earned_money = initial_money\\n        self.model.weights = weights\\n        state = self.get_state(0)\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            # Update earned_money based on the holding position\\n            if position == 1:  # Holding a Long position\\n                earned_money += (self.trend[t] - self.trend[t - self.skip]) * 4 * 12.5\\n            elif position == -1:  # Holding a Short position\\n                earned_money += (self.trend[t - self.skip] - self.trend[t]) * 4 * 12.5\\n\\n            # Execute actions\\n            if action == 1 and position == 0:  # Open Long position\\n                position = 1\\n                bought_price = self.trend[t]\\n\\n            elif action == 3 and position == 1:  # Close Long position\\n                earned_money += (self.trend[t] - bought_price) * 4 * 12.5 - 5.8  # Realize P&L and deduct commission\\n                position = 0\\n\\n            elif action == 2 and position == 0:  # Open Short position\\n                position = -1\\n                bought_price = self.trend[t]\\n\\n            elif action == 4 and position == -1:  # Close Short position\\n                earned_money += (bought_price - self.trend[t]) * 4 * 12.5 - 5.8  # Realize P&L and deduct commission\\n                position = 0\\n\\n            state = next_state\\n\\n        # Final close of any open position at the end\\n        if position == 1:  # Close Long\\n            earned_money += (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n        elif position == -1:  # Close Short\\n            earned_money += (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n\\n        return ((earned_money - initial_money) / initial_money) * 100\\n\\n    def fit(self, iterations, checkpoint):\\n        self.es.train(iterations, print_every=checkpoint)\\n\\n    def buy_tick(self):\\n        initial_money = self.initial_money\\n        state = self.get_state(0)\\n        states_long = []\\n        states_short = []\\n        states_hold = []\\n        states_close_long = []\\n        states_close_short = []\\n        states_stand_by = []\\n\\n        earned_money = initial_money\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            if position == 1:  # Holding a Long position\\n                earned_money += (self.trend[t] - self.trend[t - self.skip]) * 4 * 12.5\\n            elif position == -1:  # Holding a Short position\\n                earned_money += (self.trend[t - self.skip] - self.trend[t]) * 4 * 12.5\\n\\n            if action == 1 and position == 0:  # Long\\n                position = 1\\n                bought_price = self.trend[t]\\n                states_long.append(t)\\n                print(f\\'day {t}: buy 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 3 and position == 1:  # Close Long\\n                earned_money += (self.trend[t] - bought_price) * 4 * 12.5 - 5.8  # Realize remaining P&L and deduct commission\\n                position = 0\\n                states_close_long.append(t)\\n                print(f\\'day {t}: close long at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 2 and position == 0:  # Short\\n                position = -1\\n                bought_price = self.trend[t]\\n                states_short.append(t)\\n                print(f\\'day {t}: sell 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 4 and position == -1:  # Close Short\\n                earned_money += (bought_price - self.trend[t]) * 4 * 12.5 - 5.8  # Realize remaining P&L and deduct commission\\n                position = 0\\n                states_close_short.append(t)\\n                print(f\\'day {t}: close short at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 0 and position != 0:  # Hold\\n                states_hold.append(t)\\n                print(f\\'day {t}: hold at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            elif action == 5 and position == 0:  # StandBy\\n                states_stand_by.append(t)\\n                print(f\\'day {t}: standby at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n            state = next_state\\n\\n        # Close any open position at the end\\n        if position == 1:  # Close Long\\n            earned_money += (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n            states_close_long.append(len(self.trend) - 1)\\n            print(f\\'final day: close long at price {self.trend[-1]}, total balance {earned_money}\\')\\n\\n        elif position == -1:  # Close Short\\n            earned_money += (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n            states_close_short.append(len(self.trend) - 1)\\n            print(f\\'final day: close short at price {self.trend[-1]}, total balance {earned_money}\\')\\n\\n        roi = (earned_money / initial_money) * 100\\n        total_gains = earned_money - initial_money\\n        print(f\\'Total Gains: {total_gains}, ROI: {roi}%\\')\\n        return states_long, states_short, states_hold, states_close_long, states_close_short, states_stand_by, total_gains, roi\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n        max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n        return ticks_earned, max_drawdown, start_idx, end_idx\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        peak = cumulative_profit[0]\\n        max_drawdown = 0\\n        start_idx, end_idx = 0, 0\\n\\n        for i in range(1, len(cumulative_profit)):\\n            if cumulative_profit[i] > peak:\\n                peak = cumulative_profit[i]\\n            drawdown = peak - cumulative_profit[i]\\n            if drawdown > max_drawdown:\\n                max_drawdown = drawdown\\n                start_idx, end_idx = peak, cumulative_profit[i]\\n        return max_drawdown, start_idx, end_idx\\n', 'Check for errors. add try exception, and optimize the following code:\\n\\n\\nimport numpy as np\\nimport logging\\nimport time\\n\\nclass Deep_Evolution_Strategy():\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def adaptive_sigma(self, rewards):\\n        \"\"\"Adjust sigma based on the mean and standard deviation of rewards\"\"\"\\n        mean_reward = np.mean(rewards)\\n        std_reward = np.std(rewards)\\n        if std_reward > 0:\\n            self.sigma *= 1 + (mean_reward / std_reward)   # Adjust sigma based on performance\\n            # Cap sigma to prevent it from becoming top layer\\n            self.sigma = min(self.sigma, 1.0)   # Example cap\\n\\n    def train(self, epoch=100, print_every=1):\\n        lasttime = time.time()\\n        for i in range(epoch):\\n            logging.debug(f\\'Starting epoch {i + 1}/{epoch}\\')\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = []\\n                for w in self.weights:\\n                    x.append(np.random.randn(*w.shape))\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            self.adaptive_sigma(rewards)  # Adjust sigma based on rewards\\n\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] = w + (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T,\\n                                                                                                              rewards).T\\n            if (i + 1) % print_every == 0:\\n                logging.info(f\\'iter {i + 1}. reward: {self.reward_function(self.weights)}\\')\\n        logging.info(\\'time taken to train: %s seconds\\', time.time() - lasttime)\\n ', 'Check for errors, and try exception to the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 29, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.01\\t    Added try-except blocks in critical areas to catch and log errors.\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.01\\t    Created a method map_action_to_string to improve readability and\\n                                            maintainability.\\n[JSanchez]\\t29-Aug-2024:\\tRev. 1.01\\t    Introduced a method update_position to handle position updates based on\\n                                            actions.\\n------------------------------------------------------------------------------------------------------------------------\\n\\n\\nKey Changes Made\\n    Exception Handling: Added try-except blocks in critical areas to catch and log errors.\\n    Action Mapping: Created a method map_action_to_string to improve readability and maintainability.\\n    Position Management: Introduced a method update_position to handle position updates based on actions.\\n    State Recording: Created a method record_action_states to encapsulate the logic for recording states based on\\n    actions.\\n    Final Position Handling: Separated the logic for finalizing positions into a method finalize_position for clarity.\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        self.model = model\\n        self.window_size = window_size\\n        self.trend = trend\\n        self.skip = skip\\n        self.initial_money = initial_money\\n        self.indicators = indicators\\n        self.es = deep.Deep_Evolution_Strategy(\\n            self.model.get_weights(),\\n            self.get_tick_reward,\\n            self.POPULATION_SIZE,\\n            self.SIGMA,\\n            self.LEARNING_RATE,\\n        )\\n\\n    def act(self, sequence, position, training=False):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            # Hold, Long, Short, Close Long, Close Short, Stand By\\n            if position == 0:\\n                mask = [0, 1, 1, 0, 0, 1]  # Allow Long, Short, StandBy\\n            elif position == 1:\\n                mask = [1, 0, 1, 1, 0, 0]  # Allow Hold, Close Long\\n            elif position == -1:\\n                mask = [1, 1, 0, 0, 1, 0]  # Allow Hold, Close Short\\n            masked_decision = np.multiply(decision[0], mask)\\n            return np.argmax(masked_decision)\\n        except Exception as e:\\n            logging.error(f\"Error in action decision: {e}\")\\n            return 0  # Default to Hold if there\\'s an error\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n            block = self.trend[d: t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0: t + 1]\\n            res = [block[i + 1] - block[i] for i in range(window_size - 1)]\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n            return state\\n        except IndexError as e:\\n            logging.error(f\"Index error in get_state: {e}\")\\n            return np.zeros((1, len(self.indicators) + window_size - 1))  # Return a zero state\\n\\n    def predict(self):\\n        actions = []\\n        position = 0\\n        for t in range(len(self.trend) - self.window_size):\\n            state = self.get_state(t)\\n            action_index = self.act(state, position)\\n            actions.append(self.map_action_to_string(action_index))\\n            position = self.update_position(action_index, position)\\n        return actions\\n\\n    def map_action_to_string(self, action_index):\\n        action_map = {\\n            0: \\'Hold\\',\\n            1: \\'Buy\\',\\n            2: \\'Sell\\',\\n            3: \\'Close Long\\',\\n            4: \\'Close Short\\',\\n            5: \\'StandBy\\'\\n        }\\n        return action_map.get(action_index, \\'Hold\\')\\n\\n    def update_position(self, action_index, current_position):\\n        if action_index == 1:  # Buy\\n            return 1\\n        elif action_index == 2:  # Sell\\n            return -1\\n        elif action_index in [3, 4]:  # Close Long or Close Short\\n            return 0\\n        return current_position  # Hold or StandBy\\n\\n    def get_tick_reward(self, weights):\\n        initial_money = self.initial_money\\n        earned_money = initial_money\\n        self.model.weights = weights\\n        state = self.get_state(0)\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            # Update earned_money based on the holding position\\n            if position == 1:  # Holding a Long position\\n                earned_money += (self.trend[t] - self.trend[t - self.skip]) * 4 * 12.5\\n            elif position == -1:  # Holding a Short position\\n                earned_money += (self.trend[t - self.skip] - self.trend[t]) * 4 * 12.5\\n\\n            # Execute actions\\n            position, bought_price = self.execute_action(action, position, t)\\n\\n            state = next_state\\n\\n        # Final close of any open position at the end\\n        earned_money += self.finalize_position(position, bought_price)\\n\\n        return ((earned_money - initial_money) / initial_money) * 100\\n\\n    def execute_action(self, action, position, t):\\n        bought_price = 0\\n        if action == 1 and position == 0:  # Open Long position\\n            position = 1\\n            bought_price = self.trend[t]\\n        elif action == 3 and position == 1:  # Close Long position\\n            position = 0\\n        elif action == 2 and position == 0:  # Open Short position\\n            position = -1\\n            bought_price = self.trend[t]\\n        elif action == 4 and position == -1:  # Close Short position\\n            position = 0\\n        return position, bought_price\\n\\n    def finalize_position(self, position, bought_price):\\n        if position == 1:  # Close Long\\n            return (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n        elif position == -1:  # Close Short\\n            return (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n        return 0\\n\\n    def fit(self, iterations, checkpoint):\\n        self.es.train(iterations, print_every=checkpoint)\\n\\n    def buy_tick(self):\\n        initial_money = self.initial_money\\n        state = self.get_state(0)\\n        states_long = []\\n        states_short = []\\n        states_hold = []\\n        states_close_long = []\\n        states_close_short = []\\n        states_stand_by = []\\n\\n        earned_money = initial_money\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            # Update earned_money based on the holding position\\n            if position == 1:  # Holding a Long position\\n                earned_money += (self.trend[t] - self.trend[t - self.skip]) * 4 * 12.5\\n            elif position == -1:  # Holding a Short position\\n                earned_money += (self.trend[t - self.skip] - self.trend[t]) * 4 * 12.5\\n\\n            position, bought_price = self.execute_action(action, position, t)\\n\\n            # Record states based on actions\\n            self.record_action_states(action, position, t, states_long, states_short, states_close_long, states_close_short, states_hold, states_stand_by, earned_money)\\n\\n            state = next_state\\n\\n        # Close any open position at the end\\n        earned_money += self.finalize_position(position, bought_price)\\n\\n        roi = (earned_money / initial_money) * 100\\n        total_gains = earned_money - initial_money\\n        print(f\\'Total Gains: {total_gains}, ROI: {roi}%\\')\\n        return states_long, states_short, states_hold, states_close_long, states_close_short, states_stand_by, total_gains, roi\\n\\n    def record_action_states(self, action, position, t, states_long, states_short, states_close_long, states_close_short, states_hold, states_stand_by, earned_money):\\n        if action == 1 and position == 1:  # Long\\n            states_long.append(t)\\n            print(f\\'day {t}: buy 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n        elif action == 3 and position == 0:  # Close Long\\n            states_close_long.append(t)\\n            print(f\\'day {t}: close long at price {self.trend[t]}, total balance {earned_money}\\')\\n        elif action == 2 and position == 0:  # Short\\n            states_short.append(t)\\n            print(f\\'day {t}: sell 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n        elif action == 4 and position == -1:  # Close Short\\n            states_close_short.append(t)\\n            print(f\\'day {t}: close short at price {self.trend[t]}, total balance {earned_money}\\')\\n        elif action == 0 and position != 0:  # Hold\\n            states_hold.append(t)\\n            print(f\\'day {t}: hold at price {self.trend[t]}, total balance {earned_money}\\')\\n        elif action == 5 and position == 0:  # StandBy\\n            states_stand_by.append(t)\\n            print(f\\'day {t}: standby at price {self.trend[t]}, total balance {earned_money}\\')\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n        max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n        return ticks_earned, max_drawdown, start_idx, end_idx\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        peak = cumulative_profit[0]\\n        max_drawdown = 0\\n        start_idx, end_idx = 0, 0\\n\\n        for i in range(1, len(cumulative_profit)):\\n            if cumulative_profit[i] > peak:\\n                peak = cumulative_profit[i]\\n            drawdown = peak - cumulative_profit[i]\\n            if drawdown > max_drawdown:\\n                max_drawdown = drawdown\\n                start_idx, end_idx = peak, cumulative_profit[i]\\n        return max_drawdown, start_idx, end_idx', 'Error on decision = self.model.predict(np.array(sequence))\\n\\nclass Agent:\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        self.model = model\\n        self.window_size = window_size\\n        self.trend = trend\\n        self.skip = skip\\n        self.initial_money = initial_money\\n        self.indicators = indicators\\n        self.es = deep.Deep_Evolution_Strategy(\\n            self.model.get_weights(),\\n            self.get_tick_reward,\\n            self.POPULATION_SIZE,\\n            self.SIGMA,\\n            self.LEARNING_RATE,\\n        )\\n\\n    def act(self, sequence, position, training=False):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            # Hold, Long, Short, Close Long, Close Short, Stand By\\n            mask = [0, 1, 1, 0, 0, 1] if position == 0 else \\\\\\n                   [1, 0, 1, 1, 0, 0] if position == 1 else \\\\\\n                   [1, 1, 0, 0, 1, 0]  # position == -1\\n            masked_decision = np.multiply(decision[0], mask)\\n            return np.argmax(masked_decision)\\n        except Exception as e:\\n            logging.error(f\"Error in action decision: {e}\")\\n            return 0  # Default to Hold if there\\'s an error\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n            block = self.trend[d: t + 1] if d >= 0 else [-d * [self.trend[0]]] + self.trend[0: t + 1]\\n            res = [block[i + 1] - block[i] for i in range(window_size - 1)]\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n            return state\\n        except IndexError as e:\\n            logging.error(f\"Index error in get_state: {e}\")\\n            return np.zeros((1, len(self.indicators) + self.window_size - 1))  # Return a zero state\\n        except Exception as e:\\n            logging.error(f\"Unexpected error in get_state: {e}\")\\n            return np.zeros((1, len(self.indicators) + self.window_size - 1))  # Return a zero state\\n\\n    def predict(self):\\n        actions = []\\n        position = 0\\n        for t in range(len(self.trend) - self.window_size):\\n            state = self.get_state(t)\\n            action_index = self.act(state, position)\\n            actions.append(self.map_action_to_string(action_index))\\n            position = self.update_position(action_index, position)\\n        return actions\\n\\n    def map_action_to_string(self, action_index):\\n        action_map = {\\n            0: \\'Hold\\',\\n            1: \\'Buy\\',\\n            2: \\'Sell\\',\\n            3: \\'Close Long\\',\\n            4: \\'Close Short\\',\\n            5: \\'StandBy\\'\\n        }\\n        return action_map.get(action_index, \\'Hold\\')\\n\\n    def update_position(self, action_index, current_position):\\n        try:\\n            if action_index == 1:  # Buy\\n                return 1\\n            elif action_index == 2:  # Sell\\n                return -1\\n            elif action_index in [3, 4]:  # Close Long or Close Short\\n                return 0\\n            return current_position  # Hold or StandBy\\n        except Exception as e:\\n            logging.error(f\"Error updating position: {e}\")\\n            return current_position  # Return current position in case of error\\n\\n    def get_tick_reward(self, weights):\\n        initial_money = self.initial_money\\n        earned_money = initial_money\\n        self.model.weights = weights\\n        state = self.get_state(0)\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            # Update earned_money based on the holding position\\n            if position == 1:  # Holding a Long position\\n                earned_money += (self.trend[t] - self.trend[t - self.skip]) * 4 * 12.5\\n            elif position == -1:  # Holding a Short position\\n                earned_money += (self.trend[t - self.skip] - self.trend[t]) * 4 * 12.5\\n\\n            # Execute actions\\n            position, bought_price = self.execute_action(action, position, t)\\n\\n            state = next_state\\n\\n        # Final close of any open position at the end\\n        earned_money += self.finalize_position(position, bought_price)\\n\\n        return ((earned_money - initial_money) / initial_money) * 100\\n\\n    def execute_action(self, action, position, t):\\n        bought_price = 0\\n        try:\\n            if action == 1 and position == 0:  # Open Long position\\n                position = 1\\n                bought_price = self.trend[t]\\n            elif action == 3 and position == 1:  # Close Long position\\n                position = 0\\n            elif action == 2 and position == 0:  # Open Short position\\n                position = -1\\n                bought_price = self.trend[t]\\n            elif action == 4 and position == -1:  # Close Short position\\n                position = 0\\n        except Exception as e:\\n            logging.error(f\"Error executing action: {e}\")\\n        return position, bought_price\\n\\n    def finalize_position(self, position, bought_price):\\n        try:\\n            if position == 1:  # Close Long\\n                return (self.trend[-1] - bought_price) * 4 * 12.5 - 5.8\\n            elif position == -1:  # Close Short\\n                return (bought_price - self.trend[-1]) * 4 * 12.5 - 5.8\\n            return 0\\n        except Exception as e:\\n            logging.error(f\"Error finalizing position: {e}\")\\n            return 0\\n\\n    def fit(self, iterations, checkpoint):\\n        self.es.train(iterations, print_every=checkpoint)\\n\\n    def buy_tick(self):\\n        initial_money = self.initial_money\\n        state = self.get_state(0)\\n        states_long = []\\n        states_short = []\\n        states_hold = []\\n        states_close_long = []\\n        states_close_short = []\\n        states_stand_by = []\\n\\n        earned_money = initial_money\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state, position)\\n            next_state = self.get_state(t + 1)\\n\\n            # Update earned_money based on the holding position\\n            if position == 1:  # Holding a Long position\\n                earned_money += (self.trend[t] - self.trend[t - self.skip]) * 4 * 12.5\\n            elif position == -1:  # Holding a Short position\\n                earned_money += (self.trend[t - self.skip] - self.trend[t]) * 4 * 12.5\\n\\n            position, bought_price = self.execute_action(action, position, t)\\n\\n            # Record states based on actions\\n            self.record_action_states(action, position, t, states_long, states_short, states_close_long, states_close_short, states_hold, states_stand_by, earned_money)\\n\\n            state = next_state\\n\\n        # Close any open position at the end\\n        earned_money += self.finalize_position(position, bought_price)\\n\\n        roi = (earned_money / initial_money) * 100\\n        total_gains = earned_money - initial_money\\n        print(f\\'Total Gains: {total_gains}, ROI: {roi}%\\')\\n        return states_long, states_short, states_hold, states_close_long, states_close_short, states_stand_by, total_gains, roi\\n\\n    def record_action_states(self, action, position, t, states_long, states_short, states_close_long, states_close_short, states_hold, states_stand_by, earned_money):\\n        try:\\n            if action == 1 and position == 1:  # Long\\n                states_long.append(t)\\n                print(f\\'day {t}: buy 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n            elif action == 3 and position == 0:  # Close Long\\n                states_close_long.append(t)\\n                print(f\\'day {t}: close long at price {self.trend[t]}, total balance {earned_money}\\')\\n            elif action == 2 and position == 0:  # Short\\n                states_short.append(t)\\n                print(f\\'day {t}: sell 1 contract at price {self.trend[t]}, total balance {earned_money}\\')\\n            elif action == 4 and position == -1:  # Close Short\\n                states_close_short.append(t)\\n                print(f\\'day {t}: close short at price {self.trend[t]}, total balance {earned_money}\\')\\n            elif action == 0 and position != 0:  # Hold\\n                states_hold.append(t)\\n                print(f\\'day {t}: hold at price {self.trend[t]}, total balance {earned_money}\\')\\n            elif action == 5 and position == 0:  # StandBy\\n                states_stand_by.append(t)\\n                print(f\\'day {t}: standby at price {self.trend[t]}, total balance {earned_money}\\')\\n        except Exception as e:\\n            logging.error(f\"Error recording action states: {e}\")\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error calculating metrics: {e}\")\\n            return 0, 0, 0, 0  # Return default values in case of error\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit[i] > peak:\\n                    peak = cumulative_profit[i]\\n                drawdown = peak - cumulative_profit[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error calculating max drawdown: {e}\")\\n            return 0, 0, 0  # Return default values in case of error', \"Add try exception to the following code:\\n\\n\\nimport numpy as np\\nimport logging\\nimport time\\n\\nclass Deep_Evolution_Strategy:\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def train(self, epoch=100, print_every=1):\\n        lasttime = time.time()\\n        for i in range(epoch):\\n            logging.debug(f'Starting epoch {i + 1}/{epoch}')\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = []\\n                for w in self.weights:\\n                    x.append(np.random.randn(*w.shape))\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] = w + (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T,\\n                                                                                                              rewards).T\\n            if (i + 1) % print_every == 0:\\n                logging.info(f'iter {i + 1}. reward: {self.reward_function(self.weights)}')\\n        logging.info('time taken to train: %s seconds', time.time() - lasttime)\", \"How do I separate the winning numbers from the winning numbers columns When I read the csv file\\n\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n\\n# Generate initial DataFrame with 5 columns and 4000 rows\\n\\ndf_initial = pd.read_csv('data/Lottery_Powerball_Winning_Numbers__Beginning_2010.csv')\\n\\n\\n# Prepare data for training\\nX = df_initial.values[:-5]  # Use all rows except the last 5 for features\\ny = df_initial.values[5:]    # Use rows 5 to 4000 for target\\n\\n# Train a linear regression model for each column\\nmodels = []\\nfor i in range(5):\\n    model = LinearRegression()\\n    model.fit(X, y[:, i])\\n    models.append(model)\\n\\n# Predict the next 5 rows\\nlast_rows = df_initial.values[-5:]  # Last 5 rows to predict the next values\\npredictions = np.array([model.predict(last_rows) for model in models]).T\\n\\n# Create a DataFrame for the predicted columns\\ndf_predictions = pd.DataFrame(predictions, columns=[f'Predicted_Column{i+1}' for i in range(5)])\\n\\n# Display the predicted DataFrame\\nprint(df_predictions)\", 'How do I separate the winning numbers from the winning numbers columns When I read the csv file that contains \\nDraw Date          Winning Numbers            Multiplier\\n09/26/2020        11 21 27 36 62 24                3\\n09/30/2020        14 18 36 49 76 18                2', 'How do I separate each numbers on the winning_numbers columns\\n\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n\\n# Load the CSV file\\ndf = pd.read_csv(\\'data/Lottery_Powerball_Winning_Numbers__Beginning_2010.csv\\')\\n\\n# Display the first few rows to understand the structure\\nprint(df.head())\\n\\n# Assuming the \"Winning Numbers\" column contains numbers separated by commas\\n# Split the \"Winning Numbers\" into separate columns\\nwinning_numbers = df[\\'Winning Numbers\\'].str.split(\\',\\', expand=True)\\n\\n# Rename the columns for clarity\\nwinning_numbers.columns = [f\\'Winning_Number_{i+1}\\' for i in range(winning_numbers.shape[1])]\\n\\n# Combine the Draw Date, Multiplier, and the separated winning numbers into a new DataFrame\\ndf_separated = pd.concat([df[\\'Draw Date\\'], winning_numbers, df[\\'Multiplier\\']], axis=1)\\n\\n# Display the new DataFrame with separated winning numbers\\nprint(df_separated.head())', 'ValueError: could not convert string to float: \\'09/26/2020\\'\\n\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n\\n# Load the CSV file\\ndf = pd.read_csv(\\'data/Lottery_Powerball_Winning_Numbers__Beginning_2010.csv\\')\\n\\n# Display the first few rows to understand the structure\\nprint(df.head())\\n\\n# Split the \"Winning Numbers\" into separate columns based on spaces\\nwinning_numbers = df[\\'Winning Numbers\\'].str.split(\\' \\', expand=True)\\n\\n# Rename the columns for clarity\\nwinning_numbers.columns = [f\\'Winning_Number_{i+1}\\' for i in range(winning_numbers.shape[1])]\\n\\n# Combine the Draw Date, Multiplier, and the separated winning numbers into a new DataFrame\\ndf_separated = pd.concat([df[\\'Draw Date\\'], winning_numbers, df[\\'Multiplier\\']], axis=1)\\n\\n# Display the new DataFrame with separated winning numbers\\nprint(df_separated.head())\\n\\n\\n# Prepare data for training\\nX = df_separated.values[:-5]  # Use all rows except the last 5 for features\\ny = df_separated.values[5:]    # Use rows 5 to 4000 for target\\n\\n# Train a linear regression model for each column\\nmodels = []\\nfor i in range(5):\\n    model = LinearRegression()\\n    model.fit(X, y[:, i])\\n    models.append(model)\\n\\n# Predict the next 5 rows\\nlast_rows = df_initial.values[-5:]  # Last 5 rows to predict the next values\\npredictions = np.array([model.predict(last_rows) for model in models]).T\\n\\n# Create a DataFrame for the predicted columns\\ndf_predictions = pd.DataFrame(predictions, columns=[f\\'Predicted_Column{i+1}\\' for i in range(5)])\\n\\n# Display the predicted DataFrame\\nprint(df_predictions)', '    raise ValueError(\\nValueError: X has 8 features, but LinearRegression is expecting 6 features as input.\\n\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n\\n# Load the CSV file\\ndf = pd.read_csv(\\'data/Lottery_Powerball_Winning_Numbers__Beginning_2010.csv\\')\\n\\n# Display the first few rows to understand the structure\\nprint(df.head())\\n\\n# Split the \"Winning Numbers\" into separate columns based on spaces\\nwinning_numbers = df[\\'Winning Numbers\\'].str.split(\\' \\', expand=True)\\n\\n# Rename the columns for clarity\\nwinning_numbers.columns = [f\\'Winning_Number_{i+1}\\' for i in range(winning_numbers.shape[1])]\\n\\n# Combine the Draw Date, Multiplier, and the separated winning numbers into a new DataFrame\\ndf_separated = pd.concat([df[\\'Draw Date\\'], winning_numbers, df[\\'Multiplier\\']], axis=1)\\n\\n# Convert \"Draw Date\" to datetime and then to a numeric format (timestamp)\\ndf_separated[\\'Draw Date\\'] = pd.to_datetime(df_separated[\\'Draw Date\\'], format=\\'%m/%d/%Y\\')\\ndf_separated[\\'Draw Date\\'] = df_separated[\\'Draw Date\\'].astype(np.int64) // 10**9  # Convert to seconds since epoch\\n\\n# Convert winning numbers and multiplier to numeric\\ndf_separated[winning_numbers.columns] = df_separated[winning_numbers.columns].apply(pd.to_numeric, errors=\\'coerce\\')\\ndf_separated[\\'Multiplier\\'] = pd.to_numeric(df_separated[\\'Multiplier\\'], errors=\\'coerce\\')\\n\\n# Prepare data for training\\nX = winning_numbers.values[:-5]  # Use all rows except the last 5 for features\\ny = winning_numbers.values[5:]    # Use rows 5 to 4000 for target\\n\\n# Train a linear regression model for each column\\nmodels = []\\nfor i in range(5):\\n    model = LinearRegression()\\n    model.fit(X, y[:, i])\\n    models.append(model)\\n\\n# Predict the next 5 rows\\nlast_rows = df_separated.values[-5:]  # Last 5 rows to predict the next values\\npredictions = np.array([model.predict(last_rows) for model in models]).T\\n\\n# Create a DataFrame for the predicted columns\\ndf_predictions = pd.DataFrame(predictions, columns=[f\\'Predicted_Column{i+1}\\' for i in range(5)])\\n\\n# Display the predicted DataFrame\\nprint(df_predictions)', 'Round the df_predictions to integer\\n\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n\\n# Load the CSV file\\ndf = pd.read_csv(\\'data/Lottery_Powerball_Winning_Numbers__Beginning_2010.csv\\')\\n\\n# Display the first few rows to understand the structure\\nprint(df.head())\\n\\n# Split the \"Winning Numbers\" into separate columns based on spaces\\nwinning_numbers = df[\\'Winning Numbers\\'].str.split(\\' \\', expand=True)\\n\\n# Rename the columns for clarity\\nwinning_numbers.columns = [f\\'Winning_Number_{i+1}\\' for i in range(winning_numbers.shape[1])]\\n\\n# Combine the Draw Date, Multiplier, and the separated winning numbers into a new DataFrame\\ndf_separated = pd.concat([df[\\'Draw Date\\'], winning_numbers, df[\\'Multiplier\\']], axis=1)\\n\\n# Convert \"Draw Date\" to datetime and then to a numeric format (timestamp)\\ndf_separated[\\'Draw Date\\'] = pd.to_datetime(df_separated[\\'Draw Date\\'], format=\\'%m/%d/%Y\\')\\ndf_separated[\\'Draw Date\\'] = df_separated[\\'Draw Date\\'].astype(np.int64) // 10**9  # Convert to seconds since epoch\\n\\n# Convert winning numbers and multiplier to numeric\\ndf_separated[winning_numbers.columns] = df_separated[winning_numbers.columns].apply(pd.to_numeric, errors=\\'coerce\\')\\ndf_separated[\\'Multiplier\\'] = pd.to_numeric(df_separated[\\'Multiplier\\'], errors=\\'coerce\\')\\n\\n# Prepare data for training\\nX = df_separated[winning_numbers.columns].values[:-5]  # Use all rows except the last 5 for features\\ny = df_separated[winning_numbers.columns].values[5:]    # Use rows 5 to 4000 for target\\n\\n# Train a linear regression model for each column\\nmodels = []\\nfor i in range(len(winning_numbers.columns)):\\n    model = LinearRegression()\\n    model.fit(X, y[:, i])\\n    models.append(model)\\n\\n# Predict the next 5 rows\\nlast_rows = df_separated[winning_numbers.columns].values[-5:]  # Last 5 rows to predict the next values\\npredictions = np.array([model.predict(last_rows) for model in models]).T\\n\\n# Create a DataFrame for the predicted columns\\ndf_predictions = pd.DataFrame(predictions, columns=[f\\'Predicted_Column{i+1}\\' for i in range(len(winning_numbers.columns))])\\n\\n# Display the predicted DataFrame\\nprint(df_predictions)', 'How do I get columns 5 - 10 when reading the csv file to df\\n\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n\\n# Load the CSV file\\ndf = pd.read_csv(\\'data/megamillions.csv\\')\\n\\n# Display the first few rows to understand the structure\\nprint(df.head())\\n\\n# Split the \"Winning Numbers\" into separate columns based on spaces\\nwinning_numbers = df[\\'Winning Numbers\\'].str.split(\\' \\', expand=True)\\n\\n# Rename the columns for clarity\\nwinning_numbers.columns = [f\\'Winning_Number_{i+1}\\' for i in range(winning_numbers.shape[1])]\\n\\n# Combine the Draw Date, Multiplier, and the separated winning numbers into a new DataFrame\\ndf_separated = pd.concat([df[\\'Draw Date\\'], winning_numbers, df[\\'Multiplier\\']], axis=1)\\n\\n# Convert \"Draw Date\" to datetime and then to a numeric format (timestamp)\\ndf_separated[\\'Draw Date\\'] = pd.to_datetime(df_separated[\\'Draw Date\\'], format=\\'%m/%d/%Y\\')\\ndf_separated[\\'Draw Date\\'] = df_separated[\\'Draw Date\\'].astype(np.int64) // 10**9  # Convert to seconds since epoch\\n\\n# Convert winning numbers and multiplier to numeric\\ndf_separated[winning_numbers.columns] = df_separated[winning_numbers.columns].apply(pd.to_numeric, errors=\\'coerce\\')\\ndf_separated[\\'Multiplier\\'] = pd.to_numeric(df_separated[\\'Multiplier\\'], errors=\\'coerce\\')\\n\\n# Prepare data for training\\nX = df_separated[winning_numbers.columns].values[:-5]  # Use all rows except the last 5 for features\\ny = df_separated[winning_numbers.columns].values[5:]    # Use rows 5 to 4000 for target\\n\\n# Train a linear regression model for each column\\nmodels = []\\nfor i in range(len(winning_numbers.columns)):\\n    model = LinearRegression()\\n    model.fit(X, y[:, i])\\n    models.append(model)\\n\\n# Predict the next 5 rows\\nlast_rows = df_separated[winning_numbers.columns].values[-5:]  # Last 5 rows to predict the next values\\npredictions = np.array([model.predict(last_rows) for model in models]).T\\n\\n# Create a DataFrame for the predicted columns\\ndf_predictions = pd.DataFrame(predictions, columns=[f\\'Predicted_Column{i+1}\\' for i in range(len(winning_numbers.columns))])\\n\\n# Round the predictions to integers\\ndf_predictions = df_predictions.round().astype(int)\\n\\n# Display the predicted DataFrame\\nprint(df_predictions)', \"Correct the errors\\n\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n\\n# Load the CSV file, selecting columns 5 to 10 (0-indexed, so it's 4 to 9)\\ndf = pd.read_csv('data/megamillions.csv', usecols=range(4, 10))\\n\\n\\n# Prepare data for training\\nX = df_separated[winning_numbers.columns].values[:-5]  # Use all rows except the last 5 for features\\ny = df_separated[winning_numbers.columns].values[5:]    # Use rows 5 to 4000 for target\\n\\n# Train a linear regression model for each column\\nmodels = []\\nfor i in range(len(winning_numbers.columns)):\\n    model = LinearRegression()\\n    model.fit(X, y[:, i])\\n    models.append(model)\\n\\n# Predict the next 5 rows\\nlast_rows = df_separated[winning_numbers.columns].values[-5:]  # Last 5 rows to predict the next values\\npredictions = np.array([model.predict(last_rows) for model in models]).T\\n\\n# Create a DataFrame for the predicted columns\\ndf_predictions = pd.DataFrame(predictions, columns=[f'Predicted_Column{i+1}' for i in range(len(winning_numbers.columns))])\\n\\n# Round the predictions to integers\\ndf_predictions = df_predictions.round().astype(int)\\n\\n# Display the predicted DataFrame\\nprint(df_predictions\", \"I can't get the same numbers on a column row on the prediction\\n\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n\\n# Load the CSV file, selecting columns 5 to 10 (0-indexed, so it's 4 to 9)\\ndf = pd.read_csv('data/megamillions.csv', usecols=range(4, 10))\\n\\n# Prepare data for training\\nX = df.values[:-5]  # Use all rows except the last 5 for features\\ny = df.values[5:]    # Use rows 5 to 4000 for target\\n\\n# Train a linear regression model for each column\\nmodels = []\\nfor i in range(len(df.columns)):\\n    model = LinearRegression()\\n    model.fit(X, y[:, i])\\n    models.append(model)\\n\\n# Predict the next 5 rows\\nlast_rows = df[df.columns].values[-5:]  # Last 5 rows to predict the next values\\npredictions = np.array([model.predict(last_rows) for model in models]).T\\n\\n# Create a DataFrame for the predicted columns\\ndf_predictions = pd.DataFrame(predictions, columns=[f'Predicted_Column{i+1}' for i in range(len(df.columns))])\\n\\n# Round the predictions to integers\\ndf_predictions = df_predictions.round().astype(int)\\n\\n# Display the predicted DataFrame\\nprint(df_predictions)\", 'What other model predictions you recommend', 'can you provide me with ARIMA ', \"ValueError: 'Date' is not in list. the date is currently separate into columns 2 - 4 on the following code:\\n\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom statsmodels.tsa.arima.model import ARIMA\\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\\n\\n# Load the time series data\\n# Example: df = pd.read_csv('your_time_series_data.csv', index_col='Date', parse_dates=True)\\n# Assuming 'value' is the column with the time series data\\ndf = pd.read_csv('data/megamillions.csv', index_col='Date', parse_dates=True)\\nts = df['value']\\n\\n# Visualize the time series\\nplt.figure(figsize=(10, 6))\\nplt.plot(ts)\\nplt.title('Time Series Data')\\nplt.show()\\n\\n# Make the time series stationary (differencing)\\nts_diff = ts.diff().dropna()\\n\\n# Plot ACF and PACF to determine p and q\\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\\nplot_acf(ts_diff, ax=axes[0])\\nplot_pacf(ts_diff, ax=axes[1])\\nplt.show()\\n\\n# Fit the ARIMA model\\np = 1  # Example value, determine from PACF plot\\nd = 1  # Differencing order\\nq = 1  # Example value, determine from ACF plot\\nmodel = ARIMA(ts, order=(p, d, q))\\nmodel_fit = model.fit()\\n\\n# Summary of the model\\nprint(model_fit.summary())\\n\\n# Make predictions\\nforecast = model_fit.forecast(steps=10)  # Forecast the next 10 time steps\\nprint(forecast)\\n\\n# Plot the forecast\\nplt.figure(figsize=(10, 6))\\nplt.plot(ts, label='Original')\\nplt.plot(forecast, label='Forecast', color='red')\\nplt.title('Time Series Forecast')\\nplt.legend()\\nplt.show()\", 'pandas._libs.tslibs.parsing.DateParseError: day is out of range for month: 9 2003 14, at position 0', 'Traceback (most recent call last):\\n  File \"C:\\\\Program Files\\\\JetBrains\\\\PyCharm Community Edition 2024.1\\\\plugins\\\\python-ce\\\\helpers\\\\pydev\\\\pydevd.py\", line 1535, in _exec\\n    pydev_imports.execfile(file, globals, locals)  # execute the script\\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"C:\\\\Program Files\\\\JetBrains\\\\PyCharm Community Edition 2024.1\\\\plugins\\\\python-ce\\\\helpers\\\\pydev\\\\_pydev_imps\\\\_pydev_execfile.py\", line 18, in execfile\\n    exec(compile(contents+\"\\\\n\", file, \\'exec\\'), glob, loc)\\n  File \"C:\\\\PythonApps\\\\drl-trainer\\\\mega.py\", line 13, in <module>\\n    df[\\'Date\\'] = pd.to_datetime(df.iloc[:, 2].astype(str) + \\' \\' +\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"C:\\\\PythonApps\\\\drl-trainer\\\\venv\\\\Lib\\\\site-packages\\\\pandas\\\\core\\\\tools\\\\datetimes.py\", line 1067, in to_datetime\\n    values = convert_listlike(arg._values, format)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"C:\\\\PythonApps\\\\drl-trainer\\\\venv\\\\Lib\\\\site-packages\\\\pandas\\\\core\\\\tools\\\\datetimes.py\", line 433, in _convert_listlike_datetimes\\n    return _array_strptime_with_fallback(arg, name, utc, format, exact, errors)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"C:\\\\PythonApps\\\\drl-trainer\\\\venv\\\\Lib\\\\site-packages\\\\pandas\\\\core\\\\tools\\\\datetimes.py\", line 467, in _array_strptime_with_fallback\\n    result, tz_out = array_strptime(arg, fmt, exact=exact, errors=errors, utc=utc)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"strptime.pyx\", line 501, in pandas._libs.tslibs.strptime.array_strptime\\n  File \"strptime.pyx\", line 451, in pandas._libs.tslibs.strptime.array_strptime\\n  File \"strptime.pyx\", line 583, in pandas._libs.tslibs.strptime._parse_with_format\\nValueError: time data \"9 14 2003\" doesn\\'t match format \"%m %Y %d\", at position 0. You might want to try:\\n    - passing `format` if your strings have a consistent format;\\n    - passing `format=\\'ISO8601\\'` if your strings are all ISO8601 but not necessarily in exactly the same format;\\n    - passing `format=\\'mixed\\'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\\n\\n\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom statsmodels.tsa.arima.model import ARIMA\\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\\n\\n# Load the CSV file\\ndf = pd.read_csv(\\'data/megamillions.csv\\')\\n\\n# Combine the date columns (assuming they are in columns 2, 3, and 4)\\n# Adjust the column indices based on your actual DataFrame structure\\n# Assuming the format is \\'month year day\\'\\ndf[\\'Date\\'] = pd.to_datetime(df.iloc[:, 2].astype(str) + \\' \\' +\\n                            df.iloc[:, 4].astype(str) + \\' \\' +\\n                            df.iloc[:, 3].astype(str), format=\\'%m %Y %d\\')\\n\\n# Set the \\'Date\\' column as the index\\ndf.set_index(\\'Date\\', inplace=True)\\n\\n# Assuming \\'value\\' is the column with the time series data\\n# Replace \\'value\\' with the actual column name you want to analyze\\nts = df[\\'value\\']  # Make sure to replace \\'value\\' with the correct column name\\n\\n# Visualize the time series\\nplt.figure(figsize=(10, 6))\\nplt.plot(ts)\\nplt.title(\\'Time Series Data\\')\\nplt.show()\\n\\n# Make the time series stationary (differencing)\\nts_diff = ts.diff().dropna()\\n\\n# Plot ACF and PACF to determine p and q\\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\\nplot_acf(ts_diff, ax=axes[0])\\nplot_pacf(ts_diff, ax=axes[1])\\nplt.show()\\n\\n# Fit the ARIMA model\\np = 1  # Example value, determine from PACF plot\\nd = 1  # Differencing order\\nq = 1  # Example value, determine from ACF plot\\nmodel = ARIMA(ts, order=(p, d, q))\\nmodel_fit = model.fit()\\n\\n# Summary of the model\\nprint(model_fit.summary())\\n\\n# Make predictions\\nforecast = model_fit.forecast(steps=10)  # Forecast the next 10 time steps\\nprint(forecast)\\n\\n# Plot the forecast\\nplt.figure(figsize=(10, 6))\\nplt.plot(ts, label=\\'Original\\')\\nplt.plot(forecast, label=\\'Forecast\\', color=\\'red\\')\\nplt.title(\\'Time Series Forecast\\')\\nplt.legend()\\nplt.show()', \"Van you convert the following code to Neural Networks and also another script code with  Gradient Boosting Machines (GBM)\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n\\n# Load the CSV file, selecting columns 5 to 10 (0-indexed, so it's 4 to 9)\\ndf = pd.read_csv('data/megamillions.csv', usecols=range(4, 10))\\n\\n# Prepare data for training\\nX = df.values[:-5]  # Use all rows except the last 5 for features\\ny = df.values[5:]    # Use rows 5 to 4000 for target\\n\\n# Train a linear regression model for each column\\nmodels = []\\nfor i in range(len(df.columns)):\\n    model = LinearRegression()\\n    model.fit(X, y[:, i])\\n    models.append(model)\\n\\n# Predict the next 5 rows\\nlast_rows = df[df.columns].values[-5:]  # Last 5 rows to predict the next values\\npredictions = np.array([model.predict(last_rows) for model in models]).T\\n\\n# Create a DataFrame for the predicted columns\\ndf_predictions = pd.DataFrame(predictions, columns=[f'Predicted_Column{i+1}' for i in range(len(df.columns))])\\n\\n# Round the predictions to integers\\ndf_predictions = df_predictions.round().astype(int)\\n\\n# Display the predicted DataFrame\\nprint(df_predictions)\", \"ModuleNotFoundError: No module named 'tensorflow'\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.preprocessing import StandardScaler\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\n\\n# Load the CSV file, selecting columns 5 to 10 (0-indexed, so it's 4 to 9)\\ndf = pd.read_csv('data/megamillions.csv', usecols=range(4, 10))\\n\\n# Prepare data for training\\nX = df.values[:-5]  # Use all rows except the last 5 for features\\ny = df.values[5:]    # Use rows 5 to 4000 for target\\n\\n# Scale the features\\nscaler = StandardScaler()\\nX_scaled = scaler.fit_transform(X)\\n\\n# Train a neural network model for each column\\nmodels = []\\nfor i in range(len(df.columns)):\\n    model = Sequential()\\n    model.add(Dense(64, activation='relu', input_shape=(X_scaled.shape[1],)))\\n    model.add(Dense(32, activation='relu'))\\n    model.add(Dense(1))  # Output layer for regression\\n\\n    model.compile(optimizer='adam', loss='mean_squared_error')\\n    model.fit(X_scaled, y[:, i], epochs=100, batch_size=10, verbose=0)\\n    models.append(model)\\n\\n# Predict the next 5 rows\\nlast_rows = df[df.columns].values[-5:]  # Last 5 rows to predict the next values\\nlast_rows_scaled = scaler.transform(last_rows)  # Scale the last rows\\npredictions = np.array([model.predict(last_rows_scaled) for model in models]).T\\n\\n# Create a DataFrame for the predicted columns\\ndf_predictions = pd.DataFrame(predictions, columns=[f'Predicted_Column{i+1}' for i in range(len(df.columns))])\\n\\n# Round the predictions to integers\\ndf_predictions = df_predictions.round().astype(int)\\n\\n# Display the predicted DataFrame\\nprint(df_predictions)\", \"ModuleNotFoundError: No module named 'tensorflow'\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 17520/35267 [13:29<11:21, 26.05it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error code: 400 - {'error': {'message': \"Invalid 'messages[0].content': string too long. Expected a string with maximum length 1048576, but got a string with length 1258104 instead.\", 'type': 'invalid_request_error', 'param': 'messages[0].content', 'code': 'string_above_max_length'}}. Failed ['how do I make the graphic bars go sideways on the following code:\\n\\nimport gc\\nimport logging\\n\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\n\\ndef monitor_memory(threshold=80):\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', transaction_cost=0.01):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        if monitor_memory():\\n            agent.save()\\n            gc.collect()\\n            # break\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = delta - transaction_cost  # Reward is the change in price minus transaction cost\\n                total_profit += delta - transaction_cost\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta - transaction_cost  # Reward is the profit made minus transaction cost\\n            total_profit += delta - transaction_cost\\n\\n        # HOLD\\n        else:\\n            pass\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        agent.save()\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss))\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, transaction_cost=0.01):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {}\".format(format_currency(data.iloc[t][adjcp_field])))\\n\\n            # Calculate delta and reward for buy action\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = delta - transaction_cost\\n                total_profit += delta - transaction_cost\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta - transaction_cost  # Reward is the profit made minus transaction cost\\n            total_profit += delta - transaction_cost\\n\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Position: {}\".format(\\n                    format_currency(data.iloc[t][adjcp_field]),\\n                    format_position(data.iloc[t][adjcp_field] - bought_price)))\\n\\n        # HOLD\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'How do I call the following code:\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.02\\nCreated by Javier Sanchez, June 15, 2024\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\"\"\"\\n\\nimport os\\nimport logging\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n# Constants for default values\\nDATE_FIELD = \"Date\"\\nADJCP_FIELD = \"Adj Close\"\\nWINDOW_SIZE = 10\\nBATCH_SIZE = 32\\nEPISODE_COUNT = 50\\nMODEL_NAME = \"model_debug\"\\n\\ndef setup_logging(debug):\\n    \"\"\"Setup logging configuration.\"\"\"\\n    level = \"DEBUG\" if debug else \"INFO\"\\n    coloredlogs.install(level=level)\\n    logging.basicConfig(level=level)\\n\\ndef parse_args():\\n    \"\"\"Parse command-line arguments.\"\"\"\\n    args = docopt(__doc__)\\n    return {\\n        \"file_data\": args[\"<file-data>\"],\\n        \"train_percent\": float(args[\"<train-percent>\"]),\\n        \"date_field\": args[\"--date-field\"] or DATE_FIELD,\\n        \"adjcp_field\": args[\"--adjcp-field\"] or ADJCP_FIELD,\\n        \"window_size\": int(args[\"--window-size\"] or WINDOW_SIZE),\\n        \"batch_size\": int(args[\"--batch-size\"] or BATCH_SIZE),\\n        \"episode_count\": int(args[\"--episode-count\"] or EPISODE_COUNT),\\n        \"model_name\": args[\"--model-name\"] or MODEL_NAME,\\n        \"debug\": args[\"--debug\"]\\n    }\\n\\ndef train_and_evaluate(agent, train_data, val_data, window_size, batch_size, episode_count, adjcp_field, debug):\\n    \"\"\"Train and evaluate the agent.\"\"\"\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = -float(\\'inf\\')\\n    episode = 0\\n    trans_cost = 0.25\\n\\n    while episode < episode_count:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   window_size=window_size, prev_agent_profit=agent_profit, adjcp_field=adjcp_field,\\n                                   trans_cost=trans_cost)\\n        agent_profit = train_result[2]\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, agent.strategy)\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        episode += 1\\n\\ndef main():\\n    \"\"\"Main function to train the stock trading bot.\"\"\"\\n    args = parse_args()\\n    setup_logging(args[\"debug\"])\\n    switch_k_backend_device()\\n\\n    agent = Agent(args[\"window_size\"], strategy=\"t-dqn\", pretrained=True, model_name=args[\"model_name\"])\\n    data = load_data(args[\"file_data\"], args[\"date_field\"])\\n    take = int(len(data) * (args[\"train_percent\"] / 100))\\n    train_data = data_split(data, 0, take, args[\"adjcp_field\"])\\n    val_data = data_split(data, take, len(data), args[\"adjcp_field\"])\\n\\n    try:\\n        train_and_evaluate(agent, train_data, val_data, args[\"window_size\"], args[\"batch_size\"],\\n                           args[\"episode_count\"], args[\"adjcp_field\"], args[\"debug\"])\\n    except KeyboardInterrupt:\\n        logging.info(\"Training aborted by user.\")\\n\\nif __name__ == \"__main__\":\\n    main()', 'I get the following ERROR:root:Error parsing arguments: --date-field is not a unique prefix: --date-field, --date-field? for the following code:\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.02\\nCreated by Javier Sanchez, June 15, 2024\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n  \\nExplanation of Arguments:\\n    70: Percentage of data to use for training (the rest will be used for validation).\\n    --date-field Date: Specifies the name of the date field in your data file (default is Date).\\n    --adjcp-field Adj Close: Specifies the name of the adjusted close price field in your data file (default is Adj Close).\\n    --window-size 10: Size of the n-day window stock data representation used as the feature vector (default is 10).\\n    --batch-size 32: Number of samples to train on in one mini-batch during training (default is 32).\\n    --episode-count 50: Number of trading episodes to use for training (default is 50).\\n    --model-name model_debug: Name of the pretrained model to use (default is model_debug).\\n    --debug: Specifies whether to use verbose logs during the evaluation operation.  \\n  \\n\"\"\"\\n\\nimport os\\nimport logging\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n# Constants for default values\\nDATE_FIELD = \"Date\"\\nADJCP_FIELD = \"Adj Close\"\\nWINDOW_SIZE = 10\\nBATCH_SIZE = 32\\nEPISODE_COUNT = 50\\nMODEL_NAME = \"model_debug\"\\n\\ndef setup_logging(debug):\\n    \"\"\"Setup logging configuration.\"\"\"\\n    level = \"DEBUG\" if debug else \"INFO\"\\n    coloredlogs.install(level=level)\\n    logging.basicConfig(level=level)\\n\\n\\ndef parse_args():\\n    \"\"\"Parse command-line arguments.\"\"\"\\n    try:\\n        args = docopt(__doc__)\\n        return {\\n            \"file_data\": args[\"<file-data>\"],\\n            \"train_percent\": float(args[\"<train-percent>\"]),\\n            \"date_field\": args[\"--date-field\"] or DATE_FIELD,\\n            \"adjcp_field\": args[\"--adjcp-field\"] or ADJCP_FIELD,\\n            \"window_size\": int(args[\"--window-size\"] or WINDOW_SIZE),\\n            \"batch_size\": int(args[\"--batch-size\"] or BATCH_SIZE),\\n            \"episode_count\": int(args[\"--episode-count\"] or EPISODE_COUNT),\\n            \"model_name\": args[\"--model-name\"] or MODEL_NAME,\\n            \"debug\": args[\"--debug\"]\\n        }\\n    except Exception as e:\\n        logging.error(f\"Error parsing arguments: {e}\")\\n        exit(1)\\n\\n\\ndef train_and_evaluate(agent, train_data, val_data, window_size, batch_size, episode_count, adjcp_field, debug):\\n    \"\"\"Train and evaluate the agent.\"\"\"\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = -float(\\'inf\\')\\n    episode = 0\\n    trans_cost = 0.25\\n\\n    while episode < episode_count:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   window_size=window_size, prev_agent_profit=agent_profit, adjcp_field=adjcp_field,\\n                                   trans_cost=trans_cost)\\n        agent_profit = train_result[2]\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, agent.strategy)\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        episode += 1\\n\\ndef main():\\n    \"\"\"Main function to train the stock trading bot.\"\"\"\\n    args = parse_args()\\n    setup_logging(args[\"debug\"])\\n    switch_k_backend_device()\\n\\n    agent = Agent(args[\"window_size\"], strategy=\"t-dqn\", pretrained=True, model_name=args[\"model_name\"])\\n    data = load_data(args[\"file_data\"], args[\"date_field\"])\\n    take = int(len(data) * (args[\"train_percent\"] / 100))\\n    train_data = data_split(data, 0, take, args[\"adjcp_field\"])\\n    val_data = data_split(data, take, len(data), args[\"adjcp_field\"])\\n\\n    try:\\n        train_and_evaluate(agent, train_data, val_data, args[\"window_size\"], args[\"batch_size\"],\\n                           args[\"episode_count\"], args[\"adjcp_field\"], args[\"debug\"])\\n    except KeyboardInterrupt:\\n        logging.info(\"Training aborted by user.\")\\n\\nif __name__ == \"__main__\":\\n    main()', 'Why the following code doesn\\'t save a mode:\\n\\nimport gc\\nimport logging\\n\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\n\\ndef monitor_memory(threshold=80):\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        if monitor_memory():\\n            agent.save()\\n            gc.collect()\\n            # break\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = delta - trans_cost  # Reward is the change in price minus transaction cost\\n                total_profit += delta - trans_cost\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta - trans_cost  # Reward is the profit made minus transaction cost\\n            total_profit += delta - trans_cost\\n\\n        # HOLD\\n        else:\\n            pass\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        agent.save()\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss))\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.01):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {}\".format(format_currency(data.iloc[t][adjcp_field])))\\n\\n            # Calculate delta and reward for buy action\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = delta - trans_cost\\n                total_profit += delta - trans_cost\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta - trans_cost  # Reward is the profit made minus transaction cost\\n            total_profit += delta - trans_cost\\n\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Position: {}\".format(\\n                    format_currency(data.iloc[t][adjcp_field]),\\n                    format_position(data.iloc[t][adjcp_field] - bought_price)))\\n\\n        # HOLD\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'the following code would not run:\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.02\\nCreated by Javier Sanchez, June 15, 2024\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\\nExplanation of Arguments:\\n    path/to/your/data.csv: Path to your stock data file.\\n    70: Percentage of data to use for training (the rest will be used for validation).\\n    --date-field Date: Specifies the name of the date field in your data file (default is Date).\\n    --adjcp-field Adj Close: Specifies the name of the adjusted close price field in your data file (default is Adj Close).\\n    --window-size 10: Size of the n-day window stock data representation used as the feature vector (default is 10).\\n    --batch-size 32: Number of samples to train on in one mini-batch during training (default is 32).\\n    --episode-count 50: Number of trading episodes to use for training (default is 50).\\n    --model-name model_debug: Name of the pretrained model to use (default is model_debug).\\n    --debug: Specifies whether to use verbose logs during the evaluation operation.\\n\"\"\"\\n\\nimport os\\nimport logging\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n# Constants for default values\\nDATE_FIELD = \"Date\"\\nADJCP_FIELD = \"Adj Close\"\\nWINDOW_SIZE = 10\\nBATCH_SIZE = 32\\nEPISODE_COUNT = 50\\nMODEL_NAME = \"model_debug\"\\n\\ndef setup_logging(debug):\\n    \"\"\"Setup logging configuration.\"\"\"\\n    level = \"DEBUG\" if debug else \"INFO\"\\n    coloredlogs.install(level=level)\\n    logging.basicConfig(level=level)\\n\\ndef parse_args():\\n    \"\"\"Parse command-line arguments.\"\"\"\\n    args = docopt(__doc__)\\n    return {\\n        \"file_data\": args[\"<file-data>\"],\\n        \"train_percent\": float(args[\"<train-percent>\"]),\\n        \"date_field\": args[\"--date-field\"] or DATE_FIELD,\\n        \"adjcp_field\": args[\"--adjcp-field\"] or ADJCP_FIELD,\\n        \"window_size\": int(args[\"--window-size\"] or WINDOW_SIZE),\\n        \"batch_size\": int(args[\"--batch-size\"] or BATCH_SIZE),\\n        \"episode_count\": int(args[\"--episode-count\"] or EPISODE_COUNT),\\n        \"model_name\": args[\"--model-name\"] or MODEL_NAME,\\n        \"debug\": args[\"--debug\"]\\n    }\\n\\ndef train_and_evaluate(agent, train_data, val_data, window_size, batch_size, episode_count, adjcp_field, debug):\\n    \"\"\"Train and evaluate the agent.\"\"\"\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = -float(\\'inf\\')\\n    episode = 0\\n    trans_cost = 0.25\\n\\n    while episode < episode_count:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   window_size=window_size, prev_agent_profit=agent_profit, adjcp_field=adjcp_field,\\n                                   trans_cost=trans_cost)\\n        agent_profit = train_result[2]\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, agent.strategy)\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        episode += 1\\n\\ndef main():\\n    \"\"\"Main function to train the stock trading bot.\"\"\"\\n    args = parse_args()\\n    setup_logging(args[\"debug\"])\\n    switch_k_backend_device()\\n\\n    agent = Agent(args[\"window_size\"], strategy=\"t-dqn\", pretrained=True, model_name=args[\"model_name\"])\\n    data = load_data(args[\"file_data\"], args[\"date_field\"])\\n    take = int(len(data) * (args[\"train_percent\"] / 100))\\n    train_data = data_split(data, 0, take, args[\"adjcp_field\"])\\n    val_data = data_split(data, take, len(data), args[\"adjcp_field\"])\\n\\n    try:\\n        train_and_evaluate(agent, train_data, val_data, args[\"window_size\"], args[\"batch_size\"],\\n                           args[\"episode_count\"], args[\"adjcp_field\"], args[\"debug\"])\\n    except KeyboardInterrupt:\\n        logging.info(\"Training aborted by user.\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n', 'the following code gives args and parse_args() errors:\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.02\\nCreated by Javier Sanchez, June 15, 2024\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\\nExplanation of Arguments:\\n    path/to/your/data.csv: Path to your stock data file.\\n    70: Percentage of data to use for training (the rest will be used for validation).\\n    --date-field Date: Specifies the name of the date field in your data file (default is Date).\\n    --adjcp-field Adj Close: Specifies the name of the adjusted close price field in your data file (default is Adj Close).\\n    --window-size 10: Size of the n-day window stock data representation used as the feature vector (default is 10).\\n    --batch-size 32: Number of samples to train on in one mini-batch during training (default is 32).\\n    --episode-count 50: Number of trading episodes to use for training (default is 50).\\n    --model-name model_debug: Name of the pretrained model to use (default is model_debug).\\n    --debug: Specifies whether to use verbose logs during the evaluation operation.\\n\"\"\"\\n\\nimport os\\nimport logging\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n# Constants for default values\\nDATE_FIELD = \"Date\"\\nADJCP_FIELD = \"Adj Close\"\\nWINDOW_SIZE = 10\\nBATCH_SIZE = 32\\nEPISODE_COUNT = 50\\nMODEL_NAME = \"model_debug\"\\n\\ndef setup_logging(debug):\\n    \"\"\"Setup logging configuration.\"\"\"\\n    level = \"DEBUG\" if debug else \"INFO\"\\n    coloredlogs.install(level=level)\\n    logging.basicConfig(level=level)\\n\\ndef parse_args():\\n    \"\"\"Parse command-line arguments.\"\"\"\\n    args = docopt(__doc__)\\n    return {\\n        \"file_data\": args[\"<file-data>\"],\\n        \"train_percent\": float(args[\"<train-percent>\"]),\\n        \"date_field\": args[\"--date-field\"] or DATE_FIELD,\\n        \"adjcp_field\": args[\"--adjcp-field\"] or ADJCP_FIELD,\\n        \"window_size\": int(args[\"--window-size\"] or WINDOW_SIZE),\\n        \"batch_size\": int(args[\"--batch-size\"] or BATCH_SIZE),\\n        \"episode_count\": int(args[\"--episode-count\"] or EPISODE_COUNT),\\n        \"model_name\": args[\"--model-name\"] or MODEL_NAME,\\n        \"debug\": args[\"--debug\"]\\n    }\\n\\ndef train_and_evaluate(agent, train_data, val_data, window_size, batch_size, episode_count, adjcp_field, debug):\\n    \"\"\"Train and evaluate the agent.\"\"\"\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = -float(\\'inf\\')\\n    episode = 0\\n    trans_cost = 0.25\\n\\n    while episode < episode_count:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   window_size=window_size, prev_agent_profit=agent_profit, adjcp_field=adjcp_field,\\n                                   trans_cost=trans_cost)\\n        agent_profit = train_result[2]\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, agent.strategy)\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        episode += 1\\n\\ndef main():\\n    \"\"\"Main function to train the stock trading bot.\"\"\"\\n    args = parse_args()\\n    setup_logging(args[\"debug\"])\\n    switch_k_backend_device()\\n\\n    agent = Agent(args[\"window_size\"], strategy=\"t-dqn\", pretrained=True, model_name=args[\"model_name\"])\\n    data = load_data(args[\"file_data\"], args[\"date_field\"])\\n    take = int(len(data) * (args[\"train_percent\"] / 100))\\n    train_data = data_split(data, 0, take, args[\"adjcp_field\"])\\n    val_data = data_split(data, take, len(data), args[\"adjcp_field\"])\\n\\n    try:\\n        train_and_evaluate(agent, train_data, val_data, args[\"window_size\"], args[\"batch_size\"],\\n                           args[\"episode_count\"], args[\"adjcp_field\"], args[\"debug\"])\\n    except KeyboardInterrupt:\\n        logging.info(\"Training aborted by user.\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n', 'the following code gives Error parsing arguments: --date-field is not a unique prefix: --date-field\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.02\\nCreated by Javier Sanchez, June 15, 2024\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\\nExplanation of Arguments:\\n    path/to/your/data.csv: Path to your stock data file.\\n    70: Percentage of data to use for training (the rest will be used for validation).\\n    --date-field Date: Specifies the name of the date field in your data file (default is Date).\\n    --adjcp-field Adj Close: Specifies the name of the adjusted close price field in your data file (default is Adj Close).\\n    --window-size 10: Size of the n-day window stock data representation used as the feature vector (default is 10).\\n    --batch-size 32: Number of samples to train on in one mini-batch during training (default is 32).\\n    --episode-count 50: Number of trading episodes to use for training (default is 50).\\n    --model-name model_debug: Name of the pretrained model to use (default is model_debug).\\n    --debug: Specifies whether to use verbose logs during the evaluation operation.\\n\"\"\"\\n\\nimport os\\nimport logging\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n# Constants for default values\\nDATE_FIELD = \"Date\"\\nADJCP_FIELD = \"Adj Close\"\\nWINDOW_SIZE = 10\\nBATCH_SIZE = 32\\nEPISODE_COUNT = 50\\nMODEL_NAME = \"model_debug\"\\n\\ndef setup_logging(debug):\\n    \"\"\"Setup logging configuration.\"\"\"\\n    level = \"DEBUG\" if debug else \"INFO\"\\n    coloredlogs.install(level=level)\\n    logging.basicConfig(level=level)\\n\\ndef parse_args():\\n    \"\"\"Parse command-line arguments.\"\"\"\\n    try:\\n        args = docopt(__doc__)\\n        return {\\n            \"file_data\": args[\"<file-data>\"],\\n            \"train_percent\": float(args[\"<train-percent>\"]),\\n            \"date_field\": args[\"--date-field\"] or DATE_FIELD,\\n            \"adjcp_field\": args[\"--adjcp-field\"] or ADJCP_FIELD,\\n            \"window_size\": int(args[\"--window-size\"] or WINDOW_SIZE),\\n            \"batch_size\": int(args[\"--batch-size\"] or BATCH_SIZE),\\n            \"episode_count\": int(args[\"--episode-count\"] or EPISODE_COUNT),\\n            \"model_name\": args[\"--model-name\"] or MODEL_NAME,\\n            \"debug\": args[\"--debug\"]\\n        }\\n    except Exception as e:\\n        logging.error(f\"Error parsing arguments: {e}\")\\n        exit(1)\\n\\ndef train_and_evaluate(agent, train_data, val_data, window_size, batch_size, episode_count, adjcp_field, debug):\\n    \"\"\"Train and evaluate the agent.\"\"\"\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = -float(\\'inf\\')\\n    episode = 0\\n    trans_cost = 0.25\\n\\n    while episode < episode_count:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   window_size=window_size, prev_agent_profit=agent_profit, adjcp_field=adjcp_field,\\n                                   trans_cost=trans_cost)\\n        agent_profit = train_result[2]\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, agent.strategy)\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        episode += 1\\n\\ndef main():\\n    \"\"\"Main function to train the stock trading bot.\"\"\"\\n    args = parse_args()\\n    setup_logging(args[\"debug\"])\\n    switch_k_backend_device()\\n\\n    agent = Agent(args[\"window_size\"], strategy=\"t-dqn\", pretrained=True, model_name=args[\"model_name\"])\\n    data = load_data(args[\"file_data\"], args[\"date_field\"])\\n    take = int(len(data) * (args[\"train_percent\"] / 100))\\n    train_data = data_split(data, 0, take, args[\"adjcp_field\"])\\n    val_data = data_split(data, take, len(data), args[\"adjcp_field\"])\\n\\n    try:\\n        train_and_evaluate(agent, train_data, val_data, args[\"window_size\"], args[\"batch_size\"],\\n                           args[\"episode_count\"], args[\"adjcp_field\"], args[\"debug\"])\\n    except KeyboardInterrupt:\\n        logging.info(\"Training aborted by user.\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n', 'the following code gives ValueError: Weights for model \\'sequential\\' have not yet been created. Weights are created when the model is first called on inputs or `build()` is called with an `input_shape`.\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.02\\nCreated by Javier Sanchez, June 15, 2024\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\"\"\"\\n\\nimport os\\nimport logging\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n# Constants for default values\\nDATE_FIELD = \"Date\"\\nADJCP_FIELD = \"Adj Close\"\\nWINDOW_SIZE = 10\\nBATCH_SIZE = 32\\nEPISODE_COUNT = 50\\nMODEL_NAME = \"model_debug\"\\n\\ndef setup_logging(debug):\\n    \"\"\"Setup logging configuration.\"\"\"\\n    level = \"DEBUG\" if debug else \"INFO\"\\n    coloredlogs.install(level=level)\\n    logging.basicConfig(level=level)\\n\\ndef parse_args():\\n    \"\"\"Parse command-line arguments.\"\"\"\\n    try:\\n        args = docopt(__doc__)\\n        return {\\n            \"file_data\": args[\"<file-data>\"],\\n            \"train_percent\": float(args[\"<train-percent>\"]),\\n            \"date_field\": args[\"--date-field\"] or DATE_FIELD,\\n            \"adjcp_field\": args[\"--adjcp-field\"] or ADJCP_FIELD,\\n            \"window_size\": int(args[\"--window-size\"] or WINDOW_SIZE),\\n            \"batch_size\": int(args[\"--batch-size\"] or BATCH_SIZE),\\n            \"episode_count\": int(args[\"--episode-count\"] or EPISODE_COUNT),\\n            \"model_name\": args[\"--model-name\"] or MODEL_NAME,\\n            \"debug\": args[\"--debug\"]\\n        }\\n    except Exception as e:\\n        logging.error(f\"Error parsing arguments: {e}\")\\n        exit(1)\\n\\ndef train_and_evaluate(agent, train_data, val_data, window_size, batch_size, episode_count, adjcp_field, debug):\\n    \"\"\"Train and evaluate the agent.\"\"\"\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = -float(\\'inf\\')\\n    episode = 0\\n    trans_cost = 0.25\\n\\n    while episode < episode_count:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   window_size=window_size, prev_agent_profit=agent_profit, adjcp_field=adjcp_field,\\n                                   trans_cost=trans_cost)\\n        agent_profit = train_result[2]\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, agent.strategy)\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        episode += 1\\n\\ndef main():\\n    \"\"\"Main function to train the stock trading bot.\"\"\"\\n    args = parse_args()\\n    setup_logging(args[\"debug\"])\\n    switch_k_backend_device()\\n\\n    agent = Agent(args[\"window_size\"], strategy=\"t-dqn\", pretrained=True, model_name=args[\"model_name\"])\\n    data = load_data(args[\"file_data\"], args[\"date_field\"])\\n    take = int(len(data) * (args[\"train_percent\"] / 100))\\n    train_data = data_split(data, 0, take, args[\"adjcp_field\"])\\n    val_data = data_split(data, take, len(data), args[\"adjcp_field\"])\\n\\n    try:\\n        train_and_evaluate(agent, train_data, val_data, args[\"window_size\"], args[\"batch_size\"],\\n                           args[\"episode_count\"], args[\"adjcp_field\"], args[\"debug\"])\\n    except KeyboardInterrupt:\\n        logging.info(\"Training aborted by user.\")\\n\\nif __name__ == \"__main__\":\\n    main()', 'please check the following code:\\n\\nimport gc\\nimport logging\\n\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\n\\ndef monitor_memory(threshold=80):\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = delta - trans_cost\\n                total_profit += delta - trans_cost\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta - trans_cost\\n            total_profit += delta - trans_cost\\n\\n        # HOLD\\n        else:\\n            pass\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss))\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {}\".format(format_currency(data.iloc[t][adjcp_field])))\\n\\n            # Calculate delta and reward for buy action\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = delta - trans_cost\\n                total_profit += delta - trans_cost\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta - trans_cost  # Reward is the profit made minus transaction cost\\n            total_profit += delta - trans_cost\\n\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Position: {}\".format(\\n                    format_currency(data.iloc[t][adjcp_field]),\\n                    format_position(data.iloc[t][adjcp_field] - bought_price)))\\n\\n        # HOLD\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action\\n', 'Please improve logging memory management, and reward calculation for the following code:\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = delta - trans_cost\\n                total_profit += delta - trans_cost\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta - trans_cost\\n            total_profit += delta - trans_cost\\n\\n        # HOLD\\n        else:\\n            pass\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        logging.info(f\"Average loss for episode {episode}: {np.mean(avg_loss)}\")\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss))', 'Please improve logging memory management, and reward calculation for the following code:\\n\\nimport gc\\nimport logging\\n\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\n\\ndef monitor_memory(threshold=80):\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = delta - trans_cost\\n                total_profit += delta - trans_cost\\n                logging.debug(f\"BUY: Price: {data.iloc[t][adjcp_field]}, Reward: {reward}\")\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta - trans_cost\\n            total_profit += delta - trans_cost\\n            logging.debug(f\"SELL: Price: {data.iloc[t][adjcp_field]}, Reward: {reward}\")\\n\\n        # HOLD\\n        else:\\n            reward = 0  # Explicitly set reward to 0 for HOLD action\\n            logging.debug(f\"HOLD: Price: {data.iloc[t][adjcp_field]}\")\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {}\".format(format_currency(data.iloc[t][adjcp_field])))\\n\\n            # Calculate delta and reward for buy action\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = delta - trans_cost\\n                total_profit += delta - trans_cost\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta - trans_cost  # Reward is the profit made minus transaction cost\\n            total_profit += delta - trans_cost\\n\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Position: {}\".format(\\n                    format_currency(data.iloc[t][adjcp_field]),\\n                    format_position(data.iloc[t][adjcp_field] - bought_price)))\\n\\n        # HOLD\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action\\n', 'How can I improve the following code:\\n\\nimport logging\\nimport os\\n\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\n# Formats Position\\nformat_position = lambda price: (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n# Formats Currency\\nformat_currency = lambda price: \\'${0:.2f}\\'.format(abs(price))\\n\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\" Displays training results\\n    \"\"\"\\n    if val_position == initial_offset or val_position == 0.0:\\n        logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: USELESS  Train Loss: {:.4f}\\'\\n                     .format(strategy, result[0], result[1], format_position(result[2]), result[3]))\\n        return False\\n    else:\\n        logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: {}  Train Loss: {:.4f})\\'\\n                     .format(strategy, result[0], result[1], format_position(result[2]), format_position(val_position),\\n                             result[3], ))\\n        return True\\n\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\" Displays eval results\\n    \"\"\"\\n    if profit == initial_offset or profit == 0.0:\\n        logging.info(\\'{}: USELESS\\\\n\\'.format(model_name))\\n    else:\\n        logging.info(\\'{}: {}\\\\n\\'.format(model_name, format_position(profit)))\\n\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\"Reads stock data from csv file\\n    \"\"\"\\n\\n    df = pd.read_csv(stock_file)\\n    if date_field != \"Date\":\\n        df[\\'Date\\'] = df[date_field]\\n\\n    df = df.sort_values([date_field])\\n    df[\\'adjcp\\'] = df[adjcp_field]\\n    # Add Indicators and Pattern Recognition Functions\\n    df = add_indicators(df)\\n    return df\\n\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\"Reads stock data from csv file\\n    \"\"\"\\n    df = pd.read_csv(stock_file)\\n    df = df.sort_values([date_field])\\n    return list(df[adjcp_field])\\n\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\"\\n    split the dataset into training or testing using date\\n    :param df: (df) pandas dataframe\\n    :param start:\\n    :param end:\\n    :return: (df) pandas dataframe\\n    \"\"\"\\n    data = df[(df.index >= start) & (df.index < end)]\\n    data.reset_index()\\n    return data\\n\\n\\ndef switch_k_backend_device():\\n    \"\"\" Switches `keras` backend from GPU to CPU if required.\\n\\n    Faster computation on CPU (if using tensorflow-gpu).\\n    \"\"\"\\n    if keras.backend == \"tensorflow\":\\n        logging.debug(\"switching to TensorFlow for CPU\")\\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n\\n\\ndef get_features():\\n    \"\"\"Returns an n-day state representation ending at time t\\n    \"\"\"\\n    all_feature_name = [\\'CDL2CROWS\\',\\n                        \\'CDL3BLACKCROWS\\',\\n                        \\'CDL3INSIDE\\',\\n                        \\'CDL3LINESTRIKE\\',\\n                        \\'CDL3OUTSIDE\\',\\n                        \\'CDL3STARSINSOUTH\\',\\n                        \\'CDL3WHITESOLDIERS\\',\\n                        \\'CDLABANDONEDBABY\\',\\n                        \\'CDLADVANCEBLOCK\\',\\n                        \\'CDLBELTHOLD\\',\\n                        \\'CDLBREAKAWAY\\',\\n                        \\'CDLCLOSINGMARUBOZU\\',\\n                        \\'CDLCONCEALBABYSWALL\\',\\n                        \\'CDLCOUNTERATTACK\\',\\n                        \\'CDLDARKCLOUDCOVER\\',\\n                        \\'CDLDOJI\\',\\n                        \\'CDLDOJISTAR\\',\\n                        \\'CDLDRAGONFLYDOJI\\',\\n                        \\'CDLENGULFING\\',\\n                        \\'CDLEVENINGDOJISTAR\\',\\n                        \\'CDLEVENINGSTAR\\',\\n                        \\'CDLGAPSIDESIDEWHITE\\',\\n                        \\'CDLGRAVESTONEDOJI\\',\\n                        \\'CDLHAMMER\\',\\n                        \\'CDLHANGINGMAN\\',\\n                        \\'CDLHARAMI\\',\\n                        \\'CDLHARAMICROSS\\',\\n                        \\'CDLHIGHWAVE\\',\\n                        \\'CDLHIKKAKE\\',\\n                        \\'CDLHIKKAKEMOD\\',\\n                        \\'CDLHOMINGPIGEON\\',\\n                        \\'CDLIDENTICAL3CROWS\\',\\n                        \\'CDLINNECK\\',\\n                        \\'CDLINVERTEDHAMMER\\',\\n                        \\'CDLKICKING\\',\\n                        \\'CDLKICKINGBYLENGTH\\',\\n                        \\'CDLLADDERBOTTOM\\',\\n                        \\'CDLLONGLEGGEDDOJI\\',\\n                        \\'CDLLONGLINE\\',\\n                        \\'CDLMARUBOZU\\',\\n                        \\'CDLMATCHINGLOW\\',\\n                        \\'CDLMATHOLD\\',\\n                        \\'CDLMORNINGDOJISTAR\\',\\n                        \\'CDLMORNINGSTAR\\',\\n                        \\'CDLONNECK\\',\\n                        \\'CDLPIERCING\\',\\n                        \\'CDLRICKSHAWMAN\\',\\n                        \\'CDLRISEFALL3METHODS\\',\\n                        \\'CDLSEPARATINGLINES\\',\\n                        \\'CDLSHOOTINGSTAR\\',\\n                        \\'CDLSHORTLINE\\',\\n                        \\'CDLSPINNINGTOP\\',\\n                        \\'CDLSTALLEDPATTERN\\',\\n                        \\'CDLSTICKSANDWICH\\',\\n                        \\'CDLTAKURI\\',\\n                        \\'CDLTASUKIGAP\\',\\n                        \\'CDLTHRUSTING\\',\\n                        \\'CDLTRISTAR\\',\\n                        \\'CDLUNIQUE3RIVER\\',\\n                        \\'CDLUPSIDEGAP2CROWS\\',\\n                        \\'CDLXSIDEGAP3METHODS\\',\\n                        # \\'RSI\\',\\n                        \\'MOMENTUM\\',\\n                        # \\'DX\\',\\n                        \\'CCI\\',\\n                        # \\'WILLR\\',\\n                        \\'MFI\\',\\n                        \\'ROC\\',\\n                        # \\'ATR\\',\\n                        \\'ADX\\',\\n                        \\'PPO\\'\\n                        # \\'MACD\\',\\n                        # \\'MACDSIGNAL\\',\\n                        # \\'MACDHIST\\'\\n                        ]\\n    return all_feature_name\\n\\n\\ndef add_indicators(df):\\n    # Pattern Recognition Functions\\n    df[\\'CDL2CROWS\\'] = talib.CDL2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3BLACKCROWS\\'] = talib.CDL3BLACKCROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3INSIDE\\'] = talib.CDL3INSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3LINESTRIKE\\'] = talib.CDL3LINESTRIKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3OUTSIDE\\'] = talib.CDL3OUTSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3STARSINSOUTH\\'] = talib.CDL3STARSINSOUTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDL3WHITESOLDIERS\\'] = talib.CDL3WHITESOLDIERS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLABANDONEDBABY\\'] = talib.CDLABANDONEDBABY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLADVANCEBLOCK\\'] = talib.CDLADVANCEBLOCK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLBELTHOLD\\'] = talib.CDLBELTHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLBREAKAWAY\\'] = talib.CDLBREAKAWAY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLCLOSINGMARUBOZU\\'] = talib.CDLCLOSINGMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLCONCEALBABYSWALL\\'] = talib.CDLCONCEALBABYSWALL(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLCOUNTERATTACK\\'] = talib.CDLCOUNTERATTACK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDARKCLOUDCOVER\\'] = talib.CDLDARKCLOUDCOVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLDOJI\\'] = talib.CDLDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDOJISTAR\\'] = talib.CDLDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDRAGONFLYDOJI\\'] = talib.CDLDRAGONFLYDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLENGULFING\\'] = talib.CDLENGULFING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLEVENINGDOJISTAR\\'] = talib.CDLEVENINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLEVENINGSTAR\\'] = talib.CDLEVENINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLGAPSIDESIDEWHITE\\'] = talib.CDLGAPSIDESIDEWHITE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLGRAVESTONEDOJI\\'] = talib.CDLGRAVESTONEDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHAMMER\\'] = talib.CDLHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHANGINGMAN\\'] = talib.CDLHANGINGMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHARAMI\\'] = talib.CDLHARAMI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHARAMICROSS\\'] = talib.CDLHARAMICROSS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHIGHWAVE\\'] = talib.CDLHIGHWAVE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHIKKAKE\\'] = talib.CDLHIKKAKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHIKKAKEMOD\\'] = talib.CDLHIKKAKEMOD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHOMINGPIGEON\\'] = talib.CDLHOMINGPIGEON(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLIDENTICAL3CROWS\\'] = talib.CDLIDENTICAL3CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLINNECK\\'] = talib.CDLINNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLINVERTEDHAMMER\\'] = talib.CDLINVERTEDHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLKICKING\\'] = talib.CDLKICKING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLKICKINGBYLENGTH\\'] = talib.CDLKICKINGBYLENGTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLLADDERBOTTOM\\'] = talib.CDLLADDERBOTTOM(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLLONGLEGGEDDOJI\\'] = talib.CDLLONGLEGGEDDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLLONGLINE\\'] = talib.CDLLONGLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLMARUBOZU\\'] = talib.CDLMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLMATCHINGLOW\\'] = talib.CDLMATCHINGLOW(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLMATHOLD\\'] = talib.CDLMATHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLMORNINGDOJISTAR\\'] = talib.CDLMORNINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLMORNINGSTAR\\'] = talib.CDLMORNINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLONNECK\\'] = talib.CDLONNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLPIERCING\\'] = talib.CDLPIERCING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLRICKSHAWMAN\\'] = talib.CDLRICKSHAWMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLRISEFALL3METHODS\\'] = talib.CDLRISEFALL3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSEPARATINGLINES\\'] = talib.CDLSEPARATINGLINES(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSHOOTINGSTAR\\'] = talib.CDLSHOOTINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSHORTLINE\\'] = talib.CDLSHORTLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSPINNINGTOP\\'] = talib.CDLSPINNINGTOP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSTALLEDPATTERN\\'] = talib.CDLSTALLEDPATTERN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLSTICKSANDWICH\\'] = talib.CDLSTICKSANDWICH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLTAKURI\\'] = talib.CDLTAKURI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLTASUKIGAP\\'] = talib.CDLTASUKIGAP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLTHRUSTING\\'] = talib.CDLTHRUSTING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLTRISTAR\\'] = talib.CDLTRISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLUNIQUE3RIVER\\'] = talib.CDLUNIQUE3RIVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLUPSIDEGAP2CROWS\\'] = talib.CDLUPSIDEGAP2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLXSIDEGAP3METHODS\\'] = talib.CDLXSIDEGAP3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # Price action\\n    # https://ta-lib.github.io/ta-lib-python/func_groups/momentum_indicators.html\\n    # rsi,stochRsi,cci,williamsr,mfi,roc,atr,adx\\n    # df[\\'RSI\\'] = talib.RSI(df[\\'Close\\'], timeperiod=14)\\n    df[\\'MOMENTUM\\'] = talib.MOM(df[\\'Close\\'], timeperiod=10)\\n    # df[\\'DX\\'] = talib.stream_DX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    # df[\\'STOCHRSI\\'] = talib.STOCHRSI(df[\\'Close\\'], timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)\\n    df[\\'CCI\\'] = talib.CCI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    # df[\\'WILLR\\'] = talib.WILLR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'MFI\\'] = talib.MFI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'], timeperiod=14)\\n    df[\\'ROC\\'] = talib.ROC(df[\\'Close\\'], timeperiod=10)\\n    # df[\\'ATR\\'] = talib.ATR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'ADX\\'] = talib.ADX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'PPO\\'] = talib.PPO(df[\\'Close\\'], fastperiod=5, slowperiod=15, matype=8)\\n    # df[\\'MACD\\'], df[\\'MACDSIGNAL\\'], df[\\'MACDHIST\\'] = talib.MACD(df[\\'Close\\'], fastperiod=5, slowperiod=15, signalperiod=9)\\n    df = df.fillna(0)\\n\\n    return df\\n', 'Please improve the following code:\\n\\nimport math\\n\\nimport numpy as np\\n\\nfrom trading_bot.utils import get_features\\n\\n\\ndef sigmoid(x):\\n    \"\"\"Performs sigmoid operation\\n    \"\"\"\\n    try:\\n        if x < 0:\\n            return 1 - 1 / (1 + math.exp(x))\\n        return 1 / (1 + math.exp(-x))\\n    except Exception as err:\\n        print(\"Error in sigmoid: \" + err)\\n\\n\\ndef get_state(data, t, n_days):\\n    \"\"\"Returns an n-day state representation ending at time t\\n    \"\"\"\\n    all_feature_name = get_features()\\n    d = t - n_days + 1\\n    all_feature = np.column_stack([data[k] for k in all_feature_name])\\n    block = all_feature[d:t + 1] if d >= 0 else np.vstack([-d * [all_feature[0]], all_feature[0:t + 1]])\\n    return block\\n', 'ValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 7675 and the array at index 1 has size 67 for the following code:\\n\\nimport math\\nimport numpy as np\\nfrom trading_bot.utils import get_features\\n\\ndef sigmoid(x):\\n    \"\"\"Performs sigmoid operation.\"\"\"\\n    try:\\n        if x < 0:\\n            return 1 - 1 / (1 + math.exp(x))\\n        return 1 / (1 + math.exp(-x))\\n    except Exception as err:\\n        print(f\"Error in sigmoid: {err}\")\\n        return None\\n\\ndef get_state(data, t, n_days):\\n    \"\"\"Returns an n-day state representation ending at time t.\"\"\"\\n    all_feature_name = get_features()\\n    d = t - n_days + 1\\n\\n    # Ensure the start index is not negative\\n    if d < 0:\\n        padding = np.tile(data[all_feature_name[0]], (-d, 1))\\n        block = np.vstack([padding, data[all_feature_name][:t + 1]])\\n    else:\\n        block = data[all_feature_name][d:t + 1]\\n\\n    return block', 'TypeError: MOM() takes at most 2 positional arguments (4 given) for the following code:\\n\\nimport logging\\nimport os\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\n\\n# Formats Position\\ndef format_position(price):\\n    return (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n\\n# Formats Currency\\ndef format_currency(price):\\n    return \\'${0:.2f}\\'.format(abs(price))\\n\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\" Displays training results \"\"\"\\n    if val_position in {initial_offset, 0.0}:\\n        logging.info(f\\'{strategy}: Episode {result[0]}/{result[1]} - Train Position: {format_position(result[2])}  Val Position: USELESS  Train Loss: {result[3]:.4f}\\')\\n        return False\\n    else:\\n        logging.info(f\\'{strategy}: Episode {result[0]}/{result[1]} - Train Position: {format_position(result[2])}  Val Position: {format_position(val_position)}  Train Loss: {result[3]:.4f}\\')\\n        return True\\n\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\" Displays evaluation results \"\"\"\\n    if profit in {initial_offset, 0.0}:\\n        logging.info(f\\'{model_name}: USELESS\\\\n\\')\\n    else:\\n        logging.info(f\\'{model_name}: {format_position(profit)}\\\\n\\')\\n\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\" Reads stock data from CSV file \"\"\"\\n    df = pd.read_csv(stock_file)\\n    if date_field != \"Date\":\\n        df[\\'Date\\'] = df[date_field]\\n    df = df.sort_values([date_field])\\n    df[\\'adjcp\\'] = df[adjcp_field]\\n    return add_indicators(df)\\n\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\" Reads stock data from CSV file \"\"\"\\n    df = pd.read_csv(stock_file)\\n    return list(df.sort_values([date_field])[adjcp_field])\\n\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\" Splits the dataset into training or testing using date \"\"\"\\n    data = df[(df.index >= start) & (df.index < end)]\\n    return data.reset_index(drop=True)\\n\\n\\ndef switch_k_backend_device():\\n    \"\"\" Switches `keras` backend from GPU to CPU if required. \"\"\"\\n    if keras.backend == \"tensorflow\":\\n        logging.debug(\"Switching to TensorFlow for CPU\")\\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n\\n\\ndef get_features():\\n    \"\"\" Returns a list of feature names for technical indicators \"\"\"\\n    return [\\n        \\'CDL2CROWS\\', \\'CDL3BLACKCROWS\\', \\'CDL3INSIDE\\', \\'CDL3LINESTRIKE\\', \\'CDL3OUTSIDE\\',\\n        \\'CDL3STARSINSOUTH\\', \\'CDL3WHITESOLDIERS\\', \\'CDLABANDONEDBABY\\', \\'CDLADVANCEBLOCK\\',\\n        \\'CDLBELTHOLD\\', \\'CDLBREAKAWAY\\', \\'CDLCLOSINGMARUBOZU\\', \\'CDLCONCEALBABYSWALL\\',\\n        \\'CDLCOUNTERATTACK\\', \\'CDLDARKCLOUDCOVER\\', \\'CDLDOJI\\', \\'CDLDOJISTAR\\', \\'CDLDRAGONFLYDOJI\\',\\n        \\'CDLENGULFING\\', \\'CDLEVENINGDOJISTAR\\', \\'CDLEVENINGSTAR\\', \\'CDLGAPSIDESIDEWHITE\\',\\n        \\'CDLGRAVESTONEDOJI\\', \\'CDLHAMMER\\', \\'CDLHANGINGMAN\\', \\'CDLHARAMI\\', \\'CDLHARAMICROSS\\',\\n        \\'CDLHIGHWAVE\\', \\'CDLHIKKAKE\\', \\'CDLHIKKAKEMOD\\', \\'CDLHOMINGPIGEON\\', \\'CDLIDENTICAL3CROWS\\',\\n        \\'CDLINNECK\\', \\'CDLINVERTEDHAMMER\\', \\'CDLKICKING\\', \\'CDLKICKINGBYLENGTH\\', \\'CDLLADDERBOTTOM\\',\\n        \\'CDLLONGLEGGEDDOJI\\', \\'CDLLONGLINE\\', \\'CDLMARUBOZU\\', \\'CDLMATCHINGLOW\\', \\'CDLMATHOLD\\',\\n        \\'CDLMORNINGDOJISTAR\\', \\'CDLMORNINGSTAR\\', \\'CDLONNECK\\', \\'CDLPIERCING\\', \\'CDLRICKSHAWMAN\\',\\n        \\'CDLRISEFALL3METHODS\\', \\'CDLSEPARATINGLINES\\', \\'CDLSHOOTINGSTAR\\', \\'CDLSHORTLINE\\',\\n        \\'CDLSPINNINGTOP\\', \\'CDLSTALLEDPATTERN\\', \\'CDLSTICKSANDWICH\\', \\'CDLTAKURI\\', \\'CDLTASUKIGAP\\',\\n        \\'CDLTHRUSTING\\', \\'CDLTRISTAR\\', \\'CDLUNIQUE3RIVER\\', \\'CDLUPSIDEGAP2CROWS\\', \\'CDLXSIDEGAP3METHODS\\',\\n        \\'MOMENTUM\\', \\'CCI\\', \\'MFI\\', \\'ROC\\', \\'ADX\\', \\'PPO\\'\\n    ]\\n#\\n# def add_indicators(df):\\n#     # Pattern Recognition Functions\\n#     df[\\'CDL2CROWS\\'] = talib.CDL2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDL3BLACKCROWS\\'] = talib.CDL3BLACKCROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDL3INSIDE\\'] = talib.CDL3INSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDL3LINESTRIKE\\'] = talib.CDL3LINESTRIKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDL3OUTSIDE\\'] = talib.CDL3OUTSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDL3STARSINSOUTH\\'] = talib.CDL3STARSINSOUTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDL3WHITESOLDIERS\\'] = talib.CDL3WHITESOLDIERS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLABANDONEDBABY\\'] = talib.CDLABANDONEDBABY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n#     df[\\'CDLADVANCEBLOCK\\'] = talib.CDLADVANCEBLOCK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLBELTHOLD\\'] = talib.CDLBELTHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLBREAKAWAY\\'] = talib.CDLBREAKAWAY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLCLOSINGMARUBOZU\\'] = talib.CDLCLOSINGMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLCONCEALBABYSWALL\\'] = talib.CDLCONCEALBABYSWALL(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLCOUNTERATTACK\\'] = talib.CDLCOUNTERATTACK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLDARKCLOUDCOVER\\'] = talib.CDLDARKCLOUDCOVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n#     df[\\'CDLDOJI\\'] = talib.CDLDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLDOJISTAR\\'] = talib.CDLDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLDRAGONFLYDOJI\\'] = talib.CDLDRAGONFLYDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLENGULFING\\'] = talib.CDLENGULFING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLEVENINGDOJISTAR\\'] = talib.CDLEVENINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n#     df[\\'CDLEVENINGSTAR\\'] = talib.CDLEVENINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n#     df[\\'CDLGAPSIDESIDEWHITE\\'] = talib.CDLGAPSIDESIDEWHITE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLGRAVESTONEDOJI\\'] = talib.CDLGRAVESTONEDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLHAMMER\\'] = talib.CDLHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLHANGINGMAN\\'] = talib.CDLHANGINGMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLHARAMI\\'] = talib.CDLHARAMI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLHARAMICROSS\\'] = talib.CDLHARAMICROSS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLHIGHWAVE\\'] = talib.CDLHIGHWAVE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLHIKKAKE\\'] = talib.CDLHIKKAKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLHIKKAKEMOD\\'] = talib.CDLHIKKAKEMOD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLHOMINGPIGEON\\'] = talib.CDLHOMINGPIGEON(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLIDENTICAL3CROWS\\'] = talib.CDLIDENTICAL3CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLINNECK\\'] = talib.CDLINNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLINVERTEDHAMMER\\'] = talib.CDLINVERTEDHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLKICKING\\'] = talib.CDLKICKING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLKICKINGBYLENGTH\\'] = talib.CDLKICKINGBYLENGTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLLADDERBOTTOM\\'] = talib.CDLLADDERBOTTOM(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLLONGLEGGEDDOJI\\'] = talib.CDLLONGLEGGEDDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLLONGLINE\\'] = talib.CDLLONGLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLMARUBOZU\\'] = talib.CDLMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLMATCHINGLOW\\'] = talib.CDLMATCHINGLOW(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLMATHOLD\\'] = talib.CDLMATHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n#     df[\\'CDLMORNINGDOJISTAR\\'] = talib.CDLMORNINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n#     df[\\'CDLMORNINGSTAR\\'] = talib.CDLMORNINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n#     df[\\'CDLONNECK\\'] = talib.CDLONNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLPIERCING\\'] = talib.CDLPIERCING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLRICKSHAWMAN\\'] = talib.CDLRICKSHAWMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLRISEFALL3METHODS\\'] = talib.CDLRISEFALL3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLSEPARATINGLINES\\'] = talib.CDLSEPARATINGLINES(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLSHOOTINGSTAR\\'] = talib.CDLSHOOTINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLSHORTLINE\\'] = talib.CDLSHORTLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLSPINNINGTOP\\'] = talib.CDLSPINNINGTOP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLSTALLEDPATTERN\\'] = talib.CDLSTALLEDPATTERN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLSTICKSANDWICH\\'] = talib.CDLSTICKSANDWICH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLTAKURI\\'] = talib.CDLTAKURI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLTASUKIGAP\\'] = talib.CDLTASUKIGAP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLTHRUSTING\\'] = talib.CDLTHRUSTING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLTRISTAR\\'] = talib.CDLTRISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLUNIQUE3RIVER\\'] = talib.CDLUNIQUE3RIVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLUPSIDEGAP2CROWS\\'] = talib.CDLUPSIDEGAP2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     df[\\'CDLXSIDEGAP3METHODS\\'] = talib.CDLXSIDEGAP3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n#     # Price action\\n#     # https://ta-lib.github.io/ta-lib-python/func_groups/momentum_indicators.html\\n#     # rsi,stochRsi,cci,williamsr,mfi,roc,atr,adx\\n#     # df[\\'RSI\\'] = talib.RSI(df[\\'Close\\'], timeperiod=14)\\n#     df[\\'MOMENTUM\\'] = talib.MOM(df[\\'Close\\'], timeperiod=10)\\n#     # df[\\'DX\\'] = talib.stream_DX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n#     # df[\\'STOCHRSI\\'] = talib.STOCHRSI(df[\\'Close\\'], timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)\\n#     df[\\'CCI\\'] = talib.CCI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n#     # df[\\'WILLR\\'] = talib.WILLR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n#     df[\\'MFI\\'] = talib.MFI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'], timeperiod=14)\\n#     df[\\'ROC\\'] = talib.ROC(df[\\'Close\\'], timeperiod=10)\\n#     # df[\\'ATR\\'] = talib.ATR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n#     df[\\'ADX\\'] = talib.ADX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n#     df[\\'PPO\\'] = talib.PPO(df[\\'Close\\'], fastperiod=5, slowperiod=15, matype=8)\\n#     # df[\\'MACD\\'], df[\\'MACDSIGNAL\\'], df[\\'MACDHIST\\'] = talib.MACD(df[\\'Close\\'], fastperiod=5, slowperiod=15, signalperiod=9)\\n#\\n#     # Fill NaN values with 0\\n#     df.fillna(0, inplace=True)\\n#     return df\\n\\n\\ndef add_indicators(df):\\n    \"\"\" Adds technical indicators to the DataFrame \"\"\"\\n    indicators = {\\n        \\'CDL2CROWS\\': talib.CDL2CROWS,\\n        \\'CDL3BLACKCROWS\\': talib.CDL3BLACKCROWS,\\n        \\'CDL3INSIDE\\': talib.CDL3INSIDE,\\n        \\'CDL3LINESTRIKE\\': talib.CDL3LINESTRIKE,\\n        \\'CDL3OUTSIDE\\': talib.CDL3OUTSIDE,\\n        \\'CDL3STARSINSOUTH\\': talib.CDL3STARSINSOUTH,\\n        \\'CDL3WHITESOLDIERS\\': talib.CDL3WHITESOLDIERS,\\n        \\'CDLABANDONEDBABY\\': talib.CDLABANDONEDBABY,\\n        \\'CDLADVANCEBLOCK\\': talib.CDLADVANCEBLOCK,\\n        \\'CDLBELTHOLD\\': talib.CDLBELTHOLD,\\n        \\'CDLBREAKAWAY\\': talib.CDLBREAKAWAY,\\n        \\'CDLCLOSINGMARUBOZU\\': talib.CDLCLOSINGMARUBOZU,\\n        \\'CDLCONCEALBABYSWALL\\': talib.CDLCONCEALBABYSWALL,\\n        \\'CDLCOUNTERATTACK\\': talib.CDLCOUNTERATTACK,\\n        \\'CDLDARKCLOUDCOVER\\': talib.CDLDARKCLOUDCOVER,\\n        \\'CDLDOJI\\': talib.CDLDOJI,\\n        \\'CDLDOJISTAR\\': talib.CDLDOJISTAR,\\n        \\'CDLDRAGONFLYDOJI\\': talib.CDLDRAGONFLYDOJI,\\n        \\'CDLENGULFING\\': talib.CDLENGULFING,\\n        \\'CDLEVENINGDOJISTAR\\': talib.CDLEVENINGDOJISTAR,\\n        \\'CDLEVENINGSTAR\\': talib.CDLEVENINGSTAR,\\n        \\'CDLGAPSIDESIDEWHITE\\': talib.CDLGAPSIDESIDEWHITE,\\n        \\'CDLGRAVESTONEDOJI\\': talib.CDLGRAVESTONEDOJI,\\n        \\'CDLHAMMER\\': talib.CDLHAMMER,\\n        \\'CDLHANGINGMAN\\': talib.CDLHANGINGMAN,\\n        \\'CDLHARAMI\\': talib.CDLHARAMI,\\n        \\'CDLHARAMICROSS\\': talib.CDLHARAMICROSS,\\n        \\'CDLHIGHWAVE\\': talib.CDLHIGHWAVE,\\n        \\'CDLHIKKAKE\\': talib.CDLHIKKAKE,\\n        \\'CDLHIKKAKEMOD\\': talib.CDLHIKKAKEMOD,\\n        \\'CDLHOMINGPIGEON\\': talib.CDLHOMINGPIGEON,\\n        \\'CDLIDENTICAL3CROWS\\': talib.CDLIDENTICAL3CROWS,\\n        \\'CDLINNECK\\': talib.CDLINNECK,\\n        \\'CDLINVERTEDHAMMER\\': talib.CDLINVERTEDHAMMER,\\n        \\'CDLKICKING\\': talib.CDLKICKING,\\n        \\'CDLKICKINGBYLENGTH\\': talib.CDLKICKINGBYLENGTH,\\n        \\'CDLLADDERBOTTOM\\': talib.CDLLADDERBOTTOM,\\n        \\'CDLLONGLEGGEDDOJI\\': talib.CDLLONGLEGGEDDOJI,\\n        \\'CDLLONGLINE\\': talib.CDLLONGLINE,\\n        \\'CDLMARUBOZU\\': talib.CDLMARUBOZU,\\n        \\'CDLMATCHINGLOW\\': talib.CDLMATCHINGLOW,\\n        \\'CDLMATHOLD\\': talib.CDLMATHOLD,\\n        \\'CDLMORNINGDOJISTAR\\': talib.CDLMORNINGDOJISTAR,\\n        \\'CDLMORNINGSTAR\\': talib.CDLMORNINGSTAR,\\n        \\'CDLONNECK\\': talib.CDLONNECK,\\n        \\'CDLPIERCING\\': talib.CDLPIERCING,\\n        \\'CDLRICKSHAWMAN\\': talib.CDLRICKSHAWMAN,\\n        \\'CDLRISEFALL3METHODS\\': talib.CDLRISEFALL3METHODS,\\n        \\'CDLSEPARATINGLINES\\': talib.CDLSEPARATINGLINES,\\n        \\'CDLSHOOTINGSTAR\\': talib.CDLSHOOTINGSTAR,\\n        \\'CDLSHORTLINE\\': talib.CDLSHORTLINE,\\n        \\'CDLSPINNINGTOP\\': talib.CDLSPINNINGTOP,\\n        \\'CDLSTALLEDPATTERN\\': talib.CDLSTALLEDPATTERN,\\n        \\'CDLSTICKSANDWICH\\': talib.CDLSTICKSANDWICH,\\n        \\'CDLTAKURI\\': talib.CDLTAKURI,\\n        \\'CDLTASUKIGAP\\': talib.CDLTASUKIGAP,\\n        \\'CDLTHRUSTING\\': talib.CDLTHRUSTING,\\n        \\'CDLTRISTAR\\': talib.CDLTRISTAR,\\n        \\'CDLUNIQUE3RIVER\\': talib.CDLUNIQUE3RIVER,\\n        \\'CDLUPSIDEGAP2CROWS\\': talib.CDLUPSIDEGAP2CROWS,\\n        \\'CDLXSIDEGAP3METHODS\\': talib.CDLXSIDEGAP3METHODS,\\n        \\'MOMENTUM\\': talib.MOM,\\n        \\'CCI\\': talib.CCI,\\n        \\'MFI\\': talib.MFI,\\n        \\'ROC\\': talib.ROC,\\n        \\'ADX\\': talib.ADX,\\n        \\'PPO\\': talib.PPO\\n    }\\n\\n    for indicator, func in indicators.items():\\n        if indicator in [\\'CDLABANDONEDBABY\\', \\'CDLDARKCLOUDCOVER\\', \\'CDLEVENINGDOJISTAR\\', \\'CDLEVENINGSTAR\\', \\'CDLMATHOLD\\', \\'CDLMORNINGDOJISTAR\\', \\'CDLMORNINGSTAR\\']:\\n            df[indicator] = func(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n        else:\\n            df[indicator] = func(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n\\n    # Fill NaN values with 0\\n    df.fillna(0, inplace=True)\\n    return df\\n', 'TypeError: add_indicators.<locals>.<lambda>() takes 3 positional arguments but 4 were given on the following code:\\n\\nimport logging\\nimport os\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\n# Formats Position\\ndef format_position(price):\\n    return (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n# Formats Currency\\ndef format_currency(price):\\n    return \\'${0:.2f}\\'.format(abs(price))\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\" Displays training results \"\"\"\\n    if val_position in {initial_offset, 0.0}:\\n        logging.info(f\\'{strategy}: Episode {result[0]}/{result[1]} - Train Position: {format_position(result[2])}  Val Position: USELESS  Train Loss: {result[3]:.4f}\\')\\n        return False\\n    else:\\n        logging.info(f\\'{strategy}: Episode {result[0]}/{result[1]} - Train Position: {format_position(result[2])}  Val Position: {format_position(val_position)}  Train Loss: {result[3]:.4f}\\')\\n        return True\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\" Displays evaluation results \"\"\"\\n    if profit in {initial_offset, 0.0}:\\n        logging.info(f\\'{model_name}: USELESS\\\\n\\')\\n    else:\\n        logging.info(f\\'{model_name}: {format_position(profit)}\\\\n\\')\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\" Reads stock data from CSV file \"\"\"\\n    df = pd.read_csv(stock_file)\\n    if date_field != \"Date\":\\n        df[\\'Date\\'] = df[date_field]\\n    df = df.sort_values([date_field])\\n    df[\\'adjcp\\'] = df[adjcp_field]\\n    return add_indicators(df)\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\" Reads stock data from CSV file \"\"\"\\n    df = pd.read_csv(stock_file)\\n    return list(df.sort_values([date_field])[adjcp_field])\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\" Splits the dataset into training or testing using date \"\"\"\\n    data = df[(df.index >= start) & (df.index < end)]\\n    return data.reset_index(drop=True)\\n\\ndef switch_k_backend_device():\\n    \"\"\" Switches `keras` backend from GPU to CPU if required. \"\"\"\\n    if keras.backend == \"tensorflow\":\\n        logging.debug(\"Switching to TensorFlow for CPU\")\\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n\\ndef get_features():\\n    \"\"\" Returns a list of feature names for technical indicators \"\"\"\\n    return [\\n        \\'CDL2CROWS\\', \\'CDL3BLACKCROWS\\', \\'CDL3INSIDE\\', \\'CDL3LINESTRIKE\\', \\'CDL3OUTSIDE\\',\\n        \\'CDL3STARSINSOUTH\\', \\'CDL3WHITESOLDIERS\\', \\'CDLABANDONEDBABY\\', \\'CDLADVANCEBLOCK\\',\\n        \\'CDLBELTHOLD\\', \\'CDLBREAKAWAY\\', \\'CDLCLOSINGMARUBOZU\\', \\'CDLCONCEALBABYSWALL\\',\\n        \\'CDLCOUNTERATTACK\\', \\'CDLDARKCLOUDCOVER\\', \\'CDLDOJI\\', \\'CDLDOJISTAR\\', \\'CDLDRAGONFLYDOJI\\',\\n        \\'CDLENGULFING\\', \\'CDLEVENINGDOJISTAR\\', \\'CDLEVENINGSTAR\\', \\'CDLGAPSIDESIDEWHITE\\',\\n        \\'CDLGRAVESTONEDOJI\\', \\'CDLHAMMER\\', \\'CDLHANGINGMAN\\', \\'CDLHARAMI\\', \\'CDLHARAMICROSS\\',\\n        \\'CDLHIGHWAVE\\', \\'CDLHIKKAKE\\', \\'CDLHIKKAKEMOD\\', \\'CDLHOMINGPIGEON\\', \\'CDLIDENTICAL3CROWS\\',\\n        \\'CDLINNECK\\', \\'CDLINVERTEDHAMMER\\', \\'CDLKICKING\\', \\'CDLKICKINGBYLENGTH\\', \\'CDLLADDERBOTTOM\\',\\n        \\'CDLLONGLEGGEDDOJI\\', \\'CDLLONGLINE\\', \\'CDLMARUBOZU\\', \\'CDLMATCHINGLOW\\', \\'CDLMATHOLD\\',\\n        \\'CDLMORNINGDOJISTAR\\', \\'CDLMORNINGSTAR\\', \\'CDLONNECK\\', \\'CDLPIERCING\\', \\'CDLRICKSHAWMAN\\',\\n        \\'CDLRISEFALL3METHODS\\', \\'CDLSEPARATINGLINES\\', \\'CDLSHOOTINGSTAR\\', \\'CDLSHORTLINE\\',\\n        \\'CDLSPINNINGTOP\\', \\'CDLSTALLEDPATTERN\\', \\'CDLSTICKSANDWICH\\', \\'CDLTAKURI\\', \\'CDLTASUKIGAP\\',\\n        \\'CDLTHRUSTING\\', \\'CDLTRISTAR\\', \\'CDLUNIQUE3RIVER\\', \\'CDLUPSIDEGAP2CROWS\\', \\'CDLXSIDEGAP3METHODS\\',\\n        \\'MOMENTUM\\', \\'CCI\\', \\'MFI\\', \\'ROC\\', \\'ADX\\', \\'PPO\\'\\n    ]\\n\\ndef add_indicators(df):\\n    \"\"\" Adds technical indicators to the DataFrame \"\"\"\\n    indicators = {\\n        \\'CDL2CROWS\\': talib.CDL2CROWS,\\n        \\'CDL3BLACKCROWS\\': talib.CDL3BLACKCROWS,\\n        \\'CDL3INSIDE\\': talib.CDL3INSIDE,\\n        \\'CDL3LINESTRIKE\\': talib.CDL3LINESTRIKE,\\n        \\'CDL3OUTSIDE\\': talib.CDL3OUTSIDE,\\n        \\'CDL3STARSINSOUTH\\': talib.CDL3STARSINSOUTH,\\n        \\'CDL3WHITESOLDIERS\\': talib.CDL3WHITESOLDIERS,\\n        \\'CDLABANDONEDBABY\\': talib.CDLABANDONEDBABY,\\n        \\'CDLADVANCEBLOCK\\': talib.CDLADVANCEBLOCK,\\n        \\'CDLBELTHOLD\\': talib.CDLBELTHOLD,\\n        \\'CDLBREAKAWAY\\': talib.CDLBREAKAWAY,\\n        \\'CDLCLOSINGMARUBOZU\\': talib.CDLCLOSINGMARUBOZU,\\n        \\'CDLCONCEALBABYSWALL\\': talib.CDLCONCEALBABYSWALL,\\n        \\'CDLCOUNTERATTACK\\': talib.CDLCOUNTERATTACK,\\n        \\'CDLDARKCLOUDCOVER\\': talib.CDLDARKCLOUDCOVER,\\n        \\'CDLDOJI\\': talib.CDLDOJI,\\n        \\'CDLDOJISTAR\\': talib.CDLDOJISTAR,\\n        \\'CDLDRAGONFLYDOJI\\': talib.CDLDRAGONFLYDOJI,\\n        \\'CDLENGULFING\\': talib.CDLENGULFING,\\n        \\'CDLEVENINGDOJISTAR\\': talib.CDLEVENINGDOJISTAR,\\n        \\'CDLEVENINGSTAR\\': talib.CDLEVENINGSTAR,\\n        \\'CDLGAPSIDESIDEWHITE\\': talib.CDLGAPSIDESIDEWHITE,\\n        \\'CDLGRAVESTONEDOJI\\': talib.CDLGRAVESTONEDOJI,\\n        \\'CDLHAMMER\\': talib.CDLHAMMER,\\n        \\'CDLHANGINGMAN\\': talib.CDLHANGINGMAN,\\n        \\'CDLHARAMI\\': talib.CDLHARAMI,\\n        \\'CDLHARAMICROSS\\': talib.CDLHARAMICROSS,\\n        \\'CDLHIGHWAVE\\': talib.CDLHIGHWAVE,\\n        \\'CDLHIKKAKE\\': talib.CDLHIKKAKE,\\n        \\'CDLHIKKAKEMOD\\': talib.CDLHIKKAKEMOD,\\n        \\'CDLHOMINGPIGEON\\': talib.CDLHOMINGPIGEON,\\n        \\'CDLIDENTICAL3CROWS\\': talib.CDLIDENTICAL3CROWS,\\n        \\'CDLINNECK\\': talib.CDLINNECK,\\n        \\'CDLINVERTEDHAMMER\\': talib.CDLINVERTEDHAMMER,\\n        \\'CDLKICKING\\': talib.CDLKICKING,\\n        \\'CDLKICKINGBYLENGTH\\': talib.CDLKICKINGBYLENGTH,\\n        \\'CDLLADDERBOTTOM\\': talib.CDLLADDERBOTTOM,\\n        \\'CDLLONGLEGGEDDOJI\\': talib.CDLLONGLEGGEDDOJI,\\n        \\'CDLLONGLINE\\': talib.CDLLONGLINE,\\n        \\'CDLMARUBOZU\\': talib.CDLMARUBOZU,\\n        \\'CDLMATCHINGLOW\\': talib.CDLMATCHINGLOW,\\n        \\'CDLMATHOLD\\': talib.CDLMATHOLD,\\n        \\'CDLMORNINGDOJISTAR\\': talib.CDLMORNINGDOJISTAR,\\n        \\'CDLMORNINGSTAR\\': talib.CDLMORNINGSTAR,\\n        \\'CDLONNECK\\': talib.CDLONNECK,\\n        \\'CDLPIERCING\\': talib.CDLPIERCING,\\n        \\'CDLRICKSHAWMAN\\': talib.CDLRICKSHAWMAN,\\n        \\'CDLRISEFALL3METHODS\\': talib.CDLRISEFALL3METHODS,\\n        \\'CDLSEPARATINGLINES\\': talib.CDLSEPARATINGLINES,\\n        \\'CDLSHOOTINGSTAR\\': talib.CDLSHOOTINGSTAR,\\n        \\'CDLSHORTLINE\\': talib.CDLSHORTLINE,\\n        \\'CDLSPINNINGTOP\\': talib.CDLSPINNINGTOP,\\n        \\'CDLSTALLEDPATTERN\\': talib.CDLSTALLEDPATTERN,\\n        \\'CDLSTICKSANDWICH\\': talib.CDLSTICKSANDWICH,\\n        \\'CDLTAKURI\\': talib.CDLTAKURI,\\n        \\'CDLTASUKIGAP\\': talib.CDLTASUKIGAP,\\n        \\'CDLTHRUSTING\\': talib.CDLTHRUSTING,\\n        \\'CDLTRISTAR\\': talib.CDLTRISTAR,\\n        \\'CDLUNIQUE3RIVER\\': talib.CDLUNIQUE3RIVER,\\n        \\'CDLUPSIDEGAP2CROWS\\': talib.CDLUPSIDEGAP2CROWS,\\n        \\'CDLXSIDEGAP3METHODS\\': talib.CDLXSIDEGAP3METHODS,\\n        \\'MOMENTUM\\': lambda close: talib.MOM(close, timeperiod=10),\\n        \\'CCI\\': lambda high, low, close: talib.CCI(high, low, close, timeperiod=14),\\n        \\'MFI\\': lambda high, low, close, volume: talib.MFI(high, low, close, volume, timeperiod=14),\\n        \\'ROC\\': lambda close: talib.ROC(close, timeperiod=10),\\n        \\'ADX\\': lambda high, low, close: talib.ADX(high, low, close, timeperiod=14),\\n        \\'PPO\\': lambda close: talib.PPO(close, fastperiod=5, slowperiod=15, matype=8)\\n    }\\n\\n    for indicator, func in indicators.items():\\n        if indicator in [\\'CDLABANDONEDBABY\\', \\'CDLDARKCLOUDCOVER\\', \\'CDLEVENINGDOJISTAR\\', \\'CDLEVENINGSTAR\\', \\'CDLMATHOLD\\', \\'CDLMORNINGDOJISTAR\\', \\'CDLMORNINGSTAR\\']:\\n            df[indicator] = func(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n        elif indicator in [\\'MOMENTUM\\', \\'ROC\\', \\'PPO\\']:\\n            df[indicator] = func(df[\\'Close\\'])\\n        elif indicator == \\'MFI\\':\\n            df[indicator] = func(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'])\\n        else:\\n            df[indicator] = func(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n\\n    # Fill NaN values with 0\\n    df.fillna(0, inplace=True)\\n    return df', 'How can I improve the following code:\\n\\nimport os\\nfrom pathlib import Path\\nimport pandas as pd\\nfrom flask import Flask, jsonify, request\\nimport logging\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.ops import get_state\\nfrom trading_bot.utils import switch_k_backend_device, add_indicators\\n\\napp = Flask(__name__)\\napp.config[\\'WTF_CSRF_CHECK_DEFAULT\\'] = False\\napp.config[\\'WTF_CSRF_ENABLED\\'] = False\\n\\n# Set up logging\\nlogging.basicConfig(level=logging.INFO)\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    global agent\\n    try:\\n        rq = request.json\\n\\n        # Extract the data\\n        data = rq.get(\\'Data\\', [])\\n        # logging.info(f\"Received data: {data}\")\\n\\n        # Convert the data to a DataFrame\\n        df = pd.DataFrame(data, columns=[\\'Date\\', \\'Open\\', \\'High\\', \\'Low\\', \\'Close\\', \\'Volume\\'])\\n        # if date_field != \"Date\":\\n        #     df[\\'Date\\'] = df[date_field]\\n        #\\n        # df = df.sort_values([date_field])\\n        # df[\\'adjcp\\'] = df[adjcp_field]\\n        # Add Indicators and Pattern Recognition Functions\\n        df = add_indicators(df)\\n\\n        # Check if DataFrame is empty or has insufficient rows\\n        if df.empty or len(df) < window_size:\\n            return jsonify({\\'error\\': \\'Insufficient data for prediction\\'}), 400\\n\\n        # Prepare the state for prediction\\n        t = len(df) - 1\\n        state = get_state(df, t, window_size)\\n        # logging.info(f\"State for prediction: {state}\")\\n\\n        # Make a prediction\\n        output = agent.act(state, True)\\n        logging.info(f\"Predicted action: {output}\")\\n\\n        if output == 1:\\n            return jsonify({\\'Res\\': \\'buy\\'}), 200\\n        elif output == 2:\\n            return jsonify({\\'Res\\': \\'sell\\'}), 200\\n\\n        return jsonify({\\'Res\\': \\'hold\\'}), 200\\n\\n    except Exception as e:\\n        logging.error(f\"Error processing request: {e}\")\\n        return jsonify({\\'error\\': \\'Internal server error\\'}), 500\\n\\nif __name__ == \"__main__\":\\n    predict_data = None\\n    directory_path = os.path.realpath(__file__)\\n    folder = Path(directory_path)\\n    window_size = 60\\n    # adjcp_field = \"Close\"\\n    # date_field = \"Date\"\\n    env = None\\n    env_test = None\\n    switch_k_backend_device()\\n\\n    # Initialize the agent\\n    agent = Agent(window_size, pretrained=True, model_name=\\'models/model_dqn\\')\\n    agent.first_iter = False\\n    app.run(host=\"0.0.0.0\", debug=False, port=5020)\\n', 'On the following code, I  only get 0 on the sell for rewards:\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                total_profit += reward\\n                logging.debug(f\"BUY: Price: {data.iloc[t][adjcp_field]}, Reward: {reward}\")\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            total_profit += reward\\n            logging.debug(f\"SELL: Price: {data.iloc[t][adjcp_field]}, Reward: {reward}\")\\n\\n        # HOLD\\n        else:\\n            reward = 0  # Explicitly set reward to 0 for HOLD action\\n            logging.debug(f\"HOLD: Price: {data.iloc[t][adjcp_field]}\")\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\n', 'What is wrong with the following yaml code:', 'What is wrong with the following yaml code:\\n\\nversion: \"3.3\"\\nservices:\\n  drl-train:\\n    image: drl-train\\n    container_name: drl-train\\n    build:\\n      context: .\\n    environment:\\n      EP_COUNT: 20\\n      W_SIZE: 1440\\n      DATE_FIELD: Date\\n      ADJCP_FIELD: Close\\n      MODEL_NAME: model\\n    volumes:\\n      - ./models:/app/models\\n      - ./data:/app/data\\n\\n  drl-api:\\n    image: drl-api\\n    container_name: drl-api\\n    build:\\n      context: .\\n      dockerfile: DockerfileApi\\n    environment:\\n      MODEL_NAME: model\\n    ports:\\n      - \"5020:5020\"\\n    volumes:\\n      - ./models:/app/models\\n', 'I get the following ERROR [drl-train build 4/4] RUN pip install -r requirements.txt when I run docker-compose build drl-train for the following yaml code:\\n\\nversion: \\'3.3\\'\\nservices:\\n  drl-train:\\n    image: drl-train\\n    container_name: drl-train\\n    build:\\n      context: .\\n    environment:\\n      EP_COUNT: 20\\n      W_SIZE: 1440\\n      DATE_FIELD: Date\\n      ADJCP_FIELD: Close\\n      MODEL_NAME: model\\n    volumes:\\n      - ./models:/app/models\\n      - ./data:/app/data\\n\\n  drl-api:\\n    image: drl-api\\n    container_name: drl-api\\n    build:\\n      context: .\\n      dockerfile: DockerfileApi\\n    environment:\\n      MODEL_NAME: model\\n    ports:\\n      - \"5020:5020\"\\n    volumes:\\n      - ./models:/app/models', 'I get the following ERROR:root:Error parsing arguments: --date-field is not a unique prefix: --date-field, --date-field? for the following code:\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.02\\nCreated by Javier Sanchez, June 15, 2024\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\\nExplanation of Arguments:\\n    70: Percentage of data to use for training (the rest will be used for validation).\\n    --date-field Date: Specifies the name of the date field in your data file (default is Date).\\n    --adjcp-field Adj Close: Specifies the name of the adjusted close price field in your data file (default is Adj Close).\\n    --window-size 10: Size of the n-day window stock data representation used as the feature vector (default is 10).\\n    --batch-size 32: Number of samples to train on in one mini-batch during training (default is 32).\\n    --episode-count 50: Number of trading episodes to use for training (default is 50).\\n    --model-name model_debug: Name of the pretrained model to use (default is model_debug).\\n    --debug: Specifies whether to use verbose logs during the evaluation operation.\\n\\n\"\"\"\\n\\nimport os\\nimport logging\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n# Constants for default values\\nDATE_FIELD = \"Date\"\\nADJCP_FIELD = \"Adj Close\"\\nWINDOW_SIZE = 10\\nBATCH_SIZE = 32\\nEPISODE_COUNT = 50\\nMODEL_NAME = \"model_debug\"\\n\\ndef setup_logging(debug):\\n    \"\"\"Setup logging configuration.\"\"\"\\n    level = \"DEBUG\" if debug else \"INFO\"\\n    coloredlogs.install(level=level)\\n    logging.basicConfig(level=level)\\n\\n\\ndef parse_args():\\n    \"\"\"Parse command-line arguments.\"\"\"\\n    try:\\n        args = docopt(__doc__)\\n        return {\\n            \"file_data\": args[\"<file-data>\"],\\n            \"train_percent\": float(args[\"<train-percent>\"]),\\n            \"date_field\": args[\"--date-field\"] or DATE_FIELD,\\n            \"adjcp_field\": args[\"--adjcp-field\"] or ADJCP_FIELD,\\n            \"window_size\": int(args[\"--window-size\"] or WINDOW_SIZE),\\n            \"batch_size\": int(args[\"--batch-size\"] or BATCH_SIZE),\\n            \"episode_count\": int(args[\"--episode-count\"] or EPISODE_COUNT),\\n            \"model_name\": args[\"--model-name\"] or MODEL_NAME,\\n            \"debug\": args[\"--debug\"]\\n        }\\n    except Exception as e:\\n        logging.error(f\"Error parsing arguments: {e}\")\\n        exit(1)\\n\\n\\ndef train_and_evaluate(agent, train_data, val_data, window_size, batch_size, episode_count, adjcp_field, debug):\\n    \"\"\"Train and evaluate the agent.\"\"\"\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = -float(\\'inf\\')\\n    episode = 0\\n    trans_cost = 0.25\\n\\n    while episode < episode_count:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   window_size=window_size, prev_agent_profit=agent_profit, adjcp_field=adjcp_field,\\n                                   trans_cost=trans_cost)\\n        agent_profit = train_result[2]\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, agent.strategy)\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        episode += 1\\n\\ndef main():\\n    \"\"\"Main function to train the stock trading bot.\"\"\"\\n    args = parse_args()\\n    setup_logging(args[\"debug\"])\\n    switch_k_backend_device()\\n\\n    agent = Agent(args[\"window_size\"], strategy=\"t-dqn\", pretrained=True, model_name=args[\"model_name\"])\\n    data = load_data(args[\"file_data\"], args[\"date_field\"])\\n    take = int(len(data) * (args[\"train_percent\"] / 100))\\n    train_data = data_split(data, 0, take, args[\"adjcp_field\"])\\n    val_data = data_split(data, take, len(data), args[\"adjcp_field\"])\\n\\n    try:\\n        train_and_evaluate(agent, train_data, val_data, args[\"window_size\"], args[\"batch_size\"],\\n                           args[\"episode_count\"], args[\"adjcp_field\"], args[\"debug\"])\\n    except KeyboardInterrupt:\\n        logging.info(\"Training aborted by user.\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n', 'ERROR:root:Error parsing arguments: --date-field is not a unique prefix: --date-field, --date-field?\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.02\\nCreated by Javier Sanchez, June 15, 2024\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\\nExplanation of Arguments:\\n    70: Percentage of data to use for training (the rest will be used for validation).\\n    --date-field Date: Specifies the name of the date field in your data file (default is Date).\\n    --adjcp-field Adj Close: Specifies the name of the adjusted close price field in your data file (default is Adj Close).\\n    --window-size 10: Size of the n-day window stock data representation used as the feature vector (default is 10).\\n    --batch-size 32: Number of samples to train on in one mini-batch during training (default is 32).\\n    --episode-count 50: Number of trading episodes to use for training (default is 50).\\n    --model-name model_debug: Name of the pretrained model to use (default is model_debug).\\n    --debug: Specifies whether to use verbose logs during the evaluation operation.\\n\\n\"\"\"\\n\\nimport os\\nimport logging\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n# Constants for default values\\nDATE_FIELD = \"Date\"\\nADJCP_FIELD = \"Adj Close\"\\nWINDOW_SIZE = 10\\nBATCH_SIZE = 32\\nEPISODE_COUNT = 50\\nMODEL_NAME = \"model_debug\"\\n\\ndef setup_logging(debug):\\n    \"\"\"Setup logging configuration.\"\"\"\\n    level = \"DEBUG\" if debug else \"INFO\"\\n    coloredlogs.install(level=level)\\n    logging.basicConfig(level=level)\\n\\n\\ndef parse_args():\\n    \"\"\"Parse command-line arguments.\"\"\"\\n    try:\\n        args = docopt(__doc__)\\n        return {\\n            \"file_data\": args[\"<file-data>\"],\\n            \"train_percent\": float(args[\"<train-percent>\"]),\\n            \"date_field\": args[\"--date-field\"] or DATE_FIELD,\\n            \"adjcp_field\": args[\"--adjcp-field\"] or ADJCP_FIELD,\\n            \"window_size\": int(args[\"--window-size\"] or WINDOW_SIZE),\\n            \"batch_size\": int(args[\"--batch-size\"] or BATCH_SIZE),\\n            \"episode_count\": int(args[\"--episode-count\"] or EPISODE_COUNT),\\n            \"model_name\": args[\"--model-name\"] or MODEL_NAME,\\n            \"debug\": args[\"--debug\"]\\n        }\\n    except Exception as e:\\n        logging.error(f\"Error parsing arguments: {e}\")\\n        exit(1)\\n\\n\\ndef train_and_evaluate(agent, train_data, val_data, window_size, batch_size, episode_count, adjcp_field, debug):\\n    \"\"\"Train and evaluate the agent.\"\"\"\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = -float(\\'inf\\')\\n    episode = 0\\n    trans_cost = 0.25\\n\\n    while episode < episode_count:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   window_size=window_size, prev_agent_profit=agent_profit, adjcp_field=adjcp_field,\\n                                   trans_cost=trans_cost)\\n        agent_profit = train_result[2]\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, agent.strategy)\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        episode += 1\\n\\ndef main():\\n    \"\"\"Main function to train the stock trading bot.\"\"\"\\n    args = parse_args()\\n    setup_logging(args[\"debug\"])\\n    switch_k_backend_device()\\n\\n    agent = Agent(args[\"window_size\"], strategy=\"t-dqn\", pretrained=True, model_name=args[\"model_name\"])\\n    data = load_data(args[\"file_data\"], args[\"date_field\"])\\n    take = int(len(data) * (args[\"train_percent\"] / 100))\\n    train_data = data_split(data, 0, take, args[\"adjcp_field\"])\\n    val_data = data_split(data, take, len(data), args[\"adjcp_field\"])\\n\\n    try:\\n        train_and_evaluate(agent, train_data, val_data, args[\"window_size\"], args[\"batch_size\"],\\n                           args[\"episode_count\"], args[\"adjcp_field\"], args[\"debug\"])\\n    except KeyboardInterrupt:\\n        logging.info(\"Training aborted by user.\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n', 'I get the following ValueError: Argument(s) not recognized: {\\'jit_compile\\': False, \\'is_legacy_optimizer\\': False} on the following Code:\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2023\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\"\"\"\\n\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n\\ndef main(train_stock, train_percent, date_field, adjcp_field, window_size, batch_size, ep_count,\\n         strategy=\"t-dqn\", model_name=\"model_debug\", debug=False):\\n    \"\"\" Trains the stock trading bot using Deep Q-Learning.\\n\\n    Args: [python train.py --help]\\n    \"\"\"\\n    agent = Agent(window_size, strategy=strategy, pretrained=True, model_name=model_name)\\n    data = load_data(train_stock, date_field)\\n    take = int(len(data) * (train_percent / 100))\\n    train_data = data_split(data, 0, take, adjcp_field)\\n    val_data = data_split(data, take, int(len(data)), adjcp_field)\\n\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = -1000000000\\n    episode = 0\\n    episode_count = ep_count\\n    trans_cost = 0.25\\n\\n    while episode < episode_count:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size, trans_cost=trans_cost,\\n                                   window_size=window_size, prev_agent_profit=agent_profit, adjcp_field=adjcp_field)\\n        agent_profit = train_result[2]\\n\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, strategy)\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        # if episode >= episode_count:\\n        #     break\\n        episode += 1\\n\\n\\nif __name__ == \"__main__\":\\n    args = docopt(__doc__)\\n\\n    file_data = args[\"<file-data>\"]\\n    train_percent = args[\"<train-percent>\"]\\n\\n    date_field = args[\"--date-field\"]\\n    adjcp_field = args[\"--adjcp-field\"]\\n    window_size = int(args[\"--window-size\"])\\n    batch_size = int(args[\"--batch-size\"])\\n    ep_count = int(args[\"--episode-count\"])\\n    model_name = args[\"--model-name\"]\\n    debug = args[\"--debug\"]\\n\\n    coloredlogs.install(level=\"DEBUG\")\\n    switch_k_backend_device()\\n\\n    try:\\n        # dqn\\n        main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n             ep_count, \"dqn\", f\"{model_name}_dqn\", debug)\\n\\n        # # t-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"t-dqn\", f\"{model_name}_tdqn\", debug)\\n        #\\n        # # double-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"double-dqn\", f\"{model_name}_ddqn\", debug)\\n\\n    except KeyboardInterrupt:\\n        print(\"Aborted!\")\\n\\n', 'Optimize the following code:\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2023\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\"\"\"\\n\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n\\ndef main(train_stock, train_percent, date_field, adjcp_field, window_size, batch_size, ep_count,\\n         strategy=\"t-dqn\", model_name=\"model_debug\", debug=False):\\n    \"\"\" Trains the stock trading bot using Deep Q-Learning.\\n\\n    Args: [python train.py --help]\\n    \"\"\"\\n    agent = Agent(window_size, strategy=strategy, pretrained=True, model_name=model_name)\\n    data = load_data(train_stock, date_field)\\n    take = int(len(data) * (train_percent / 100))\\n    train_data = data_split(data, 0, take, adjcp_field)\\n    val_data = data_split(data, take, int(len(data)), adjcp_field)\\n\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = -1000000000\\n    episode = 0\\n    episode_count = ep_count\\n    trans_cost = 0.25\\n\\n    while episode < episode_count:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size, trans_cost=trans_cost,\\n                                   window_size=window_size, prev_agent_profit=agent_profit, adjcp_field=adjcp_field)\\n        agent_profit = train_result[2]\\n\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, strategy)\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        # if episode >= episode_count:\\n        #     break\\n        episode += 1\\n\\n\\nif __name__ == \"__main__\":\\n    args = docopt(__doc__)\\n\\n    file_data = args[\"<file-data>\"]\\n    train_percent = args[\"<train-percent>\"]\\n\\n    date_field = args[\"--date-field\"]\\n    adjcp_field = args[\"--adjcp-field\"]\\n    window_size = int(args[\"--window-size\"])\\n    batch_size = int(args[\"--batch-size\"])\\n    ep_count = int(args[\"--episode-count\"])\\n    model_name = args[\"--model-name\"]\\n    debug = args[\"--debug\"]\\n\\n    coloredlogs.install(level=\"DEBUG\")\\n    switch_k_backend_device()\\n\\n    try:\\n        # dqn\\n        main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n             ep_count, \"dqn\", f\"{model_name}_dqn\", debug)\\n\\n        # # t-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"t-dqn\", f\"{model_name}_tdqn\", debug)\\n        #\\n        # # double-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"double-dqn\", f\"{model_name}_ddqn\", debug)\\n\\n    except KeyboardInterrupt:\\n        print(\"Aborted!\")\\n', 'Local variable \\'average_loss\\' might be referenced before assignment  for the following code:\\n\\nimport gc\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"Monitor memory usage and log if it exceeds the threshold.\"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        logging.warning(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:  # BUY\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                total_profit += reward\\n                logging.debug(f\"BUY: Price: {data.iloc[t][adjcp_field]}, Reward: {reward}\")\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:  # SELL\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            total_profit += reward\\n            logging.debug(f\"SELL: Price: {data.iloc[t][adjcp_field]}, Reward: {reward}\")\\n\\n        # Ensure that the HOLD action is handled correctly\\n        else: # HOLD\\n            reward = 0  # Explicitly set reward to 0 for HOLD action\\n            logging.debug(f\"HOLD: Price: {data.iloc[t][adjcp_field]}\")\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {}\".format(format_currency(data.iloc[t][adjcp_field])))\\n\\n            # Calculate delta and reward for buy action\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                total_profit += reward\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            total_profit += reward\\n\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Position: {}\".format(\\n                    format_currency(data.iloc[t][adjcp_field]),\\n                    format_position(data.iloc[t][adjcp_field] - bought_price)))\\n\\n        # HOLD\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', '[drl-train build 4/4] RUN pip install -r requirements.txt:\\n1.123 ERROR: Could not find a version that satisfies the requirement absl-py==2.1.0 (from versions: 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.1.10, 0.1.11, 0.1.12, 0.1.13, 0.2.0, 0.2.1, 0.2.2, 0.3.0, 0.4.0, 0.4.1, 0.5.0, 0.6.0, 0.6.1, 0.7.0, 0.7.1, 0.8.0, 0.8.1, 0.9.0, 0.10.0, 0.11.0, 0.12.0, 0.13.0, 0.14.0, 0.14.1, 0.15.0, 1.0.0, 1.1.0, 1.2.0, 1.3.0, 1.4.0)\\n\\nfor the following yaml:\\n\\nversion: \\'3.3\\'\\nservices:\\n  drl-train:\\n    image: drl-train\\n    container_name: drl-train\\n    build:\\n      context: .\\n    environment:\\n      EP_COUNT: 20\\n      W_SIZE: 1440\\n      DATE_FIELD: Date\\n      ADJCP_FIELD: Close\\n      MODEL_NAME: model\\n    volumes:\\n      - ./models:/app/models\\n      - ./data:/app/data\\n\\n  drl-api:\\n    image: drl-api\\n    container_name: drl-api\\n    build:\\n      context: .\\n      dockerfile: DockerfileApi\\n    environment:\\n      MODEL_NAME: model\\n    ports:\\n      - \"5020:5020\"\\n    volumes:\\n      - ./models:/app/models', 'how do i fix the following ERROR: Could not find a version that satisfies the requirement jupyter_server_terminals==0.5.3 (from versions: none) on the yaml file:\\n\\nversion: \\'3.3\\'\\nservices:\\n  drl-train:\\n    image: drl-train\\n    container_name: drl-train\\n    build:\\n      context: .\\n    environment:\\n      EP_COUNT: 20\\n      W_SIZE: 1440\\n      DATE_FIELD: Date\\n      ADJCP_FIELD: Close\\n      MODEL_NAME: model\\n    volumes:\\n      - ./models:/app/models\\n      - ./data:/app/data\\n\\n  drl-api:\\n    image: drl-api\\n    container_name: drl-api\\n    build:\\n      context: .\\n      dockerfile: DockerfileApi\\n    environment:\\n      MODEL_NAME: model\\n    ports:\\n      - \"5020:5020\"\\n    volumes:\\n      - ./models:/app/models\\n', 'ValueError: Argument(s) not recognized: {\\'jit_compile\\': False, \\'is_legacy_optimizer\\': False}\\nwhen trying to execute the following code:\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2023\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\"\"\"\\n\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n\\ndef main(train_stock, train_percent, date_field, adjcp_field, window_size, batch_size, ep_count,\\n         strategy=\"t-dqn\", model_name=\"model_debug\", debug=False):\\n    \"\"\" Trains the stock trading bot using Deep Q-Learning. Args: [python train.py --help]\\n    \"\"\"\\n    agent = Agent(window_size, strategy=strategy, pretrained=True, model_name=model_name)\\n    data = load_data(train_stock, date_field)\\n    take = int(len(data) * (train_percent / 100))\\n    train_data = data_split(data, 0, take, adjcp_field)\\n    val_data = data_split(data, take, int(len(data)), adjcp_field)\\n\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = -1000000000\\n    episode = 0\\n    episode_count = ep_count\\n    trans_cost: float = 0.25\\n\\n    while episode < episode_count:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   trans_cost=trans_cost, window_size=window_size, prev_agent_profit=agent_profit,\\n                                   adjcp_field=adjcp_field)\\n        agent_profit = train_result[2]\\n\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, strategy)\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        episode += 1\\n\\n\\nif __name__ == \"__main__\":\\n    args = docopt(__doc__)\\n\\n    file_data = args[\"<file-data>\"]\\n    train_percent = args[\"<train-percent>\"]\\n\\n    date_field = args[\"--date-field\"]\\n    adjcp_field = args[\"--adjcp-field\"]\\n    window_size = int(args[\"--window-size\"])\\n    batch_size = int(args[\"--batch-size\"])\\n    ep_count = int(args[\"--episode-count\"])\\n    model_name = args[\"--model-name\"]\\n    debug = args[\"--debug\"]\\n\\n    coloredlogs.install(level=\"DEBUG\")\\n    switch_k_backend_device()\\n\\n    try:\\n        # dqn\\n        main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n             ep_count, \"dqn\", f\"{model_name}_dqn\", debug)\\n\\n        # # t-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"t-dqn\", f\"{model_name}_tdqn\", debug)\\n        #\\n        # # double-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"double-dqn\", f\"{model_name}_ddqn\", debug)\\n\\n    except KeyboardInterrupt:\\n        print(\"Aborted!\")\\n', 'ValueError: Weights for model \\'sequential\\' have not yet been created. Weights are created when the model is first called on inputs or `build()` is called with an `input_shape`. When executing the following code:\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.02\\nCreated by Javier Sanchez, June 15, 2024\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\"\"\"\\n\\nimport os\\nimport logging\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n# Constants for default values\\nDATE_FIELD = \"Date\"\\nADJCP_FIELD = \"Adj Close\"\\nWINDOW_SIZE = 10\\nBATCH_SIZE = 32\\nEPISODE_COUNT = 50\\nMODEL_NAME = \"model_debug\"\\n\\ndef setup_logging(debug):\\n    \"\"\"Setup logging configuration.\"\"\"\\n    level = \"DEBUG\" if debug else \"INFO\"\\n    coloredlogs.install(level=level)\\n    logging.basicConfig(level=level)\\n\\ndef parse_args():\\n    \"\"\"Parse command-line arguments.\"\"\"\\n    try:\\n        args = docopt(__doc__)\\n        return {\\n            \"file_data\": args[\"<file-data>\"],\\n            \"train_percent\": float(args[\"<train-percent>\"]),\\n            \"date_field\": args[\"--date-field\"] or DATE_FIELD,\\n            \"adjcp_field\": args[\"--adjcp-field\"] or ADJCP_FIELD,\\n            \"window_size\": int(args[\"--window-size\"] or WINDOW_SIZE),\\n            \"batch_size\": int(args[\"--batch-size\"] or BATCH_SIZE),\\n            \"episode_count\": int(args[\"--episode-count\"] or EPISODE_COUNT),\\n            \"model_name\": args[\"--model-name\"] or MODEL_NAME,\\n            \"debug\": args[\"--debug\"]\\n        }\\n    except Exception as e:\\n        logging.error(f\"Error parsing arguments: {e}\")\\n        exit(1)\\n\\ndef train_and_evaluate(agent, train_data, val_data, window_size, batch_size, episode_count, adjcp_field, debug):\\n    \"\"\"Train and evaluate the agent.\"\"\"\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = -float(\\'inf\\')\\n    episode = 0\\n    trans_cost = 0.25\\n\\n    while episode < episode_count:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   window_size=window_size, prev_agent_profit=agent_profit, adjcp_field=adjcp_field,\\n                                   trans_cost=trans_cost)\\n        agent_profit = train_result[2]\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, agent.strategy)\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        episode += 1\\n\\ndef main():\\n    \"\"\"Main function to train the stock trading bot.\"\"\"\\n    args = parse_args()\\n    setup_logging(args[\"debug\"])\\n    switch_k_backend_device()\\n\\n    agent = Agent(args[\"window_size\"], strategy=\"t-dqn\", pretrained=True, model_name=args[\"model_name\"])\\n    data = load_data(args[\"file_data\"], args[\"date_field\"])\\n    take = int(len(data) * (args[\"train_percent\"] / 100))\\n    train_data = data_split(data, 0, take, args[\"adjcp_field\"])\\n    val_data = data_split(data, take, len(data), args[\"adjcp_field\"])\\n\\n    try:\\n        train_and_evaluate(agent, train_data, val_data, args[\"window_size\"], args[\"batch_size\"],\\n                           args[\"episode_count\"], args[\"adjcp_field\"], args[\"debug\"])\\n    except KeyboardInterrupt:\\n        logging.info(\"Training aborted by user.\")\\n\\nif __name__ == \"__main__\":\\n    main()', 'How do make the following code faster and more reliable:\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2023\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\"\"\"\\n\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n\\ndef main(train_stock, train_percent, date_field, adjcp_field, window_size, batch_size, ep_count,\\n         strategy=\"t-dqn\", model_name=\"model_debug\", debug=False):\\n    \"\"\" Trains the stock trading bot using Deep Q-Learning. Args: [python train.py --help]\\n    \"\"\"\\n    agent = Agent(window_size, strategy=strategy, pretrained=True, model_name=model_name)\\n    data = load_data(train_stock, date_field)\\n    take = int(len(data) * (train_percent / 100))\\n    train_data = data_split(data, 0, take, adjcp_field)\\n    val_data = data_split(data, take, int(len(data)), adjcp_field)\\n\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = -1000000000\\n    episode = 0\\n    episode_count = ep_count\\n    trans_cost: float = 0.25\\n\\n    while episode < episode_count:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   trans_cost=trans_cost, window_size=window_size, prev_agent_profit=agent_profit,\\n                                   adjcp_field=adjcp_field)\\n        agent_profit = train_result[2]\\n\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, strategy)\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        episode += 1\\n\\n\\nif __name__ == \"__main__\":\\n    args = docopt(__doc__)\\n\\n    file_data = args[\"<file-data>\"]\\n    train_percent = args[\"<train-percent>\"]\\n\\n    date_field = args[\"--date-field\"]\\n    adjcp_field = args[\"--adjcp-field\"]\\n    window_size = int(args[\"--window-size\"])\\n    batch_size = int(args[\"--batch-size\"])\\n    ep_count = int(args[\"--episode-count\"])\\n    model_name = args[\"--model-name\"]\\n    debug = args[\"--debug\"]\\n\\n    coloredlogs.install(level=\"DEBUG\")\\n    switch_k_backend_device()\\n\\n    try:\\n        # dqn\\n        main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n             ep_count, \"dqn\", f\"{model_name}_dqn\", debug)\\n\\n        # # t-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"t-dqn\", f\"{model_name}_tdqn\", debug)\\n        #\\n        # # double-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"double-dqn\", f\"{model_name}_ddqn\", debug)\\n\\n    except KeyboardInterrupt:\\n        print(\"Aborted!\")\\n', 'How can I use Use Multiprocessing on the following code:\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2023\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\"\"\"\\n\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\ndef main(train_stock, train_percent, date_field, adjcp_field, window_size, batch_size, ep_count,\\n         strategy=\"t-dqn\", model_name=\"model_debug\", debug=False):\\n    \"\"\" Trains the stock trading bot using Deep Q-Learning. \"\"\"\\n    agent = Agent(window_size, strategy=strategy, pretrained=True, model_name=model_name)\\n    data = load_data(train_stock, date_field)\\n    take = int(len(data) * (train_percent / 100))\\n    train_data = data_split(data, 0, take, adjcp_field)\\n    val_data = data_split(data, take, len(data), adjcp_field)\\n\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = float(\\'-inf\\')\\n    trans_cost = 0.25\\n\\n    for episode in range(ep_count):\\n        train_result = train_model(agent, episode, train_data, ep_count=ep_count, batch_size=batch_size,\\n                                   trans_cost=trans_cost, window_size=window_size, prev_agent_profit=agent_profit,\\n                                   adjcp_field=adjcp_field)\\n        agent_profit = train_result[2]\\n\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, strategy)\\n\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n\\n        # Optional: Save model or implement early stopping here\\n\\n# def main(train_stock, train_percent, date_field, adjcp_field, window_size, batch_size, ep_count,\\n#          strategy=\"t-dqn\", model_name=\"model_debug\", debug=False):\\n#     \"\"\" Trains the stock trading bot using Deep Q-Learning. Args: [python train.py --help]\\n#     \"\"\"\\n#     agent = Agent(window_size, strategy=strategy, pretrained=True, model_name=model_name)\\n#     data = load_data(train_stock, date_field)\\n#     take = int(len(data) * (train_percent / 100))\\n#     train_data = data_split(data, 0, take, adjcp_field)\\n#     val_data = data_split(data, take, int(len(data)), adjcp_field)\\n#\\n#     initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n#     agent_profit = 0\\n#     prev_agent_profit = -1000000000\\n#     episode = 0\\n#     episode_count = ep_count\\n#     trans_cost: float = 0.25\\n#\\n#     while episode < episode_count:\\n#         train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n#                                    trans_cost=trans_cost, window_size=window_size, prev_agent_profit=agent_profit,\\n#                                    adjcp_field=adjcp_field)\\n#         agent_profit = train_result[2]\\n#\\n#         if agent_profit > 0:\\n#             val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n#             show_train_result(train_result, val_result, initial_offset, strategy)\\n#         if agent_profit > prev_agent_profit:\\n#             prev_agent_profit = agent_profit\\n#         episode += 1\\n\\n\\nif __name__ == \"__main__\":\\n    args = docopt(__doc__)\\n\\n    file_data = args[\"<file-data>\"]\\n    train_percent = args[\"<train-percent>\"]\\n\\n    date_field = args[\"--date-field\"]\\n    adjcp_field = args[\"--adjcp-field\"]\\n    window_size = int(args[\"--window-size\"])\\n    batch_size = int(args[\"--batch-size\"])\\n    ep_count = int(args[\"--episode-count\"])\\n    model_name = args[\"--model-name\"]\\n    debug = args[\"--debug\"]\\n\\n    coloredlogs.install(level=\"DEBUG\")\\n    switch_k_backend_device()\\n\\n    try:\\n        # dqn\\n        main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n             ep_count, \"dqn\", f\"{model_name}_dqn\", debug)\\n\\n        # # t-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"t-dqn\", f\"{model_name}_tdqn\", debug)\\n        #\\n        # # double-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"double-dqn\", f\"{model_name}_ddqn\", debug)\\n\\n    except KeyboardInterrupt:\\n        print(\"Aborted!\")\\n', 'How can I use Use Multiprocessing on the following code:\\n\\nimport gc\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"Monitor memory usage and log if it exceeds the threshold.\"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        logging.warning(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:  # BUY\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                total_profit += reward\\n                logging.debug(f\"BUY: Price: {data.iloc[t][adjcp_field]}, Reward: {reward}\")\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:  # SELL\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            total_profit += reward\\n            logging.debug(f\"SELL: Price: {data.iloc[t][adjcp_field]}, Reward: {reward}\")\\n\\n        # HOLD\\n        else:  # HOLD\\n            reward = 0  # Explicitly set reward to 0 for HOLD action\\n            logging.debug(f\"HOLD: Price: {data.iloc[t][adjcp_field]}\")\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {}\".format(format_currency(data.iloc[t][adjcp_field])))\\n\\n            # Calculate delta and reward for buy action\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                total_profit += reward\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            total_profit += reward\\n\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Position: {}\".format(\\n                    format_currency(data.iloc[t][adjcp_field]),\\n                    format_position(data.iloc[t][adjcp_field] - bought_price)))\\n\\n        # HOLD\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'TypeError: train_model() got multiple values for argument \\'ep_count\\' for the following code:\\n\\nimport gc\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\nfrom multiprocessing import Pool, cpu_count\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"Monitor memory usage and log if it exceeds the threshold.\"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        logging.warning(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n\\ndef train_single_episode(args):\\n    \"\"\"Function to train a single episode.\"\"\"\\n    agent, episode, data, ep_count, batch_size, window_size, prev_agent_profit, adjcp_field, trans_cost = args\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:  # BUY\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                total_profit += reward\\n                logging.debug(f\"BUY: Price: {data.iloc[t][adjcp_field]}, Reward: {reward}\")\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:  # SELL\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            total_profit += reward\\n            logging.debug(f\"SELL: Price: {data.iloc[t][adjcp_field]}, Reward: {reward}\")\\n\\n        # HOLD\\n        else:  # HOLD\\n            reward = 0  # Explicitly set reward to 0 for HOLD action\\n            logging.debug(f\"HOLD: Price: {data.iloc[t][adjcp_field]}\")\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, total_profit, average_loss\\n\\n\\ndef train_model(agent, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    \"\"\"Train the model using multiprocessing.\"\"\"\\n    # Prepare arguments for multiprocessing\\n    args = [(agent, episode, data, ep_count, batch_size, window_size, prev_agent_profit, adjcp_field, trans_cost) for episode in range(ep_count)]\\n\\n    # Use multiprocessing Pool to parallelize training\\n    with Pool(cpu_count()) as pool:\\n        results = pool.map(train_single_episode, args)\\n\\n    return results\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {}\".format(format_currency(data.iloc[t][adjcp_field])))\\n\\n            # Calculate delta and reward for buy action\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                total_profit += reward\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            total_profit += reward\\n\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Position: {}\".format(\\n                    format_currency(data.iloc[t][adjcp_field]),\\n                    format_position(data.iloc[t][adjcp_field] - bought_price)))\\n\\n        # HOLD\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action\\n', 'When executing the docker-compose build drl-train, it gives command not found: Docker-compose', 'I get the following ValueError: File not found: filepath=models/model_dqn.keras. Please ensure the file is an accessible `.keras` zip file. when i try to execute the following code:\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2023\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\"\"\"\\n\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n\\ndef main(train_stock, train_percent, date_field, adjcp_field, window_size, batch_size, ep_count,\\n         strategy=\"t-dqn\", model_name=\"model_debug\", debug=False):\\n    \"\"\" Trains the stock trading bot using Deep Q-Learning. Args: [python train.py --help]\\n    \"\"\"\\n    agent = Agent(window_size, strategy=strategy, pretrained=True, model_name=model_name)\\n    data = load_data(train_stock, date_field)\\n    take = int(len(data) * (train_percent / 100))\\n    train_data = data_split(data, 0, take, adjcp_field)\\n    val_data = data_split(data, take, int(len(data)), adjcp_field)\\n\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = -1000000000\\n    episode = 0\\n    episode_count = ep_count\\n    trans_cost: float = 0.25\\n\\n    while episode < episode_count:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   trans_cost=trans_cost, window_size=window_size, prev_agent_profit=agent_profit,\\n                                   adjcp_field=adjcp_field)\\n        agent_profit = train_result[2]\\n\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, strategy)\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        episode += 1\\n\\n\\nif __name__ == \"__main__\":\\n    args = docopt(__doc__)\\n\\n    file_data = args[\"<file-data>\"]\\n    train_percent = args[\"<train-percent>\"]\\n\\n    date_field = args[\"--date-field\"]\\n    adjcp_field = args[\"--adjcp-field\"]\\n    window_size = int(args[\"--window-size\"])\\n    batch_size = int(args[\"--batch-size\"])\\n    ep_count = int(args[\"--episode-count\"])\\n    model_name = args[\"--model-name\"]\\n    debug = args[\"--debug\"]\\n\\n    coloredlogs.install(level=\"DEBUG\")\\n    switch_k_backend_device()\\n\\n    try:\\n        # dqn\\n        main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n             ep_count, \"dqn\", f\"{model_name}_dqn\", debug)\\n\\n        # # t-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"t-dqn\", f\"{model_name}_tdqn\", debug)\\n        #\\n        # # double-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"double-dqn\", f\"{model_name}_ddqn\", debug)\\n\\n    except KeyboardInterrupt:\\n        print(\"Aborted!\")\\n', 'File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\\nFile \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\\nFile \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\\nOSError: Unable to open file (file signature not found)\\nWhen executing the following code:\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2023\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\"\"\"\\n\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n\\ndef main(train_stock, train_percent, date_field, adjcp_field, window_size, batch_size, ep_count,\\n         strategy=\"t-dqn\", model_name=\"model_debug\", debug=False):\\n    \"\"\" Trains the stock trading bot using Deep Q-Learning. Args: [python train.py --help]\\n    \"\"\"\\n    agent = Agent(window_size, strategy=strategy, pretrained=True, model_name=model_name)\\n    data = load_data(train_stock, date_field)\\n    take = int(len(data) * (train_percent / 100))\\n    train_data = data_split(data, 0, take, adjcp_field)\\n    val_data = data_split(data, take, int(len(data)), adjcp_field)\\n\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = -1000000000\\n    episode = 0\\n    episode_count = ep_count\\n    trans_cost: float = 0.25\\n\\n    while episode < episode_count:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   trans_cost=trans_cost, window_size=window_size, prev_agent_profit=agent_profit,\\n                                   adjcp_field=adjcp_field)\\n        agent_profit = train_result[2]\\n\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, strategy)\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        episode += 1\\n\\n\\nif __name__ == \"__main__\":\\n    args = docopt(__doc__)\\n\\n    file_data = args[\"<file-data>\"]\\n    train_percent = args[\"<train-percent>\"]\\n\\n    date_field = args[\"--date-field\"]\\n    adjcp_field = args[\"--adjcp-field\"]\\n    window_size = int(args[\"--window-size\"])\\n    batch_size = int(args[\"--batch-size\"])\\n    ep_count = int(args[\"--episode-count\"])\\n    model_name = args[\"--model-name\"]\\n    debug = args[\"--debug\"]\\n\\n    coloredlogs.install(level=\"DEBUG\")\\n    switch_k_backend_device()\\n\\n    try:\\n        # dqn\\n        main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n             ep_count, \"dqn\", f\"{model_name}_dqn\", debug)\\n\\n        # # t-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"t-dqn\", f\"{model_name}_tdqn\", debug)\\n        #\\n        # # double-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"double-dqn\", f\"{model_name}_ddqn\", debug)\\n\\n    except KeyboardInterrupt:\\n        print(\"Aborted!\")\\n \\n', 'when to running docker-compose build in macbook pro i get the following failed to solve: process \"/bin/sh -c tar -xvf ta-lib.tar.gz && cd ta-lib/ && ./configure --prefix=/usr && make && make install\" did not complete successfully: exit code: 1', 'ERROR: Could not find a version that satisfies the requirement nvidia-nvjitlink-cu12==12.6.20 (from versions: 0.0.1.dev5)\\nERROR: No matching distribution found for nvidia-nvjitlink-cu12==12.6.20\\n', 'failed to solve: process \"/bin/sh -c wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz &&   tar -xvzf ta-lib-0.4.0-src.tar.gz &&   cd ta-lib/ &&   ./configure --prefix=/usr &&   make &&   make install\" did not complete successfully: exit code: 1\\n', 'How do I print the data from the following System.Collections.Generic.List`1[NinjaTrader.Custom.SmartStrategies.Utils.TradingData]', 'How do I print the data from the following that comes from dataSent variable System.Collections.Generic.List`1[NinjaTrader.Custom.SmartStrategies.Utils.TradingData]\\n\\n', '  How do I print the dataToSent values\\n\\n           var request = new TradingData()\\n           {\\n               Date = Time[0].ToString(\"G\"),\\n               Open = Open[0],\\n               High = High[0],\\n               Low = Low[0],\\n               Close = Close[0],\\n               Volume = Volume[0],\\n           };\\n           _data.Enqueue(request);\\n                var dataToSent = _data.Take(60).ToList();\\n                var result = _api.Predict(new TradingBotApiRequest()\\n                {\\n                    Data = dataToSent\\n                });\\n\\n', 'Why is the \"except IndexError\" keeps getting trigger and the buy and sell reward is not correct for the following code:\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:  # BUY\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            if len(agent.inventory) > 1:\\n                try:\\n                    previous_price = agent.inventory[-2]\\n                    delta = data.iloc[t][adjcp_field] - previous_price\\n                    reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                    total_profit += reward\\n                    logging.debug(f\"BUY: Price: {data.iloc[t][adjcp_field] - previous_price}, Reward: {reward}\")\\n                except IndexError:\\n                    # delta = 0\\n                    # previous_price = agent.inventory[-2]\\n                    delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n                    logging.debug(f\"BUY----: Price: {delta}\")\\n                total_profit += delta\\n\\n        # SELL\\n        elif action == 2:  # SELL\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            if len(agent.inventory) > 0:\\n                try:\\n                    bought_price = agent.inventory.pop(0)\\n                    delta = data.iloc[t][adjcp_field] - bought_price\\n                    reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                    total_profit += reward\\n                    logging.debug(f\"SELL: Price: {data.iloc[t][adjcp_field] - bought_price}, Reward: {reward}\")\\n                except IndexError:\\n                    # delta = 0\\n                    delta = data.iloc[t][adjcp_field] - data.iloc[t + window_size][adjcp_field]\\n                    logging.debug(f\"SELL----: Price: {delta}\")\\n                total_profit += delta\\n\\n        # HOLD\\n        else:  # HOLD\\n            reward = 0  # Explicitly set reward to 0 for HOLD action\\n            logging.debug(f\"HOLD: Price: {data.iloc[t][adjcp_field]}, Reward: {reward}\")\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n', 'Why is the \"except IndexError\" keeps getting trigger and the buy and sell reward is not correct for the following code:\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:  # BUY\\n                try:\\n                    previous_price = agent.inventory[-2]\\n                    delta = data.iloc[t][adjcp_field] - previous_price\\n                    reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                    total_profit += reward\\n                    logging.debug(f\"BUY: Price: {data.iloc[t][adjcp_field] - previous_price}, Reward: {reward}\")\\n                except IndexError:\\n                    # delta = 0\\n                    # previous_price = agent.inventory[-2]\\n                    delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n                    logging.debug(f\"BUY----: Price: {delta}\")\\n                total_profit += delta\\n\\n        # SELL\\n        elif action == 2:  # SELL\\n                try:\\n                    bought_price = agent.inventory.pop(0)\\n                    delta = data.iloc[t][adjcp_field] - bought_price\\n                    reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                    total_profit += reward\\n                    logging.debug(f\"SELL: Price: {data.iloc[t][adjcp_field] - bought_price}, Reward: {reward}\")\\n                except IndexError:\\n                    # delta = 0\\n                    delta = data.iloc[t][adjcp_field] - data.iloc[t + window_size][adjcp_field]\\n                    logging.debug(f\"SELL----: Price: {delta}\")\\n                total_profit += delta\\n\\n        # HOLD\\n        else:  # HOLD\\n            reward = 0  # Explicitly set reward to 0 for HOLD action\\n            logging.debug(f\"HOLD: Price: {data.iloc[t][adjcp_field]}, Reward: {reward}\")\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n', 'Check the following code for error:\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:  # BUY\\n            agent.inventory.append(data.iloc[t][adjcp_field])  # Add the current price to inventory\\n            if len(agent.inventory) > 1:\\n                try:\\n                    previous_price = agent.inventory[-2]\\n                    delta = data.iloc[t][adjcp_field] - previous_price\\n                    reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                    total_profit += reward\\n                    logging.debug(f\"BUY: Price: {data.iloc[t][adjcp_field] - previous_price}, Reward: {reward}\")\\n                except IndexError:\\n                    delta = 0  # Set delta to 0 if there\\'s an issue\\n                    logging.debug(\"BUY: Unable to access previous price, delta set to 0.\")\\n            else:\\n                delta = 0  # No previous price to compare\\n                logging.debug(\"BUY: Inventory has less than 2 items, delta set to 0.\")\\n            total_profit += delta\\n\\n        # SELL\\n        elif action == 2:  # SELL\\n            if len(agent.inventory) > 0:\\n                bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n                delta = data.iloc[t][adjcp_field] - bought_price\\n                reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                total_profit += reward\\n                logging.debug(f\"SELL: Price: {data.iloc[t][adjcp_field] - bought_price}, Reward: {reward}\")\\n            else:\\n                delta = 0  # No item to sell\\n                logging.debug(\"SELL: Inventory is empty, delta set to 0.\")\\n            total_profit += delta\\n\\n        # Ensure safe access to data\\n        if t + window_size < len(data):\\n            # Use t + window_size safely\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        else:\\n            logging.debug(\"Index out of bounds for data access with t + window_size.\")\\n        \\n        \\n        # HOLD\\n        else:  # HOLD\\n            reward = 0  # Explicitly set reward to 0 for HOLD action\\n            logging.debug(f\"HOLD: Price: {data.iloc[t][adjcp_field]}, Reward: {reward}\")\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\n', 'Why is the SELL Price is always a negative\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:  # BUY\\n            agent.inventory.append(data.iloc[t][adjcp_field])  # Add the current price to inventory\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                total_profit += reward\\n                logging.debug(f\"BUY: Price: {data.iloc[t][adjcp_field] - previous_price}, Reward: {reward}\")\\n            else:\\n                delta = 0  # No previous price to compare\\n                logging.debug(\"BUY: Inventory has less than 2 items, delta set to 0.\")\\n            total_profit += delta\\n\\n        # SELL\\n        elif action == 2:  # SELL\\n            if len(agent.inventory) > 0:\\n                bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n                delta = data.iloc[t][adjcp_field] - bought_price\\n                reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                total_profit += reward\\n                logging.debug(f\"SELL: Price: {data.iloc[t][adjcp_field] - bought_price}, Reward: {reward}\")\\n            else:\\n                delta = 0  # No item to sell\\n                logging.debug(\"SELL: Inventory is empty, delta set to 0.\")\\n            total_profit += delta\\n\\n        # HOLD\\n        else:  # HOLD\\n            reward = 0  # Explicitly set reward to 0 for HOLD action\\n            logging.debug(f\"HOLD: Price: {data.iloc[t][adjcp_field]}, Reward: {reward}\")\\n\\n        # Ensure safe access to data\\n        if t + window_size < len(data):\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        else:\\n            logging.debug(\"Index out of bounds for data access with t + window_size.\")\\n            delta = 0  # Set delta to 0 if out of bounds\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\n', 'Check and optimize the following code:\\n\\nimport gc\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"Monitor memory usage and log if it exceeds the threshold.\"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        logging.warning(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:  # BUY\\n            agent.inventory.append(data.iloc[t][adjcp_field])  # Add the current price to inventory\\n            if len(agent.inventory) > 1:\\n                bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n                current_price = data.iloc[t][adjcp_field]\\n                # previous_price = agent.inventory[-2]\\n                delta = current_price - bought_price\\n                reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                total_profit += reward\\n                logging.debug(f\"BUY: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n            else:\\n                delta = 0  # No previous price to compare\\n                logging.debug(\"BUY: Inventory has less than 2 items, delta set to 0.\")\\n            total_profit += delta\\n\\n        # SELL\\n        elif action == 2:  # SELL\\n            if len(agent.inventory) > 0:\\n                if len(agent.inventory) > 0:\\n                    bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n                    current_price = data.iloc[t][adjcp_field]\\n                    delta = bought_price - current_price\\n                    reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                    total_profit += reward\\n                    logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n                else:\\n                    delta = 0  # No item to sell\\n                    logging.debug(\"SELL: Inventory is empty, delta set to 0.\")\\n                total_profit += delta\\n            else:\\n                delta = 0  # No item to sell\\n                logging.debug(\"SELL: Inventory is empty, delta set to 0.\")\\n            total_profit += delta\\n\\n        # HOLD\\n        else:  # HOLD\\n            reward = 0  # Explicitly set reward to 0 for HOLD action\\n            logging.debug(f\"HOLD: Price: {data.iloc[t][adjcp_field]}, Reward: {reward}\")\\n\\n        # Ensure safe access to data\\n        if t + window_size < len(data):\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        else:\\n            logging.debug(\"Index out of bounds for data access with t + window_size.\")\\n            delta = 0  # Set delta to 0 if out of bounds\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n        # Save the model if profitable\\n        if total_profit > 0 and total_profit > prev_agent_profit:\\n            try:\\n                agent.save()  # Save the model\\n                logging.info(\"Model saved after profitable episode.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # BUY\\n        if action == 1:\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            agent.inventory.append(data.iloc[t][adjcp_field])  # Add the current price to inventory\\n            if len(agent.inventory) > 1:\\n                bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n                current_price = data.iloc[t][adjcp_field]\\n                # previous_price = agent.inventory[-2]\\n                delta = current_price - bought_price\\n                reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                total_profit += reward\\n                logging.debug(f\"BUY: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n            else:\\n                delta = 0  # No previous price to compare\\n                logging.debug(\"BUY: Inventory has less than 2 items, delta set to 0.\")\\n            total_profit += delta\\n\\n\\n        # SELL\\n        elif action == 2:\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if len(agent.inventory) > 0:\\n                if len(agent.inventory) > 0:\\n                    bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n                    current_price = data.iloc[t][adjcp_field]\\n                    delta = bought_price - current_price\\n                    reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n                    total_profit += reward\\n                    logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n                else:\\n                    delta = 0  # No item to sell\\n                    logging.debug(\"SELL: Inventory is empty, delta set to 0.\")\\n                total_profit += delta\\n            else:\\n                delta = 0  # No item to sell\\n                logging.debug(\"SELL: Inventory is empty, delta set to 0.\")\\n            total_profit += delta\\n\\n        # HOLD\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'Why is it the self.model.predict(state, verbose=0) is always giving me 0, how is the best way to predict from a model\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions.\"\"\"\\n        if not is_eval and random.random() <= self.epsilon:\\n            return random.randrange(self.action_size)\\n\\n        # if self.first_iter:\\n        #     self.first_iter = False\\n        #     return 1  # Make a definite buy on the first iteration\\n\\n        action_probs = self.model.predict(state, verbose=0)\\n        return np.argmax(action_probs[0])', 'The model that is saved after we train is always giving me 0, how can I fix it to make it more reliable\\n\\nimport gc\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"Monitor memory usage and log if it exceeds the threshold.\"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        logging.warning(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n\\ndef execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n    \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n    current_price = data.iloc[t][adjcp_field]\\n    reward = 0\\n    delta = 0\\n\\n    if action == 1:  # BUY\\n        agent.inventory.append(current_price)  # Add the current price to inventory\\n        if len(agent.inventory) > 1:\\n            bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            delta = current_price - bought_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            logging.debug(f\"BUY: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.debug(\"BUY: Inventory has less than 2 items, delta set to 0.\")\\n    elif action == 2:  # SELL\\n        if len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            delta = bought_price - current_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.debug(\"SELL: Inventory is empty, delta set to 0.\")\\n    return reward, delta\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # Execute trade and get reward and delta\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # Execute trade and log history\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        if action == 1:\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n        elif action == 2:\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'What will be the best way to reduce data stock noise when the data is loaded:\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\" Reads stock data from CSV file \"\"\"\\n    df = pd.read_csv(stock_file)\\n    if date_field != \"Date\":\\n        df[\\'Date\\'] = df[date_field]\\n    df = df.sort_values([date_field])\\n    df[\\'adjcp\\'] = df[adjcp_field]\\n    return add_indicators(df)', 'What will be the best way to reduce data stock noise on the following code:\\n\\nimport logging\\nimport os\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\n\\n# Formats Position\\ndef format_position(price):\\n    return (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n\\n# Formats Currency\\ndef format_currency(price):\\n    return \\'${0:.2f}\\'.format(abs(price))\\n\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\" Displays training results \"\"\"\\n    if val_position in {initial_offset, 0.0}:\\n        logging.info(f\\'{strategy}: Episode {result[0]}/{result[1]} - Train Position: {format_position(result[2])}  Val Position: USELESS Train Loss: {result[3]:.4f}\\')\\n        return False\\n    else:\\n        logging.info(f\\'{strategy}: Episode {result[0]}/{result[1]} - Train Position: {format_position(result[2])}  Val Position: {format_position(val_position)}  Train Loss: {result[3]:.4f}\\')\\n        return True\\n\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\" Displays evaluation results \"\"\"\\n    if profit in {initial_offset, 0.0}:\\n        logging.info(f\\'{model_name}: USELESS\\\\n\\')\\n    else:\\n        logging.info(f\\'{model_name}: {format_position(profit)}\\\\n\\')\\n\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\" Reads stock data from CSV file \"\"\"\\n    df = pd.read_csv(stock_file)\\n    if date_field != \"Date\":\\n        df[\\'Date\\'] = df[date_field]\\n    df = df.sort_values([date_field])\\n    df[\\'adjcp\\'] = df[adjcp_field]\\n    return add_indicators(df)\\n\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\" Reads stock data from CSV file \"\"\"\\n    df = pd.read_csv(stock_file)\\n    return list(df.sort_values([date_field])[adjcp_field])\\n\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\" Splits the dataset into training or testing using date \"\"\"\\n    data = df[(df.index >= start) & (df.index < end)]\\n    return data.reset_index(drop=True)\\n\\n\\ndef switch_k_backend_device():\\n    \"\"\" Switches `keras` backend from GPU to CPU if required. \"\"\"\\n    if keras.backend == \"tensorflow\":\\n        logging.debug(\"Switching to TensorFlow for CPU\")\\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n\\n\\ndef get_features():\\n    \"\"\" Returns a list of feature names for technical indicators \"\"\"\\n    return [\\n        \\'CDL2CROWS\\', \\'CDL3BLACKCROWS\\', \\'CDL3INSIDE\\', \\'CDL3LINESTRIKE\\', \\'CDL3OUTSIDE\\',\\n        \\'CDL3STARSINSOUTH\\', \\'CDL3WHITESOLDIERS\\', \\'CDLABANDONEDBABY\\', \\'CDLADVANCEBLOCK\\',\\n        \\'CDLBELTHOLD\\', \\'CDLBREAKAWAY\\', \\'CDLCLOSINGMARUBOZU\\', \\'CDLCONCEALBABYSWALL\\',\\n        \\'CDLCOUNTERATTACK\\', \\'CDLDARKCLOUDCOVER\\', \\'CDLDOJI\\', \\'CDLDOJISTAR\\', \\'CDLDRAGONFLYDOJI\\',\\n        \\'CDLENGULFING\\', \\'CDLEVENINGDOJISTAR\\', \\'CDLEVENINGSTAR\\', \\'CDLGAPSIDESIDEWHITE\\',\\n        \\'CDLGRAVESTONEDOJI\\', \\'CDLHAMMER\\', \\'CDLHANGINGMAN\\', \\'CDLHARAMI\\', \\'CDLHARAMICROSS\\',\\n        \\'CDLHIGHWAVE\\', \\'CDLHIKKAKE\\', \\'CDLHIKKAKEMOD\\', \\'CDLHOMINGPIGEON\\', \\'CDLIDENTICAL3CROWS\\',\\n        \\'CDLINNECK\\', \\'CDLINVERTEDHAMMER\\', \\'CDLKICKING\\', \\'CDLKICKINGBYLENGTH\\', \\'CDLLADDERBOTTOM\\',\\n        \\'CDLLONGLEGGEDDOJI\\', \\'CDLLONGLINE\\', \\'CDLMARUBOZU\\', \\'CDLMATCHINGLOW\\', \\'CDLMATHOLD\\',\\n        \\'CDLMORNINGDOJISTAR\\', \\'CDLMORNINGSTAR\\', \\'CDLONNECK\\', \\'CDLPIERCING\\', \\'CDLRICKSHAWMAN\\',\\n        \\'CDLRISEFALL3METHODS\\', \\'CDLSEPARATINGLINES\\', \\'CDLSHOOTINGSTAR\\', \\'CDLSHORTLINE\\',\\n        \\'CDLSPINNINGTOP\\', \\'CDLSTALLEDPATTERN\\', \\'CDLSTICKSANDWICH\\', \\'CDLTAKURI\\', \\'CDLTASUKIGAP\\',\\n        \\'CDLTHRUSTING\\', \\'CDLTRISTAR\\', \\'CDLUNIQUE3RIVER\\', \\'CDLUPSIDEGAP2CROWS\\', \\'CDLXSIDEGAP3METHODS\\',\\n        \\'RSI\\', \\'MOMENTUM\\', \\'CCI\\', \\'MFI\\', \\'ROC\\', \\'ADX\\', \\'PPO\\'\\n    ]\\n\\n\\ndef add_indicators(df):\\n    \"\"\" Adds technical indicators to the DataFrame \"\"\"\\n    indicators = {\\n        \\'CDL2CROWS\\': talib.CDL2CROWS,\\n        \\'CDL3BLACKCROWS\\': talib.CDL3BLACKCROWS,\\n        \\'CDL3INSIDE\\': talib.CDL3INSIDE,\\n        \\'CDL3LINESTRIKE\\': talib.CDL3LINESTRIKE,\\n        \\'CDL3OUTSIDE\\': talib.CDL3OUTSIDE,\\n        \\'CDL3STARSINSOUTH\\': talib.CDL3STARSINSOUTH,\\n        \\'CDL3WHITESOLDIERS\\': talib.CDL3WHITESOLDIERS,\\n        \\'CDLABANDONEDBABY\\': talib.CDLABANDONEDBABY,\\n        \\'CDLADVANCEBLOCK\\': talib.CDLADVANCEBLOCK,\\n        \\'CDLBELTHOLD\\': talib.CDLBELTHOLD,\\n        \\'CDLBREAKAWAY\\': talib.CDLBREAKAWAY,\\n        \\'CDLCLOSINGMARUBOZU\\': talib.CDLCLOSINGMARUBOZU,\\n        \\'CDLCONCEALBABYSWALL\\': talib.CDLCONCEALBABYSWALL,\\n        \\'CDLCOUNTERATTACK\\': talib.CDLCOUNTERATTACK,\\n        \\'CDLDARKCLOUDCOVER\\': talib.CDLDARKCLOUDCOVER,\\n        \\'CDLDOJI\\': talib.CDLDOJI,\\n        \\'CDLDOJISTAR\\': talib.CDLDOJISTAR,\\n        \\'CDLDRAGONFLYDOJI\\': talib.CDLDRAGONFLYDOJI,\\n        \\'CDLENGULFING\\': talib.CDLENGULFING,\\n        \\'CDLEVENINGDOJISTAR\\': talib.CDLEVENINGDOJISTAR,\\n        \\'CDLEVENINGSTAR\\': talib.CDLEVENINGSTAR,\\n        \\'CDLGAPSIDESIDEWHITE\\': talib.CDLGAPSIDESIDEWHITE,\\n        \\'CDLGRAVESTONEDOJI\\': talib.CDLGRAVESTONEDOJI,\\n        \\'CDLHAMMER\\': talib.CDLHAMMER,\\n        \\'CDLHANGINGMAN\\': talib.CDLHANGINGMAN,\\n        \\'CDLHARAMI\\': talib.CDLHARAMI,\\n        \\'CDLHARAMICROSS\\': talib.CDLHARAMICROSS,\\n        \\'CDLHIGHWAVE\\': talib.CDLHIGHWAVE,\\n        \\'CDLHIKKAKE\\': talib.CDLHIKKAKE,\\n        \\'CDLHIKKAKEMOD\\': talib.CDLHIKKAKEMOD,\\n        \\'CDLHOMINGPIGEON\\': talib.CDLHOMINGPIGEON,\\n        \\'CDLIDENTICAL3CROWS\\': talib.CDLIDENTICAL3CROWS,\\n        \\'CDLINNECK\\': talib.CDLINNECK,\\n        \\'CDLINVERTEDHAMMER\\': talib.CDLINVERTEDHAMMER,\\n        \\'CDLKICKING\\': talib.CDLKICKING,\\n        \\'CDLKICKINGBYLENGTH\\': talib.CDLKICKINGBYLENGTH,\\n        \\'CDLLADDERBOTTOM\\': talib.CDLLADDERBOTTOM,\\n        \\'CDLLONGLEGGEDDOJI\\': talib.CDLLONGLEGGEDDOJI,\\n        \\'CDLLONGLINE\\': talib.CDLLONGLINE,\\n        \\'CDLMARUBOZU\\': talib.CDLMARUBOZU,\\n        \\'CDLMATCHINGLOW\\': talib.CDLMATCHINGLOW,\\n        \\'CDLMATHOLD\\': talib.CDLMATHOLD,\\n        \\'CDLMORNINGDOJISTAR\\': talib.CDLMORNINGDOJISTAR,\\n        \\'CDLMORNINGSTAR\\': talib.CDLMORNINGSTAR,\\n        \\'CDLONNECK\\': talib.CDLONNECK,\\n        \\'CDLPIERCING\\': talib.CDLPIERCING,\\n        \\'CDLRICKSHAWMAN\\': talib.CDLRICKSHAWMAN,\\n        \\'CDLRISEFALL3METHODS\\': talib.CDLRISEFALL3METHODS,\\n        \\'CDLSEPARATINGLINES\\': talib.CDLSEPARATINGLINES,\\n        \\'CDLSHOOTINGSTAR\\': talib.CDLSHOOTINGSTAR,\\n        \\'CDLSHORTLINE\\': talib.CDLSHORTLINE,\\n        \\'CDLSPINNINGTOP\\': talib.CDLSPINNINGTOP,\\n        \\'CDLSTALLEDPATTERN\\': talib.CDLSTALLEDPATTERN,\\n        \\'CDLSTICKSANDWICH\\': talib.CDLSTICKSANDWICH,\\n        \\'CDLTAKURI\\': talib.CDLTAKURI,\\n        \\'CDLTASUKIGAP\\': talib.CDLTASUKIGAP,\\n        \\'CDLTHRUSTING\\': talib.CDLTHRUSTING,\\n        \\'CDLTRISTAR\\': talib.CDLTRISTAR,\\n        \\'CDLUNIQUE3RIVER\\': talib.CDLUNIQUE3RIVER,\\n        \\'CDLUPSIDEGAP2CROWS\\': talib.CDLUPSIDEGAP2CROWS,\\n        \\'CDLXSIDEGAP3METHODS\\': talib.CDLXSIDEGAP3METHODS,\\n        \\'RSI\\': lambda close: talib.RSI(close, timeperiod=14),\\n        \\'MOMENTUM\\': lambda close: talib.MOM(close, timeperiod=10),\\n        \\'CCI\\': lambda high, low, close: talib.CCI(high, low, close, timeperiod=14),\\n        \\'MFI\\': lambda high, low, close, volume: talib.MFI(high, low, close, volume, timeperiod=14),\\n        \\'ROC\\': lambda close: talib.ROC(close, timeperiod=10),\\n        \\'ADX\\': lambda high, low, close: talib.ADX(high, low, close, timeperiod=14),\\n        \\'PPO\\': lambda close: talib.PPO(close, fastperiod=5, slowperiod=15, matype=8)\\n    }\\n\\n    for indicator, func in indicators.items():\\n        if indicator in [\\'CDLABANDONEDBABY\\', \\'CDLDARKCLOUDCOVER\\', \\'CDLEVENINGDOJISTAR\\', \\'CDLEVENINGSTAR\\', \\'CDLMATHOLD\\',\\n                         \\'CDLMORNINGDOJISTAR\\', \\'CDLMORNINGSTAR\\']:\\n            df[indicator] = func(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n        elif indicator in [\\'RSI\\', \\'MOMENTUM\\', \\'ROC\\', \\'PPO\\']:\\n            df[indicator] = func(df[\\'Close\\'])\\n        elif indicator == \\'MFI\\':\\n            df[indicator] = func(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'])\\n        elif indicator in [\\'CCI\\', \\'ADX\\']:\\n            df[indicator] = func(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n        else:\\n            df[indicator] = func(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n\\n    # Fill NaN values with 0\\n    df.fillna(0, inplace=True)\\n    return df', 'How do i fix the following so I don\\'t get \"Inventory has less than 2 items, delta set to 0.\" on the following code:\\n\\nimport gc\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"Monitor memory usage and log if it exceeds the threshold.\"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        logging.warning(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n\\ndef execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n    \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n    current_price = data.iloc[t][adjcp_field]\\n    reward = 0\\n    delta = 0\\n\\n    if action == 1:  # BUY\\n        agent.inventory.append(current_price)  # Add the current price to inventory\\n        if len(agent.inventory) > 1:\\n            bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            delta = current_price - bought_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            logging.debug(f\"BUY: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.debug(\"BUY: Inventory has less than 2 items, delta set to 0.\")\\n    elif action == 2:  # SELL\\n        if len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            delta = bought_price - current_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.debug(\"SELL: Inventory is empty, delta set to 0.\")\\n    return reward, delta\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # Execute trade and get reward and delta\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n            logging.info(f\"Episode {episode}, Step {t}, Loss: {loss:.4f}\")\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # Execute trade and log history\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        if action == 1:\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n        elif action == 2:\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'How do i fix the following so I don\\'t get \"Inventory is empty, delta set to 0.\" on the following code:\\n\\nimport gc\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"Monitor memory usage and log if it exceeds the threshold.\"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        logging.warning(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n    \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n    current_price = data.iloc[t][adjcp_field]\\n    reward = 0\\n    delta = 0\\n\\n    if action == 1:  # BUY\\n        agent.inventory.append(current_price)  # Add the current price to inventory\\n        if len(agent.inventory) > 1:\\n            bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            delta = current_price - bought_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            logging.debug(f\"BUY: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.debug(\"BUY: Inventory has less than 2 items, delta set to 0.\")\\n    elif action == 2:  # SELL\\n        if len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            delta = bought_price - current_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.debug(\"SELL: Inventory is empty, delta set to 0.\")\\n    return reward, delta\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # Execute trade and get reward and delta\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n            logging.info(f\"Episode {episode}, Step {t}, Loss: {loss:.4f}\")\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # Execute trade and log history\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        if action == 1:\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n        elif action == 2:\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'What is wrong with the following code:\\n\\nimport gc\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"Monitor memory usage and log if it exceeds the threshold.\"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        logging.warning(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n# def execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n#     \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n#     current_price = data.iloc[t][adjcp_field]\\n#     reward = 0\\n#     delta = 0\\n#\\n#     if action == 1:  # BUY\\n#         agent.inventory.append(current_price)  # Add the current price to inventory\\n#         if len(agent.inventory) > 1:\\n#             bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n#             delta = current_price - bought_price\\n#             reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n#             logging.debug(f\"BUY: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n#         else:\\n#             logging.debug(\"BUY: Inventory has less than 2 items, delta set to 0.\")\\n#     elif action == 2:  # SELL\\n#         if len(agent.inventory) > 0:\\n#             bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n#             delta = bought_price - current_price\\n#             reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n#             logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n#         else:\\n#             logging.debug(\"SELL: Inventory is empty, delta set to 0.\")\\n#     return reward, delta\\n\\ndef execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n    \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n    current_price = data.iloc[t][adjcp_field]\\n    reward = 0\\n    delta = 0\\n\\n    if action == 1:  # BUY\\n        agent.inventory.append(current_price)  # Add the current price to inventory\\n        logging.debug(f\"BUY: Current Price: {current_price}, Inventory: {agent.inventory}\")\\n\\n        # Only calculate delta if there are at least 2 items in inventory\\n        if len(agent.inventory) > 1:\\n            bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            delta = current_price - bought_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            logging.debug(f\"BUY: Bought Price: {bought_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.debug(\"BUY: Inventory has less than 2 items, no delta calculated.\")\\n\\n    elif action == 2:  # SELL\\n        if len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            delta = bought_price - current_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            logging.debug(\\n                f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.debug(\"SELL: Inventory is empty, no trade executed.\")\\n\\n    return reward, delta\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # Execute trade and get reward and delta\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n            logging.info(f\"Episode {episode}, Step {t}, Loss: {loss:.4f}\")\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # Execute trade and log history\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        if action == 1:\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n        elif action == 2:\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'How can I improve the handling of actions in the evaluate_model function:\\n\\nimport gc\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"Monitor memory usage and log if it exceeds the threshold.\"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        logging.warning(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n# def execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n#     \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n#     current_price = data.iloc[t][adjcp_field]\\n#     reward = 0\\n#     delta = 0\\n#\\n#     if action == 1:  # BUY\\n#         agent.inventory.append(current_price)  # Add the current price to inventory\\n#         if len(agent.inventory) > 1:\\n#             bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n#             delta = current_price - bought_price\\n#             reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n#             logging.debug(f\"BUY: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n#         else:\\n#             logging.debug(\"BUY: Inventory has less than 2 items, delta set to 0.\")\\n#     elif action == 2:  # SELL\\n#         if len(agent.inventory) > 0:\\n#             bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n#             delta = bought_price - current_price\\n#             reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n#             logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n#         else:\\n#             logging.debug(\"SELL: Inventory is empty, delta set to 0.\")\\n#     return reward, delta\\n\\ndef execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n    \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n    current_price = data.iloc[t][adjcp_field]\\n    reward = 0\\n    delta = 0\\n\\n    if action == 1:  # BUY\\n        agent.inventory.append(current_price)  # Add the current price to inventory\\n        logging.debug(f\"BUY: Current Price: {current_price}, Inventory: {agent.inventory}\")\\n\\n        # Calculate delta if there are at least 2 items in inventory\\n        if len(agent.inventory) > 1:\\n            bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            delta = current_price - bought_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            logging.debug(f\"BUY: Bought Price: {bought_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.debug(\"BUY: Inventory has less than 2 items, no delta calculated.\")\\n\\n    elif action == 2:  # SELL\\n        if len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            delta = bought_price - current_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.warning(\"SELL: Inventory is empty, no trade executed.\")\\n\\n    return reward, delta\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # Execute trade and get reward and delta\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n            logging.info(f\"Episode {episode}, Step {t}, Loss: {loss:.4f}\")\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # Execute trade and log history\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        if action == 1:\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n        elif action == 2:\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'In the train_model function, if the agent.act(state) method does not return a valid action (e.g., if it returns an unexpected value), it could lead to unintended behavior. How can I Ensure that the action returned is validated.\\n\\nimport gc\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"Monitor memory usage and log if it exceeds the threshold.\"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        logging.warning(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n# def execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n#     \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n#     current_price = data.iloc[t][adjcp_field]\\n#     reward = 0\\n#     delta = 0\\n#\\n#     if action == 1:  # BUY\\n#         agent.inventory.append(current_price)  # Add the current price to inventory\\n#         if len(agent.inventory) > 1:\\n#             bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n#             delta = current_price - bought_price\\n#             reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n#             logging.debug(f\"BUY: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n#         else:\\n#             logging.debug(\"BUY: Inventory has less than 2 items, delta set to 0.\")\\n#     elif action == 2:  # SELL\\n#         if len(agent.inventory) > 0:\\n#             bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n#             delta = bought_price - current_price\\n#             reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n#             logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n#         else:\\n#             logging.debug(\"SELL: Inventory is empty, delta set to 0.\")\\n#     return reward, delta\\n\\ndef execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n    \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n    current_price = data.iloc[t][adjcp_field]\\n    reward = 0\\n    delta = 0\\n\\n    if action == 1:  # BUY\\n        agent.inventory.append(current_price)  # Add the current price to inventory\\n        logging.debug(f\"BUY: Current Price: {current_price}, Inventory: {agent.inventory}\")\\n\\n        # Calculate delta if there are at least 2 items in inventory\\n        if len(agent.inventory) > 1:\\n            bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            delta = current_price - bought_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            logging.debug(f\"BUY: Bought Price: {bought_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.debug(\"BUY: Inventory has less than 2 items, no delta calculated.\")\\n\\n    elif action == 2:  # SELL\\n        if len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            delta = bought_price - current_price\\n            reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n            logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.warning(\"SELL: Inventory is empty, no trade executed.\")\\n\\n    return reward, delta\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # Execute trade and get reward and delta\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n            logging.info(f\"Episode {episode}, Step {t}, Loss: {loss:.4f}\")\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    action_map = {\\n        1: \"BUY\",\\n        2: \"SELL\",\\n        0: \"HOLD\"  # Assuming 0 is the HOLD action\\n    }\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # Validate action\\n        if action not in action_map:\\n            logging.warning(f\"Unexpected action {action} at time {t}. Defaulting to HOLD.\")\\n            action = 0  # Default to HOLD if action is invalid\\n\\n        # Execute trade and log history\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        # Log the action taken\\n        history.append((data.iloc[t][adjcp_field], action_map[action]))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'How to I get the bought_price and current_price for the following:\\n\\ndef execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n    \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n    current_price = data.iloc[t][adjcp_field]\\n    reward = 0\\n    delta = 0\\n\\n    if action == 1:  # BUY\\n        agent.inventory.append(current_price)  # Add the current price to inventory\\n        logging.debug(f\"BUY: Current Price: {current_price}, Inventory: {agent.inventory}\")\\n\\n        # Calculate delta if there are at least 2 items in inventory\\n        if len(agent.inventory) > 1:\\n            # bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            bought_price = agent.inventory[-2]\\n            # delta = current_price - bought_price\\n            delta = ((current_price - bought_price) * 4 * 12.5) - trans_cost\\n            reward = max(delta, 0)  # Ensure reward is not negative\\n            logging.debug(f\"BUY: Bought Price: {bought_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.debug(\"BUY: Inventory has less than 2 items, no delta calculated.\")\\n\\n    elif action == 2:  # SELL\\n        if len(agent.inventory) > 0:\\n            # bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            bought_price = agent.inventory[-2]\\n            # delta = bought_price - current_price\\n            delta = ((bought_price - current_price) * 4 * 12.5) - trans_cost\\n            reward = max(delta, 0)  # Ensure reward is not negative\\n            logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.warning(\"SELL: Inventory is empty, no trade executed.\")\\n\\n    return reward, delta\\n\\n', 'How to I get the bought_price and current_price for the following:\\n\\ndef execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n    \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n    current_price = data.iloc[t][adjcp_field]\\n    reward = 0\\n    delta = 0\\n\\n    if action == 1:  # BUY\\n        agent.inventory.append(current_price)  # Add the current price to inventory\\n        logging.debug(f\"BUY: Current Price: {current_price}, Inventory: {agent.inventory}\")\\n\\n        # Calculate delta if there are at least 2 items in inventory\\n        if len(agent.inventory) > 1:\\n            # bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            _price = agent.inventory[-2]\\n            delta = ((current_price - _price) * 4 * 12.5) - trans_cost\\n            reward = max(delta, 0)  # Ensure reward is not negative\\n            logging.debug(f\"BUY: Bought Price: {_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.debug(\"BUY: Inventory has less than 2 items, no delta calculated.\")\\n\\n    elif action == 2:  # SELL\\n        if len(agent.inventory) > 0:\\n            # bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            _price = agent.inventory[-2]\\n            delta = ((_price - current_price) * 4 * 12.5) - trans_cost\\n            reward = max(delta, 0)  # Ensure reward is not negative\\n            logging.debug(f\"SELL: Bought Price: {_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.warning(\"SELL: Inventory is empty, no trade executed.\")\\n\\n    return reward, delta\\n', 'How do I get a positive delta for the action == 1 and action == 2 for the following code:\\n\\ndef execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n    \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n    current_price = data.iloc[t][adjcp_field]\\n    reward = 0\\n    delta = 0\\n\\n    if action == 1:  # BUY\\n        agent.inventory.append(current_price)  # Add the current price to inventory\\n        logging.debug(f\"BUY: Current Price: {current_price}, Inventory: {agent.inventory}\")\\n\\n        # Calculate delta if there are at least 2 items in inventory\\n        if len(agent.inventory) > 1:\\n            # bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            bought_price = agent.inventory[-2]  # Use the second last item as the bought price\\n            delta = ((current_price - bought_price) * 4 * 12.5) - trans_cost\\n            reward = max(delta, 0)  # Ensure reward is not negative\\n            logging.debug(f\"BUY: Bought Price: {bought_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.debug(\"BUY: Inventory has less than 2 items, no delta calculated.\")\\n\\n    elif action == 2:  # SELL\\n        if len(agent.inventory) > 0:\\n            # bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n            bought_price = agent.inventory[-2]  # Use the second last item as the bought price\\n            delta = ((bought_price - current_price) * 4 * 12.5) - trans_cost\\n            reward = max(delta, 0)  # Ensure reward is not negative\\n            logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.warning(\"SELL: Inventory is empty, no trade executed.\")\\n\\n    return reward, delta\\n', 'ERROR:root:Error processing request: \\'adjcp\\' on the following code:\\n\\nimport os\\nfrom pathlib import Path\\nimport pandas as pd\\nfrom flask import Flask, jsonify, request\\nimport logging\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.ops import get_state\\nfrom trading_bot.utils import switch_k_backend_device, add_indicators\\nfrom scipy.signal import savgol_filter\\n\\napp = Flask(__name__)\\napp.config[\\'WTF_CSRF_CHECK_DEFAULT\\'] = False\\napp.config[\\'WTF_CSRF_ENABLED\\'] = False\\n\\n# Set up logging\\nlogging.basicConfig(level=logging.INFO)\\n\\n# Global variables\\nwindow_size = 60\\nagent = None\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    global agent\\n    try:\\n        rq = request.json\\n        data = rq.get(\\'Data\\', [])\\n\\n        # Convert the data to a DataFrame\\n        df = pd.DataFrame(data, columns=[\\'Date\\', \\'Open\\', \\'High\\', \\'Low\\', \\'Close\\', \\'Volume\\'])\\n\\n        # Add Indicators and Pattern Recognition Functions\\n        df = add_indicators(df)\\n        df[\\'adjcp\\'] = savgol_filter(df[\\'adjcp\\'], window_length=11, polyorder=2)\\n        # Verify the number of features\\n        logging.info(f\"Number of features: {len(df.columns)}\")\\n\\n        # Check if DataFrame is empty or has insufficient rows\\n        if df.empty or len(df) < window_size:\\n            return jsonify({\\'error\\': \\'Insufficient data for prediction\\'}), 400\\n\\n        # Prepare the state for prediction\\n        t = len(df) - 1\\n        state = get_state(df, t, window_size)\\n\\n        # Make a prediction\\n        output = agent.act(state, True)\\n        logging.info(f\"Predicted action: {output}\")\\n\\n        # Map output to action\\n        action_map = {1: \\'buy\\', 2: \\'sell\\'}\\n        result = action_map.get(output, \\'hold\\')\\n\\n        return jsonify({\\'Res\\': result}), 200\\n\\n    except Exception as e:\\n        logging.error(f\"Error processing request: {e}\")\\n        return jsonify({\\'error\\': \\'Internal server error\\'}), 500\\n\\n\\nif __name__ == \"__main__\":\\n    directory_path = os.path.realpath(__file__)\\n    folder = Path(directory_path)\\n    switch_k_backend_device()\\n\\n    # Initialize the agent\\n    agent = Agent(window_size, pretrained=True, model_name=\\'models/model_dqn\\')\\n    app.run(host=\"0.0.0.0\", debug=False, port=5020)', 'ERROR:root:Column \\'adjcp\\' not found in DataFrame after adding indicators.\\n\\nimport os\\nfrom pathlib import Path\\nimport pandas as pd\\nfrom flask import Flask, jsonify, request\\nimport logging\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.ops import get_state\\nfrom trading_bot.utils import switch_k_backend_device, add_indicators\\nfrom scipy.signal import savgol_filter\\n\\napp = Flask(__name__)\\napp.config[\\'WTF_CSRF_CHECK_DEFAULT\\'] = False\\napp.config[\\'WTF_CSRF_ENABLED\\'] = False\\n\\n# Set up logging\\nlogging.basicConfig(level=logging.INFO)\\n\\n# Global variables\\nwindow_size = 60\\nagent = None\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    global agent\\n    try:\\n        rq = request.json\\n        data = rq.get(\\'Data\\', [])\\n\\n        # Convert the data to a DataFrame\\n        df = pd.DataFrame(data, columns=[\\'Date\\', \\'Open\\', \\'High\\', \\'Low\\', \\'Close\\', \\'Volume\\'])\\n        if date_field != \"Date\":\\n            df[\\'Date\\'] = df[date_field]\\n        df = df.sort_values([date_field])\\n        df[\\'adjcp\\'] = df[adjcp_field]\\n        \\n        # Check if \\'adjcp\\' column exists after adding indicators\\n        if \\'adjcp\\' not in df.columns:\\n            logging.error(\"Column \\'adjcp\\' not found in DataFrame after adding indicators.\")\\n            return jsonify({\\'error\\': \"Column \\'adjcp\\' not found in DataFrame.\"}), 400\\n\\n        # Apply Savitzky-Golay filter to \\'adjcp\\'\\n        df[\\'adjcp\\'] = savgol_filter(df[\\'adjcp\\'], window_length=11, polyorder=2)\\n\\n        # Add Indicators and Pattern Recognition Functions\\n        df = add_indicators(df)\\n\\n        # Verify the number of features\\n        logging.info(f\"Number of features: {len(df.columns)}\")\\n\\n        # Check if DataFrame is empty or has insufficient rows\\n        if df.empty or len(df) < window_size:\\n            return jsonify({\\'error\\': \\'Insufficient data for prediction\\'}), 400\\n\\n        # Prepare the state for prediction\\n        t = len(df) - 1\\n        state = get_state(df, t, window_size)\\n\\n        # Make a prediction\\n        output = agent.act(state, True)\\n        logging.info(f\"Predicted action: {output}\")\\n\\n        # Map output to action\\n        action_map = {1: \\'buy\\', 2: \\'sell\\'}\\n        result = action_map.get(output, \\'hold\\')\\n\\n        return jsonify({\\'Res\\': result}), 200\\n\\n    except Exception as e:\\n        logging.error(f\"Error processing request: {e}\")\\n        return jsonify({\\'error\\': \\'Internal server error\\'}), 500\\n    \\n# @app.route(\\'/predict\\', methods=[\\'POST\\'])\\n# def predict():\\n#     global agent\\n#     try:\\n#         rq = request.json\\n#         data = rq.get(\\'Data\\', [])\\n#\\n#         # Convert the data to a DataFrame\\n#         df = pd.DataFrame(data, columns=[\\'Date\\', \\'Open\\', \\'High\\', \\'Low\\', \\'Close\\', \\'Volume\\'])\\n#\\n#         # Add Indicators and Pattern Recognition Functions\\n#         df = add_indicators(df)\\n#         df[\\'adjcp\\'] = savgol_filter(df[\\'adjcp\\'], window_length=11, polyorder=2)\\n#         # Verify the number of features\\n#         logging.info(f\"Number of features: {len(df.columns)}\")\\n#\\n#         # Check if DataFrame is empty or has insufficient rows\\n#         if df.empty or len(df) < window_size:\\n#             return jsonify({\\'error\\': \\'Insufficient data for prediction\\'}), 400\\n#\\n#         # Prepare the state for prediction\\n#         t = len(df) - 1\\n#         state = get_state(df, t, window_size)\\n#\\n#         # Make a prediction\\n#         output = agent.act(state, True)\\n#         logging.info(f\"Predicted action: {output}\")\\n#\\n#         # Map output to action\\n#         action_map = {1: \\'buy\\', 2: \\'sell\\'}\\n#         result = action_map.get(output, \\'hold\\')\\n#\\n#         return jsonify({\\'Res\\': result}), 200\\n#\\n#     except Exception as e:\\n#         logging.error(f\"Error processing request: {e}\")\\n#         return jsonify({\\'error\\': \\'Internal server error\\'}), 500\\n\\n\\nif __name__ == \"__main__\":\\n    directory_path = os.path.realpath(__file__)\\n    folder = Path(directory_path)\\n    switch_k_backend_device()\\n\\n    # Initialize the agent\\n    agent = Agent(window_size, pretrained=True, model_name=\\'models/model_dqn\\')\\n    app.run(host=\"0.0.0.0\", debug=False, port=5020)', 'No mattter how I train the model the action_map.get keeps giving me 0 for the output\\n\\nimport os\\nfrom pathlib import Path\\nimport pandas as pd\\nfrom flask import Flask, jsonify, request\\nimport logging\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.ops import get_state\\nfrom trading_bot.utils import switch_k_backend_device, add_indicators\\n\\napp = Flask(__name__)\\napp.config[\\'WTF_CSRF_CHECK_DEFAULT\\'] = False\\napp.config[\\'WTF_CSRF_ENABLED\\'] = False\\n\\n# Set up logging\\nlogging.basicConfig(level=logging.INFO)\\n\\n# Global variables\\nwindow_size = 60\\nagent = None\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    global agent\\n    try:\\n        rq = request.json\\n        data = rq.get(\\'Data\\', [])\\n\\n        # Convert the data to a DataFrame\\n        df = pd.DataFrame(data, columns=[\\'Date\\', \\'Open\\', \\'High\\', \\'Low\\', \\'Close\\', \\'Volume\\'])\\n\\n        # Add Indicators and Pattern Recognition Functions\\n        df = add_indicators(df)\\n\\n        # Check if DataFrame is empty or has insufficient rows\\n        if df.empty or len(df) < window_size:\\n            return jsonify({\\'error\\': \\'Insufficient data for prediction\\'}), 400\\n\\n        # Prepare the state for prediction\\n        t = len(df) - 1\\n        state = get_state(df, t, window_size)\\n\\n        # Make a prediction\\n        output = agent.act(state, True)\\n        logging.info(f\"Predicted action: {output}\")\\n\\n        # Map output to action\\n        action_map = {1: \\'buy\\', 2: \\'sell\\'}\\n        result = action_map.get(output, \\'hold\\')\\n\\n        return jsonify({\\'Res\\': result}), 200\\n\\n    except Exception as e:\\n        logging.error(f\"Error processing request: {e}\")\\n        return jsonify({\\'error\\': \\'Internal server error\\'}), 500\\n\\n\\nif __name__ == \"__main__\":\\n    directory_path = os.path.realpath(__file__)\\n    folder = Path(directory_path)\\n    switch_k_backend_device()\\n\\n    # Initialize the agent\\n    agent = Agent(window_size, pretrained=True, model_name=\\'models/model_dqn\\')\\n    app.run(host=\"0.0.0.0\", debug=False, port=5020)', 'Why when I train the model it continues giving me a Hold, instead of a Buy or Sell on the following code:\\n\\nimport gc\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"Monitor memory usage and log if it exceeds the threshold.\"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        logging.warning(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n    \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n    current_price = data.iloc[t][adjcp_field]\\n    reward = 0\\n    delta = 0\\n\\n    if action == 1:  # BUY\\n        agent.inventory.append(current_price)  # Add the current price to inventory\\n        logging.debug(f\"BUY: Current Price: {current_price}, Inventory: {agent.inventory}\")\\n\\n        # Calculate delta if there are at least 2 items in inventory\\n        if len(agent.inventory) > 1:\\n            bought_price = agent.inventory[-2]  # Use the second last item as the bought price\\n            delta = ((current_price - bought_price) * 4 * 12.5) - trans_cost\\n            reward = max(delta, 0)  # Ensure reward is not negative\\n            logging.debug(f\"BUY: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.debug(\"BUY: Inventory has less than 2 items, no delta calculated.\")\\n\\n    elif action == 2:  # SELL\\n        if len(agent.inventory) > 1:\\n            bought_price = agent.inventory[-2]  # Use the second last item as the bought price\\n            delta = ((bought_price - current_price) * 4 * 12.5) - trans_cost\\n            reward = max(delta, 0)  # Ensure reward is not negative\\n            logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.warning(\"SELL: Inventory is empty, no trade executed.\")\\n\\n    return reward, delta\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    action_map = {\\n        1: \"BUY\",\\n        2: \"SELL\",\\n        0: \"HOLD\"  # Assuming 0 is the HOLD action\\n    }\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # Validate action\\n        if action not in action_map:\\n            logging.warning(f\"Unexpected action {action} at time {t}. Defaulting to HOLD.\")\\n            action = 0  # Default to HOLD if action is invalid\\n\\n        # Execute trade and get reward and delta\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n            logging.info(f\"Episode {episode}, Step {t}, Loss: {loss:.4f}\")\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    action_map = {\\n        1: \"BUY\",\\n        2: \"SELL\",\\n        0: \"HOLD\"  # Assuming 0 is the HOLD action\\n    }\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # Validate action\\n        if action not in action_map:\\n            logging.warning(f\"Unexpected action {action} at time {t}. Defaulting to HOLD.\")\\n            action = 0  # Default to HOLD if action is invalid\\n\\n        # Execute trade and log history\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        # Log the action taken\\n        history.append((data.iloc[t][adjcp_field], action_map[action]))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', \"How do I incorporate the TA-Lib library and add the indicators RSI, CCI, PPO, Momentum and eliminate selected_indicators on the following code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport modules.agent_function as agent_funct\\nimport modules.model_function as model_funct\\n\\n# Load and preprocess data\\ndf1 = pd.read_csv('data/20240718-ES SEP24-6Range.csv')\\nrows_count = (len(df1) + 1) # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10) # calculate 20% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[1:rows_toTrain, :].copy()\\n\\n# Check if 'Date' column exists, otherwise create it\\nif 'Date' not in df1.columns:\\n    df1['Date'] = pd.date_range(start='1/1/2000', periods=len(df1), freq='D')\\n\\n# Convert columns to float, excluding the 'Date' column\\ndef convert_to_float(lst):\\n    return [float(i) for i in lst]\\n\\nfor col in df1.columns:\\n    if col != 'Date':  # Skip conversion for 'Date' column\\n        df1[col] = convert_to_float(df1[col].tolist())\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    'macd', 'ppo', 'momentum', 'QQEDiff1', 'QQEDiff2', 'QQEDiff3', 'RSIMACDDiff'\\n]\\n\\n# Extract the selected indicators\\nindicators = {indicator: df1[indicator].tolist() for indicator in selected_indicators}\\nclose = df1['Close'].tolist()\\n\\n\\ndef step_by_step_prediction(agent, trend):\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t]\\n            print('bar %d: buy 1 contract at price %f, total balance %f' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t]\\n            print('bar %d: sell 1 contract at price %f, total balance %f' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n\\n# Update agent initialization with MACD and signal line\\nwindow_size = 60\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# logging.info('Starting training...')\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info('Training complete. Starting prediction...')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, 'output/model.pkl')\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color='black', alpha=0.5, lw=1.)\\nplt.plot(close, '.', markersize=10, color='green', alpha=0.5, label='buying signal', markevery=states_buy)\\nplt.plot(close, '.', markersize=10, color='red', alpha=0.5, label='selling signal', markevery=states_sell)\\nplt.title(f'total gains {total_gains}, total investment {invest}%')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv('data/20240718-ES SEP24-6Range.csv')\\ndf2 = df2.iloc[rows_toTrain + 1:, :].copy()\\n# Save DataFrame to CSV file\\ndf2.to_csv('data/ApiTest.csv', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load('output/model.pkl')\\n\\nfor col in df2.columns:\\n    if col != 'Date':  # Skip conversion for 'Date' column\\n        df2[col] = convert_to_float(df2[col].tolist())\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: df2[indicator].tolist() for indicator in selected_indicators}\\nclose_test = df2['Close'].tolist()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest['Close'] = close_test\\ntest['Signal'] = 0\\ntest.loc[test.index.intersection(states_buyT), 'Signal'] = 1\\ntest.loc[test.index.intersection(states_sellT), 'Signal'] = -1\\ntest['Market_Returns'] = test['Close'].pct_change()\\ntest['Strategy_Returns'] = test['Market_Returns'] * test['Signal'].shift(1)\\ntest['Cumulative_Profit'] = (test['Strategy_Returns'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test['Cumulative_Profit'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color='black', alpha=0.5, lw=1.)\\nplt.plot(close_test, '.', markersize=10, color='green', alpha=0.5, label='buying signal', markevery=states_buyT)\\nplt.plot(close_test, '.', markersize=10, color='red', alpha=0.5, label='selling signal', markevery=states_sellT)\\nplt.title(f'total gains {total_gainsT}, ROI {roiT}%')\\nplt.legend()\\nplt.show()\\n\\n# Save the test results\\n#joblib.dump(agentTest, 'agent_test.pkl')\\n\\n# Load and test the saved model\\nloaded_model = joblib.load('output/model.pkl')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color='black', alpha=0.5, lw=1.)\\nplt.plot(close, '.', markersize=10, color='green', alpha=0.5, label='buying signal', markevery=loaded_states_buy)\\nplt.plot(close, '.', markersize=10, color='red', alpha=0.5, label='selling signal', markevery=loaded_states_sell)\\nplt.title(f'total gains {loaded_total_gains}, total investment {loaded_invest}%')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)\\n\", 'What is wrong with the following Code:\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport talib  # Import TA-Lib\\nimport modules.agent_function as agent_funct\\nimport modules.model_function as model_funct\\nfrom modules.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n# train_stock = \\'data/dataFile.csv\\'\\n# date_field = \"Date\"\\n# adjcp_field = \"Close\"\\n# smoothing_method = \\'sma\\'\\n# window_size = 20\\n# # Load and preprocess data\\n# df1 = load_data(train_stock, date_field, adjcp_field, smoothing_method=\\'savgol\\', window_size=window_size)\\n\\n\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nrows_count = (len(df1) + 1)  # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[1:rows_toTrain, :].copy()\\n\\n# Check if \\'Date\\' column exists, otherwise create it\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\ndef convert_to_float(lst):\\n    return [float(i) for i in lst]\\n\\nfor col in df1.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df1[col] = convert_to_float(df1[col].tolist())\\n\\n\\n# Calculate indicators using TA-Lib\\n# df1[\\'RSI\\'] = talib.RSI(df1[\\'Close\\'], timeperiod=14)  # RSI - Relative Strength Index\\n# df1[\\'ROC\\'] = talib.ROC(df1[\\'Close\\'], timeperiod=14)  # ROC - Rate of change\\n# df1[\\'ADX\\'] = talib.ADX(df1[\\'High\\'], df1[\\'Low\\'], df1[\\'Close\\'], timeperiod=14)  # ADX - Average Directional Movement Index\\n# df1[\\'CCI\\'] = talib.CCI(df1[\\'High\\'], df1[\\'Low\\'], df1[\\'Close\\'], timeperiod=14)  # CCI - Commodity Channel Index\\n# df1[\\'PPO\\'] = talib.PPO(df1[\\'Close\\'], fastperiod=12, slowperiod=26, matype=0)  # PPO - Percentage Price Oscillator\\n# df1[\\'Momentum\\'] = talib.MOM(df1[\\'Close\\'], timeperiod=10)  # MOM - Momentum\\n\\n# Fill NaN values with 0\\n# df1.fillna(0, inplace=True)\\nselected_indicators = add_indicators(df1)\\n\\n# Define the list of indicators to be used\\n# selected_indicators = [\\'ADX\\', \\'CCI\\', \\'PPO\\', \\'Momentum\\']  # Updated indicators list\\n\\n# Extract the selected indicators\\nindicators = {indicator: df1[indicator].tolist() for indicator in selected_indicators}\\nclose = df1[\\'Close\\'].tolist()\\n\\n\\ndef add_indicators(df1):\\n    \"\"\" Adds technical indicators to the DataFrame \"\"\"\\n    indicators = {\\n        \\'CDL2CROWS\\': talib.CDL2CROWS,\\n        \\'CDL3BLACKCROWS\\': talib.CDL3BLACKCROWS,\\n        \\'CDL3INSIDE\\': talib.CDL3INSIDE,\\n        \\'CDL3LINESTRIKE\\': talib.CDL3LINESTRIKE,\\n        \\'CDL3OUTSIDE\\': talib.CDL3OUTSIDE,\\n        \\'CDL3STARSINSOUTH\\': talib.CDL3STARSINSOUTH,\\n        \\'CDL3WHITESOLDIERS\\': talib.CDL3WHITESOLDIERS,\\n        \\'CDLABANDONEDBABY\\': talib.CDLABANDONEDBABY,\\n        \\'CDLADVANCEBLOCK\\': talib.CDLADVANCEBLOCK,\\n        \\'CDLBELTHOLD\\': talib.CDLBELTHOLD,\\n        \\'CDLBREAKAWAY\\': talib.CDLBREAKAWAY,\\n        \\'CDLCLOSINGMARUBOZU\\': talib.CDLCLOSINGMARUBOZU,\\n        \\'CDLCONCEALBABYSWALL\\': talib.CDLCONCEALBABYSWALL,\\n        \\'CDLCOUNTERATTACK\\': talib.CDLCOUNTERATTACK,\\n        \\'CDLDARKCLOUDCOVER\\': talib.CDLDARKCLOUDCOVER,\\n        \\'CDLDOJI\\': talib.CDLDOJI,\\n        \\'CDLDOJISTAR\\': talib.CDLDOJISTAR,\\n        \\'CDLDRAGONFLYDOJI\\': talib.CDLDRAGONFLYDOJI,\\n        \\'CDLENGULFING\\': talib.CDLENGULFING,\\n        \\'CDLEVENINGDOJISTAR\\': talib.CDLEVENINGDOJISTAR,\\n        \\'CDLEVENINGSTAR\\': talib.CDLEVENINGSTAR,\\n        \\'CDLGAPSIDESIDEWHITE\\': talib.CDLGAPSIDESIDEWHITE,\\n        \\'CDLGRAVESTONEDOJI\\': talib.CDLGRAVESTONEDOJI,\\n        \\'CDLHAMMER\\': talib.CDLHAMMER,\\n        \\'CDLHANGINGMAN\\': talib.CDLHANGINGMAN,\\n        \\'CDLHARAMI\\': talib.CDLHARAMI,\\n        \\'CDLHARAMICROSS\\': talib.CDLHARAMICROSS,\\n        \\'CDLHIGHWAVE\\': talib.CDLHIGHWAVE,\\n        \\'CDLHIKKAKE\\': talib.CDLHIKKAKE,\\n        \\'CDLHIKKAKEMOD\\': talib.CDLHIKKAKEMOD,\\n        \\'CDLHOMINGPIGEON\\': talib.CDLHOMINGPIGEON,\\n        \\'CDLIDENTICAL3CROWS\\': talib.CDLIDENTICAL3CROWS,\\n        \\'CDLINNECK\\': talib.CDLINNECK,\\n        \\'CDLINVERTEDHAMMER\\': talib.CDLINVERTEDHAMMER,\\n        \\'CDLKICKING\\': talib.CDLKICKING,\\n        \\'CDLKICKINGBYLENGTH\\': talib.CDLKICKINGBYLENGTH,\\n        \\'CDLLADDERBOTTOM\\': talib.CDLLADDERBOTTOM,\\n        \\'CDLLONGLEGGEDDOJI\\': talib.CDLLONGLEGGEDDOJI,\\n        \\'CDLLONGLINE\\': talib.CDLLONGLINE,\\n        \\'CDLMARUBOZU\\': talib.CDLMARUBOZU,\\n        \\'CDLMATCHINGLOW\\': talib.CDLMATCHINGLOW,\\n        \\'CDLMATHOLD\\': talib.CDLMATHOLD,\\n        \\'CDLMORNINGDOJISTAR\\': talib.CDLMORNINGDOJISTAR,\\n        \\'CDLMORNINGSTAR\\': talib.CDLMORNINGSTAR,\\n        \\'CDLONNECK\\': talib.CDLONNECK,\\n        \\'CDLPIERCING\\': talib.CDLPIERCING,\\n        \\'CDLRICKSHAWMAN\\': talib.CDLRICKSHAWMAN,\\n        \\'CDLRISEFALL3METHODS\\': talib.CDLRISEFALL3METHODS,\\n        \\'CDLSEPARATINGLINES\\': talib.CDLSEPARATINGLINES,\\n        \\'CDLSHOOTINGSTAR\\': talib.CDLSHOOTINGSTAR,\\n        \\'CDLSHORTLINE\\': talib.CDLSHORTLINE,\\n        \\'CDLSPINNINGTOP\\': talib.CDLSPINNINGTOP,\\n        \\'CDLSTALLEDPATTERN\\': talib.CDLSTALLEDPATTERN,\\n        \\'CDLSTICKSANDWICH\\': talib.CDLSTICKSANDWICH,\\n        \\'CDLTAKURI\\': talib.CDLTAKURI,\\n        \\'CDLTASUKIGAP\\': talib.CDLTASUKIGAP,\\n        \\'CDLTHRUSTING\\': talib.CDLTHRUSTING,\\n        \\'CDLTRISTAR\\': talib.CDLTRISTAR,\\n        \\'CDLUNIQUE3RIVER\\': talib.CDLUNIQUE3RIVER,\\n        \\'CDLUPSIDEGAP2CROWS\\': talib.CDLUPSIDEGAP2CROWS,\\n        \\'CDLXSIDEGAP3METHODS\\': talib.CDLXSIDEGAP3METHODS,\\n        \\'MOMENTUM\\': lambda close: talib.MOM(close, timeperiod=10),\\n        \\'ADX\\': lambda high, low, close: talib.ADX(high, low, close, timeperiod=14),\\n        \\'PPO\\': lambda close: talib.PPO(close, fastperiod=5, slowperiod=15, matype=8)\\n    }\\n\\n    for indicator, func in indicators.items():\\n        if indicator in [\\'CDLABANDONEDBABY\\', \\'CDLDARKCLOUDCOVER\\', \\'CDLEVENINGDOJISTAR\\', \\'CDLEVENINGSTAR\\',\\n                         \\'CDLMATHOLD\\',\\n                         \\'CDLMORNINGDOJISTAR\\', \\'CDLMORNINGSTAR\\']:\\n            df1[indicator] = func(df1[\\'Open\\'], df1[\\'High\\'], df1[\\'Low\\'], df1[\\'Close\\'], penetration=0)\\n        elif indicator in [\\'MOMENTUM\\', \\'PPO\\']:\\n            df1[indicator] = func(df1[\\'Close\\'])\\n        elif indicator in [\\'ADX\\']:\\n            df1[indicator] = func(df1[\\'High\\'], df1[\\'Low\\'], df1[\\'Close\\'])\\n        else:\\n            df1[indicator] = func(df1[\\'Open\\'], df1[\\'High\\'], df1[\\'Low\\'], df1[\\'Close\\'])\\n\\n    # Fill NaN values with 0\\n    df1.fillna(0, inplace=True)\\n    return df1\\n\\ndef step_by_step_prediction(agent, trend):\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t]\\n            print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t]\\n            print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n# Update agent initialization with the new indicators\\nwindow_size = 60\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# logging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info(\\'Training complete. Starting prediction...\\')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/20240718-ES SEP24-6Range.csv\\')\\ndf2 = df2.iloc[rows_toTrain + 1:, :].copy()\\n# Save DataFrame to CSV file\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\nfor col in df2.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df2[col] = convert_to_float(df2[col].tolist())\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: df2[indicator].tolist() for indicator in selected_indicators}\\nclose_test = df2[\\'Close\\'].tolist()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Save the test results\\n# joblib.dump(agentTest, \\'agent_test.pkl\\')\\n\\n# Load and test the saved model\\nloaded_model = joblib.load(\\'output/model.pkl\\')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\nplt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)', 'I get IndexError: list index out of range when window_size is set to 10 on the following code:\\n\\nimport gc\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"Monitor memory usage and log if it exceeds the threshold.\"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        logging.warning(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n# def execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n#     \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n#     current_price = data.iloc[t][adjcp_field]\\n#     reward = 0\\n#     delta = 0\\n#\\n#     if action == 1:  # BUY\\n#         agent.inventory.append(current_price)  # Add the current price to inventory\\n#         if len(agent.inventory) > 1:\\n#             bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n#             delta = current_price - bought_price\\n#             reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n#             logging.debug(f\"BUY: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n#         else:\\n#             logging.debug(\"BUY: Inventory has less than 2 items, delta set to 0.\")\\n#     elif action == 2:  # SELL\\n#         if len(agent.inventory) > 0:\\n#             bought_price = agent.inventory.pop(0)  # Remove the first item from inventory\\n#             delta = bought_price - current_price\\n#             reward = max(delta - trans_cost, 0)  # Ensure reward is not negative\\n#             logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n#         else:\\n#             logging.debug(\"SELL: Inventory is empty, delta set to 0.\")\\n#     return reward, delta\\n\\ndef execute_trade(action, agent, data, t, adjcp_field, trans_cost):\\n    \"\"\"Execute trade based on the action and return the profit and delta.\"\"\"\\n    current_price = data.iloc[t][adjcp_field]\\n    reward = 0\\n    delta = 0\\n\\n    if action == 1:  # BUY\\n        agent.inventory.append(current_price)  # Add the current price to inventory\\n        logging.debug(f\"BUY: Current Price: {current_price}, Inventory: {agent.inventory}\")\\n\\n        # Calculate delta if there are at least 2 items in inventory\\n        if len(agent.inventory) > 1:\\n            bought_price = agent.inventory[-2]  # Use the second last item as the bought price\\n            delta = ((current_price - bought_price) * 4 * 12.5) - trans_cost\\n            reward = max(delta, 0)  # Ensure reward is not negative\\n            logging.debug(f\"BUY: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.debug(\"BUY: Inventory has less than 2 items, no delta calculated.\")\\n\\n    elif action == 2:  # SELL\\n        if len(agent.inventory) > 0:\\n            bought_price = agent.inventory[-2]  # Use the second last item as the bought price\\n            delta = ((bought_price - current_price) * 4 * 12.5) - trans_cost\\n            reward = max(delta, 0)  # Ensure reward is not negative\\n            logging.debug(f\"SELL: Bought Price: {bought_price}, Current Price: {current_price}, Delta: {delta}, Reward: {reward}\")\\n        else:\\n            logging.warning(\"SELL: Inventory is empty, no trade executed.\")\\n\\n    return reward, delta\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\', trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n    average_loss = 0  # Initialize average_loss\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    action_map = {\\n        1: \"BUY\",\\n        2: \"SELL\",\\n        0: \"HOLD\"  # Assuming 0 is the HOLD action\\n    }\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        # Monitor memory usage and save model if necessary\\n        if monitor_memory():\\n            try:\\n                agent.save()\\n                logging.info(\"Model saved due to high memory usage.\")\\n            except Exception as e:\\n                logging.error(f\"Error saving model during memory check: {e}\")\\n            gc.collect()\\n\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state)\\n\\n        # Validate action\\n        if action not in action_map:\\n            logging.warning(f\"Unexpected action {action} at time {t}. Defaulting to HOLD.\")\\n            action = 0  # Default to HOLD if action is invalid\\n\\n        # Execute trade and get reward and delta\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n            logging.info(f\"Episode {episode}, Step {t}, Loss: {loss:.4f}\")\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Log average loss\\n    if avg_loss:\\n        average_loss = np.mean(avg_loss)\\n        logging.info(f\"Average loss for episode {episode}: {average_loss:.4f}\")\\n\\n    return episode, ep_count, total_profit, average_loss\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field, trans_cost=0.25):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    action_map = {\\n        1: \"BUY\",\\n        2: \"SELL\",\\n        0: \"HOLD\"  # Assuming 0 is the HOLD action\\n    }\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # Select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # Validate action\\n        if action not in action_map:\\n            logging.warning(f\"Unexpected action {action} at time {t}. Defaulting to HOLD.\")\\n            action = 0  # Default to HOLD if action is invalid\\n\\n        # Execute trade and log history\\n        reward, delta = execute_trade(action, agent, data, t, adjcp_field, trans_cost)\\n        total_profit += reward + delta\\n\\n        # Log the action taken\\n        history.append((data.iloc[t][adjcp_field], action_map[action]))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 68), found shape=(None, 64) for the following code:\\n\\nimport logging\\nimport os\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\nfrom scipy.ndimage import gaussian_filter1d\\nfrom scipy.signal import savgol_filter\\n\\n# Formats Position\\ndef format_position(price):\\n    return (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n# Formats Currency\\ndef format_currency(price):\\n    return \\'${0:.2f}\\'.format(abs(price))\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\" Displays training results \"\"\"\\n    if val_position in {initial_offset, 0.0}:\\n        logging.info(f\\'{strategy}: Episode {result[0]}/{result[1]} - Train Position: {format_position(result[2])}  Val Position: USELESS Train Loss: {result[3]:.4f}\\')\\n        return False\\n    else:\\n        logging.info(f\\'{strategy}: Episode {result[0]}/{result[1]} - Train Position: {format_position(result[2])}  Val Position: {format_position(val_position)}  Train Loss: {result[3]:.4f}\\')\\n        return True\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\" Displays evaluation results \"\"\"\\n    if profit in {initial_offset, 0.0}:\\n        logging.info(f\\'{model_name}: USELESS\\\\n\\')\\n    else:\\n        logging.info(f\\'{model_name}: {format_position(profit)}\\\\n\\')\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\", smoothing_method=\\'sma\\', window_size=20):\\n    \"\"\" Reads stock data from CSV file and applies noise reduction \"\"\"\\n    df = pd.read_csv(stock_file)\\n    if date_field != \"Date\":\\n        df[\\'Date\\'] = df[date_field]\\n    df = df.sort_values([date_field])\\n    df[\\'adjcp\\'] = df[adjcp_field]\\n\\n    # Apply noise reduction\\n    if smoothing_method == \\'sma\\':\\n        df[\\'adjcp\\'] = df[\\'adjcp\\'].rolling(window=window_size).mean()\\n    elif smoothing_method == \\'ema\\':\\n        df[\\'adjcp\\'] = df[\\'adjcp\\'].ewm(span=window_size, adjust=False).mean()\\n    elif smoothing_method == \\'gaussian\\':\\n        df[\\'adjcp\\'] = gaussian_filter1d(df[\\'adjcp\\'], sigma=2)\\n    elif smoothing_method == \\'savgol\\':\\n        df[\\'adjcp\\'] = savgol_filter(df[\\'adjcp\\'], window_length=11, polyorder=2)\\n\\n    df.dropna(inplace=True)  # Drop rows with NaN values after smoothing\\n    return add_indicators(df)\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\" Reads stock data from CSV file \"\"\"\\n    df = pd.read_csv(stock_file)\\n    return list(df.sort_values([date_field])[adjcp_field])\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\" Splits the dataset into training or testing using date \"\"\"\\n    data = df[(df.index >= start) & (df.index < end)]\\n    return data.reset_index(drop=True)\\n\\ndef switch_k_backend_device():\\n    \"\"\" Switches `keras` backend from GPU to CPU if required. \"\"\"\\n    if keras.backend == \"tensorflow\":\\n        logging.debug(\"Switching to TensorFlow for CPU\")\\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n\\ndef get_features():\\n    \"\"\" Returns a list of feature names for technical indicators \"\"\"\\n    return [\\n        \\'CDL2CROWS\\', \\'CDL3BLACKCROWS\\', \\'CDL3INSIDE\\', \\'CDL3LINESTRIKE\\', \\'CDL3OUTSIDE\\',\\n        \\'CDL3STARSINSOUTH\\', \\'CDL3WHITESOLDIERS\\', \\'CDLABANDONEDBABY\\', \\'CDLADVANCEBLOCK\\',\\n        \\'CDLBELTHOLD\\', \\'CDLBREAKAWAY\\', \\'CDLCLOSINGMARUBOZU\\', \\'CDLCONCEALBABYSWALL\\',\\n        \\'CDLCOUNTERATTACK\\', \\'CDLDARKCLOUDCOVER\\', \\'CDLDOJI\\', \\'CDLDOJISTAR\\', \\'CDLDRAGONFLYDOJI\\',\\n        \\'CDLENGULFING\\', \\'CDLEVENINGDOJISTAR\\', \\'CDLEVENINGSTAR\\', \\'CDLGAPSIDESIDEWHITE\\',\\n        \\'CDLGRAVESTONEDOJI\\', \\'CDLHAMMER\\', \\'CDLHANGINGMAN\\', \\'CDLHARAMI\\', \\'CDLHARAMICROSS\\',\\n        \\'CDLHIGHWAVE\\', \\'CDLHIKKAKE\\', \\'CDLHIKKAKEMOD\\', \\'CDLHOMINGPIGEON\\', \\'CDLIDENTICAL3CROWS\\',\\n        \\'CDLINNECK\\', \\'CDLINVERTEDHAMMER\\', \\'CDLKICKING\\', \\'CDLKICKINGBYLENGTH\\', \\'CDLLADDERBOTTOM\\',\\n        \\'CDLLONGLEGGEDDOJI\\', \\'CDLLONGLINE\\', \\'CDLMARUBOZU\\', \\'CDLMATCHINGLOW\\', \\'CDLMATHOLD\\',\\n        \\'CDLMORNINGDOJISTAR\\', \\'CDLMORNINGSTAR\\', \\'CDLONNECK\\', \\'CDLPIERCING\\', \\'CDLRICKSHAWMAN\\',\\n        \\'CDLRISEFALL3METHODS\\', \\'CDLSEPARATINGLINES\\', \\'CDLSHOOTINGSTAR\\', \\'CDLSHORTLINE\\',\\n        \\'CDLSPINNINGTOP\\', \\'CDLSTALLEDPATTERN\\', \\'CDLSTICKSANDWICH\\', \\'CDLTAKURI\\', \\'CDLTASUKIGAP\\',\\n        \\'CDLTHRUSTING\\', \\'CDLTRISTAR\\', \\'CDLUNIQUE3RIVER\\', \\'CDLUPSIDEGAP2CROWS\\', \\'CDLXSIDEGAP3METHODS\\',\\n        \\'MOMENTUM\\', \\'ADX\\', \\'PPO\\'\\n    ]\\n\\n\\n    # \\'RSI\\', \\'MOMENTUM\\', \\'CCI\\', \\'MFI\\', \\'ROC\\', \\'ADX\\', \\'PPO\\'\\ndef add_indicators(df):\\n    \"\"\" Adds technical indicators to the DataFrame \"\"\"\\n    indicators = {\\n        \\'CDL2CROWS\\': talib.CDL2CROWS,\\n        \\'CDL3BLACKCROWS\\': talib.CDL3BLACKCROWS,\\n        \\'CDL3INSIDE\\': talib.CDL3INSIDE,\\n        \\'CDL3LINESTRIKE\\': talib.CDL3LINESTRIKE,\\n        \\'CDL3OUTSIDE\\': talib.CDL3OUTSIDE,\\n        \\'CDL3STARSINSOUTH\\': talib.CDL3STARSINSOUTH,\\n        \\'CDL3WHITESOLDIERS\\': talib.CDL3WHITESOLDIERS,\\n        \\'CDLABANDONEDBABY\\': talib.CDLABANDONEDBABY,\\n        \\'CDLADVANCEBLOCK\\': talib.CDLADVANCEBLOCK,\\n        \\'CDLBELTHOLD\\': talib.CDLBELTHOLD,\\n        \\'CDLBREAKAWAY\\': talib.CDLBREAKAWAY,\\n        \\'CDLCLOSINGMARUBOZU\\': talib.CDLCLOSINGMARUBOZU,\\n        \\'CDLCONCEALBABYSWALL\\': talib.CDLCONCEALBABYSWALL,\\n        \\'CDLCOUNTERATTACK\\': talib.CDLCOUNTERATTACK,\\n        \\'CDLDARKCLOUDCOVER\\': talib.CDLDARKCLOUDCOVER,\\n        \\'CDLDOJI\\': talib.CDLDOJI,\\n        \\'CDLDOJISTAR\\': talib.CDLDOJISTAR,\\n        \\'CDLDRAGONFLYDOJI\\': talib.CDLDRAGONFLYDOJI,\\n        \\'CDLENGULFING\\': talib.CDLENGULFING,\\n        \\'CDLEVENINGDOJISTAR\\': talib.CDLEVENINGDOJISTAR,\\n        \\'CDLEVENINGSTAR\\': talib.CDLEVENINGSTAR,\\n        \\'CDLGAPSIDESIDEWHITE\\': talib.CDLGAPSIDESIDEWHITE,\\n        \\'CDLGRAVESTONEDOJI\\': talib.CDLGRAVESTONEDOJI,\\n        \\'CDLHAMMER\\': talib.CDLHAMMER,\\n        \\'CDLHANGINGMAN\\': talib.CDLHANGINGMAN,\\n        \\'CDLHARAMI\\': talib.CDLHARAMI,\\n        \\'CDLHARAMICROSS\\': talib.CDLHARAMICROSS,\\n        \\'CDLHIGHWAVE\\': talib.CDLHIGHWAVE,\\n        \\'CDLHIKKAKE\\': talib.CDLHIKKAKE,\\n        \\'CDLHIKKAKEMOD\\': talib.CDLHIKKAKEMOD,\\n        \\'CDLHOMINGPIGEON\\': talib.CDLHOMINGPIGEON,\\n        \\'CDLIDENTICAL3CROWS\\': talib.CDLIDENTICAL3CROWS,\\n        \\'CDLINNECK\\': talib.CDLINNECK,\\n        \\'CDLINVERTEDHAMMER\\': talib.CDLINVERTEDHAMMER,\\n        \\'CDLKICKING\\': talib.CDLKICKING,\\n        \\'CDLKICKINGBYLENGTH\\': talib.CDLKICKINGBYLENGTH,\\n        \\'CDLLADDERBOTTOM\\': talib.CDLLADDERBOTTOM,\\n        \\'CDLLONGLEGGEDDOJI\\': talib.CDLLONGLEGGEDDOJI,\\n        \\'CDLLONGLINE\\': talib.CDLLONGLINE,\\n        \\'CDLMARUBOZU\\': talib.CDLMARUBOZU,\\n        \\'CDLMATCHINGLOW\\': talib.CDLMATCHINGLOW,\\n        \\'CDLMATHOLD\\': talib.CDLMATHOLD,\\n        \\'CDLMORNINGDOJISTAR\\': talib.CDLMORNINGDOJISTAR,\\n        \\'CDLMORNINGSTAR\\': talib.CDLMORNINGSTAR,\\n        \\'CDLONNECK\\': talib.CDLONNECK,\\n        \\'CDLPIERCING\\': talib.CDLPIERCING,\\n        \\'CDLRICKSHAWMAN\\': talib.CDLRICKSHAWMAN,\\n        \\'CDLRISEFALL3METHODS\\': talib.CDLRISEFALL3METHODS,\\n        \\'CDLSEPARATINGLINES\\': talib.CDLSEPARATINGLINES,\\n        \\'CDLSHOOTINGSTAR\\': talib.CDLSHOOTINGSTAR,\\n        \\'CDLSHORTLINE\\': talib.CDLSHORTLINE,\\n        \\'CDLSPINNINGTOP\\': talib.CDLSPINNINGTOP,\\n        \\'CDLSTALLEDPATTERN\\': talib.CDLSTALLEDPATTERN,\\n        \\'CDLSTICKSANDWICH\\': talib.CDLSTICKSANDWICH,\\n        \\'CDLTAKURI\\': talib.CDLTAKURI,\\n        \\'CDLTASUKIGAP\\': talib.CDLTASUKIGAP,\\n        \\'CDLTHRUSTING\\': talib.CDLTHRUSTING,\\n        \\'CDLTRISTAR\\': talib.CDLTRISTAR,\\n        \\'CDLUNIQUE3RIVER\\': talib.CDLUNIQUE3RIVER,\\n        \\'CDLUPSIDEGAP2CROWS\\': talib.CDLUPSIDEGAP2CROWS,\\n        \\'CDLXSIDEGAP3METHODS\\': talib.CDLXSIDEGAP3METHODS,\\n        \\'MOMENTUM\\': lambda close: talib.MOM(close, timeperiod=10),\\n        \\'ADX\\': lambda high, low, close: talib.ADX(high, low, close, timeperiod=14),\\n        \\'PPO\\': lambda close: talib.PPO(close, fastperiod=5, slowperiod=15, matype=8)\\n    }\\n\\n    # \\'RSI\\': lambda close: talib.RSI(close, timeperiod=14),\\n    # \\'MOMENTUM\\': lambda close: talib.MOM(close, timeperiod=10),\\n    # \\'CCI\\': lambda high, low, close: talib.CCI(high, low, close, timeperiod=14),\\n    # \\'MFI\\': lambda high, low, close, volume: talib.MFI(high, low, close, volume, timeperiod=14),\\n    # \\'ROC\\': lambda close: talib.ROC(close, timeperiod=10),\\n    # \\'ADX\\': lambda high, low, close: talib.ADX(high, low, close, timeperiod=14),\\n    # \\'PPO\\': lambda close: talib.PPO(close, fastperiod=5, slowperiod=15, matype=8)\\n\\n    for indicator, func in indicators.items():\\n        if indicator in [\\'CDLABANDONEDBABY\\', \\'CDLDARKCLOUDCOVER\\', \\'CDLEVENINGDOJISTAR\\', \\'CDLEVENINGSTAR\\', \\'CDLMATHOLD\\',\\n                         \\'CDLMORNINGDOJISTAR\\', \\'CDLMORNINGSTAR\\']:\\n            df[indicator] = func(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n        elif indicator in [\\'MOMENTUM\\',\\'PPO\\']:\\n            df[indicator] = func(df[\\'Close\\'])\\n        # elif indicator == \\'MFI\\':\\n        #     df[indicator] = func(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'])\\n        elif indicator in [\\'ADX\\']:\\n            df[indicator] = func(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n        else:\\n            df[indicator] = func(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n\\n    # Fill NaN values with 0\\n    df.fillna(0, inplace=True)\\n    return df', 'ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 68), found shape=(None, 64) for the following code:\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2023\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\"\"\"\\n\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\ndef main(train_stock, train_percent, date_field, adjcp_field, window_size, batch_size, ep_count,\\n         strategy=\"t-dqn\", model_name=\"model_debug\", debug=False):\\n    \"\"\" Trains the stock trading bot using Deep Q-Learning. \"\"\"\\n    agent = Agent(window_size, strategy=strategy, pretrained=True, model_name=model_name)\\n    data = load_data(train_stock, date_field,adjcp_field, smoothing_method=\\'savgol\\', window_size=window_size)\\n    take = int(len(data) * (train_percent / 100))\\n    train_data = data_split(data, 0, take, adjcp_field)\\n    val_data = data_split(data, take, len(data), adjcp_field)\\n\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = float(\\'-inf\\')\\n    trans_cost = 0.25\\n\\n    for episode in range(ep_count):\\n        train_result = train_model(agent, episode, train_data, ep_count=ep_count, batch_size=batch_size,\\n                                   trans_cost=trans_cost, window_size=window_size, prev_agent_profit=agent_profit,\\n                                   adjcp_field=adjcp_field)\\n        agent_profit = train_result[2]\\n\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, strategy)\\n\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n\\n        # Optional: Save model or implement early stopping here\\n\\n# def main(train_stock, train_percent, date_field, adjcp_field, window_size, batch_size, ep_count,\\n#          strategy=\"t-dqn\", model_name=\"model_debug\", debug=False):\\n#     \"\"\" Trains the stock trading bot using Deep Q-Learning. Args: [python train.py --help]\\n#     \"\"\"\\n#     agent = Agent(window_size, strategy=strategy, pretrained=True, model_name=model_name)\\n#     data = load_data(train_stock, date_field)\\n#     take = int(len(data) * (train_percent / 100))\\n#     train_data = data_split(data, 0, take, adjcp_field)\\n#     val_data = data_split(data, take, int(len(data)), adjcp_field)\\n#\\n#     initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n#     agent_profit = 0\\n#     prev_agent_profit = -1000000000\\n#     episode = 0\\n#     episode_count = ep_count\\n#     trans_cost: float = 0.25\\n#\\n#     while episode < episode_count:\\n#         train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n#                                    trans_cost=trans_cost, window_size=window_size, prev_agent_profit=agent_profit,\\n#                                    adjcp_field=adjcp_field)\\n#         agent_profit = train_result[2]\\n#\\n#         if agent_profit > 0:\\n#             val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n#             show_train_result(train_result, val_result, initial_offset, strategy)\\n#         if agent_profit > prev_agent_profit:\\n#             prev_agent_profit = agent_profit\\n#         episode += 1\\n\\n\\nif __name__ == \"__main__\":\\n    args = docopt(__doc__)\\n\\n    file_data = args[\"<file-data>\"]\\n    train_percent = args[\"<train-percent>\"]\\n\\n    date_field = args[\"--date-field\"]\\n    adjcp_field = args[\"--adjcp-field\"]\\n    window_size = int(args[\"--window-size\"])\\n    batch_size = int(args[\"--batch-size\"])\\n    ep_count = int(args[\"--episode-count\"])\\n    model_name = args[\"--model-name\"]\\n    debug = args[\"--debug\"]\\n\\n    coloredlogs.install(level=\"DEBUG\")\\n    switch_k_backend_device()\\n\\n    try:\\n        # dqn\\n        main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n             ep_count, \"dqn\", f\"{model_name}_dqn\", debug)\\n\\n        # # t-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"t-dqn\", f\"{model_name}_tdqn\", debug)\\n        #\\n        # # double-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"double-dqn\", f\"{model_name}_ddqn\", debug)\\n\\n    except KeyboardInterrupt:\\n        print(\"Aborted!\")\\n', '  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\\nKeyError: \\'MOMENTUM\\'\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom modules.indicator_utils import load_data, convert_to_float, add_indicators  # Import the new module\\n\\n\\n\\nstock_file =\\'data/dataFile.csv\\'\\ndf1 = load_data(stock_file)\\nrows_count = (len(df1) + 1)  # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[1:rows_toTrain, :].copy()\\n\\n# Check if \\'Date\\' column exists, otherwise create it\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\nfor col in df1.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df1[col] = convert_to_float(df1[col].tolist())\\n\\n# Define the list of indicators to be used\\nselected_indicators = add_indicators(df1)\\n\\n# Extract the selected indicators\\nindicators = {indicator: df1[indicator].tolist() for indicator in selected_indicators}\\nclose = df1[\\'Close\\'].tolist()\\n\\ndef step_by_step_prediction(agent, trend):\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t]\\n            print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t]\\n            print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n# Update agent initialization with the new indicators\\nwindow_size = 60\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# logging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info(\\'Training complete. Starting prediction...\\')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain + 1:, :].copy()\\n# Save DataFrame to CSV file\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\nfor col in df2.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df2[col] = convert_to_float(df2[col].tolist())\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: df2[indicator].tolist() for indicator in selected_indicators}\\nclose_test = df2[\\'Close\\'].tolist()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Save the test results\\n# joblib.dump(agentTest, \\'agent_test.pkl\\')\\n\\n# Load and test the saved model\\nloaded_model = joblib.load(\\'output/model.pkl\\')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\nplt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)', 'How do I perform the following:\\n# Define the list of indicators to be used\\nselected_indicators = add_indicators(df1)\\n \\n# Extract the selected indicators\\nindicators = {indicator: df1[indicator].tolist() for indicator in selected_indicators}\\n\\non the following api.py code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nfrom modules.indicator_utils import convert_to_float, add_indicators  # Import the new module\\n\\napp = Flask(__name__)\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    data = request.get_json()\\n    trend = data[\\'trend\\']\\n    indicators = data[\\'indicators\\']\\n    t = int(data[\\'t\\'])\\n    window_size = int(data[\\'window_size\\'])\\n\\n    state = get_state(trend, indicators, t, window_size)\\n    decision = model.predict(state)\\n    action = np.argmax(decision[0])\\n\\n    response = {\\'action\\': int(action)}\\n    return jsonify(response)\\n\\n\\nif __name__==\\'__main__\\':\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n    app.run(host=\"0.0.0.0\", debug=False, port=5020)', 'How do I define the list of indicators to be used from the data using \\nadd_indicators() and then  extracting the selected indicators and adding it to indicator list\\n\\non the following api.py code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nfrom modules.indicator_utils import convert_to_float, add_indicators  # Import the new module\\n\\napp = Flask(__name__)\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    data = request.get_json()\\n    trend = data[\\'trend\\']\\n    indicators = data[\\'indicators\\']\\n    t = int(data[\\'t\\'])\\n    window_size = int(data[\\'window_size\\'])\\n\\n    state = get_state(trend, indicators, t, window_size)\\n    decision = model.predict(state)\\n    action = np.argmax(decision[0])\\n\\n    response = {\\'action\\': int(action)}\\n    return jsonify(response)\\n\\n\\nif __name__==\\'__main__\\':\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n    app.run(host=\"0.0.0.0\", debug=False, port=5020)', 'using the data = request.get_json()', 'Optimize the following code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nfrom modules.indicator_utils import convert_to_float, add_indicators  # Import the new module\\n\\napp = Flask(__name__)\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    data = request.get_json()\\n\\n    trend = data[\\'trend\\']\\n    # indicators = data[\\'indicators\\']\\n    # Define the list of indicators to be used\\n    selected_indicators = add_indicators(data)\\n\\n    # Extract the selected indicators\\n    indicators = {indicator: data[indicator].tolist() for indicator in selected_indicators}\\n\\n    t = int(data[\\'t\\'])\\n    window_size = int(data[\\'window_size\\'])\\n\\n    state = get_state(trend, indicators, t, window_size)\\n    decision = model.predict(state)\\n    action = np.argmax(decision[0])\\n\\n    response = {\\'action\\': int(action)}\\n    return jsonify(response)\\n\\n\\nif __name__==\\'__main__\\':\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n    app.run(host=\"0.0.0.0\", debug=False, port=5020)', 'how do I separate the def add_indicators(df1): and the def convert_to_float(lst): to separate python  module in order to be called from the following code:\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport talib  # Import TA-Lib\\nimport modules.agent_function as agent_funct\\nimport modules.model_function as model_funct\\n\\n\\ndef convert_to_float(lst):\\n    return [float(i) for i in lst]\\n\\ndef add_indicators(df1):\\n    \"\"\" Adds technical indicators to the DataFrame \"\"\"\\n    indicators = {\\n        \\'MOMENTUM\\': lambda close: talib.MOM(close, timeperiod=10),\\n        \\'ADX\\': lambda high, low, close: talib.ADX(high, low, close, timeperiod=14),\\n        \\'PPO\\': lambda close: talib.PPO(close, fastperiod=5, slowperiod=15, matype=8)\\n    }\\n\\n    for indicator, func in indicators.items():\\n        if indicator == \\'MOMENTUM\\':\\n            df1[indicator] = func(df1[\\'Close\\'])\\n        elif indicator == \\'ADX\\':\\n            df1[indicator] = func(df1[\\'High\\'], df1[\\'Low\\'], df1[\\'Close\\'])\\n        elif indicator == \\'PPO\\':\\n            df1[indicator] = func(df1[\\'Close\\'])\\n\\n    # Fill NaN values with 0\\n    df1.fillna(0, inplace=True)\\n    return list(indicators.keys())\\n\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nrows_count = (len(df1) + 1)  # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[1:rows_toTrain, :].copy()\\n\\n# Check if \\'Date\\' column exists, otherwise create it\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\ndef convert_to_float(lst):\\n    return [float(i) for i in lst]\\n\\nfor col in df1.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df1[col] = convert_to_float(df1[col].tolist())\\n\\n# Define the list of indicators to be used\\nselected_indicators = add_indicators(df1)\\n\\n# Extract the selected indicators\\nindicators = {indicator: df1[indicator].tolist() for indicator in selected_indicators}\\nclose = df1[\\'Close\\'].tolist()\\n\\ndef step_by_step_prediction(agent, trend):\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t]\\n            print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t]\\n            print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n# Update agent initialization with the new indicators\\nwindow_size = 60\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# logging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info(\\'Training complete. Starting prediction...\\')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain + 1:, :].copy()\\n# Save DataFrame to CSV file\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\nfor col in df2.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df2[col] = convert_to_float(df2[col].tolist())\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: df2[indicator].tolist() for indicator in selected_indicators}\\nclose_test = df2[\\'Close\\'].tolist()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Save the test results\\n# joblib.dump(agentTest, \\'agent_test.pkl\\')\\n\\n# Load and test the saved model\\nloaded_model = joblib.load(\\'output/model.pkl\\')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\nplt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)\\n', 'how do I print the values of the return payload  on the following code:\\n\\n\\t\\tprivate object CreateJsonPayload(int _loopNo)\\n\\t\\t{\\t\\t\\n\\t\\t    var date = new List<DateTime>();          \\n\\t\\t    var open = new List<double>();\\t\\t\\t\\n\\t\\t    var high = new List<double>();\\t\\t\\t\\n\\t\\t    var low = new List<double>();\\t\\t\\t\\n\\t\\t    var close = new List<double>();\\t\\t\\t\\n\\t\\t    var volume = new List<double>();\\t\\t\\t\\n\\t\\t    var trend = new List<double>();\\n\\t\\t    var ppo = new List<double>();\\n\\t\\t    var adx = new List<double>();\\n\\t\\t    var adxr = new List<double>();\\n\\t\\t    var momentum = new List<double>();\\n\\t\\t\\n\\t\\t    for (int i = _loopNo; i >= 0; i--)\\n\\t\\t    {\\n\\t\\t        date.Add(Times[0][i]);\\n\\t\\t        open.Add(Open[i]);\\n\\t\\t        high.Add(High[i]);\\n\\t\\t        low.Add(Low[i]);\\n\\t\\t        close.Add(Close[i]);\\n\\t\\t        volume.Add(Volume[i]);\\t\\t\\t\\n\\t\\t        trend.Add(Close[i]);\\n\\t\\t        // Uncomment and implement these if needed\\n\\t\\t        // ppo.Add(PPO(_ppoFast, _ppoSlow, _ppoSmooth)[i]);\\n\\t\\t        // momentum.Add(Momentum(_momentumPeriod)[i]);\\n\\t\\t        // adx.Add(ADX(_adxPeriod)[i]);\\n\\t\\t        // adxr.Add(ADX(_adxPeriod)[i]);\\n\\t\\t    }\\n\\t\\t\\t\\n\\t\\t    var indicators = new Dictionary<string, object>\\n\\t\\t    {\\n\\t\\t        { \"Date\", date.Select(d => d.ToString(\"yyyy-MM-dd HH:mm:ss\")).ToList() }, // Convert to desired string format with date and time\\n\\t\\t        { \"Open\", open },\\n\\t\\t        { \"High\", high },\\n\\t\\t        { \"Low\", low },\\n\\t\\t        { \"Close\", close },\\n\\t\\t        { \"Volume\", volume }\\n\\t\\t    };\\t\\t\\t\\n\\t\\t\\n\\t\\t    var payload = new\\n\\t\\t    {\\n\\t\\t        t = 29,\\n\\t\\t        trend = trend,\\n\\t\\t        indicators = indicators,\\n\\t\\t        window_size = 60\\n\\t\\t    };\\n\\t\\t\\t\\n\\t\\t    Print(\"Payload \" + payload);\\n\\t\\t\\n\\t\\t    return payload;\\n\\t\\t}\\n', 'how do I print the values of the return payload on the following code:\\n\\n\\tprivate object CreateJsonPayload(int _loopNo)\\n\\t{\\t\\t\\n\\t    var date = new List<DateTime>();          \\n\\t    var open = new List<double>();\\t\\t\\t\\n\\t    var high = new List<double>();\\t\\t\\t\\n\\t    var low = new List<double>();\\t\\t\\t\\n\\t    var close = new List<double>();\\t\\t\\t\\n\\t    var volume = new List<double>();\\t\\t\\t\\n\\t    var trend = new List<double>();\\n\\t    var ppo = new List<double>();\\n\\t    var adx = new List<double>();\\n\\t    var adxr = new List<double>();\\n\\t    var momentum = new List<double>();\\n\\t\\n\\t    for (int i = _loopNo; i >= 0; i--)\\n\\t    {\\n\\t        date.Add(Times[0][i]);\\n\\t        open.Add(Open[i]);\\n\\t        high.Add(High[i]);\\n\\t        low.Add(Low[i]);\\n\\t        close.Add(Close[i]);\\n\\t        volume.Add(Volume[i]);\\t\\t\\t\\n\\t        trend.Add(Close[i]);\\n\\t        // Uncomment and implement these if needed\\n\\t        // ppo.Add(PPO(_ppoFast, _ppoSlow, _ppoSmooth)[i]);\\n\\t        // momentum.Add(Momentum(_momentumPeriod)[i]);\\n\\t        // adx.Add(ADX(_adxPeriod)[i]);\\n\\t        // adxr.Add(ADX(_adxPeriod)[i]);\\n\\t    }\\n\\t\\t\\n\\t    var indicators = new Dictionary<string, object>\\n\\t    {\\n\\t        { \"Date\", date.Select(d => d.ToString(\"yyyy-MM-dd HH:mm:ss\")).ToList() }, // Convert to desired string format with date and time\\n\\t        { \"Open\", open },\\n\\t        { \"High\", high },\\n\\t        { \"Low\", low },\\n\\t        { \"Close\", close },\\n\\t        { \"Volume\", volume }\\n\\t    };\\t\\t\\t\\n\\t\\n\\t    var payload = new\\n\\t    {\\n\\t        t = 29,\\n\\t        trend = trend,\\n\\t        indicators = indicators,\\n\\t        window_size = 60\\n\\t    };\\n\\t\\n\\t    return payload;\\n\\t}', '\\'Formatting\\' is an ambiguous reference between \\'Newtonsoft.Json.Formatting\\' and \\'System.Xml.Formatting\\'\\n\\nprivate object CreateJsonPayload(int _loopNo)\\n{\\t\\t\\n    var date = new List<DateTime>();          \\n    var open = new List<double>();\\t\\t\\t\\n    var high = new List<double>();\\t\\t\\t\\n    var low = new List<double>();\\t\\t\\t\\n    var close = new List<double>();\\t\\t\\t\\n    var volume = new List<double>();\\t\\t\\t\\n    var trend = new List<double>();\\n    var ppo = new List<double>();\\n    var adx = new List<double>();\\n    var adxr = new List<double>();\\n    var momentum = new List<double>();\\n\\n    for (int i = _loopNo; i >= 0; i--)\\n    {\\n        date.Add(Times[0][i]);\\n        open.Add(Open[i]);\\n        high.Add(High[i]);\\n        low.Add(Low[i]);\\n        close.Add(Close[i]);\\n        volume.Add(Volume[i]);\\t\\t\\t\\n        trend.Add(Close[i]);\\n        // Uncomment and implement these if needed\\n        // ppo.Add(PPO(_ppoFast, _ppoSlow, _ppoSmooth)[i]);\\n        // momentum.Add(Momentum(_momentumPeriod)[i]);\\n        // adx.Add(ADX(_adxPeriod)[i]);\\n        // adxr.Add(ADX(_adxPeriod)[i]);\\n    }\\n\\t\\n    var indicators = new Dictionary<string, object>\\n    {\\n        { \"Date\", date.Select(d => d.ToString(\"yyyy-MM-dd HH:mm:ss\")).ToList() }, // Convert to desired string format with date and time\\n        { \"Open\", open },\\n        { \"High\", high },\\n        { \"Low\", low },\\n        { \"Close\", close },\\n        { \"Volume\", volume }\\n    };\\t\\t\\t\\n\\n    var payload = new\\n    {\\n        t = 29,\\n        trend = trend,\\n        indicators = indicators,\\n        window_size = 60\\n    };\\n\\t\\n    // Print the payload in a readable JSON format\\n    string jsonPayload = JsonConvert.SerializeObject(payload, Formatting.Indented);\\n    Console.WriteLine(\"Payload: \" + jsonPayload); // Use Console.WriteLine for printing\\n\\n    return payload;\\n}', 'ValueError: Could not convert string to float\" \\'Date\\'\\n\\n\\t\\tprivate object CreateJsonPayload(int _loopNo)\\n\\t\\t{\\t\\t\\n\\t\\t    var date = new List<DateTime>();          \\n\\t\\t    var open = new List<double>();\\t\\t\\t\\n\\t\\t    var high = new List<double>();\\t\\t\\t\\n\\t\\t    var low = new List<double>();\\t\\t\\t\\n\\t\\t    var close = new List<double>();\\t\\t\\t\\n\\t\\t    var volume = new List<double>();\\t\\t\\t\\n\\t\\t    var trend = new List<double>();\\n\\t\\t    var ppo = new List<double>();\\n\\t\\t    var adx = new List<double>();\\n\\t\\t    var adxr = new List<double>();\\n\\t\\t    var momentum = new List<double>();\\n\\t\\t\\n\\t\\t    for (int i = _loopNo; i >= 0; i--)\\n\\t\\t    {\\n\\t\\t        date.Add(Times[0][i]);\\n\\t\\t        open.Add(Open[i]);\\n\\t\\t        high.Add(High[i]);\\n\\t\\t        low.Add(Low[i]);\\n\\t\\t        close.Add(Close[i]);\\n\\t\\t        volume.Add(Volume[i]);\\t\\t\\t\\n\\t\\t        trend.Add(Close[i]);\\n\\t\\t        // Uncomment and implement these if needed\\n\\t\\t        // ppo.Add(PPO(_ppoFast, _ppoSlow, _ppoSmooth)[i]);\\n\\t\\t        // momentum.Add(Momentum(_momentumPeriod)[i]);\\n\\t\\t        // adx.Add(ADX(_adxPeriod)[i]);\\n\\t\\t        // adxr.Add(ADX(_adxPeriod)[i]);\\n\\t\\t    }\\n\\t\\t\\t\\n\\t\\t    var indicators = new Dictionary<string, object>\\n\\t\\t    {\\n\\t\\t        { \"Date\", date }, // Convert to desired string format with date and time\\n\\t\\t        { \"Open\", open },\\n\\t\\t        { \"High\", high },\\n\\t\\t        { \"Low\", low },\\n\\t\\t        { \"Close\", close },\\n\\t\\t        { \"Volume\", volume }\\n\\t\\t    };\\t\\t\\t\\n\\n//\\t\\t    // Print the payload values\\n//\\t\\t    Print(\"Payload Values:\");\\n//\\t\\t    foreach (var indicator in indicators)\\n//\\t\\t    {\\n//\\t\\t        Print($\"{indicator.Key}: {string.Join(\", \", indicator.Value)}\");\\n//\\t\\t    }\\t\\t\\n\\t\\t\\t\\n\\t\\t    var payload = new\\n\\t\\t    {\\n                t = 29,\\n                trend = trend,\\n                indicators = indicators,\\n                window_size = 30\\n\\t\\t    };\\n\\t\\t\\t\\n//\\t\\t    // Print the payload in a readable JSON format\\n//\\t\\t    string jsonPayload = JsonConvert.SerializeObject(payload, Formatting.Indented);\\n//\\t\\t    Print(\"Payload: \" + jsonPayload);\\n\\t\\t\\t\\n\\t\\t\\t// Print the payload in a readable JSON format\\n\\t\\t    string jsonPayload = JsonConvert.SerializeObject(payload, Newtonsoft.Json.Formatting.Indented); // Specify the full namespace\\n\\t\\t    Print(\"Payload: \" + jsonPayload); // Use Console.WriteLine for printing\\n\\n\\t\\t\\n\\t\\t    return payload;\\n\\t\\t}', 'Check the following\\n\\n\\t\\tprivate object CreateJsonPayload(int _loopNo)\\n\\t\\t{\\t\\t\\n\\t\\t\\tvar date = new List<DateTime>(); \\n\\t\\t    var open = new List<double>();\\t\\t\\t\\n\\t\\t    var high = new List<double>();\\t\\t\\t\\n\\t\\t    var low = new List<double>();\\t\\t\\t\\n\\t\\t    var close = new List<double>();\\t\\t\\t\\n\\t\\t    var volume = new List<double>();\\t\\t\\t\\n\\t\\t    var trend = new List<double>();\\n\\t\\t\\n\\t\\t    for (int i = _loopNo; i >= 0; i--)\\n\\t\\t    {\\n\\t\\t\\t\\tdate.Add(Times[0][i]);\\n\\t\\t        open.Add(Open[i]);\\n\\t\\t        high.Add(High[i]);\\n\\t\\t        low.Add(Low[i]);\\n\\t\\t        close.Add(Close[i]);\\n\\t\\t        volume.Add(Volume[i]);\\t\\t\\t\\n\\t\\t        trend.Add(Close[i]);\\n\\t\\t    }\\n\\t\\t\\t\\n\\t\\t    var indicators = new Dictionary<string, object>\\n\\t\\t    {\\n \\t\\t\\t\\t{ \"Date\", date.Select(d => d.ToString(\"yyyy-MM-dd HH:mm:ss\")).ToList() }, // Convert DateTime to string\\n\\t\\t        { \"Open\", open },\\n\\t\\t        { \"High\", high },\\n\\t\\t        { \"Low\", low },\\n\\t\\t        { \"Close\", close },\\n\\t\\t        { \"Volume\", volume }\\n\\t\\t    };\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t    var payload = new\\n\\t\\t    {\\n                t = 29,\\n                trend = trend,\\n                indicators = indicators,\\n                window_size = 30\\n\\t\\t    };\\n\\t\\t\\t\\n\\t\\t    // Print the payload values\\n\\t\\t    Print(\"Payload Values:\");\\n\\t\\t    foreach (var indicator in payload)\\n\\t\\t    {\\n\\t\\t        foreach (var kvp in indicator)\\n\\t\\t        {\\n\\t\\t            if (kvp.Key == \"Date\")\\n\\t\\t            {\\n\\t\\t                // Handle Date separately as it is a List<string>\\n\\t\\t                Print($\"{kvp.Key}: {string.Join(\", \", (List<string>)kvp.Value)}\");\\n\\t\\t            }\\n\\t\\t            else\\n\\t\\t            {\\n\\t\\t                // Handle other entries as List<double>\\n\\t\\t                Print($\"{kvp.Key}: {string.Join(\", \", (List<double>)kvp.Value)}\");\\n\\t\\t            }\\n\\t\\t        }\\n\\t\\t    }\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t// Print the payload in a readable JSON format\\n\\t\\t    string jsonPayload = JsonConvert.SerializeObject(payload, Newtonsoft.Json.Formatting.Indented); // Specify the full namespace\\n\\t\\t    Print(\"Payload: \" + jsonPayload); // Use Console.WriteLine for printing\\n\\n\\t\\t    return payload;\\n\\t\\t}', 'Argument 1: cannot convert from \\'System.DateTime\\' to \\'string\\' on the following code:     \\n\\n       private object CreateJsonPayload(int _loopNo)\\n        {\\t\\t\\n            var date = new List<DateTime>();          \\n\\t\\t\\tvar open = new List<double>();\\t\\t\\t\\n\\t\\t\\tvar high = new List<double>();\\t\\t\\t\\n\\t\\t\\tvar low = new List<double>();\\t\\t\\t\\n\\t\\t\\tvar close = new List<double>();\\t\\t\\t\\n\\t\\t\\tvar volume = new List<double>();\\t\\t\\t\\n            var trend = new List<double>();\\n            var ppo = new List<double>();\\n            var adx = new List<double>();\\n            var adxr = new List<double>();\\n\\t\\t\\tvar momentum = new List<double>();\\n\\n            for (int i = _loopNo; i >= 0; i--)\\n            {\\n\\t\\t\\t\\tdate.Add(Times[0][i]);\\n\\t\\t\\t\\topen.Add(Open[i]);\\n\\t\\t\\t\\thigh.Add(High[i]);\\n\\t\\t\\t\\tlow.Add(Low[i]);\\n\\t\\t\\t\\tclose.Add(Close[i]);\\n\\t\\t\\t\\tvolume.Add(Volume[i]);\\t\\t\\t\\n\\t\\t\\t\\ttrend.Add(Close[i]);\\n//                ppo.Add(PPO(_ppoFast, _ppoSlow, _ppoSmooth)[i]);\\n//\\t\\t\\t\\t  momentum.Add(Momentum(_momentumPeriod)[i]);\\n//\\t\\t\\t\\t  adx.Add(ADX(_adxPeriod)[i]);\\n//\\t\\t\\t\\t  adx.Add(ADX(_adxPeriod)[i]);\\n            }\\n\\t\\t\\t\\n//            var indicators = new Dictionary<string, List<double>>\\n//            {\\t\\t\\t\\t\\n//                { \"Date\", date }, // Convert to desired string format with date and time\\n//                { \"Opem\", open },\\n//                { \"High\", high },\\n//                { \"Low\", low },\\n//                { \"Close\", close },\\n//                { \"Volume\", volume }\\n\\t\\t\\t\\n//            };\\n\\t\\t    var indicators = new List<Dictionary<string, object>>\\n\\t\\t    {\\n\\t\\t        new Dictionary<string, object>\\n\\t\\t        {\\n\\t\\t            { \"Date\", date.Select(d => DateTime.Parse(d).ToString(\"yyyy-MM-dd HH:mm:ss\")).ToList() }, // Convert to desired string format with date and time\\n\\t\\t            { \"Open\", open },\\n\\t\\t            { \"High\", high },\\n\\t\\t            { \"Low\", low },\\n\\t\\t            { \"Close\", close },\\n\\t\\t            { \"Volume\", volume }\\n\\t\\t        }\\n\\t\\t    };\\n\\t\\t\\tvar payload = new\\n            {\\n                t = 29,\\n                trend = trend,\\n                indicators = indicators,\\n                window_size = 60\\n            };\\n\\t\\t\\t\\n\\t\\t\\tPrint(\"Payload \" + payload);\\n\\n            return payload;\\n        }\\t\\t', 'Argument 1: cannot convert from \\'System.DateTime\\' to \\'System.Collection.GenericList,double>\\' on the following code:\\n\\n\\t\\tprivate object CreateJsonPayload(int _loopNo)\\n\\t\\t{\\t\\t\\n\\t\\t    var date = new List<DateTime>();          \\n\\t\\t    var open = new List<double>();\\t\\t\\t\\n\\t\\t    var high = new List<double>();\\t\\t\\t\\n\\t\\t    var low = new List<double>();\\t\\t\\t\\n\\t\\t    var close = new List<double>();\\t\\t\\t\\n\\t\\t    var volume = new List<double>();\\t\\t\\t\\n\\t\\t    var trend = new List<double>();\\n\\t\\t    var ppo = new List<double>();\\n\\t\\t    var adx = new List<double>();\\n\\t\\t    var adxr = new List<double>();\\n\\t\\t    var momentum = new List<double>();\\n\\t\\t\\n\\t\\t    for (int i = _loopNo; i >= 0; i--)\\n\\t\\t    {\\n\\t\\t        date.Add(Times[0][i]);\\n\\t\\t        open.Add(Open[i]);\\n\\t\\t        high.Add(High[i]);\\n\\t\\t        low.Add(Low[i]);\\n\\t\\t        close.Add(Close[i]);\\n\\t\\t        volume.Add(Volume[i]);\\t\\t\\t\\n\\t\\t        trend.Add(Close[i]);\\n\\t\\t        // Uncomment and implement these if needed\\n\\t\\t        // ppo.Add(PPO(_ppoFast, _ppoSlow, _ppoSmooth)[i]);\\n\\t\\t        // momentum.Add(Momentum(_momentumPeriod)[i]);\\n\\t\\t        // adx.Add(ADX(_adxPeriod)[i]);\\n\\t\\t        // adxr.Add(ADX(_adxPeriod)[i]);\\n\\t\\t    }\\n\\t\\t\\t\\n//\\t\\t    var indicators = new List<Dictionary<string, object>>\\n//\\t\\t    {\\n//\\t\\t        new Dictionary<string, object>\\n//\\t\\t        {\\n//\\t\\t            { \"Date\", date.Select(d => d.ToString(\"yyyy-MM-dd HH:mm:ss\")).ToList() }, // Convert to desired string format with date and time\\n//\\t\\t            { \"Open\", open },\\n//\\t\\t            { \"High\", high },\\n//\\t\\t            { \"Low\", low },\\n//\\t\\t            { \"Close\", close },\\n//\\t\\t            { \"Volume\", volume }\\n//\\t\\t        }\\n//\\t\\t    };\\n\\t\\t\\t\\n            var indicators = new Dictionary<string, List<double>>\\n\\t\\t    {\\n\\t            { \"Date\", date.Select(d => d.ToString(\"yyyy-MM-dd HH:mm:ss\")).ToList() }, // Convert to desired string format with date and time\\n\\t            { \"Open\", open },\\n\\t            { \"High\", high },\\n\\t            { \"Low\", low },\\n\\t            { \"Close\", close },\\n\\t            { \"Volume\", volume }\\n            };\\t\\t\\t\\n\\t\\t\\n\\t\\t    var payload = new\\n\\t\\t    {\\n\\t\\t        t = 29,\\n\\t\\t        trend = trend,\\n\\t\\t        indicators = indicators,\\n\\t\\t        window_size = 60\\n\\t\\t    };\\n\\t\\t\\t\\n\\t\\t    Print(\"Payload \" + payload);\\n\\t\\t\\n\\t\\t    return payload;\\n\\t\\t}\\t\\n', 'ValueError: shapes (1,33) and (63,500) not aligned: 33 (dim 1) != 63 (dim 0) \\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\n\\napp = Flask(__name__)\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    data = request.get_json()\\n    trend = data[\\'trend\\']\\n    indicators = data[\\'indicators\\']\\n    t = int(data[\\'t\\'])\\n    window_size = int(data[\\'window_size\\'])\\n\\n    state = get_state(trend, indicators, t, window_size)\\n    decision = model.predict(state)\\n    action = np.argmax(decision[0])\\n\\n    response = {\\'action\\': int(action)}\\n    return jsonify(response)\\n\\n\\nif __name__==\\'__main__\\':\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n    app.run(host=\"0.0.0.0\", debug=False, port=5020)', 'How do I incorporate the Savitzky-Golay filter to the indicators on the following code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom modules.indicator_utils import convert_to_float  # Import the new module\\nfrom sklearn.preprocessing import StandardScaler\\n\\nsns.set()\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQESmoothed\\', \\'QQEFast\\', \\'QQESlow\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n]\\n\\n# Load and preprocess data\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nrows_count = (len(df1) + 1)  # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[1:rows_toTrain, :].copy()\\n\\n# Check if \\'Date\\' column exists, otherwise create it\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\nfor col in df1.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df1[col] = convert_to_float(df1[col].tolist())\\n\\n# Define the list of indicators to be used\\n# selected_indicators = add_indicators(df1)\\n\\n# Extract the selected indicators\\n# indicators = {indicator: df1[indicator].tolist() for indicator in selected_indicators}\\nindicators = {indicator: np.array(df1[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\n# close = df1[\\'Close\\'].tolist()\\nclose = np.array(df1[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Verify that the indicators are added to the DataFrame\\nprint(\"Columns in DataFrame after adding indicators:\", df1.columns)\\n\\n# Apply scaling to the indicators\\nscalers = {}\\nfor indicator in selected_indicators:\\n    scaler = StandardScaler()\\n    indicators[indicator] = scaler.fit_transform(indicators[indicator])\\n    scalers[indicator] = scaler\\n\\n# Save the scalers for future use\\njoblib.dump(scalers, \\'scalers.pkl\\')\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\nindicators = {k: v.flatten() for k, v in indicators.items()}\\nclose = close.flatten()\\n\\n# Verify that the indicators are added to the DataFrame\\n# print(\"Columns in DataFrame after adding indicators:\", df1.columns)\\n\\ndef step_by_step_prediction(agent, trend):\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t]\\n            print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t]\\n            print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n# Update agent initialization with the new indicators\\nwindow_size = 60\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# logging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info(\\'Training complete. Starting prediction...\\')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain + 1:, :].copy()\\n# Save DataFrame to CSV file\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\nfor col in df2.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df2[col] = convert_to_float(df2[col].tolist())\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: np.array(df2[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose_test = np.array(df2[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Load the scalers and apply to test data\\nscalers = joblib.load(\\'scalers.pkl\\')\\nfor indicator in selected_indicators:\\n    test_indicators[indicator] = scalers[indicator].transform(test_indicators[indicator])\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\ntest_indicators = {k: v.flatten() for k, v in test_indicators.items()}\\nclose_test = close_test.flatten()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Save the test results\\n# joblib.dump(agentTest, \\'agent_test.pkl\\')\\n\\n# Load and test the saved model\\nloaded_model = joblib.load(\\'output/model.pkl\\')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\nplt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)\\n', \"ValueError: If mode is 'interp', window_length must be less than or equal to the size of x.\", 'raise ValueError(\"If mode is \\'interp\\', window_length must be less \"\\nValueError: If mode is \\'interp\\', window_length must be less than or equal to the size of x.\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom modules.indicator_utils import convert_to_float  # Import the new module\\nfrom sklearn.preprocessing import StandardScaler\\nfrom scipy.signal import savgol_filter  # Import the Savitzky-Golay filter\\n\\nsns.set()\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQESmoothed\\', \\'QQEFast\\', \\'QQESlow\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n]\\n\\n# Load and preprocess data\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nrows_count = (len(df1) + 1)  # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[1:rows_toTrain, :].copy()\\n\\n# Check if \\'Date\\' column exists, otherwise create it\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\nfor col in df1.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df1[col] = convert_to_float(df1[col].tolist())\\n\\n# Extract the selected indicators\\nindicators = {indicator: np.array(df1[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose = np.array(df1[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Apply Savitzky-Golay filter to each indicator\\nwindow_length = 5  # Adjust this value as needed (must be odd)\\npolyorder = 2      # Adjust this value as needed\\n\\nfor indicator in selected_indicators:\\n    indicators[indicator] = savgol_filter(indicators[indicator], window_length, polyorder)\\n\\n# Verify that the indicators are added to the DataFrame\\nprint(\"Columns in DataFrame after adding indicators:\", df1.columns)\\n\\n# Apply scaling to the indicators\\nscalers = {}\\nfor indicator in selected_indicators:\\n    scaler = StandardScaler()\\n    indicators[indicator] = scaler.fit_transform(indicators[indicator])\\n    scalers[indicator] = scaler\\n\\n# Save the scalers for future use\\njoblib.dump(scalers, \\'scalers.pkl\\')\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\nindicators = {k: v.flatten() for k, v in indicators.items()}\\nclose = close.flatten()\\n\\n# The rest of your code remains unchanged...\\n\\ndef step_by_step_prediction(agent, trend):\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t]\\n            print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t]\\n            print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n# Update agent initialization with the new indicators\\nwindow_size = 60\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# logging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info(\\'Training complete. Starting prediction...\\')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain + 1:, :].copy()\\n# Save DataFrame to CSV file\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\nfor col in df2.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df2[col] = convert_to_float(df2[col].tolist())\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: np.array(df2[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose_test = np.array(df2[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Load the scalers and apply to test data\\nscalers = joblib.load(\\'scalers.pkl\\')\\nfor indicator in selected_indicators:\\n    test_indicators[indicator] = scalers[indicator].transform(test_indicators[indicator])\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\ntest_indicators = {k: v.flatten() for k, v in test_indicators.items()}\\nclose_test = close_test.flatten()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Save the test results\\n# joblib.dump(agentTest, \\'agent_test.pkl\\')\\n\\n# Load and test the saved model\\nloaded_model = joblib.load(\\'output/model.pkl\\')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\nplt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)', 'ValueError: Expected 2D array, got 1D array instead:\\narray=[ 3.43514675  3.3364697   3.23724617 ... -4.36661947 -4.47681188\\n -4.58816764].\\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom modules.indicator_utils import convert_to_float  # Import the new module\\nfrom sklearn.preprocessing import StandardScaler\\nfrom scipy.signal import savgol_filter  # Import the Savitzky-Golay filter\\n\\nsns.set()\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQESmoothed\\', \\'QQEFast\\', \\'QQESlow\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n]\\n\\n# Load and preprocess data\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nrows_count = (len(df1) + 1)  # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[1:rows_toTrain, :].copy()\\n\\n# Check if \\'Date\\' column exists, otherwise create it\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\nfor col in df1.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df1[col] = convert_to_float(df1[col].tolist())\\n\\n# Extract the selected indicators\\nindicators = {indicator: np.array(df1[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose = np.array(df1[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Apply Savitzky-Golay filter to each indicator\\npolyorder = 2  # Adjust this value as needed\\nwindow_size = 60\\n\\nfor indicator in selected_indicators:\\n    # Get the current indicator data\\n    indicator_data = indicators[indicator].flatten()  # Flatten to 1D for the filter\\n\\n    # Check if the length of the indicator data is less than the desired window length\\n    if len(indicator_data) < window_size:\\n        print(f\"Warning: The length of {indicator} is less than the window length. Skipping filter.\")\\n        continue  # Skip filtering for this indicator\\n\\n    # Apply the Savitzky-Golay filter\\n    indicators[indicator] = savgol_filter(indicator_data, window_size, polyorder)\\n\\n# Verify that the indicators are added to the DataFrame\\nprint(\"Columns in DataFrame after adding indicators:\", df1.columns)\\n\\n\\n# Apply scaling to the indicators\\nscalers = {}\\nfor indicator in selected_indicators:\\n    scaler = StandardScaler()\\n    indicators[indicator] = scaler.fit_transform(indicators[indicator])\\n    scalers[indicator] = scaler\\n\\n# Save the scalers for future use\\njoblib.dump(scalers, \\'scalers.pkl\\')\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\nindicators = {k: v.flatten() for k, v in indicators.items()}\\nclose = close.flatten()\\n\\n# The rest of your code remains unchanged...\\n\\ndef step_by_step_prediction(agent, trend):\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t]\\n            print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t]\\n            print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n# Update agent initialization with the new indicators\\nwindow_size = 60\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# logging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info(\\'Training complete. Starting prediction...\\')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain + 1:, :].copy()\\n# Save DataFrame to CSV file\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\nfor col in df2.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df2[col] = convert_to_float(df2[col].tolist())\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: np.array(df2[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose_test = np.array(df2[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Load the scalers and apply to test data\\nscalers = joblib.load(\\'scalers.pkl\\')\\nfor indicator in selected_indicators:\\n    test_indicators[indicator] = scalers[indicator].transform(test_indicators[indicator])\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\ntest_indicators = {k: v.flatten() for k, v in test_indicators.items()}\\nclose_test = close_test.flatten()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Save the test results\\n# joblib.dump(agentTest, \\'agent_test.pkl\\')\\n\\n# Load and test the saved model\\nloaded_model = joblib.load(\\'output/model.pkl\\')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\nplt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)', 'How do I incorporate the Kalman filters to the indicators on the following code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom modules.indicator_utils import convert_to_float  # Import the new module\\nfrom sklearn.preprocessing import StandardScaler\\n\\nsns.set()\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQESmoothed\\', \\'QQEFast\\', \\'QQESlow\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n]\\n\\n# Load and preprocess data\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nrows_count = (len(df1) + 1)  # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[1:rows_toTrain, :].copy()\\n\\n# Check if \\'Date\\' column exists, otherwise create it\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\nfor col in df1.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df1[col] = convert_to_float(df1[col].tolist())\\n\\n# Define the list of indicators to be used\\n# selected_indicators = add_indicators(df1)\\n\\n# Extract the selected indicators\\n# indicators = {indicator: df1[indicator].tolist() for indicator in selected_indicators}\\nindicators = {indicator: np.array(df1[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\n# close = df1[\\'Close\\'].tolist()\\nclose = np.array(df1[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Verify that the indicators are added to the DataFrame\\nprint(\"Columns in DataFrame after adding indicators:\", df1.columns)\\n\\n# Apply scaling to the indicators\\nscalers = {}\\nfor indicator in selected_indicators:\\n    scaler = StandardScaler()\\n    indicators[indicator] = scaler.fit_transform(indicators[indicator])\\n    scalers[indicator] = scaler\\n\\n# Save the scalers for future use\\njoblib.dump(scalers, \\'scalers.pkl\\')\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\nindicators = {k: v.flatten() for k, v in indicators.items()}\\nclose = close.flatten()\\n\\n# Verify that the indicators are added to the DataFrame\\n# print(\"Columns in DataFrame after adding indicators:\", df1.columns)\\n\\ndef step_by_step_prediction(agent, trend):\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t]\\n            print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t]\\n            print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n# Update agent initialization with the new indicators\\nwindow_size = 60\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# logging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info(\\'Training complete. Starting prediction...\\')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain + 1:, :].copy()\\n# Save DataFrame to CSV file\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\nfor col in df2.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df2[col] = convert_to_float(df2[col].tolist())\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: np.array(df2[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose_test = np.array(df2[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Load the scalers and apply to test data\\nscalers = joblib.load(\\'scalers.pkl\\')\\nfor indicator in selected_indicators:\\n    test_indicators[indicator] = scalers[indicator].transform(test_indicators[indicator])\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\ntest_indicators = {k: v.flatten() for k, v in test_indicators.items()}\\nclose_test = close_test.flatten()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Save the test results\\n# joblib.dump(agentTest, \\'agent_test.pkl\\')\\n\\n# Load and test the saved model\\nloaded_model = joblib.load(\\'output/model.pkl\\')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\nplt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)\\n', \"Can you check the following code for any errors:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\nfrom pykalman import KalmanFilter  # Import the Kalman filter\\n\\nsns.set()\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    'adx', 'momentum', 'ppoValue', 'ppoSmoothed', 'QQEDiff1', 'QQEDiff2', 'QQEDiff3'\\n]\\n\\n# Load and preprocess data\\ndf1 = pd.read_csv('data/dataFile.csv')\\nrows_count = (len(df1) + 1)  # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[1:rows_toTrain, :].copy()\\n\\n# Check if 'Date' column exists, otherwise create it\\nif 'Date' not in df1.columns:\\n    df1['Date'] = pd.date_range(start='1/1/2000', periods=len(df1), freq='D')\\n\\n\\n# Convert columns to float, excluding the 'Date' column\\ndef convert_to_float(lst):\\n    return [float(i) for i in lst]\\n\\n\\nfor col in df1.columns:\\n    if col != 'Date':  # Skip conversion for 'Date' column\\n        df1[col] = convert_to_float(df1[col].tolist())\\n\\n# Extract the selected indicators\\nindicators = {indicator: np.array(df1[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose = np.array(df1['Close'].tolist()).reshape(-1, 1)\\n\\n# Apply scaling to the indicators\\nscalers = {}\\nfor indicator in selected_indicators:\\n    scaler = StandardScaler()\\n    indicators[indicator] = scaler.fit_transform(indicators[indicator])\\n    scalers[indicator] = scaler\\n\\n# Save the scalers for future use\\njoblib.dump(scalers, 'output/scalers.pkl')\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\nindicators = {k: v.flatten() for k, v in indicators.items()}\\nclose = close.flatten()\\n\\ndef step_by_step_prediction(agent, trend):\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t]\\n            print('bar %d: buy 1 contract at price %f, total balance %f' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t]\\n            print('bar %d: sell 1 contract at price %f, total balance %f' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n# Update agent initialization with the new indicators\\nwindow_size = 30\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# logging.info('Starting training...')\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info('Training complete. Starting prediction...')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, 'output/model.pkl')\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color='black', alpha=0.5, lw=1.)\\nplt.plot(close, '.', markersize=10, color='green', alpha=0.5, label='buying signal', markevery=states_buy)\\nplt.plot(close, '.', markersize=10, color='red', alpha=0.5, label='selling signal', markevery=states_sell)\\nplt.title(f'total gains {total_gains}, total investment {invest}%')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv('data/dataFile.csv')\\ndf2 = df2.iloc[rows_toTrain + 1:, :].copy()\\n# Save DataFrame to CSV file\\ndf2.to_csv('data/ApiTest.csv', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load('output/model.pkl')\\n\\nfor col in df2.columns:\\n    if col != 'Date':  # Skip conversion for 'Date' column\\n        df2[col] = convert_to_float(df2[col].tolist())\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: np.array(df2[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose_test = np.array(df2['Close'].tolist()).reshape(-1, 1)\\n\\n# Load the scalers and apply to test data\\nscalers = joblib.load('output/scalers.pkl')\\nfor indicator in selected_indicators:\\n    test_indicators[indicator] = scalers[indicator].transform(test_indicators[indicator])\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\ntest_indicators = {k: v.flatten() for k, v in test_indicators.items()}\\nclose_test = close_test.flatten()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest['Close'] = close_test\\ntest['Signal'] = 0\\ntest.loc[test.index.intersection(states_buyT), 'Signal'] = 1\\ntest.loc[test.index.intersection(states_sellT), 'Signal'] = -1\\ntest['Market_Returns'] = test['Close'].pct_change()\\ntest['Strategy_Returns'] = test['Market_Returns'] * test['Signal'].shift(1)\\ntest['Cumulative_Profit'] = (test['Strategy_Returns'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test['Cumulative_Profit'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color='black', alpha=0.5, lw=1.)\\nplt.plot(close_test, '.', markersize=10, color='green', alpha=0.5, label='buying signal', markevery=states_buyT)\\nplt.plot(close_test, '.', markersize=10, color='red', alpha=0.5, label='selling signal', markevery=states_sellT)\\nplt.title(f'total gains {total_gainsT}, ROI {roiT}%')\\nplt.legend()\\nplt.show()\\n\\n# Save the test results\\n# joblib.dump(agentTest, 'agent_test.pkl')\\n\\n# Load and test the saved model\\nloaded_model = joblib.load('output/model.pkl')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color='black', alpha=0.5, lw=1.)\\nplt.plot(close, '.', markersize=10, color='green', alpha=0.5, label='buying signal', markevery=loaded_states_buy)\\nplt.plot(close, '.', markersize=10, color='red', alpha=0.5, label='selling signal', markevery=loaded_states_sell)\\nplt.title(f'total gains {loaded_total_gains}, total investment {loaded_invest}%')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)\", 'How can i add relational inductive biases to the \"def buy_tick(self)\" and the \"def get_tick_reward(self, weights):\" on the following code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.01\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            return np.argmax(decision[0])\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                if action == 1 and (position == 0 or position == -1):\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5)\\n                        earned_money += -5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n\\n                elif action == 2 and (position == 0 or position == 1):\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5)\\n                        earned_money += -5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return ((earned_money) / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            self.es.train(iterations, print_every=checkpoint)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                if action == 1 and (position == 0 or position == -1):\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\'% (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\'% (t, self.trend[t], earned_money))\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n', 'What else do you recommend', 'File \"C:\\\\PythonApps\\\\drl-trainer\\\\venv\\\\Lib\\\\site-packages\\\\pandas\\\\core\\\\common.py\", line 573, in require_length_match\\n    raise ValueError(\\nValueError: Length of values (1598) does not match length of index (2952)', 'File \"C:\\\\PythonApps\\\\drl-trainer\\\\venv\\\\Lib\\\\site-packages\\\\pandas\\\\core\\\\common.py\", line 573, in require_length_match\\n    raise ValueError(\\nValueError: Length of values (1598) does not match length of index (2952)\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\nfrom pykalman import KalmanFilter  # Import the Kalman filter\\n\\nsns.set()\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n]\\n\\n# Load and preprocess data\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nrows_count = len(df1)  # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[0:rows_toTrain, :].copy()  # Start from 0\\n\\n# Check if \\'Date\\' column exists, otherwise create it\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\ndef convert_to_float(lst):\\n    return [float(i) for i in lst if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit()]\\n\\nfor col in df1.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df1[col] = convert_to_float(df1[col].tolist())\\n\\n# Extract the selected indicators\\nindicators = {indicator: np.array(df1[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose = np.array(df1[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Apply scaling to the indicators\\nscalers = {}\\nfor indicator in selected_indicators:\\n    scaler = StandardScaler()\\n    indicators[indicator] = scaler.fit_transform(indicators[indicator])\\n    scalers[indicator] = scaler\\n\\n# Save the scalers for future use\\njoblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\nindicators = {k: v.flatten() for k, v in indicators.items()}\\nclose = close.flatten()\\n\\n# Define the step-by-step prediction function\\ndef step_by_step_prediction(agent, trend):\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t]\\n            print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t]\\n            print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n# Update agent initialization with the new indicators\\nwindow_size = 30\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# logging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info(\\'Training complete. Starting prediction...\\')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\n# Plotting results\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain:, :].copy()  # Ensure correct indexing\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\nfor col in df2.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df2[col] = convert_to_float(df2[col].tolist())\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: np.array(df2[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose_test = np.array(df2[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Load the scalers and apply to test data\\nscalers = joblib.load(\\'output/scalers.pkl\\')\\nfor indicator in selected_indicators:\\n    test_indicators[indicator] = scalers[indicator].transform(test_indicators[indicator])\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\ntest_indicators = {k: v.flatten() for k, v in test_indicators.items()}\\nclose_test = close_test.flatten()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Load and test the saved model\\nloaded_model = joblib.load(\\'output/model.pkl\\')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\nplt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)', 'Traceback (most recent call last):\\n  File \"C:\\\\PythonApps\\\\drl-trainer\\\\Scaler.py\", line 49, in <module>\\n    df1[col] = convert_to_float(df1[col].tolist())\\n    ~~~^^^^^\\n  File \"C:\\\\PythonApps\\\\drl-trainer\\\\venv\\\\Lib\\\\site-packages\\\\pandas\\\\core\\\\frame.py\", line 4311, in __setitem__\\n    self._set_item(key, value)\\n  File \"C:\\\\PythonApps\\\\drl-trainer\\\\venv\\\\Lib\\\\site-packages\\\\pandas\\\\core\\\\frame.py\", line 4524, in _set_item\\n    value, refs = self._sanitize_column(value)\\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"C:\\\\PythonApps\\\\drl-trainer\\\\venv\\\\Lib\\\\site-packages\\\\pandas\\\\core\\\\frame.py\", line 5266, in _sanitize_column\\n    com.require_length_match(value, self.index)\\n  File \"C:\\\\PythonApps\\\\drl-trainer\\\\venv\\\\Lib\\\\site-packages\\\\pandas\\\\core\\\\common.py\", line 573, in require_length_match\\n    raise ValueError(\\nValueError: Length of values (1598) does not match length of index (2952)\\n\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\nfrom pykalman import KalmanFilter  # Import the Kalman filter\\n\\nsns.set()\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n]\\n\\n# Load and preprocess data\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nrows_count = len(df1)  # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[0:rows_toTrain, :].copy()  # Start from 0\\n\\n# Check if \\'Date\\' column exists, otherwise create it\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\ndef convert_to_float(lst):\\n    return [float(i) for i in lst if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit()]\\n\\nfor col in df1.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df1[col] = convert_to_float(df1[col].tolist())\\n\\n# Extract the selected indicators\\nindicators = {indicator: np.array(df1[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose = np.array(df1[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Apply scaling to the indicators\\nscalers = {}\\nfor indicator in selected_indicators:\\n    scaler = StandardScaler()\\n    indicators[indicator] = scaler.fit_transform(indicators[indicator])\\n    scalers[indicator] = scaler\\n\\n# Save the scalers for future use\\njoblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\nindicators = {k: v.flatten() for k, v in indicators.items()}\\nclose = close.flatten()\\n\\n# Define the step-by-step prediction function\\ndef step_by_step_prediction(agent, trend):\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t]\\n            print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t]\\n            print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n# Update agent initialization with the new indicators\\nwindow_size = 30\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# logging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info(\\'Training complete. Starting prediction...\\')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\n# Plotting results\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain:, :].copy()  # Ensure correct indexing\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\nfor col in df2.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df2[col] = convert_to_float(df2[col].tolist())\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: np.array(df2[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose_test = np.array(df2[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Load the scalers and apply to test data\\nscalers = joblib.load(\\'output/scalers.pkl\\')\\nfor indicator in selected_indicators:\\n    test_indicators[indicator] = scalers[indicator].transform(test_indicators[indicator])\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\ntest_indicators = {k: v.flatten() for k, v in test_indicators.items()}\\nclose_test = close_test.flatten()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Load and test the saved model\\nloaded_model = joblib.load(\\'output/model.pkl\\')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\nplt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)', 'make some  improvements, make sure that the calculate_metrics method in your agent class is implemented correctly, and ensure the convert_to_float function is defined is used consistently.\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\nfrom pykalman import KalmanFilter  # Import the Kalman filter\\n\\nsns.set()\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n]\\n\\n# Load and preprocess data\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nrows_count = len(df1)  # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[0:rows_toTrain, :].copy()  # Start from 0\\n\\n# Check if \\'Date\\' column exists, otherwise create it\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\ndef convert_to_float(lst):\\n    return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\nfor col in df1.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        converted_values = convert_to_float(df1[col].tolist())\\n        if len(converted_values) == len(df1):\\n            df1[col] = converted_values\\n        else:\\n            print(f\"Length mismatch for column {col}: {len(converted_values)} vs {len(df1)}\")\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n# Extract the selected indicators\\nindicators = {indicator: np.array(df1[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose = np.array(df1[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Apply scaling to the indicators\\nscalers = {}\\nfor indicator in selected_indicators:\\n    scaler = StandardScaler()\\n    indicators[indicator] = scaler.fit_transform(indicators[indicator])\\n    scalers[indicator] = scaler\\n\\n# Save the scalers for future use\\njoblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\nindicators = {k: v.flatten() for k, v in indicators.items()}\\nclose = close.flatten()\\n\\n# Define the step-by-step prediction function\\ndef step_by_step_prediction(agent, trend):\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t]\\n            print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t]\\n            print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n# Update agent initialization with the new indicators\\nwindow_size = 30\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# logging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info(\\'Training complete. Starting prediction...\\')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\n# Plotting results\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain:, :].copy()  # Ensure correct indexing\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\nfor col in df2.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        df2[col] = convert_to_float(df2[col].tolist())\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: np.array(df2[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose_test = np.array(df2[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Load the scalers and apply to test data\\nscalers = joblib.load(\\'output/scalers.pkl\\')\\nfor indicator in selected_indicators:\\n    test_indicators[indicator] = scalers[indicator].transform(test_indicators[indicator])\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\ntest_indicators = {k: v.flatten() for k, v in test_indicators.items()}\\nclose_test = close_test.flatten()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Load and test the saved model\\nloaded_model = joblib.load(\\'output/model.pkl\\')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\nplt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)', 'How can I incorporate meta learning on the \"def get_tick_reward(self, weights):\" and on the \"def buy_tick(self):\" on the following code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\n#   Suggested Modifications\\n#   Incorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\n#   methods.\\n#   Contextual Information: Include historical price movements or trends to provide context for current actions.\\n#   Reward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\n#   the actions taken.\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.01\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            return np.argmax(decision[0])\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]  # Example RSI indicator\\n                cci = self.indicators[\\'cci\\'][t]  # Example CCI indicator\\n                adx = self.indicators[\\'adx\\'][t]  # Example ADX indicator\\n\\n                # Modify reward based on indicators\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5)\\n                        earned_money += -5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    # Reward shaping based on RSI and CCI\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5)\\n                        earned_money += -5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    # Reward shaping based on ADX\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return ((earned_money) / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            self.es.train(iterations, print_every=checkpoint)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                cci = self.indicators[\\'cci\\'][t]\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n', 'is it better to  incorporate meta learning and relational inductive biases to a deep reinforcement learning training environment, or just have only one of them.', 'Combining meta-learning with relational inductive biases on the following code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.01\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            return np.argmax(decision[0])\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                if action == 1 and (position == 0 or position == -1):\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5)\\n                        earned_money += -5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n\\n                elif action == 2 and (position == 0 or position == 1):\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5)\\n                        earned_money += -5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return ((earned_money) / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            self.es.train(iterations, print_every=checkpoint)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                if action == 1 and (position == 0 or position == -1):\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\'% (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\'% (t, self.trend[t], earned_money))\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n', 'Any more adjustments to it?', 'how is the optimal way to reset the memory before start the training', 'How is the optimal way to reset the memory before starting the training of a machine learning model to ensure that the training process starts with a clean slate. On the following code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\n#   Improvements Made:\\n#   Consistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\n#   potential conversion errors by filling with NaN if necessary.\\n#   Length Check: Added length checks before assigning converted values back to the DataFrame to avoid length\\n#   mismatches.\\n#   Implemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\n#   correctly and used in the testing phase.\\n\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\nsns.set()\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n]\\n\\n# Load and preprocess data\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nrows_count = len(df1)  # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[0:rows_toTrain, :].copy()  # Start from 0\\n\\n# Check if \\'Date\\' column exists, otherwise create it\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\ndef convert_to_float(lst):\\n    return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n\\nfor col in df1.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        converted_values = convert_to_float(df1[col].tolist())\\n        if len(converted_values) == len(df1):\\n            df1[col] = converted_values\\n        else:\\n            print(f\"Length mismatch for column {col}: {len(converted_values)} vs {len(df1)}\")\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n# Extract the selected indicators\\nindicators = {indicator: np.array(df1[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose = np.array(df1[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Apply scaling to the indicators\\nscalers = {}\\nfor indicator in selected_indicators:\\n    scaler = StandardScaler()\\n    indicators[indicator] = scaler.fit_transform(indicators[indicator])\\n    scalers[indicator] = scaler\\n\\n# Save the scalers for future use\\njoblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\nindicators = {k: v.flatten() for k, v in indicators.items()}\\nclose = close.flatten()\\n\\n\\n# Define the step-by-step prediction function\\ndef step_by_step_prediction(agent, trend):\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t]\\n            print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t]\\n            print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n\\n# Update agent initialization with the new indicators\\nwindow_size = 30\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# logging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info(\\'Training complete. Starting prediction...\\')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\n# Plotting results\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain:, :].copy()  # Ensure correct indexing\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\nfor col in df2.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        converted_values = convert_to_float(df2[col].tolist())\\n        if len(converted_values) == len(df2):\\n            df2[col] = converted_values\\n        else:\\n            print(f\"Length mismatch for column {col}: {len(converted_values)} vs {len(df2)}\")\\n            df2[col] = pd.Series(converted_values, index=df2.index)  # Fill with NaN if lengths differ\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: np.array(df2[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose_test = np.array(df2[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Load the scalers and apply to test data\\nscalers = joblib.load(\\'output/scalers.pkl\\')\\nfor indicator in selected_indicators:\\n    test_indicators[indicator] = scalers[indicator].transform(test_indicators[indicator])\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\ntest_indicators = {k: v.flatten() for k, v in test_indicators.items()}\\nclose_test = close_test.flatten()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Load and test the saved model\\nloaded_model = joblib.load(\\'output/model.pkl\\')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\nplt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)', 'Fix the following code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\n#   Improvements Made:\\n#   Consistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\n#   potential conversion errors by filling with NaN if necessary.\\n#   Length Check: Added length checks before assigning converted values back to the DataFrame to avoid length\\n#   mismatches.\\n#   Implemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\n#   correctly and used in the testing phase.\\n\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n\\n# Before starting the training process\\nimport gc\\nimport tensorflow as tf\\nfrom keras import backend as K\\nimport torch\\n\\n\\nsns.set()\\n\\n# Clear previous variables\\ndel model, agent, scalers, indicators, close, df1, df2\\n\\n# Run garbage collection\\ngc.collect()\\n\\n# Reset TensorFlow session if using TensorFlow/Keras\\ntf.compat.v1.reset_default_graph()\\nK.clear_session()\\n\\n# Free up GPU memory if using PyTorch\\ntorch.cuda.empty_cache()\\n\\n# Now you can proceed with your training setup\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# Start training\\nagent.fit(iterations=500, checkpoint=10)\\n\\n\\n\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n]\\n\\n# Load and preprocess data\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nrows_count = len(df1)  # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[0:rows_toTrain, :].copy()  # Start from 0\\n\\n# Check if \\'Date\\' column exists, otherwise create it\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\ndef convert_to_float(lst):\\n    return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n\\nfor col in df1.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        converted_values = convert_to_float(df1[col].tolist())\\n        if len(converted_values) == len(df1):\\n            df1[col] = converted_values\\n        else:\\n            print(f\"Length mismatch for column {col}: {len(converted_values)} vs {len(df1)}\")\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n# Extract the selected indicators\\nindicators = {indicator: np.array(df1[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose = np.array(df1[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Apply scaling to the indicators\\nscalers = {}\\nfor indicator in selected_indicators:\\n    scaler = StandardScaler()\\n    indicators[indicator] = scaler.fit_transform(indicators[indicator])\\n    scalers[indicator] = scaler\\n\\n# Save the scalers for future use\\njoblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\nindicators = {k: v.flatten() for k, v in indicators.items()}\\nclose = close.flatten()\\n\\n\\n# Define the step-by-step prediction function\\ndef step_by_step_prediction(agent, trend):\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t]\\n            print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t]\\n            print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n\\n# Update agent initialization with the new indicators\\nwindow_size = 30\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# logging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info(\\'Training complete. Starting prediction...\\')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\n# Plotting results\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain:, :].copy()  # Ensure correct indexing\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\nfor col in df2.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        converted_values = convert_to_float(df2[col].tolist())\\n        if len(converted_values) == len(df2):\\n            df2[col] = converted_values\\n        else:\\n            print(f\"Length mismatch for column {col}: {len(converted_values)} vs {len(df2)}\")\\n            df2[col] = pd.Series(converted_values, index=df2.index)  # Fill with NaN if lengths differ\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: np.array(df2[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose_test = np.array(df2[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Load the scalers and apply to test data\\nscalers = joblib.load(\\'output/scalers.pkl\\')\\nfor indicator in selected_indicators:\\n    test_indicators[indicator] = scalers[indicator].transform(test_indicators[indicator])\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\ntest_indicators = {k: v.flatten() for k, v in test_indicators.items()}\\nclose_test = close_test.flatten()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Load and test the saved model\\nloaded_model = joblib.load(\\'output/model.pkl\\')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\nplt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)', '2024-08-22 23:57:15.965302: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\\n2024-08-22 23:57:18.304402: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\\nTraceback (most recent call last):\\n  File \"C:\\\\PythonApps\\\\drl-trainer\\\\Scaler.py\", line 120, in <module>\\n    import torch\\n  File \"C:\\\\PythonApps\\\\drl-trainer\\\\venv\\\\Lib\\\\site-packages\\\\torch\\\\__init__.py\", line 148, in <module>\\n    raise err\\nOSError: [WinError 126] The specified module could not be found. Error loading \"C:\\\\PythonApps\\\\drl-trainer\\\\venv\\\\Lib\\\\site-packages\\\\torch\\\\lib\\\\fbgemm.dll\" or one of its dependencies.\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\n#   Improvements Made:\\n#   Consistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\n#   potential conversion errors by filling with NaN if necessary.\\n#   Length Check: Added length checks before assigning converted values back to the DataFrame to avoid length\\n#   mismatches.\\n#   Implemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\n#   correctly and used in the testing phase.\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Set seaborn style\\nsns.set()\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n]\\n\\n# Load and preprocess data\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nrows_count = len(df1)  # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[0:rows_toTrain, :].copy()  # Start from 0\\n\\n# Check if \\'Date\\' column exists, otherwise create it\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\ndef convert_to_float(lst):\\n    return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n\\nfor col in df1.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        converted_values = convert_to_float(df1[col].tolist())\\n        if len(converted_values) == len(df1):\\n            df1[col] = converted_values\\n        else:\\n            print(f\"Length mismatch for column {col}: {len(converted_values)} vs {len(df1)}\")\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n# Extract the selected indicators\\nindicators = {indicator: np.array(df1[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose = np.array(df1[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Apply scaling to the indicators\\nscalers = {}\\nfor indicator in selected_indicators:\\n    scaler = StandardScaler()\\n    indicators[indicator] = scaler.fit_transform(indicators[indicator])\\n    scalers[indicator] = scaler\\n\\n# Save the scalers for future use\\njoblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\nindicators = {k: v.flatten() for k, v in indicators.items()}\\nclose = close.flatten()\\n\\n# Define the step-by-step prediction function\\ndef step_by_step_prediction(agent, trend):\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t]\\n            print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t]\\n            print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n# Update agent initialization with the new indicators\\nwindow_size = 30\\nskip = 1\\ninitial_money = 10000\\n\\n# Clear previous variables and reset memory before starting training\\nimport gc\\nimport tensorflow as tf\\nfrom keras import backend as K\\nimport torch\\n\\n# Clear previous variables\\ndel model, agent, scalers, indicators, close, df1, df2\\n\\n# Run garbage collection\\ngc.collect()\\n\\n# Reset TensorFlow session if using TensorFlow/Keras\\ntf.compat.v1.reset_default_graph()\\nK.clear_session()\\n\\n# Free up GPU memory if using PyTorch\\ntorch.cuda.empty_cache()\\n\\n# Now you can proceed with your training setup\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# Start training\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info(\\'Training complete. Starting prediction...\\')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\n# Plotting results\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain:, :].copy()  # Ensure correct indexing\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\nfor col in df2.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        converted_values = convert_to_float(df2[col].tolist())\\n        if len(converted_values) == len(df2):\\n            df2[col] = converted_values\\n        else:\\n            print(f\"Length mismatch for column {col}: {len(converted_values)} vs {len(df2)}\")\\n            df2[col] = pd.Series(converted_values, index=df2.index)  # Fill with NaN if lengths differ\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: np.array(df2[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose_test = np.array(df2[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Load the scalers and apply to test data\\nscalers = joblib.load(\\'output/scalers.pkl\\')\\nfor indicator in selected_indicators:\\n    test_indicators[indicator] = scalers[indicator].transform(test_indicators[indicator])\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\ntest_indicators = {k: v.flatten() for k, v in test_indicators.items()}\\nclose_test = close_test.flatten()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Load and test the saved model\\nloaded_model = joblib.load(\\'output/model.pkl\\')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\nplt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)\\n', 'Why is the following runs so slow:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\n#   Improvements Made:\\n#   Consistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\n#   potential conversion errors by filling with NaN if necessary.\\n#   Length Check: Added length checks before assigning converted values back to the DataFrame to avoid length\\n#   mismatches.\\n#   Implemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\n#   correctly and used in the testing phase.\\n\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n\\nsns.set()\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n]\\n\\n# Load and preprocess data\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nrows_count = len(df1)  # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[0:rows_toTrain, :].copy()  # Start from 0\\n\\n# Check if \\'Date\\' column exists, otherwise create it\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\ndef convert_to_float(lst):\\n    return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n\\nfor col in df1.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        converted_values = convert_to_float(df1[col].tolist())\\n        if len(converted_values) == len(df1):\\n            df1[col] = converted_values\\n        else:\\n            print(f\"Length mismatch for column {col}: {len(converted_values)} vs {len(df1)}\")\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n# Extract the selected indicators\\nindicators = {indicator: np.array(df1[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose = np.array(df1[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Apply scaling to the indicators\\nscalers = {}\\nfor indicator in selected_indicators:\\n    scaler = StandardScaler()\\n    indicators[indicator] = scaler.fit_transform(indicators[indicator])\\n    scalers[indicator] = scaler\\n\\n# Save the scalers for future use\\njoblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\nindicators = {k: v.flatten() for k, v in indicators.items()}\\nclose = close.flatten()\\n\\n\\n# Define the step-by-step prediction function\\ndef step_by_step_prediction(agent, trend):\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t]\\n            print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t]\\n            print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n\\n# Update agent initialization with the new indicators\\nwindow_size = 30\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# logging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info(\\'Training complete. Starting prediction...\\')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\n# Plotting results\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain:, :].copy()  # Ensure correct indexing\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\nfor col in df2.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        converted_values = convert_to_float(df2[col].tolist())\\n        if len(converted_values) == len(df2):\\n            df2[col] = converted_values\\n        else:\\n            print(f\"Length mismatch for column {col}: {len(converted_values)} vs {len(df2)}\")\\n            df2[col] = pd.Series(converted_values, index=df2.index)  # Fill with NaN if lengths differ\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: np.array(df2[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose_test = np.array(df2[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Load the scalers and apply to test data\\nscalers = joblib.load(\\'output/scalers.pkl\\')\\nfor indicator in selected_indicators:\\n    test_indicators[indicator] = scalers[indicator].transform(test_indicators[indicator])\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\ntest_indicators = {k: v.flatten() for k, v in test_indicators.items()}\\nclose_test = close_test.flatten()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Load and test the saved model\\nloaded_model = joblib.load(\\'output/model.pkl\\')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\nplt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)', 'How do I add the scalers to the indicators \"indicators = data[\\'indicators\\']\" and Flatten the arrays after scaling for compatibility with the rest of the code\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# import pandas as pd\\n# from modules.indicator_utils import convert_to_float, add_indicators  # Import the new module\\n\\napp = Flask(__name__)\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    data = request.get_json()\\n    trend = data[\\'trend\\']\\n    indicators = data[\\'indicators\\']\\n\\n# Load the scalers and apply to test data\\nscalers = joblib.load(\\'output/scalers.pkl\\')\\nfor indicator in selected_indicators:\\n    test_indicators[indicator] = scalers[indicator].transform(test_indicators[indicator])\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\ntest_indicators = {k: v.flatten() for k, v in test_indicators.items()}\\n\\n    t = int(data[\\'t\\'])\\n    window_size = int(data[\\'window_size\\'])\\n\\n    state = get_state(trend, indicators, t, window_size)\\n    decision = model.predict(state)\\n    action = np.argmax(decision[0])\\n\\n    response = {\\'action\\': int(action)}\\n    return jsonify(response)\\n\\n\\nif __name__==\\'__main__\\':\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n    # Load the scalers and apply to test data\\n    scalers = joblib.load(\\'output/scalers.pkl\\')\\n    app.run(host=\"0.0.0.0\", debug=False, port=5010)\\n', \"Check the following code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.01\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        self.model = model\\n        self.window_size = window_size\\n        self.trend =  np.array(trend)  # Ensure trend is a numpy array\\n        self.skip = skip\\n        self.initial_money = initial_money\\n        self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n        self.es = deep.Deep_Evolution_Strategy(\\n            self.model.get_weights(),\\n            self.get_tick_reward,\\n            self.POPULATION_SIZE,\\n            self.SIGMA,\\n            self.LEARNING_RATE,\\n        )\\n\\n    def act(self, sequence):\\n        decision = self.model.predict(np.array(sequence))\\n        return np.argmax(decision[0])\\n\\n    def get_state(self, t):\\n        window_size = self.window_size + 1\\n        d = t - window_size + 1\\n\\n        # Ensure that block has the correct length\\n        if d >= 0:\\n            block = self.trend[d:t + 1]\\n        else:\\n            # Pad with the first value if there's not enough data\\n            block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n        res = []\\n        for i in range(len(block) - 1):\\n            res.append(block[i + 1] - block[i])\\n\\n        # Ensure the length of res matches the expected window_size\\n        if len(res) < self.window_size:\\n            res = np.pad(res, (self.window_size - len(res), 0), 'constant')\\n\\n        indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n        state = np.array([res + indicators_state])\\n\\n        return state\\n\\n    def predict(self):\\n        actions = []\\n        for t in range(len(self.trend) - self.window_size):\\n            state = self.get_state(t)\\n            action_index = self.act(state)\\n            if action_index == 0:\\n                actions.append('Hold')\\n            elif action_index == 1:\\n                actions.append('Buy')\\n            elif action_index == 2:\\n                actions.append('Sell')\\n        return actions\\n\\n    def get_tick_reward(self, weights):\\n        initial_money = self.initial_money\\n        earned_money = 0\\n        self.model.weights = weights\\n        state = self.get_state(0)\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and (position == 0 or position == -1):\\n                if position == -1:\\n                    earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5)\\n                    earned_money += -5.8\\n                position = 1\\n                bought_price = self.trend[t] + 0.25\\n\\n            elif action == 2 and (position == 0 or position == 1):\\n                if position == 1:\\n                    earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5)\\n                    earned_money += -5.8\\n                position = -1\\n                bought_price = self.trend[t] - 0.25\\n\\n            state = next_state\\n        logging.debug(f'Earned money: {earned_money}')\\n        return ((earned_money) / initial_money) * 100\\n\\n    def fit(self, iterations, checkpoint):\\n        self.es.train(iterations, print_every=checkpoint)\\n\\n    def buy_tick(self):\\n        initial_money = self.initial_money\\n        state = self.get_state(0)\\n        states_sell = []\\n        states_buy = []\\n\\n        earned_money = 0\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and (position == 0 or position == -1):\\n                if position == -1:\\n                    earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                position = 1\\n                states_buy.append(t)\\n                bought_price = self.trend[t] + 0.25\\n                print('bar %d: buy 1 contract at price %f, total balance %f'% (t, self.trend[t], earned_money))\\n\\n            elif action == 2 and (position == 0 or position == 1):\\n                if position == 1:\\n                    earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                position = -1\\n                states_sell.append(t)\\n                bought_price = self.trend[t] - 0.25\\n                print('bar %d: sell 1 contract at price %f, total balance %f'% (t, self.trend[t], earned_money))\\n\\n            state = next_state\\n\\n        roi = (earned_money / initial_money) * 100\\n        total_gains = earned_money\\n        return states_buy, states_sell, total_gains, roi\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n        max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n        return ticks_earned, max_drawdown, start_idx, end_idx\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        peak = cumulative_profit.iloc[0]\\n        max_drawdown = 0\\n        start_idx, end_idx = 0, 0\\n\\n        for i in range(1, len(cumulative_profit)):\\n            if cumulative_profit.iloc[i] > peak:\\n                peak = cumulative_profit.iloc[i]\\n            drawdown = peak - cumulative_profit.iloc[i]\\n            if drawdown > max_drawdown:\\n                max_drawdown = drawdown\\n                start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n        return max_drawdown, start_idx, end_idx\\n\", \" Add error handling to ensure robustness., ensure the state calculation is efficient and handles edge cases properly. Make sure all data inputs are consistently numpy arrays. for the following code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.01\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        self.model = model\\n        self.window_size = window_size\\n        self.trend = np.array(trend)  # Ensure trend is a numpy array\\n        self.skip = skip\\n        self.initial_money = initial_money\\n        self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n        self.es = deep.Deep_Evolution_Strategy(\\n            self.model.get_weights(),\\n            self.get_tick_reward,\\n            self.POPULATION_SIZE,\\n            self.SIGMA,\\n            self.LEARNING_RATE,\\n        )\\n\\n    def act(self, sequence):\\n        decision = self.model.predict(np.array(sequence))\\n        return np.argmax(decision[0])\\n\\n    def get_state(self, t):\\n        window_size = self.window_size + 1\\n        d = t - window_size + 1\\n\\n        # Ensure that block has the correct length\\n        if d >= 0:\\n            block = self.trend[d:t + 1]\\n        else:\\n            # Pad with the first value if there's not enough data\\n            block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n        res = []\\n        for i in range(len(block) - 1):\\n            res.append(block[i + 1] - block[i])\\n\\n        # Ensure the length of res matches the expected window_size\\n        if len(res) < self.window_size:\\n            res = np.pad(res, (self.window_size - len(res), 0), 'constant')\\n\\n        indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n        state = np.array([res + indicators_state])\\n\\n        return state\\n\\n    def predict(self):\\n        actions = []\\n        for t in range(len(self.trend) - self.window_size):\\n            state = self.get_state(t)\\n            action_index = self.act(state)\\n            if action_index == 0:\\n                actions.append('Hold')\\n            elif action_index == 1:\\n                actions.append('Buy')\\n            elif action_index == 2:\\n                actions.append('Sell')\\n        return actions\\n\\n    def get_tick_reward(self, weights):\\n        initial_money = self.initial_money\\n        earned_money = 0\\n        self.model.weights = weights\\n        state = self.get_state(0)\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and (position == 0 or position == -1):\\n                if position == -1:\\n                    earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5)\\n                    earned_money += -5.8\\n                position = 1\\n                bought_price = self.trend[t] + 0.25\\n\\n            elif action == 2 and (position == 0 or position == 1):\\n                if position == 1:\\n                    earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5)\\n                    earned_money += -5.8\\n                position = -1\\n                bought_price = self.trend[t] - 0.25\\n\\n            state = next_state\\n        logging.debug(f'Earned money: {earned_money}')\\n        return ((earned_money) / initial_money) * 100\\n\\n    def fit(self, iterations, checkpoint):\\n        self.es.train(iterations, print_every=checkpoint)\\n\\n    def buy_tick(self):\\n        initial_money = self.initial_money\\n        state = self.get_state(0)\\n        states_sell = []\\n        states_buy = []\\n\\n        earned_money = 0\\n        position = 0\\n        bought_price = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state)\\n            next_state = self.get_state(t + 1)\\n\\n            if action == 1 and (position == 0 or position == -1):\\n                if position == -1:\\n                    earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                position = 1\\n                states_buy.append(t)\\n                bought_price = self.trend[t] + 0.25\\n                print('bar %d: buy 1 contract at price %f, total balance %f'% (t, self.trend[t], earned_money))\\n\\n            elif action == 2 and (position == 0 or position == 1):\\n                if position == 1:\\n                    earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                position = -1\\n                states_sell.append(t)\\n                bought_price = self.trend[t] - 0.25\\n                print('bar %d: sell 1 contract at price %f, total balance %f'% (t, self.trend[t], earned_money))\\n\\n            state = next_state\\n\\n        roi = (earned_money / initial_money) * 100\\n        total_gains = earned_money\\n        return states_buy, states_sell, total_gains, roi\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n        max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n        return ticks_earned, max_drawdown, start_idx, end_idx\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        peak = cumulative_profit.iloc[0]\\n        max_drawdown = 0\\n        start_idx, end_idx = 0, 0\\n\\n        for i in range(1, len(cumulative_profit)):\\n            if cumulative_profit.iloc[i] > peak:\\n                peak = cumulative_profit.iloc[i]\\n            drawdown = peak - cumulative_profit.iloc[i]\\n            if drawdown > max_drawdown:\\n                max_drawdown = drawdown\\n                start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n        return max_drawdown, start_idx, end_idx\", 'Check the following code for improvements/optimization:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\nimport numpy as np\\n\\n\\nclass Model:\\n    def __init__(self, input_size, layer_size, output_size):\\n        self.weights = [\\n            np.random.randn(input_size, layer_size),\\n            np.random.randn(layer_size, output_size),\\n            np.random.randn(1, layer_size),\\n        ]\\n\\n    def predict(self, inputs):\\n        feed = np.dot(inputs, self.weights[0]) + self.weights[-1]\\n        decision = np.dot(feed, self.weights[1])\\n        return decision\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def set_weights(self, weights):\\n        self.weights = weights\\n\\n\\n', ' Instead of using np.random.randn() for weight initialization, consider using np.random.normal() with specified mean and standard deviation for more control over the initialization. Add error handling in the set_weights method to ensure that the provided weights have the correct shape.  For the following code\\n\\n\\n\\nimport numpy as np\\n\\nclass Model:\\n    def __init__(self, input_size: int, layer_size: int, output_size: int):\\n        \"\"\"\\n        Initializes the model with random weights.\\n\\n        Parameters:\\n        - input_size: Number of input features.\\n        - layer_size: Number of neurons in the hidden layer.\\n        - output_size: Number of output classes.\\n        \"\"\"\\n        self.weights = [\\n            np.random.normal(0, 0.1, (input_size, layer_size)),  # Input to hidden layer weights\\n            np.random.normal(0, 0.1, (layer_size, output_size)),  # Hidden to output layer weights\\n        ]\\n        self.biases = [\\n            np.random.normal(0, 0.1, (1, layer_size)),  # Bias for hidden layer\\n            np.random.normal(0, 0.1, (1, output_size)),  # Bias for output layer\\n        ]\\n\\n    def predict(self, inputs: np.ndarray) -> np.ndarray:\\n        \"\"\"\\n        Makes a prediction based on the input data.\\n\\n        Parameters:\\n        - inputs: Input data for prediction.\\n\\n        Returns:\\n        - Decision: Output predictions.\\n        \"\"\"\\n        feed = np.dot(inputs, self.weights[0]) + self.biases[0]\\n        decision = np.dot(feed, self.weights[1]) + self.biases[1]\\n        return decision\\n\\n    def get_weights(self) -> list:\\n        \"\"\"Returns the current weights of the model.\"\"\"\\n        return self.weights\\n\\n    def set_weights(self, weights: list):\\n        \"\"\"\\n        Sets the weights of the model.\\n\\n        Parameters:\\n        - weights: New weights to set for the model.\\n        \"\"\"\\n        if len(weights) != len(self.weights):\\n            raise ValueError(\"Weights must have the same length as the current weights.\")\\n        self.weights = weights', \"Optimize and add error handling on the following code:\\n\\nimport numpy as np\\nimport time\\n\\nclass Deep_Evolution_Strategy():\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def train(self, epoch=200, print_every=1):\\n        lasttime = time.time()\\n        for i in range(epoch):\\n            # logging.debug(f'Starting epoch {i+1}/{epoch}')\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = []\\n                for w in self.weights:\\n                    x.append(np.random.randn(*w.shape))\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] = w + (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T, rewards).T\\n                if (i + 1) % print_every == 0:\\n                    print(\\n                        'iter %d. reward: %f'\\n                        % (i + 1, self.reward_function(self.weights))\\n                    )\\n                    break\\n            print('time taken to train:', time.time() - lasttime, 'seconds')\\n\\n\\n\", 'ValueError: operands could not be broadcast together with shapes (500,3) (37,500) ', 'ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (60,) + inhomogeneous part.\\n', 'ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (60,) + inhomogeneous part.\\n\\nimport numpy as np\\nimport time\\n\\nclass Deep_Evolution_Strategy:\\n    def __init__(self, weights: list, reward_function: callable, population_size: int, sigma: float, learning_rate: float):\\n        \"\"\"\\n        Initializes the Deep Evolution Strategy.\\n\\n        Parameters:\\n        - weights: Initial weights of the model.\\n        - reward_function: Function to evaluate the performance of the weights.\\n        - population_size: Number of individuals in the population.\\n        - sigma: Standard deviation for weight perturbation.\\n        - learning_rate: Learning rate for updating weights.\\n        \"\"\"\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights: list, population: np.ndarray) -> list:\\n        \"\"\"\\n        Generates a new set of weights by adding noise to the current weights.\\n\\n        Parameters:\\n        - weights: Current weights of the model.\\n        - population: Perturbations for the weights.\\n\\n        Returns:\\n        - List of new weights with added noise.\\n        \"\"\"\\n        return [weights[i] + self.sigma * population[i] for i in range(len(weights))]\\n\\n    def get_weights(self) -> list:\\n        \"\"\"Returns the current weights of the model.\"\"\"\\n        return self.weights\\n\\n    def train(self, epoch: int = 200, print_every: int = 1):\\n        \"\"\"\\n        Trains the model using the Deep Evolution Strategy.\\n\\n        Parameters:\\n        - epoch: Number of training iterations.\\n        - print_every: Frequency of printing the reward.\\n        \"\"\"\\n        lasttime = time.time()\\n        for i in range(epoch):\\n            population = [np.random.randn(*w.shape) for w in self.weights for _ in range(self.population_size)]\\n            population = np.array(population).reshape(self.population_size, len(self.weights), *self.weights[0].shape)\\n            rewards = np.zeros(self.population_size)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            # Normalize rewards\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n\\n            for index in range(len(self.weights)):\\n                A = population[:, index]\\n                if A.shape[1:] != self.weights[index].shape:\\n                    raise ValueError(f\"Shape mismatch: A.shape[1:] = {A.shape[1:]}, weights[{index}].shape = {self.weights[index].shape}\")\\n                self.weights[index] += (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T, rewards)\\n\\n            if (i + 1) % print_every == 0:\\n                print(f\\'iter {i + 1}. reward: {self.reward_function(self.weights)}\\')\\n\\n        print(\\'Time taken to train:\\', time.time() - lasttime, \\'seconds\\')', 'Check the following for any errors and optimization\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Set random seed for reproducibility\\nnp.random.seed(42)\\nsns.set()\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n]\\n\\n# Load and preprocess data\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nrows_count = len(df1)  # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[0:rows_toTrain, :].copy()  # Start from 0\\n\\n# Check if \\'Date\\' column exists, otherwise create it\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\ndef convert_to_float(lst):\\n    return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n\\nfor col in df1.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        converted_values = convert_to_float(df1[col].tolist())\\n        if len(converted_values) == len(df1):\\n            df1[col] = converted_values\\n        else:\\n            print(f\"Length mismatch for column {col}: {len(converted_values)} vs {len(df1)}\")\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n# Extract the selected indicators\\nindicators = {indicator: np.array(df1[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose = np.array(df1[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Apply scaling to the indicators\\nscalers = {}\\nfor indicator in selected_indicators:\\n    scaler = StandardScaler()\\n    indicators[indicator] = scaler.fit_transform(indicators[indicator])\\n    scalers[indicator] = scaler\\n\\n# Save the scalers for future use\\njoblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\nindicators = {k: v.flatten() for k, v in indicators.items()}\\nclose = close.flatten()\\n\\n\\n# Define the step-by-step prediction function\\ndef step_by_step_prediction(agent, trend, window_size):\\n    initial_money = agent.initial_money\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t] + 0.25\\n            print(\\'day %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t] - 0.25\\n            print(\\'day %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n\\n# Update agent initialization with the new indicators\\nwindow_size = 60\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# logging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info(\\'Training complete. Starting prediction...\\')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\n# Plotting results\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain:, :].copy()  # Ensure correct indexing\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\nfor col in df2.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        converted_values = convert_to_float(df2[col].tolist())\\n        if len(converted_values) == len(df2):\\n            df2[col] = converted_values\\n        else:\\n            print(f\"Length mismatch for column {col}: {len(converted_values)} vs {len(df2)}\")\\n            df2[col] = pd.Series(converted_values, index=df2.index)  # Fill with NaN if lengths differ\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: np.array(df2[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose_test = np.array(df2[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Load the scalers and apply to test data\\nscalers = joblib.load(\\'output/scalers.pkl\\')\\nfor indicator in selected_indicators:\\n    test_indicators[indicator] = scalers[indicator].transform(test_indicators[indicator])\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\ntest_indicators = {k: v.flatten() for k, v in test_indicators.items()}\\nclose_test = close_test.flatten()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Load and test the saved model\\nloaded_model = joblib.load(\\'output/model.pkl\\')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\nplt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)', 'optimize the following code:\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Set random seed for reproducibility\\nnp.random.seed(42)\\nsns.set()\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n]\\n\\n# Load and preprocess data\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nrows_count = len(df1)  # get the number of rows on the csv file\\ncalc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\nrows_toTrain = int(rows_count - calc_perc)\\n\\n# Copy the numbers of records to df1 for training\\ndf1 = df1.iloc[0:rows_toTrain, :].copy()  # Start from 0\\n\\n# Check if \\'Date\\' column exists, otherwise create it\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\ndef convert_to_float(lst):\\n    return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\nfor col in df1.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        converted_values = convert_to_float(df1[col].tolist())\\n        if len(converted_values) == len(df1):\\n            df1[col] = converted_values\\n        else:\\n            print(f\"Length mismatch for column {col}: {len(converted_values)} vs {len(df1)}\")\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n# Extract the selected indicators\\nindicators = {indicator: np.array(df1[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose = np.array(df1[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Apply scaling to the indicators\\nscalers = {}\\nfor indicator in selected_indicators:\\n    scaler = StandardScaler()\\n    indicators[indicator] = scaler.fit_transform(indicators[indicator])\\n    scalers[indicator] = scaler\\n\\n# Save the scalers for future use\\njoblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\nindicators = {k: v.flatten() for k, v in indicators.items()}\\nclose = close.flatten()\\n\\n# Define the step-by-step prediction function\\ndef step_by_step_prediction(agent, trend, window_size):\\n    initial_money = agent.initial_money\\n    state = agent.get_state(0)\\n    position = 0\\n    bought_price = 0\\n    earned_money = 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(state)\\n        next_state = agent.get_state(t + 1)\\n\\n        if action == 1 and (position == 0 or position == -1):\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t]) * 4 * 12.5) - 5.8\\n            position = 1\\n            bought_price = trend[t] + 0.25\\n            print(\\'day %d: buy 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        elif action == 2 and (position == 0 or position == 1):\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price) * 4 * 12.5) - 5.8\\n            position = -1\\n            bought_price = trend[t] - 0.25\\n            print(\\'day %d: sell 1 contract at price %f, total balance %f\\' % (t, trend[t], earned_money))\\n\\n        state = next_state\\n\\n# Update agent initialization with the new indicators\\nwindow_size = 60\\nskip = 1\\ninitial_money = 10000\\n\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=indicators\\n)\\n\\n# logging.info(\\'Starting training...\\')\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# logging.info(\\'Training complete. Starting prediction...\\')\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\n# Plotting results\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\nplt.title(f\\'total gains {total_gains}, total investment {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\')\\ndf2 = df2.iloc[rows_toTrain:, :].copy()  # Ensure correct indexing\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\nfor col in df2.columns:\\n    if col != \\'Date\\':  # Skip conversion for \\'Date\\' column\\n        converted_values = convert_to_float(df2[col].tolist())\\n        if len(converted_values) == len(df2):\\n            df2[col] = converted_values\\n        else:\\n            print(f\"Length mismatch for column {col}: {len(converted_values)} vs {len(df2)}\")\\n            df2[col] = pd.Series(converted_values, index=df2.index)  # Fill with NaN if lengths differ\\n\\n# Extract the selected indicators for testing\\ntest_indicators = {indicator: np.array(df2[indicator].tolist()).reshape(-1, 1) for indicator in selected_indicators}\\nclose_test = np.array(df2[\\'Close\\'].tolist()).reshape(-1, 1)\\n\\n# Load the scalers and apply to test data\\nscalers = joblib.load(\\'output/scalers.pkl\\')\\nfor indicator in selected_indicators:\\n    test_indicators[indicator] = scalers[indicator].transform(test_indicators[indicator])\\n\\n# Flatten the arrays after scaling for compatibility with the rest of the code\\ntest_indicators = {k: v.flatten() for k, v in test_indicators.items()}\\nclose_test = close_test.flatten()\\n\\nagentTest = agent_funct.Agent(\\n    model=model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\n\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame()\\ntest[\\'Close\\'] = close_test\\ntest[\\'Signal\\'] = 0\\ntest.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\nplt.title(f\\'total gains {total_gainsT}, ROI {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Load and test the saved model\\nloaded_model = joblib.load(\\'output/model.pkl\\')\\nloaded_agent = agent_funct.Agent(\\n    model=loaded_model,\\n    window_size=window_size,\\n    trend=close_test,\\n    skip=skip,\\n    initial_money=initial_money,\\n    indicators=test_indicators\\n)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nfig1 = plt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\nplt.title(f\\'total gains {loaded_total_gains}, total investment {loaded_invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)', 'After the training is done the step by step prediction gives me a loss\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    EPSILON = 0.1  # Exploration rate\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            if np.random.rand() < self.EPSILON:  # Explore\\n                action = np.random.choice([0, 1, 2])  # Randomly choose an action\\n            else:  # Exploit\\n                decision = self.model.predict(np.array(sequence))\\n                action = np.argmax(decision[0])\\n\\n            # Add small noise to the action for exploration\\n            noise = np.random.normal(0, 0.1)  # Small noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]  # Uncomment if needed\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                        if adx > 25:  # Strong trend condition\\n                            earned_money += 5  # Bonus for selling in a strong trend\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                        if adx > 25:  # Strong trend condition\\n                            earned_money += 5  # Bonus for selling in a strong trend\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if rsi > 70:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return ((earned_money) / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            self.es.train(iterations, print_every=checkpoint)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n ', 'Change the Epsilon noise injection to Gaussian Noise in the following code:\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    EPSILON = 0.1  # Exploration rate\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            if np.random.rand() < self.EPSILON:  # Explore\\n                action = np.random.choice([0, 1, 2])  # Randomly choose an action\\n            else:  # Exploit\\n                decision = self.model.predict(np.array(sequence))\\n                action = np.argmax(decision[0])\\n\\n            # Add small noise to the action for exploration\\n            noise = np.random.normal(0, 0.1)  # Small noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]  # Uncomment if needed\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return ((earned_money) / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            self.es.train(iterations, print_every=checkpoint)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n', 'How do I reset all variables used on the following code before it executing it on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Set random seed for reproducibility\\nnp.random.seed(42)\\nsns.set()\\n\\n# Define the list of indicators to be used\\nselected_indicators = [\\n    \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n]\\n\\n# Load and preprocess data\\ndf1 = pd.read_csv(\\'data/dataFile.csv\\')\\nrows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n# Copy the records to df1 for training\\ndf1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n# Ensure \\'Date\\' column exists\\nif \\'Date\\' not in df1.columns:\\n    df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n# Convert columns to float, excluding the \\'Date\\' column\\ndef convert_to_float(lst):\\n    return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n# Apply conversion and handle length mismatches\\nfor col in df1.columns:\\n    if col != \\'Date\\':\\n        converted_values = convert_to_float(df1[col].tolist())\\n        df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n# Extract and scale the selected indicators\\nindicators = {indicator: StandardScaler().fit_transform(df1[indicator].values.reshape(-1, 1)).flatten() for indicator in selected_indicators}\\nclose = df1[\\'Close\\'].values.flatten()\\n\\n# Save the scalers for future use\\njoblib.dump({indicator: StandardScaler() for indicator in selected_indicators}, \\'output/scalers.pkl\\')\\n\\n# Define the step-by-step prediction function\\ndef step_by_step_prediction(agent, trend):\\n    position, bought_price, earned_money = 0, 0, 0\\n\\n    for t in range(0, len(trend) - 1, agent.skip):\\n        action = agent.act(agent.get_state(t))\\n        if action == 1 and (position == 0 or position == -1):  # Buy\\n            if position == -1:\\n                earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n            position, bought_price = 1, trend[t] + 0.25\\n            print(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n        elif action == 2 and (position == 0 or position == 1):  # Sell\\n            if position == 1:\\n                earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n            position, bought_price = -1, trend[t] - 0.25\\n            print(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n# Initialize agent\\nwindow_size, skip, initial_money = 60, 1, 10000\\nmodel = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\nagent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n# Train the agent\\nagent.fit(iterations=500, checkpoint=10)\\n\\n# Perform predictions\\nstates_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n# Save the model\\njoblib.dump(model, \\'output/model.pkl\\')\\n\\n# Plotting results\\nplt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\nplt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Testing\\ndf2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n# Load the trained model\\nmodel = joblib.load(\\'output/model.pkl\\')\\n\\n# Convert test data\\nfor col in df2.columns:\\n    if col != \\'Date\\':\\n        converted_values = convert_to_float(df2[col].tolist())\\n        df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n# Extract and scale the selected indicators for testing\\ntest_indicators = {indicator: StandardScaler().fit_transform(df2[indicator].values.reshape(-1, 1)).flatten() for indicator in selected_indicators}\\nclose_test = df2[\\'Close\\'].values.flatten()\\n\\n# Initialize test agent\\nagentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\nstates_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n# Aligning the indices for testing data\\ntest = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\ntest.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\ntest.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\ntest[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\ntest[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\ntest[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n# Calculate metrics\\nticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n# Plotting test results\\nplt.figure(figsize=(15, 5))\\nplt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\nplt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\nplt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Load and test the saved model\\nloaded_model = joblib.load(\\'output/model.pkl\\')\\nloaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\nloaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n# Plot the loaded model results\\nplt.figure(figsize=(15, 5))\\nplt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\nplt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\nplt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\nplt.legend()\\nplt.show()\\n\\n# Perform step-by-step predictions\\nstep_by_step_prediction(agentTest, close_test)', 'Add the Gaussian noise with the minimal noise to the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            return np.argmax(decision[0])\\n\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]  # Uncomment if needed\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return ((earned_money) / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            self.es.train(iterations, print_every=checkpoint)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n', \"What would you recommend on the following code, to make it learn better on the train:\\n\\nimport numpy as np\\nimport time\\n\\n\\nclass Deep_Evolution_Strategy():\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def train(self, epoch=200, print_every=1):\\n        lasttime = time.time()\\n        for i in range(epoch):\\n            # logging.debug(f'Starting epoch {i+1}/{epoch}')\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = []\\n                for w in self.weights:\\n                    x.append(np.random.randn(*w.shape))\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] = w + (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T, rewards).T\\n                if (i + 1) % print_every == 0:\\n                    print(\\n                        'iter %d. reward: %f'\\n                        % (i + 1, self.reward_function(self.weights))\\n                    )\\n                    break\\n            print('time taken to train:', time.time() - lasttime, 'seconds')\\n\\n\\n\", 'what do I need to call the following:\\n    def train(self, epoch=200, print_every=1, update_frequency=5, patience=10):\\nfrom the \"def fit(self, iterations, checkpoint):\" on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]  # Uncomment if needed\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return ((earned_money) / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            self.es.train(iterations, print_every=checkpoint)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise', 'How can I use update_frequency on the following code:\\n\\ndef train(self, epoch=200, print_every=1, update_frequency=5, patience=10):\\n    lasttime = time.time()\\n    previous_mean_reward = -np.inf\\n    no_improvement_count = 0\\n\\n    for i in range(epoch):\\n        population = []\\n        rewards = np.zeros(self.population_size)\\n        for k in range(self.population_size):\\n            x = [np.random.randn(*w.shape) + np.random.uniform(-0.01, 0.01, w.shape) for w in self.weights]\\n            population.append(x)\\n\\n        for k in range(self.population_size):\\n            weights_population = self._get_weight_from_population(self.weights, population[k])\\n            rewards[k] = self.reward_function(weights_population)\\n\\n        rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n        self.adaptive_sigma(rewards)  # Adjust sigma based on rewards\\n\\n        for index, w in enumerate(self.weights):\\n            A = np.array([p[index] for p in population])\\n            self.weights[index] += (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T, rewards).T\\n\\n        if (i + 1) % print_every == 0:\\n            avg_reward = np.mean(rewards)\\n            print(f\\'Epoch {i + 1}/{epoch}, Average Reward: {avg_reward}\\')\\n\\n            if avg_reward < previous_mean_reward:\\n                no_improvement_count += 1\\n            else:\\n                no_improvement_count = 0\\n                previous_mean_reward = avg_reward\\n\\n            if no_improvement_count >= patience:\\n                print(\"Early stopping...\")\\n                break\\n\\n    print(\\'Time taken to train:\\', time.time() - lasttime, \\'seconds\\')\\n', 'AttributeError: \\'Deep_Evolution_Strategy\\' object has no attribute \\'adaptive_sigma\\' on teh following code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\nimport numpy as np\\nimport time\\n\\n\\nclass Deep_Evolution_Strategy():\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def train(self, epoch=200, print_every=1, update_frequency=5, patience=10):\\n        lasttime = time.time()\\n        previous_mean_reward = -np.inf\\n        no_improvement_count = 0\\n\\n        for i in range(epoch):\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = [np.random.randn(*w.shape) + np.random.uniform(-0.01, 0.01, w.shape) for w in self.weights]\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            self.adaptive_sigma(rewards)  # Adjust sigma based on rewards\\n\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] += (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T,\\n                                                                                                           rewards).T\\n\\n            if (i + 1) % print_every == 0:\\n                avg_reward = np.mean(rewards)\\n                print(f\\'Epoch {i + 1}/{epoch}, Average Reward: {avg_reward}\\')\\n\\n                if avg_reward < previous_mean_reward:\\n                    no_improvement_count += 1\\n                else:\\n                    no_improvement_count = 0\\n                    previous_mean_reward = avg_reward\\n\\n                if no_improvement_count >= patience:\\n                    print(\"Early stopping...\")\\n                    break\\n\\n        print(\\'Time taken to train:\\', time.time() - lasttime, \\'seconds\\')\\n\\n\\n', 'add a progress bar for the train on the following code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\nimport numpy as np\\nimport time\\n\\n\\nclass Deep_Evolution_Strategy():\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def adaptive_sigma(self, rewards):\\n        \"\"\"Adjust sigma based on the mean and standard deviation of rewards.\"\"\"\\n        mean_reward = np.mean(rewards)\\n        std_reward = np.std(rewards)\\n        if std_reward > 0:\\n            self.sigma *= 1 + (mean_reward / std_reward)  # Adjust sigma based on performance\\n\\n    def train(self, epoch=200, print_every=1, update_frequency=5, patience=10):\\n        previous_mean_reward = -np.inf\\n        no_improvement_count = 0\\n\\n        for i in range(epoch):\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = [np.random.randn(*w.shape) + np.random.uniform(-0.01, 0.01, w.shape) for w in self.weights]\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            self.adaptive_sigma(rewards)  # Adjust sigma based on rewards\\n\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] += (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T, rewards).T\\n\\n            if (i + 1) % print_every == 0:\\n                avg_reward = np.mean(rewards)\\n                print(f\\'Epoch {i + 1}/{epoch}, Average Reward: {avg_reward}\\')\\n\\n                if avg_reward < previous_mean_reward:\\n                    no_improvement_count += 1\\n                else:\\n                    no_improvement_count = 0\\n                    previous_mean_reward = avg_reward\\n\\n                if no_improvement_count >= patience:\\n                    print(\"Early stopping...\")\\n                    break\\n', \"add the Epsilon-Greedy with a small noise to train on the following code:\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\nimport numpy as np\\nimport time\\n\\nclass Deep_Evolution_Strategy():\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def train(self, epoch=200, print_every=1):\\n        lasttime = time.time()\\n        for i in range(epoch):\\n            # logging.debug(f'Starting epoch {i+1}/{epoch}')\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = []\\n                for w in self.weights:\\n                    x.append(np.random.randn(*w.shape))\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] = w + (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T, rewards).T\\n                if (i + 1) % print_every == 0:\\n                    print(\\n                        'iter %d. reward: %f'\\n                        % (i + 1, self.reward_function(self.weights))\\n                    )\\n                    break\\n            print('time taken to train:', time.time() - lasttime, 'seconds')\\n\\n\\n\", 'Check the following code for error, and also make the buy and hold the same for sell and hold on the \"def get_tick_reward(self, weights):\"\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.01\\n    EPSILON = 0.1  # Exploration rate\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            if np.random.rand() < self.EPSILON:  # Explore\\n                action = np.random.choice([0, 1, 2])  # Randomly choose an action\\n            else:  # Exploit\\n                decision = self.model.predict(np.array(sequence))\\n                action = np.argmax(decision[0])\\n\\n            # Add small noise to the action for exploration\\n            noise = np.random.normal(0, 0.1)  # Small noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5)\\n                        earned_money += -5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5)\\n                        earned_money += -5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        # Example: Increase sigma or modify weights\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return ((earned_money) / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            self.es.train(iterations, print_every=checkpoint)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n', 'Is the buy and hold the same for the sell and hold also for the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.01\\n    EPSILON = 0.1  # Exploration rate\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            if np.random.rand() < self.EPSILON:  # Explore\\n                action = np.random.choice([0, 1, 2])  # Randomly choose an action\\n            else:  # Exploit\\n                decision = self.model.predict(np.array(sequence))\\n                action = np.argmax(decision[0])\\n\\n            # Add small noise to the action for exploration\\n            noise = np.random.normal(0, 0.1)  # Small noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]  # Uncomment if needed\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return ((earned_money) / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            self.es.train(iterations, print_every=checkpoint)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n', 'Improved the get_tick_reward method to ensure that the buy and hold logic is consistent with the sell and hold logic on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.01\\n    EPSILON = 0.1  # Exploration rate\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            if np.random.rand() < self.EPSILON:  # Explore\\n                action = np.random.choice([0, 1, 2])  # Randomly choose an action\\n            else:  # Exploit\\n                decision = self.model.predict(np.array(sequence))\\n                action = np.argmax(decision[0])\\n\\n            # Add small noise to the action for exploration\\n            noise = np.random.normal(0, 0.1)  # Small noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]  # Uncomment if needed\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return ((earned_money) / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            self.es.train(iterations, print_every=checkpoint)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n', 'How can I update an existing .pkl model with new trained data on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n\\ndef run_retrading_simulation(model):\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_count = (len(df1) + 1)  # get the number of rows on the csv file\\n    calc_perc = int(rows_count * .10)  # calculate 20% from the rows_count\\n    rows_toTrain = int(rows_count - calc_perc)\\n    df1 = df1.iloc[rows_toTrain - 2:, :].copy()\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract and scale the selected indicators\\n    indicators = {indicator: StandardScaler().fit_transform(df1[indicator].values.reshape(-1, 1)).flatten() for indicator in selected_indicators}\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump({indicator: StandardScaler() for indicator in selected_indicators}, \\'output/scalers.pkl\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    # model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                              indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n    # Save the model\\n    # torch.save(model, \\'output/model.pkl\\')\\n    joblib.save(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n\\nif __name__==\\'__main__\\':\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n    run_retrading_simulation(model)\\n', \"optimize and check for errors on the following code:\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef run_retrading_simulation(model):\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        'rsi', 'adx', 'momentum', 'ppoValue', 'ppoSmoothed', 'QQEDiff1', 'QQEDiff2', 'QQEDiff3'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv('data/dataFile.csv')\\n    rows_count = (len(df1) + 1)  # get the number of rows on the csv file\\n    calc_perc = int(rows_count * .10)  # calculate 10% from the rows_count\\n    rows_toTrain = int(rows_count - calc_perc)\\n    df1 = df1.iloc[rows_toTrain - 2:, :].copy()\\n\\n    # Ensure 'Date' column exists\\n    if 'Date' not in df1.columns:\\n        df1['Date'] = pd.date_range(start='1/1/2000', periods=len(df1), freq='D')\\n\\n    # Convert columns to float, excluding the 'Date' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace('.', '', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != 'Date':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract and scale the selected indicators\\n    indicators = {indicator: StandardScaler().fit_transform(df1[indicator].values.reshape(-1, 1)).flatten() for indicator in selected_indicators}\\n    close = df1['Close'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump({indicator: StandardScaler() for indicator in selected_indicators}, 'output/scalers.pkl')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                              indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n    # Save the updated model\\n    joblib.dump(model, 'output/model.pkl')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color='black', alpha=0.5, lw=1.)\\n    plt.plot(close, '.', markersize=10, color='green', alpha=0.5, label='buying signal', markevery=states_buy)\\n    plt.plot(close, '.', markersize=10, color='red', alpha=0.5, label='selling signal', markevery=states_sell)\\n    plt.title(f'Total gains: {total_gains}, Total investment: {invest}%')\\n    plt.legend()\\n    plt.show()\\n\\nif __name__=='__main__':\\n    # Load the trained model\\n    model = joblib.load('output/model.pkl')\\n    run_retrading_simulation(model)\", \"add try-except blocks around file operations and data processing to handle potential errors gracefully. Consider using logging instead of print statements for better tracking of the program's execution, especially in production environments.\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nfrom modules import agent_function as agent_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef run_retrading_simulation(model):\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        'rsi', 'adx', 'momentum', 'ppoValue', 'ppoSmoothed', 'QQEDiff1', 'QQEDiff2', 'QQEDiff3'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv('data/dataFile.csv')\\n\\n    # Calculate the number of rows to train\\n    rows_count = len(df1)\\n    rows_toTrain = int(rows_count * 0.90)  # Use 90% of the data for training\\n    df1 = df1.iloc[rows_toTrain - 2:].copy()\\n\\n    # Ensure 'Date' column exists\\n    if 'Date' not in df1.columns:\\n        df1['Date'] = pd.date_range(start='1/1/2000', periods=len(df1), freq='D')\\n\\n    # Convert columns to float, excluding the 'Date' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace('.', '', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != 'Date':\\n            df1[col] = convert_to_float(df1[col].tolist())\\n\\n    # Extract and scale the selected indicators\\n    indicators = {indicator: StandardScaler().fit_transform(df1[indicator].values.reshape(-1, 1)).flatten() for indicator in selected_indicators}\\n    close = df1['Close'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump({indicator: StandardScaler() for indicator in selected_indicators}, 'output/scalers.pkl')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                              indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n    # Save the updated model\\n    joblib.dump(model, 'output/model.pkl')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color='black', alpha=0.5, lw=1.)\\n    plt.plot(close, '.', markersize=10, color='green', alpha=0.5, label='buying signal', markevery=states_buy)\\n    plt.plot(close, '.', markersize=10, color='red', alpha=0.5, label='selling signal', markevery=states_sell)\\n    plt.title(f'Total gains: {total_gains}, Total investment: {invest}%')\\n    plt.legend()\\n    plt.show()\\n\\nif __name__ == '__main__':\\n    # Load the trained model\\n    model = joblib.load('output/model.pkl')\\n    run_retrading_simulation(model)\", 'Check the following for errors:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n------------------------------------------------------------------------------------------------------------------------\\n\\n\"\"\"\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nfrom sklearn.preprocessing import StandardScaler\\n\\napp = Flask(__name__)\\n\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    data = request.get_json()\\n    trend = data[\\'trend\\']\\n    indicators = data[\\'indicators\\']\\n\\n    # # Load the scalers\\n    # scalers = joblib.load(\\'output/scalers.pkl\\')\\n\\n    # Extract and scale the selected indicators\\n    indicators = {indicator: StandardScaler().fit_transform(indicators[indicators].values.reshape(-1, 1)).flatten() for\\n                  indicator in indicators[indicators]}\\n\\n    # Scale the indicators using the loaded scalers\\n    # for indicator in indicators.keys():\\n    #     if indicator in scalers:\\n    #         indicators[indicator] = scalers[indicator].transform(np.array(indicators[indicator]).reshape(-1, 1)).flatten()\\n\\n    # Now indicators are scaled and flattened\\n    t = int(data[\\'t\\'])\\n    window_size = int(data[\\'window_size\\'])\\n\\n    state = get_state(trend, indicators, t, window_size)\\n    decision = model.predict(state)\\n    action = np.argmax(decision[0])\\n\\n    response = {\\'action\\': int(action)}\\n    return jsonify(response)\\n\\n\\nif __name__==\\'__main__\\':\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n    app.run(host=\"0.0.0.0\", debug=False, port=5010)', 'When the following api code is called, how can I also trigger the re-training with the new data that was sent to the api for prediction without adding any delays to it\\n\\n\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nimport logging\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\napp = Flask(__name__)\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    try:\\n        data = request.get_json()\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        # Load the scalers\\n        scalers = joblib.load(\\'output/scalers.pkl\\')\\n        logging.info(\"Scalers loaded successfully.\")\\n\\n        # Scale the indicators using the loaded scalers\\n        for indicator in indicators.keys():\\n            if indicator in scalers:\\n                # Check if the scaler is fitted\\n                if hasattr(scalers[indicator], \\'scale_\\'):\\n                    indicators[indicator] = scalers[indicator].transform(np.array(indicators[indicator]).reshape(-1, 1)).flatten()\\n                else:\\n                    logging.error(f\"Scaler for {indicator} is not fitted.\")\\n                    return jsonify({\\'error\\': f\\'Scaler for {indicator} is not fitted.\\'}), 500\\n            else:\\n                logging.warning(f\"Scaler for {indicator} not found.\")\\n\\n        # Now indicators are scaled and flattened\\n        t = int(data[\\'t\\'])\\n        window_size = int(data[\\'window_size\\'])\\n\\n        state = get_state(trend, indicators, t, window_size)\\n        decision = model.predict(state)\\n        action = np.argmax(decision[0])\\n\\n        response = {\\'action\\': int(action)}\\n        return jsonify(response)\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n        return jsonify({\\'error\\': \\'File not found\\'}), 500\\n    except KeyError as e:\\n        logging.error(f\"Key error: {e}\")\\n        return jsonify({\\'error\\': \\'Key error\\'}), 400\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n        return jsonify({\\'error\\': \\'An error occurred\\'}), 500\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        app.run(host=\"0.0.0.0\", debug=False, port=5000)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")', 'trigger re-training with the new data sent to the API for prediction without introducing delays, when a decrease in performance metrics average rewards.\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nimport logging\\nimport threading\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\napp = Flask(__name__)\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\ndef retrain_model(data):\\n    # This function will handle the re-training logic\\n    try:\\n        # Load the new data for re-training\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        # Load the scalers\\n        scalers = joblib.load(\\'output/scalers.pkl\\')\\n\\n        # Scale the indicators using the loaded scalers\\n        for indicator in indicators.keys():\\n            if indicator in scalers:\\n                indicators[indicator] = scalers[indicator].transform(np.array(indicators[indicator]).reshape(-1, 1)).flatten()\\n\\n        # Here you would add your model training logic\\n        # For example:\\n        # model.fit(new_data, new_labels)\\n\\n        # Save the updated model\\n        joblib.dump(model, \\'output/model.pkl\\')\\n        logging.info(\"Model re-trained and saved successfully.\")\\n\\n    except Exception as e:\\n        logging.error(f\"An error occurred during re-training: {e}\")\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    try:\\n        data = request.get_json()\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        # Load the scalers\\n        scalers = joblib.load(\\'output/scalers.pkl\\')\\n        logging.info(\"Scalers loaded successfully.\")\\n\\n        # Scale the indicators using the loaded scalers\\n        for indicator in indicators.keys():\\n            if indicator in scalers:\\n                if hasattr(scalers[indicator], \\'scale_\\'):\\n                    indicators[indicator] = scalers[indicator].transform(np.array(indicators[indicator]).reshape(-1, 1)).flatten()\\n                else:\\n                    logging.error(f\"Scaler for {indicator} is not fitted.\")\\n                    return jsonify({\\'error\\': f\\'Scaler for {indicator} is not fitted.\\'}), 500\\n            else:\\n                logging.warning(f\"Scaler for {indicator} not found.\")\\n\\n        # Now indicators are scaled and flattened\\n        t = int(data[\\'t\\'])\\n        window_size = int(data[\\'window_size\\'])\\n\\n        state = get_state(trend, indicators, t, window_size)\\n        decision = model.predict(state)\\n        action = np.argmax(decision[0])\\n\\n        # Start re-training in a background thread\\n        threading.Thread(target=retrain_model, args=(data,)).start()\\n\\n        response = {\\'action\\': int(action)}\\n        return jsonify(response)\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n        return jsonify({\\'error\\': \\'File not found\\'}), 500\\n    except KeyError as e:\\n        logging.error(f\"Key error: {e}\")\\n        return jsonify({\\'error\\': \\'Key error\\'}), 400\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n        return jsonify({\\'error\\': \\'An error occurred\\'}), 500\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        app.run(host=\"0.0.0.0\", debug=False, port=5000)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")', 'Check for errors on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n------------------------------------------------------------------------------------------------------------------------\\n\\n\"\"\"\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nimport logging\\nimport threading\\nfrom sklearn.preprocessing import StandardScaler\\nfrom modules import agent_function as agent_funct\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\napp = Flask(__name__)\\n\\n# Global variable to track average rewards\\naverage_rewards = 0.0\\nperformance_threshold = 0.5  # Set your threshold for average rewards\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\ndef retrain_model(data):\\n    try:\\n        # Load the new data for re-training\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        # Load the scalers\\n        scalers = joblib.load(\\'output/scalers.pkl\\')\\n\\n        # Scale the indicators using the loaded scalers\\n        for indicator in indicators.keys():\\n            if indicator in scalers:\\n                indicators[indicator] = scalers[indicator].transform(np.array(indicators[indicator]).reshape(-1, 1)).flatten()\\n\\n        # Here you would add your model training logic\\n        # For example:\\n        # model.fit(new_data, new_labels)\\n        # Extract and scale the selected indicators\\n\\n        close = df1[\\'Close\\'].values.flatten()\\n\\n        # Save the scalers for future use\\n        joblib.dump(scalers, \\'output/scalers.pkl\\')\\n        logging.info(\"Scalers saved successfully.\")\\n\\n        # Initialize agent\\n        window_size, skip, initial_money = 30, 1, 10000\\n        agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                                  indicators=indicators)\\n\\n        # Train the agent\\n        agent.fit(iterations=500, checkpoint=10)\\n        logging.info(\"Agent training completed successfully.\")\\n\\n        # Perform predictions\\n        states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n        # Save the updated model\\n        joblib.dump(model, \\'output/model.pkl\\')\\n        logging.info(\"Model re-trained and saved successfully.\")\\n\\n    except Exception as e:\\n        logging.error(f\"An error occurred during re-training: {e}\")\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    global average_rewards  # Use the global variable to track average rewards\\n    try:\\n        data = request.get_json()\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        # Load the scalers\\n        scalers = joblib.load(\\'output/scalers.pkl\\')\\n        logging.info(\"Scalers loaded successfully.\")\\n\\n        # Scale the indicators using the loaded scalers\\n        for indicator in indicators.keys():\\n            if indicator in scalers:\\n                if hasattr(scalers[indicator], \\'scale_\\'):\\n                    indicators[indicator] = scalers[indicator].transform(np.array(indicators[indicator]).reshape(-1, 1)).flatten()\\n                else:\\n                    logging.error(f\"Scaler for {indicator} is not fitted.\")\\n                    return jsonify({\\'error\\': f\\'Scaler for {indicator} is not fitted.\\'}), 500\\n            else:\\n                logging.warning(f\"Scaler for {indicator} not found.\")\\n\\n        # Now indicators are scaled and flattened\\n        t = int(data[\\'t\\'])\\n        window_size = int(data[\\'window_size\\'])\\n\\n        state = get_state(trend, indicators, t, window_size)\\n        decision = model.predict(state)\\n        action = np.argmax(decision[0])\\n\\n        # Update average rewards based on the decision (this is just an example)\\n        # You should replace this with your actual logic to calculate average rewards\\n        new_reward = np.random.rand()  # Simulated reward for demonstration\\n        average_rewards = (average_rewards + new_reward) / 2  # Update average rewards\\n\\n        # Check if average rewards are below the threshold\\n        if average_rewards < performance_threshold:\\n            logging.info(\"Average rewards below threshold, triggering re-training.\")\\n            threading.Thread(target=retrain_model, args=(data,)).start()\\n\\n        response = {\\'action\\': int(action)}\\n        return jsonify(response)\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n        return jsonify({\\'error\\': \\'File not found\\'}), 500\\n    except KeyError as e:\\n        logging.error(f\"Key error: {e}\")\\n        return jsonify({\\'error\\': \\'Key error\\'}), 400\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n        return jsonify({\\'error\\': \\'An error occurred\\'}), 500\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        app.run(host=\"0.0.0.0\", debug=False, port=5000)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")', 'Save the scalers after the retrain is done, When using threading, ensure that the model and scalers are accessed in a thread-safe manner. If multiple threads might access or modify shared resources, consider using locks. consider adding more specific error handling for different types of exceptions, especially around model training and data processing.\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nimport logging\\nimport threading\\nfrom sklearn.preprocessing import StandardScaler\\nfrom modules import agent_function as agent_funct\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\napp = Flask(__name__)\\n\\n# Global variable to track average rewards\\naverage_rewards = 0.0\\nperformance_threshold = 0.5  # Set your threshold for average rewards\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\ndef retrain_model(data):\\n    global average_rewards  # Ensure we can access the global variable\\n    try:\\n        # Load the new data for re-training\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        # Load the scalers\\n        scalers = joblib.load(\\'output/scalers.pkl\\')\\n\\n        # Scale the indicators using the loaded scalers\\n        for indicator in indicators.keys():\\n            if indicator in scalers:\\n                indicators[indicator] = scalers[indicator].transform(np.array(indicators[indicator]).reshape(-1, 1)).flatten()\\n\\n        # Here you would add your model training logic\\n        # For example:\\n        # model.fit(new_data, new_labels)\\n\\n        # Assuming you have a way to get the \\'Close\\' prices for training\\n        close = trend  # Replace with actual close prices if needed\\n\\n        # Initialize agent\\n        window_size, skip, initial_money = 30, 1, 10000\\n        agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                                  indicators=indicators)\\n\\n        # Train the agent\\n        agent.fit(iterations=500, checkpoint=10)\\n        logging.info(\"Agent training completed successfully.\")\\n\\n        # Perform predictions\\n        states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n        # Save the updated model\\n        joblib.dump(model, \\'output/model.pkl\\')\\n        logging.info(\"Model re-trained and saved successfully.\")\\n\\n        # Optionally, update average rewards based on training results\\n        # average_rewards = calculate_average_rewards(states_buy, states_sell)  # Implement this function\\n\\n    except Exception as e:\\n        logging.error(f\"An error occurred during re-training: {e}\")\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    global average_rewards  # Use the global variable to track average rewards\\n    try:\\n        data = request.get_json()\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        # Load the scalers\\n        scalers = joblib.load(\\'output/scalers.pkl\\')\\n        logging.info(\"Scalers loaded successfully.\")\\n\\n        # Scale the indicators using the loaded scalers\\n        for indicator in indicators.keys():\\n            if indicator in scalers:\\n                if hasattr(scalers[indicator], \\'scale_\\'):\\n                    indicators[indicator] = scalers[indicator].transform(np.array(indicators[indicator]).reshape(-1, 1)).flatten()\\n                else:\\n                    logging.error(f\"Scaler for {indicator} is not fitted.\")\\n                    return jsonify({\\'error\\': f\\'Scaler for {indicator} is not fitted.\\'}), 500\\n            else:\\n                logging.warning(f\"Scaler for {indicator} not found.\")\\n\\n        # Now indicators are scaled and flattened\\n        t = int(data[\\'t\\'])\\n        window_size = int(data[\\'window_size\\'])\\n\\n        state = get_state(trend, indicators, t, window_size)\\n        decision = model.predict(state)\\n        action = np.argmax(decision[0])\\n\\n        # Update average rewards based on the decision (this is just an example)\\n        new_reward = np.random.rand()  # Simulated reward for demonstration\\n        average_rewards = (average_rewards + new_reward) / 2  # Update average rewards\\n\\n        # Check if average rewards are below the threshold\\n        if average_rewards < performance_threshold:\\n            logging.info(\"Average rewards below threshold, triggering re-training.\")\\n            threading.Thread(target=retrain_model, args=(data,)).start()\\n\\n        response = {\\'action\\': int(action)}\\n        return jsonify(response)\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n        return jsonify({\\'error\\': \\'File not found\\'}), 500\\n    except KeyError as e:\\n        logging.error(f\"Key error: {e}\")\\n        return jsonify({\\'error\\': \\'Key error\\'}), 400\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n        return jsonify({\\'error\\': \\'An error occurred\\'}), 500\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        app.run(host=\"0.0.0.0\", debug=False, port=5000)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")', 'If the rewards are being generated from the agent.py based on past performance how can i get the average_rewards for it to work in order for the re-train to be trigger on the following code:\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nimport logging\\nimport threading\\nfrom sklearn.preprocessing import StandardScaler\\nfrom modules import agent_function as agent_funct\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\napp = Flask(__name__)\\n\\n# Global variable to track average rewards\\naverage_rewards = 0.0\\nperformance_threshold = 0.5  # Set your threshold for average rewards\\n\\n# Threading lock for thread-safe access to model and scalers\\nlock = threading.Lock()\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\ndef retrain_model(data):\\n    global average_rewards  # Ensure we can access the global variable\\n    try:\\n        # Load the new data for re-training\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        # Load the scalers\\n        with lock:\\n            scalers = joblib.load(\\'output/scalers.pkl\\')\\n\\n        # Scale the indicators using the loaded scalers\\n        for indicator in indicators.keys():\\n            if indicator in scalers:\\n                indicators[indicator] = scalers[indicator].transform(np.array(indicators[indicator]).reshape(-1, 1)).flatten()\\n\\n        # Here you would add your model training logic\\n        # For example:\\n        # model.fit(new_data, new_labels)\\n\\n        # Assuming you have a way to get the \\'Close\\' prices for training\\n        close = trend  # Replace with actual close prices if needed\\n\\n        # Initialize agent\\n        window_size, skip, initial_money = 30, 1, 10000\\n        agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                                  indicators=indicators)\\n\\n        # Train the agent\\n        agent.fit(iterations=500, checkpoint=10)\\n        logging.info(\"Agent training completed successfully.\")\\n\\n        # Perform predictions\\n        states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n        # Save the updated model\\n        with lock:\\n            joblib.dump(model, \\'output/model.pkl\\')\\n            joblib.dump(scalers, \\'output/scalers.pkl\\')  # Save scalers after retraining\\n            logging.info(\"Model and scalers re-trained and saved successfully.\")\\n\\n        # Optionally, update average rewards based on training results\\n        # average_rewards = calculate_average_rewards(states_buy, states_sell)  # Implement this function\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found during retraining: {e}\")\\n    except ValueError as e:\\n        logging.error(f\"Value error during retraining: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred during re-training: {e}\")\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    global average_rewards  # Use the global variable to track average rewards\\n    try:\\n        data = request.get_json()\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        # Load the scalers\\n        with lock:\\n            scalers = joblib.load(\\'output/scalers.pkl\\')\\n        logging.info(\"Scalers loaded successfully.\")\\n\\n        # Scale the indicators using the loaded scalers\\n        for indicator in indicators.keys():\\n            if indicator in scalers:\\n                if hasattr(scalers[indicator], \\'scale_\\'):\\n                    indicators[indicator] = scalers[indicator].transform(np.array(indicators[indicator]).reshape(-1, 1)).flatten()\\n                else:\\n                    logging.error(f\"Scaler for {indicator} is not fitted.\")\\n                    return jsonify({\\'error\\': f\\'Scaler for {indicator} is not fitted.\\'}), 500\\n            else:\\n                logging.warning(f\"Scaler for {indicator} not found.\")\\n\\n        # Now indicators are scaled and flattened\\n        t = int(data[\\'t\\'])\\n        window_size = int(data[\\'window_size\\'])\\n\\n        state = get_state(trend, indicators, t, window_size)\\n        decision = model.predict(state)\\n        action = np.argmax(decision[0])\\n\\n        # Update average rewards based on the decision (this is just an example)\\n        new_reward = np.random.rand()  # Simulated reward for demonstration\\n        average_rewards = (average_rewards + new_reward) / 2  # Update average rewards\\n\\n        # Check if average rewards are below the threshold\\n        if average_rewards < performance_threshold:\\n            logging.info(\"Average rewards below threshold, triggering re-training.\")\\n            threading.Thread(target=retrain_model, args=(data,)).start()\\n\\n        response = {\\'action\\': int(action)}\\n        return jsonify(response)\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n        return jsonify({\\'error\\': \\'File not found\\'}), 500\\n    except KeyError as e:\\n        logging.error(f\"Key error: {e}\")\\n        return jsonify({\\'error\\': \\'Key error\\'}), 400\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n        return jsonify({\\'error\\': \\'An error occurred\\'}), 500\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        with lock:\\n            model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        app.run(host=\"0.0.0.0\", debug=False, port=5000)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")', 'Make sure that the rewards are return during training on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]  # Uncomment if needed\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return ((earned_money) / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise', 'ensure that the rewards are returned during training on the following code:\\n\\nimport numpy as np\\nimport time\\n\\n\\nclass Deep_Evolution_Strategy():\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def adaptive_sigma(self, rewards):\\n        \"\"\"Adjust sigma based on the mean and standard deviation of rewards.\"\"\"\\n        mean_reward = np.mean(rewards)\\n        std_reward = np.std(rewards)\\n        if std_reward > 0:\\n            self.sigma *= 1 + (mean_reward / std_reward)  # Adjust sigma based on performance\\n\\n    def train(self, epoch=200, print_every=1, update_frequency=5, patience=10):\\n        previous_mean_reward = -np.inf\\n        no_improvement_count = 0\\n\\n        for i in range(epoch):\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = [np.random.randn(*w.shape) + np.random.uniform(-0.01, 0.01, w.shape) for w in self.weights]\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            self.adaptive_sigma(rewards)  # Adjust sigma based on rewards\\n\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] += (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T, rewards).T\\n\\n            if (i + 1) % print_every == 0:\\n                avg_reward = np.mean(rewards)\\n                print(f\\'Epoch {i + 1}/{epoch}, Average Reward: {avg_reward}\\')\\n\\n                if avg_reward < previous_mean_reward:\\n                    no_improvement_count += 1\\n                else:\\n                    no_improvement_count = 0\\n                    previous_mean_reward = avg_reward\\n\\n                if no_improvement_count >= patience:\\n                    print(\"Early stopping...\")\\n                    break\\n', 'create the code', 'Do I need for the scalers to be fitted on the following code:\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n------------------------------------------------------------------------------------------------------------------------\\n\\n\"\"\"\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nimport logging\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\napp = Flask(__name__)\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    try:\\n        data = request.get_json()\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        # Load the scalers\\n        scalers = joblib.load(\\'output/scalers.pkl\\')\\n        logging.info(\"Scalers loaded successfully.\")\\n\\n        # Scale the indicators using the loaded scalers\\n        for indicator in indicators.keys():\\n            if indicator in scalers:\\n                # Check if the scaler is fitted\\n                if hasattr(scalers[indicator], \\'scale_\\'):\\n                    indicators[indicator] = scalers[indicator].transform(np.array(indicators[indicator]).reshape(-1, 1)).flatten()\\n                else:\\n                    logging.error(f\"Scaler for {indicator} is not fitted.\")\\n                    return jsonify({\\'error\\': f\\'Scaler for {indicator} is not fitted.\\'}), 500\\n            else:\\n                logging.warning(f\"Scaler for {indicator} not found.\")\\n\\n        # Now indicators are scaled and flattened\\n        t = int(data[\\'t\\'])\\n        window_size = int(data[\\'window_size\\'])\\n\\n        state = get_state(trend, indicators, t, window_size)\\n        decision = model.predict(state)\\n        action = np.argmax(decision[0])\\n\\n        response = {\\'action\\': int(action)}\\n        return jsonify(response)\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n        return jsonify({\\'error\\': \\'File not found\\'}), 500\\n    except KeyError as e:\\n        logging.error(f\"Key error: {e}\")\\n        return jsonify({\\'error\\': \\'Key error\\'}), 400\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n        return jsonify({\\'error\\': \\'An error occurred\\'}), 500\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        app.run(host=\"0.0.0.0\", debug=False, port=5000)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")', 'How can we re-train every 15 minutes with the new data that is provided for prediction without introducing delays on the following code:\\n\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nimport logging\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\napp = Flask(__name__)\\n\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    try:\\n        data = request.get_json()\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        # Load the scalers\\n        scalers = joblib.load(\\'output/scalers.pkl\\')\\n        logging.info(\"Scalers loaded successfully.\")\\n\\n        # Scale the indicators using the loaded scalers\\n        for indicator in indicators.keys():\\n            if indicator in scalers:\\n                # Check if the scaler is fitted\\n                if hasattr(scalers[indicator], \\'scale_\\'):\\n                    indicators[indicator] = scalers[indicator].transform(np.array(indicators[indicator]).reshape(-1, 1)).flatten()\\n                else:\\n                    logging.error(f\"Scaler for {indicator} is not fitted.\")\\n                    return jsonify({\\'error\\': f\\'Scaler for {indicator} is not fitted.\\'}), 500\\n            else:\\n                logging.warning(f\"Scaler for {indicator} not found.\")\\n\\n        # Now indicators are scaled and flattened\\n        t = int(data[\\'t\\'])\\n        window_size = int(data[\\'window_size\\'])\\n\\n        state = get_state(trend, indicators, t, window_size)\\n        decision = model.predict(state)\\n        action = np.argmax(decision[0])\\n\\n        response = {\\'action\\': int(action)}\\n        return jsonify(response)\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n        return jsonify({\\'error\\': \\'File not found\\'}), 500\\n    except KeyError as e:\\n        logging.error(f\"Key error: {e}\")\\n        return jsonify({\\'error\\': \\'Key error\\'}), 400\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n        return jsonify({\\'error\\': \\'An error occurred\\'}), 500\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        app.run(host=\"0.0.0.0\", debug=False, port=5000)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")', 'What else can I provide to the following Deep Reinforcement Learning environment make it more robust:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract and scale the selected indicators\\n    scalers = {}\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        scaler = StandardScaler()\\n        indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n        scalers[indicator] = scaler  # Save the fitted scaler\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n    # Define the step-by-step prediction function\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                print(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                print(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract and scale the selected indicators for testing\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = scalers[indicator].transform(df2[indicator].values.reshape(-1, 1)).flatten()\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]  # Uncomment if needed\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return ((earned_money) / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\nimport numpy as np\\n\\nclass Deep_Evolution_Strategy():\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def adaptive_sigma(self, rewards):\\n        \"\"\"Adjust sigma based on the mean and standard deviation of rewards.\"\"\"\\n        mean_reward = np.mean(rewards)\\n        std_reward = np.std(rewards)\\n        if std_reward > 0:\\n            self.sigma *= 1 + (mean_reward / std_reward)  # Adjust sigma based on performance\\n\\n    def train(self, epoch=200, print_every=1, update_frequency=5, patience=10):\\n        previous_mean_reward = -np.inf\\n        no_improvement_count = 0\\n        all_rewards = []  # To store rewards from all epochs\\n\\n        for i in range(epoch):\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = [np.random.randn(*w.shape) + np.random.uniform(-0.01, 0.01, w.shape) for w in self.weights]\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            self.adaptive_sigma(rewards)  # Adjust sigma based on rewards\\n\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] += (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T, rewards).T\\n\\n            # Store the average reward for this epoch\\n            avg_reward = np.mean(rewards)\\n            all_rewards.append(avg_reward)\\n\\n            if (i + 1) % print_every == 0:\\n                print(f\\'Epoch {i + 1}/{epoch}, Average Reward: {avg_reward}\\')\\n\\n                if avg_reward < previous_mean_reward:\\n                    no_improvement_count += 1\\n                else:\\n                    no_improvement_count = 0\\n                    previous_mean_reward = avg_reward\\n\\n                if no_improvement_count >= patience:\\n                    print(\"Early stopping...\")\\n                    break\\n\\n        return all_rewards  # Return all rewards collected during training\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\nimport numpy as np\\n\\n\\nclass Model:\\n    def __init__(self, input_size: int, layer_size: int, output_size: int):\\n        \"\"\"\\n        Initializes the model with random weights.\\n\\n        Parameters:\\n        - input_size: Number of input features.\\n        - layer_size: Number of neurons in the hidden layer.\\n        - output_size: Number of output classes.\\n        \"\"\"\\n        self.weights = [\\n            np.random.normal(0, 0.1, (input_size, layer_size)),  # Input to hidden layer weights\\n            np.random.normal(0, 0.1, (layer_size, output_size)),  # Hidden to output layer weights\\n        ]\\n        self.biases = [\\n            np.random.normal(0, 0.1, (1, layer_size)),  # Bias for hidden layer\\n            np.random.normal(0, 0.1, (1, output_size)),  # Bias for output layer\\n        ]\\n\\n    def predict(self, inputs: np.ndarray) -> np.ndarray:\\n        \"\"\"\\n        Makes a prediction based on the input data.\\n\\n        Parameters:\\n        - inputs: Input data for prediction.\\n\\n        Returns:\\n        - Decision: Output predictions.\\n        \"\"\"\\n        feed = np.dot(inputs, self.weights[0]) + self.biases[0]\\n        decision = np.dot(feed, self.weights[1]) + self.biases[1]\\n        return decision\\n\\n    def get_weights(self) -> list:\\n        \"\"\"Returns the current weights of the model.\"\"\"\\n        return self.weights\\n\\n    def set_weights(self, weights: list):\\n        \"\"\"\\n        Sets the weights of the model.\\n\\n        Parameters:\\n        - weights: New weights to set for the model.\\n        \"\"\"\\n        if len(weights) != len(self.weights):\\n            raise ValueError(\"Weights must have the same length as the current weights.\")\\n\\n        for i in range(len(weights)):\\n            if weights[i].shape != self.weights[i].shape:\\n                raise ValueError(\\n                    f\"Weight matrix {i} must have shape {self.weights[i].shape}, but got {weights[i].shape}.\")\\n\\n        self.weights = weights', 'Modify the reward function to account for risk. For example, use the Sharpe Ratio or Sortino Ratio to evaluate the performance of trades, rewarding the agent for achieving higher risk-adjusted returns.\\n Introduce a penalty for holding positions too long without action, encouraging the agent to make timely decisions.\\nImplement an experience replay buffer to store past experiences (state, action, reward, next state). This allows the agent to learn from past actions and improve stability during training.\\nUse an adaptive learning rate strategy (e.g., Adam optimizer) to adjust the learning rate during training based on the performance of the model.\\nfor the following codes:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract and scale the selected indicators\\n    scalers = {}\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        scaler = StandardScaler()\\n        indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n        scalers[indicator] = scaler  # Save the fitted scaler\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n    # Define the step-by-step prediction function\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                print(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                print(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract and scale the selected indicators for testing\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = scalers[indicator].transform(df2[indicator].values.reshape(-1, 1)).flatten()\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]  # Uncomment if needed\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return ((earned_money) / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                # cci = self.indicators[\\'cci\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\nimport numpy as np\\n\\nclass Deep_Evolution_Strategy():\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def adaptive_sigma(self, rewards):\\n        \"\"\"Adjust sigma based on the mean and standard deviation of rewards.\"\"\"\\n        mean_reward = np.mean(rewards)\\n        std_reward = np.std(rewards)\\n        if std_reward > 0:\\n            self.sigma *= 1 + (mean_reward / std_reward)  # Adjust sigma based on performance\\n\\n    def train(self, epoch=200, print_every=1, update_frequency=5, patience=10):\\n        previous_mean_reward = -np.inf\\n        no_improvement_count = 0\\n        all_rewards = []  # To store rewards from all epochs\\n\\n        for i in range(epoch):\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = [np.random.randn(*w.shape) + np.random.uniform(-0.01, 0.01, w.shape) for w in self.weights]\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            self.adaptive_sigma(rewards)  # Adjust sigma based on rewards\\n\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] += (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T, rewards).T\\n\\n            # Store the average reward for this epoch\\n            avg_reward = np.mean(rewards)\\n            all_rewards.append(avg_reward)\\n\\n            if (i + 1) % print_every == 0:\\n                print(f\\'Epoch {i + 1}/{epoch}, Average Reward: {avg_reward}\\')\\n\\n                if avg_reward < previous_mean_reward:\\n                    no_improvement_count += 1\\n                else:\\n                    no_improvement_count = 0\\n                    previous_mean_reward = avg_reward\\n\\n                if no_improvement_count >= patience:\\n                    print(\"Early stopping...\")\\n                    break\\n\\n        return all_rewards  # Return all rewards collected during training\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\t[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n# \\t[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n# \\t[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n#\\nimport numpy as np\\n\\n\\nclass Model:\\n    def __init__(self, input_size: int, layer_size: int, output_size: int):\\n        \"\"\"\\n        Initializes the model with random weights.\\n\\n        Parameters:\\n        - input_size: Number of input features.\\n        - layer_size: Number of neurons in the hidden layer.\\n        - output_size: Number of output classes.\\n        \"\"\"\\n        self.weights = [\\n            np.random.normal(0, 0.1, (input_size, layer_size)),  # Input to hidden layer weights\\n            np.random.normal(0, 0.1, (layer_size, output_size)),  # Hidden to output layer weights\\n        ]\\n        self.biases = [\\n            np.random.normal(0, 0.1, (1, layer_size)),  # Bias for hidden layer\\n            np.random.normal(0, 0.1, (1, output_size)),  # Bias for output layer\\n        ]\\n\\n    def predict(self, inputs: np.ndarray) -> np.ndarray:\\n        \"\"\"\\n        Makes a prediction based on the input data.\\n\\n        Parameters:\\n        - inputs: Input data for prediction.\\n\\n        Returns:\\n        - Decision: Output predictions.\\n        \"\"\"\\n        feed = np.dot(inputs, self.weights[0]) + self.biases[0]\\n        decision = np.dot(feed, self.weights[1]) + self.biases[1]\\n        return decision\\n\\n    def get_weights(self) -> list:\\n        \"\"\"Returns the current weights of the model.\"\"\"\\n        return self.weights\\n\\n    def set_weights(self, weights: list):\\n        \"\"\"\\n        Sets the weights of the model.\\n\\n        Parameters:\\n        - weights: New weights to set for the model.\\n        \"\"\"\\n        if len(weights) != len(self.weights):\\n            raise ValueError(\"Weights must have the same length as the current weights.\")\\n\\n        for i in range(len(weights)):\\n            if weights[i].shape != self.weights[i].shape:\\n                raise ValueError(\\n                    f\"Weight matrix {i} must have shape {self.weights[i].shape}, but got {weights[i].shape}.\")\\n\\n        self.weights = weights', 'Any recommendations that can be done to the following codes:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract and scale the selected indicators\\n    scalers = {}\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        scaler = StandardScaler()\\n        indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n        scalers[indicator] = scaler  # Save the fitted scaler\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n    # Define the step-by-step prediction function\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                print(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                print(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract and scale the selected indicators for testing\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = scalers[indicator].transform(df2[indicator].values.reshape(-1, 1)).flatten()\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()\\n\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added logging to track the execution flow and errors\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added ltry-except blocks around file operations and data processing to\\n                                            catch specific exceptions like FileNotFoundError and\\n                                            pd.errors.EmptyDataError\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added Informative logging messages to indicate successful operations\\n                                            and errors.\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added to ensure that the scalers are fitted with the re-train data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nLogging Configuration: Set up logging to track the execution flow and errors. The log messages include timestamps\\nand severity levels.\\n\\nTry-Except Blocks: Added try-except blocks around file operations and data processing to catch specific exceptions\\nlike FileNotFoundError and pd.errors.EmptyDataError. A general exception handler is also included to catch any\\nother unexpected errors.\\n\\nLogging Messages: Informative logging messages are added to indicate successful operations and errors.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers: The code now saves the fitted scalers instead of new instances, preventing the \"not fitted\"\\nerror when you attempt to use them.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\n\\ndef run_retrading_simulation(model):\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    try:\\n        # Load and preprocess data\\n        df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n        logging.info(\"Data loaded successfully.\")\\n\\n        # Calculate the number of rows to train\\n        rows_count = len(df1)\\n        rows_toTrain = int(rows_count * 0.90)  # Use 90% of the data for training\\n        df1 = df1.iloc[rows_toTrain - 2:].copy()\\n\\n        # Ensure \\'Date\\' column exists\\n        if \\'Date\\' not in df1.columns:\\n            df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n        # Convert columns to float, excluding the \\'Date\\' column\\n        def convert_to_float(lst):\\n            return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n        # Apply conversion and handle length mismatches\\n        for col in df1.columns:\\n            if col != \\'Date\\':\\n                df1[col] = convert_to_float(df1[col].tolist())\\n\\n        logging.info(\"Data preprocessing completed successfully.\")\\n\\n        # Extract and scale the selected indicators\\n        scalers = {}\\n        indicators = {}\\n        for indicator in selected_indicators:\\n            scaler = StandardScaler()\\n            indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n            scalers[indicator] = scaler  # Save the fitted scaler\\n\\n        close = df1[\\'Close\\'].values.flatten()\\n\\n        # Save the scalers for future use\\n        joblib.dump(scalers, \\'output/scalers.pkl\\')\\n        logging.info(\"Scalers saved successfully.\")\\n\\n        # Initialize agent\\n        window_size, skip, initial_money = 30, 1, 10000\\n        agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                                  indicators=indicators)\\n\\n        # Train the agent\\n        agent.fit(iterations=500, checkpoint=10)\\n        logging.info(\"Agent training completed successfully.\")\\n\\n        # Perform predictions\\n        states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n        # Save the updated model\\n        joblib.dump(model, \\'output/model.pkl\\')\\n        logging.info(\"Model saved successfully.\")\\n\\n        # Plotting results\\n        plt.figure(figsize=(15, 5))\\n        plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n        plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n    except pd.errors.EmptyDataError:\\n        logging.error(\"No data found in the CSV file.\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        run_retrading_simulation(model)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")\\n\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n------------------------------------------------------------------------------------------------------------------------\\n\\n\"\"\"\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nimport logging\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\napp = Flask(__name__)\\n\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    try:\\n        data = request.get_json()\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        # Load the scalers\\n        scalers = joblib.load(\\'output/scalers.pkl\\')\\n        logging.info(\"Scalers loaded successfully.\")\\n\\n        # Scale the indicators using the loaded scalers\\n        for indicator in indicators.keys():\\n            if indicator in scalers:\\n                # Check if the scaler is fitted\\n                if hasattr(scalers[indicator], \\'scale_\\'):\\n                    indicators[indicator] = scalers[indicator].transform(np.array(indicators[indicator]).reshape(-1, 1)).flatten()\\n                else:\\n                    logging.error(f\"Scaler for {indicator} is not fitted.\")\\n                    return jsonify({\\'error\\': f\\'Scaler for {indicator} is not fitted.\\'}), 500\\n            else:\\n                logging.warning(f\"Scaler for {indicator} not found.\")\\n\\n        # Now indicators are scaled and flattened\\n        t = int(data[\\'t\\'])\\n        window_size = int(data[\\'window_size\\'])\\n\\n        state = get_state(trend, indicators, t, window_size)\\n        decision = model.predict(state)\\n        action = np.argmax(decision[0])\\n\\n        response = {\\'action\\': int(action)}\\n        return jsonify(response)\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n        return jsonify({\\'error\\': \\'File not found\\'}), 500\\n    except KeyError as e:\\n        logging.error(f\"Key error: {e}\")\\n        return jsonify({\\'error\\': \\'Key error\\'}), 400\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n        return jsonify({\\'error\\': \\'An error occurred\\'}), 500\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        app.run(host=\"0.0.0.0\", debug=False, port=5000)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")\\n\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n            self.experience_replay = []  # Initialize experience replay buffer\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n            holding_penalty = 0  # Penalty for holding positions too long\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Penalty for holding positions too long\\n                if position != 0:\\n                    holding_penalty += 1\\n                    if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                        earned_money -= 5  # Penalty for holding too long\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                # Store experience in replay buffer\\n                self.experience_replay.append((state, action, earned_money, next_state))\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return ((earned_money) / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n\\n\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n# \\tVersion 1.01\\n# \\tCreated by Javier Sanchez, June 15, 2023\\n# \\t---------------------------------------------------------------------------------------------------------------------------------------------------------\\n#\\n# \\tRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n# \\t--------------------', 'Why are we penalizing for holding positions too long without action. On the following Code:\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n            self.experience_replay = []  # Initialize experience replay buffer\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n            holding_penalty = 0  # Penalty for holding positions too long\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Penalty for holding positions too long\\n                if position != 0:\\n                    holding_penalty += 1\\n                    if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                        earned_money -= 5  # Penalty for holding too long\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                # Store experience in replay buffer\\n                self.experience_replay.append((state, action, earned_money, next_state))\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return (earned_money / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n', 'Why is it that when the deep reinforcement learning is trained, and we get 10% of new data to test using \\nagentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\non train.py, and we execute the re-retrain.py we get different dollars amount\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract and scale the selected indicators\\n    scalers = {}\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        scaler = StandardScaler()\\n        indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n        scalers[indicator] = scaler  # Save the fitted scaler\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n    # Define the step-by-step prediction function\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                print(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                print(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract and scale the selected indicators for testing\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = scalers[indicator].transform(df2[indicator].values.reshape(-1, 1)).flatten()\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()\\n\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added logging to track the execution flow and errors\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added ltry-except blocks around file operations and data processing to\\n                                            catch specific exceptions like FileNotFoundError and\\n                                            pd.errors.EmptyDataError\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added Informative logging messages to indicate successful operations\\n                                            and errors.\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added to ensure that the scalers are fitted with the re-train data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nLogging Configuration: Set up logging to track the execution flow and errors. The log messages include timestamps\\nand severity levels.\\n\\nTry-Except Blocks: Added try-except blocks around file operations and data processing to catch specific exceptions\\nlike FileNotFoundError and pd.errors.EmptyDataError. A general exception handler is also included to catch any\\nother unexpected errors.\\n\\nLogging Messages: Informative logging messages are added to indicate successful operations and errors.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers: The code now saves the fitted scalers instead of new instances, preventing the \"not fitted\"\\nerror when you attempt to use them.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\n\\ndef run_retrading_simulation(model):\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    try:\\n        # Load and preprocess data\\n        df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n        logging.info(\"Data loaded successfully.\")\\n\\n        # Calculate the number of rows to train\\n        rows_count = len(df1)\\n        rows_toTrain = int(rows_count * 0.90)  # Use 90% of the data for training\\n        df1 = df1.iloc[rows_toTrain - 2:].copy()\\n\\n        # Ensure \\'Date\\' column exists\\n        if \\'Date\\' not in df1.columns:\\n            df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n        # Convert columns to float, excluding the \\'Date\\' column\\n        def convert_to_float(lst):\\n            return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n        # Apply conversion and handle length mismatches\\n        for col in df1.columns:\\n            if col != \\'Date\\':\\n                df1[col] = convert_to_float(df1[col].tolist())\\n\\n        logging.info(\"Data preprocessing completed successfully.\")\\n\\n        # Extract and scale the selected indicators\\n        scalers = {}\\n        indicators = {}\\n        for indicator in selected_indicators:\\n            scaler = StandardScaler()\\n            indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n            scalers[indicator] = scaler  # Save the fitted scaler\\n\\n        close = df1[\\'Close\\'].values.flatten()\\n\\n        # Save the scalers for future use\\n        joblib.dump(scalers, \\'output/scalers.pkl\\')\\n        logging.info(\"Scalers saved successfully.\")\\n\\n        # Initialize agent\\n        window_size, skip, initial_money = 30, 1, 10000\\n        agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                                  indicators=indicators)\\n\\n        # Train the agent\\n        agent.fit(iterations=500, checkpoint=10)\\n        logging.info(\"Agent training completed successfully.\")\\n\\n        # Perform predictions\\n        states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n        # Save the updated model\\n        joblib.dump(model, \\'output/model.pkl\\')\\n        logging.info(\"Model saved successfully.\")\\n\\n        # Plotting results\\n        plt.figure(figsize=(15, 5))\\n        plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n        plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n    except pd.errors.EmptyDataError:\\n        logging.error(\"No data found in the CSV file.\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        run_retrading_simulation(model)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")\\n', 'Verify that the same data is being used for both training and testing, and that preprocessing steps are applied uniformly.\\n Ensure that the model weights are saved and loaded correctly, and that the same model architecture is used.\\nSet a random seed at the beginning of both scripts to ensure reproducibility.\\nFor the following codes:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract and scale the selected indicators\\n    scalers = {}\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        scaler = StandardScaler()\\n        indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n        scalers[indicator] = scaler  # Save the fitted scaler\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n    # Define the step-by-step prediction function\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                print(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                print(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract and scale the selected indicators for testing\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = scalers[indicator].transform(df2[indicator].values.reshape(-1, 1)).flatten()\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added logging to track the execution flow and errors\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added ltry-except blocks around file operations and data processing to\\n                                            catch specific exceptions like FileNotFoundError and\\n                                            pd.errors.EmptyDataError\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added Informative logging messages to indicate successful operations\\n                                            and errors.\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added to ensure that the scalers are fitted with the re-train data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nLogging Configuration: Set up logging to track the execution flow and errors. The log messages include timestamps\\nand severity levels.\\n\\nTry-Except Blocks: Added try-except blocks around file operations and data processing to catch specific exceptions\\nlike FileNotFoundError and pd.errors.EmptyDataError. A general exception handler is also included to catch any\\nother unexpected errors.\\n\\nLogging Messages: Informative logging messages are added to indicate successful operations and errors.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers: The code now saves the fitted scalers instead of new instances, preventing the \"not fitted\"\\nerror when you attempt to use them.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\n\\ndef run_retrading_simulation(model):\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    try:\\n        # Load and preprocess data\\n        df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n        logging.info(\"Data loaded successfully.\")\\n\\n        # Calculate the number of rows to train\\n        rows_count = len(df1)\\n        rows_toTrain = int(rows_count * 0.90)  # Use 90% of the data for training\\n        df1 = df1.iloc[rows_toTrain - 2:].copy()\\n\\n        # Ensure \\'Date\\' column exists\\n        if \\'Date\\' not in df1.columns:\\n            df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n        # Convert columns to float, excluding the \\'Date\\' column\\n        def convert_to_float(lst):\\n            return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n        # Apply conversion and handle length mismatches\\n        for col in df1.columns:\\n            if col != \\'Date\\':\\n                df1[col] = convert_to_float(df1[col].tolist())\\n\\n        logging.info(\"Data preprocessing completed successfully.\")\\n\\n        # Extract and scale the selected indicators\\n        scalers = {}\\n        indicators = {}\\n        for indicator in selected_indicators:\\n            scaler = StandardScaler()\\n            indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n            scalers[indicator] = scaler  # Save the fitted scaler\\n\\n        close = df1[\\'Close\\'].values.flatten()\\n\\n        # Save the scalers for future use\\n        joblib.dump(scalers, \\'output/scalers.pkl\\')\\n        logging.info(\"Scalers saved successfully.\")\\n\\n        # Initialize agent\\n        window_size, skip, initial_money = 30, 1, 10000\\n        agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                                  indicators=indicators)\\n\\n        # Train the agent\\n        agent.fit(iterations=500, checkpoint=10)\\n        logging.info(\"Agent training completed successfully.\")\\n\\n        # Perform predictions\\n        states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n        # Save the updated model\\n        joblib.dump(model, \\'output/model.pkl\\')\\n        logging.info(\"Model saved successfully.\")\\n\\n        # Plotting results\\n        plt.figure(figsize=(15, 5))\\n        plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n        plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n    except pd.errors.EmptyDataError:\\n        logging.error(\"No data found in the CSV file.\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        run_retrading_simulation(model)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")\\n', 'Set a Random Seed: Ensure that the random seed is set at the beginning of both scripts.\\nConsistent Data Splitting: Ensure that the same data is used for training and testing.\\nUniform Preprocessing: Apply the same preprocessing steps to both training and testing data.\\nSave and Load Model Weights Correctly: Ensure that the model weights are saved and loaded correctly.\\nLogging: Add logging to track the execution flow and any potential issues.\\nfor the following codes:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract and scale the selected indicators\\n    scalers = {}\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        scaler = StandardScaler()\\n        indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n        scalers[indicator] = scaler  # Save the fitted scaler\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n    # Define the step-by-step prediction function\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                print(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                print(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract and scale the selected indicators for testing\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = scalers[indicator].transform(df2[indicator].values.reshape(-1, 1)).flatten()\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added logging to track the execution flow and errors\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added ltry-except blocks around file operations and data processing to\\n                                            catch specific exceptions like FileNotFoundError and\\n                                            pd.errors.EmptyDataError\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added Informative logging messages to indicate successful operations\\n                                            and errors.\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added to ensure that the scalers are fitted with the re-train data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nLogging Configuration: Set up logging to track the execution flow and errors. The log messages include timestamps\\nand severity levels.\\n\\nTry-Except Blocks: Added try-except blocks around file operations and data processing to catch specific exceptions\\nlike FileNotFoundError and pd.errors.EmptyDataError. A general exception handler is also included to catch any\\nother unexpected errors.\\n\\nLogging Messages: Informative logging messages are added to indicate successful operations and errors.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers: The code now saves the fitted scalers instead of new instances, preventing the \"not fitted\"\\nerror when you attempt to use them.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef run_retrading_simulation(model):\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    try:\\n        # Load and preprocess data\\n        df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n        logging.info(\"Data loaded successfully.\")\\n\\n        # Calculate the number of rows to train\\n        rows_count = len(df1)\\n        rows_toTrain = int(rows_count * 0.90)  # Use 90% of the data for training\\n        df1 = df1.iloc[rows_toTrain - 2:].copy()\\n\\n        # Ensure \\'Date\\' column exists\\n        if \\'Date\\' not in df1.columns:\\n            df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n        # Convert columns to float, excluding the \\'Date\\' column\\n        def convert_to_float(lst):\\n            return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n        # Apply conversion and handle length mismatches\\n        for col in df1.columns:\\n            if col != \\'Date\\':\\n                df1[col] = convert_to_float(df1[col].tolist())\\n\\n        logging.info(\"Data preprocessing completed successfully.\")\\n\\n        # Extract and scale the selected indicators\\n        scalers = {}\\n        indicators = {}\\n        for indicator in selected_indicators:\\n            scaler = StandardScaler()\\n            indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n            scalers[indicator] = scaler  # Save the fitted scaler\\n\\n        close = df1[\\'Close\\'].values.flatten()\\n\\n        # Save the scalers for future use\\n        joblib.dump(scalers, \\'output/scalers.pkl\\')\\n        logging.info(\"Scalers saved successfully.\")\\n\\n        # Initialize agent\\n        window_size, skip, initial_money = 30, 1, 10000\\n        agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                                  indicators=indicators)\\n\\n        # Train the agent\\n        agent.fit(iterations=500, checkpoint=10)\\n        logging.info(\"Agent training completed successfully.\")\\n\\n        # Perform predictions\\n        states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n        # Save the updated model\\n        joblib.dump(model, \\'output/model.pkl\\')\\n        logging.info(\"Model saved successfully.\")\\n\\n        # Plotting results\\n        plt.figure(figsize=(15, 5))\\n        plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n        plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n    except pd.errors.EmptyDataError:\\n        logging.error(\"No data found in the CSV file.\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        run_retrading_simulation(model)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n            self.experience_replay = []  # Initialize experience replay buffer\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n            holding_penalty = 0  # Penalty for holding positions too long\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Penalty for holding positions too long\\n                if position != 0:\\n                    holding_penalty += 1\\n                    if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                        earned_money -= 5  # Penalty for holding too long\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                # Store experience in replay buffer\\n                self.experience_replay.append((state, action, earned_money, next_state))\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return (earned_money / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\n\\nReward Function:\\n\\nSharpe Ratio: The reward function now evaluates the performance of trades using the Sharpe Ratio.\\nPenalty for Holding: Introduced a penalty for holding positions too long without action.\\n\"\"\"\\n\\nimport numpy as np\\n\\nclass Deep_Evolution_Strategy():\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def adaptive_sigma(self, rewards):\\n        \"\"\"Adjust sigma based on the mean and standard deviation of rewards.\"\"\"\\n        mean_reward = np.mean(rewards)\\n        std_reward = np.std(rewards)\\n        if std_reward > 0:\\n            self.sigma *= 1 + (mean_reward / std_reward)  # Adjust sigma based on performance\\n\\n    def train(self, epoch=200, print_every=1, update_frequency=5, patience=10):\\n        previous_mean_reward = -np.inf\\n        no_improvement_count = 0\\n        all_rewards = []  # To store rewards from all epochs\\n\\n        for i in range(epoch):\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = [np.random.randn(*w.shape) + np.random.uniform(-0.01, 0.01, w.shape) for w in self.weights]\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            self.adaptive_sigma(rewards)  # Adjust sigma based on rewards\\n\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] += (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T, rewards).T\\n\\n            # Store the average reward for this epoch\\n            avg_reward = np.mean(rewards)\\n            all_rewards.append(avg_reward)\\n\\n            if (i + 1) % print_every == 0:\\n                print(f\\'Epoch {i + 1}/{epoch}, Average Reward: {avg_reward}\\')\\n\\n                if avg_reward < previous_mean_reward:\\n       ', 'Use the last 10% of the data for re-training on the following code\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef run_retrading_simulation(model):\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    try:\\n        # Load and preprocess data\\n        df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n        logging.info(\"Data loaded successfully.\")\\n\\n        # Calculate the number of rows to train\\n        rows_count = len(df1)\\n        rows_toTrain = int(rows_count * 0.90)  # Use 90% of the data for training\\n        df1 = df1.iloc[rows_toTrain - 2:].copy()\\n\\n        # Ensure \\'Date\\' column exists\\n        if \\'Date\\' not in df1.columns:\\n            df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n        # Convert columns to float, excluding the \\'Date\\' column\\n        def convert_to_float(lst):\\n            return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n        # Apply conversion and handle length mismatches\\n        for col in df1.columns:\\n            if col != \\'Date\\':\\n                df1[col] = convert_to_float(df1[col].tolist())\\n\\n        logging.info(\"Data preprocessing completed successfully.\")\\n\\n        # Extract and scale the selected indicators\\n        scalers = {}\\n        indicators = {}\\n        for indicator in selected_indicators:\\n            scaler = StandardScaler()\\n            indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n            scalers[indicator] = scaler  # Save the fitted scaler\\n\\n        close = df1[\\'Close\\'].values.flatten()\\n\\n        # Save the scalers for future use\\n        joblib.dump(scalers, \\'output/scalers.pkl\\')\\n        logging.info(\"Scalers saved successfully.\")\\n\\n        # Initialize agent\\n        window_size, skip, initial_money = 30, 1, 10000\\n        agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                                  indicators=indicators)\\n\\n        # Train the agent\\n        agent.fit(iterations=500, checkpoint=10)\\n        logging.info(\"Agent training completed successfully.\")\\n\\n        # Perform predictions\\n        states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n        # Save the updated model\\n        joblib.dump(model, \\'output/model.pkl\\')\\n        logging.info(\"Model saved successfully.\")\\n\\n        # Plotting results\\n        plt.figure(figsize=(15, 5))\\n        plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n        plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n    except pd.errors.EmptyDataError:\\n        logging.error(\"No data found in the CSV file.\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        run_retrading_simulation(model)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")', \" Use the last 10% of the data for re-training on the following   \\n\\n # Testing\\n    df2 = pd.read_csv('data/dataFile.csv').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv('data/ApiTest.csv', index=False)\", 'Use the last 10% of the data for on the following \\ndf2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\ndf2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract and scale the selected indicators\\n    scalers = {}\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        scaler = StandardScaler()\\n        indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n        scalers[indicator] = scaler  # Save the fitted scaler\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n    # Define the step-by-step prediction function\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                print(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                print(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract and scale the selected indicators for testing\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = scalers[indicator].transform(df2[indicator].values.reshape(-1, 1)).flatten()\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()', 'After make all the changes to the following codes, I keep getting a negative total gains for the testing and re-training:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract and scale the selected indicators\\n    scalers = {}\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        scaler = StandardScaler()\\n        indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n        scalers[indicator] = scaler  # Save the fitted scaler\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n    # Define the step-by-step prediction function\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                print(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                print(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract and scale the selected indicators for testing\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = scalers[indicator].transform(df2[indicator].values.reshape(-1, 1)).flatten()\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added logging to track the execution flow and errors\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added ltry-except blocks around file operations and data processing to\\n                                            catch specific exceptions like FileNotFoundError and\\n                                            pd.errors.EmptyDataError\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added Informative logging messages to indicate successful operations\\n                                            and errors.\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added to ensure that the scalers are fitted with the re-train data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nLogging Configuration: Set up logging to track the execution flow and errors. The log messages include timestamps\\nand severity levels.\\n\\nTry-Except Blocks: Added try-except blocks around file operations and data processing to catch specific exceptions\\nlike FileNotFoundError and pd.errors.EmptyDataError. A general exception handler is also included to catch any\\nother unexpected errors.\\n\\nLogging Messages: Informative logging messages are added to indicate successful operations and errors.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers: The code now saves the fitted scalers instead of new instances, preventing the \"not fitted\"\\nerror when you attempt to use them.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\n\\ndef run_retrading_simulation(model):\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    try:\\n        # Load and preprocess data\\n        df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n        logging.info(\"Data loaded successfully.\")\\n\\n        # Calculate the number of rows to train\\n        rows_count = len(df1)\\n        rows_toTrain = int(rows_count * 0.10)  # Use last 10% of the data for training\\n        df1 = df1.iloc[-rows_toTrain:].copy()  # Select the last 10%\\n\\n        # Ensure \\'Date\\' column exists\\n        if \\'Date\\' not in df1.columns:\\n            df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n        # Convert columns to float, excluding the \\'Date\\' column\\n        def convert_to_float(lst):\\n            return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n        # Apply conversion and handle length mismatches\\n        for col in df1.columns:\\n            if col != \\'Date\\':\\n                df1[col] = convert_to_float(df1[col].tolist())\\n\\n        logging.info(\"Data preprocessing completed successfully.\")\\n\\n        # Extract and scale the selected indicators\\n        scalers = {}\\n        indicators = {}\\n        for indicator in selected_indicators:\\n            scaler = StandardScaler()\\n            indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n            scalers[indicator] = scaler  # Save the fitted scaler\\n\\n        close = df1[\\'Close\\'].values.flatten()\\n\\n        # Save the scalers for future use\\n        joblib.dump(scalers, \\'output/scalers.pkl\\')\\n        logging.info(\"Scalers saved successfully.\")\\n\\n        # Initialize agent\\n        window_size, skip, initial_money = 30, 1, 10000\\n        agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                                  indicators=indicators)\\n\\n        # Train the agent\\n        agent.fit(iterations=500, checkpoint=10)\\n        logging.info(\"Agent training completed successfully.\")\\n\\n        # Perform predictions\\n        states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n        # Save the updated model\\n        joblib.dump(model, \\'output/model.pkl\\')\\n        logging.info(\"Model saved successfully.\")\\n\\n        # Plotting results\\n        plt.figure(figsize=(15, 5))\\n        plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n        plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n    except pd.errors.EmptyDataError:\\n        logging.error(\"No data found in the CSV file.\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        run_retrading_simulation(model)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")\\n\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n            self.experience_replay = []  # Initialize experience replay buffer\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n            holding_penalty = 0  # Penalty for holding positions too long\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Penalty for holding positions too long\\n                if position != 0:\\n                    holding_penalty += 1\\n                    if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                        earned_money -= 5  # Penalty for holding too long\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                # Store experience in replay buffer\\n                self.experience_replay.append((state, action, earned_money, next_state))\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return (earned_money / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\n\\nReward Function:\\n\\nSharpe Ratio: The reward function now evaluates the performance of trades using the Sharpe Ratio.\\nPenalty for Holding: Introduced a penalty for holding positions too long without action.\\n\"\"\"\\n\\nimport numpy as np\\n\\nclass Deep_Evolution_Strategy():\\n    def __init__(self, weights, reward_function, population_size, sigma, learning_rate):\\n        self.weights = weights\\n        self.reward_function = reward_function\\n        self.population_size = population_size\\n        self.sigma = sigma\\n        self.learning_rate = learning_rate\\n\\n    def _get_weight_from_population(self, weights, population):\\n        weights_population = []\\n        for index, i in enumerate(population):\\n            jittered = self.sigma * i\\n            weights_population.append(weights[index] + jittered)\\n        return weights_population\\n\\n    def get_weights(self):\\n        return self.weights\\n\\n    def adaptive_sigma(self, rewards):\\n        \"\"\"Adjust sigma based on the mean and standard deviation of rewards.\"\"\"\\n        mean_reward = np.mean(rewards)\\n        std_reward = np.std(rewards)\\n        if std_reward > 0:\\n            self.sigma *= 1 + (mean_reward / std_reward)  # Adjust sigma based on performance\\n\\n    def train(self, epoch=200, print_every=1, update_frequency=5, patience=10):\\n        previous_mean_reward = -np.inf\\n        no_improvement_count = 0\\n        all_rewards = []  # To store rewards from all epochs\\n\\n        for i in range(epoch):\\n            population = []\\n            rewards = np.zeros(self.population_size)\\n            for k in range(self.population_size):\\n                x = [np.random.randn(*w.shape) + np.random.uniform(-0.01, 0.01, w.shape) for w in self.weights]\\n                population.append(x)\\n\\n            for k in range(self.population_size):\\n                weights_population = self._get_weight_from_population(self.weights, population[k])\\n                rewards[k] = self.reward_function(weights_population)\\n\\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\\n            self.adaptive_sigma(rewards)  # Adjust sigma based on rewards\\n\\n            for index, w in enumerate(self.weights):\\n                A = np.array([p[index] for p in population])\\n                self.weights[index] += (self.learning_rate / (self.population_size * self.sigma)) * np.dot(A.T, rewards).T\\n\\n            # Store the average reward for this epoch\\n            avg_reward = np.mean(rewards)\\n            all_rewards.append(avg_reward)\\n\\n            if (i + 1) % print_every == 0:\\n                print(f\\'Epoch {i + 1}/{epoch}, Average Reward: {avg_reward}\\')\\n\\n                if avg_reward < previous_mean_reward:\\n                    no_improvement_count += 1\\n                else:\\n                    no_improvement_count = 0\\n                    previous_mean_reward = avg_reward\\n\\n                if no_improvement_count >= patience:\\n                    print(\"Early stopping...\")\\n                    break\\n\\n        return all_rewards  # Re', 'What learning rate should it be change to', 'Add the learning rate option to the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract and scale the selected indicators\\n    scalers = {}\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        scaler = StandardScaler()\\n        indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n        scalers[indicator] = scaler  # Save the fitted scaler\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n    # Define the step-by-step prediction function\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                logging.info(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                logging.info(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n    logging.info(f\\'Training completed. Total gains: {total_gains}, Total investment: {invest}%\\')\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract and scale the selected indicators for testing\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = scalers[indicator].transform(df2[indicator].values.reshape(-1, 1)).flatten()\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n    logging.info(f\\'Testing completed. Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n    logging.info(f\\'Loaded model testing completed. Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()', 'The for indicator in indicators.keys(): loop on the \\nindicators[indicator] = scalers[indicator].transform(np.array(indicators[indicator]).reshape(-1, 1)).flatten()\\nI get the following RROR - An error occurred: This StandardScaler instance is not fitted yet. Call \\'fit\\' with appropriate arguments before using this estimator.\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n------------------------------------------------------------------------------------------------------------------------\\n\\n\"\"\"\\n\\nimport numpy as np\\nfrom flask import Flask, request, jsonify\\nimport joblib\\nimport logging\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\napp = Flask(__name__)\\n\\ndef get_state(trend, indicators, t, window_size):\\n    window_size = window_size + 1\\n    d = t - window_size + 1\\n    block = trend[d : t + 1] if d >= 0 else -d * [trend[0]] + trend[0 : t + 1]\\n    res = []\\n    for i in range(window_size - 1):\\n        res.append(block[i + 1] - block[i])\\n    indicators_state = [indicators[indicator][t] for indicator in indicators]\\n    state = np.array([res + indicators_state])\\n    return state\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    try:\\n        data = request.get_json()\\n        trend = data[\\'trend\\']\\n        indicators = data[\\'indicators\\']\\n\\n        # Load the scalers\\n        scalers = joblib.load(\\'output/scalers.pkl\\')\\n        logging.info(\"Scalers loaded successfully.\")\\n\\n        # Scale the indicators using the loaded scalers\\n        for indicator in indicators.keys():\\n            if indicator in scalers:\\n                indicators[indicator] = scalers[indicator].transform(np.array(indicators[indicator]).reshape(-1, 1)).flatten()\\n\\n        # Now indicators are scaled and flattened\\n        t = int(data[\\'t\\'])\\n        window_size = int(data[\\'window_size\\'])\\n\\n        state = get_state(trend, indicators, t, window_size)\\n        decision = model.predict(state)\\n        action = np.argmax(decision[0])\\n\\n        response = {\\'action\\': int(action)}\\n        return jsonify(response)\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n        return jsonify({\\'error\\': \\'File not found\\'}), 500\\n    except KeyError as e:\\n        logging.error(f\"Key error: {e}\")\\n        return jsonify({\\'error\\': \\'Key error\\'}), 400\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n        return jsonify({\\'error\\': \\'An error occurred\\'}), 500\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        app.run(host=\"0.0.0.0\", debug=False, port=5000)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")', 'Call \\'fit\\' with appropriate arguments before using this estimator,\" indicates that the StandardScaler instances in your scalers dictionary were not fitted with data before being saved and then loaded. for the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract and scale the selected indicators\\n    indicators = {indicator: StandardScaler().fit_transform(df1[indicator].values.reshape(-1, 1)).flatten() for indicator in selected_indicators}\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump({indicator: StandardScaler() for indicator in selected_indicators}, \\'output/scalers.pkl\\')\\n\\n    # Define the step-by-step prediction function\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                print(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                print(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract and scale the selected indicators for testing\\n    test_indicators = {indicator: StandardScaler().fit_transform(df2[indicator].values.reshape(-1, 1)).flatten() for indicator in selected_indicators}\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()\\n', 'Call \\'fit\\' with appropriate arguments before using this estimator,\" indicates that the StandardScaler instances in your scalers dictionary were not fitted with data before being saved and then loaded. for the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added logging to track the execution flow and errors\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added ltry-except blocks around file operations and data processing to\\n                                            catch specific exceptions like FileNotFoundError and\\n                                            pd.errors.EmptyDataError\\n[JSanchez]\\t25-Jun-2024:\\tRev. 1.03\\t    Added  Informative logging messages to indicate successful operations\\n                                            and errors.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nLogging Configuration: Set up logging to track the execution flow and errors. The log messages include timestamps\\nand severity levels.\\n\\nTry-Except Blocks: Added try-except blocks around file operations and data processing to catch specific exceptions\\nlike FileNotFoundError and pd.errors.EmptyDataError. A general exception handler is also included to catch any\\nother unexpected errors.\\n\\nLogging Messages: Informative logging messages are added to indicate successful operations and errors.\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\n\\ndef run_retrading_simulation(model):\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    try:\\n        # Load and preprocess data\\n        df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n        logging.info(\"Data loaded successfully.\")\\n\\n        # Calculate the number of rows to train\\n        rows_count = len(df1)\\n        rows_toTrain = int(rows_count * 0.90)  # Use 90% of the data for training\\n        df1 = df1.iloc[rows_toTrain - 2:].copy()\\n\\n        # Ensure \\'Date\\' column exists\\n        if \\'Date\\' not in df1.columns:\\n            df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n        # Convert columns to float, excluding the \\'Date\\' column\\n        def convert_to_float(lst):\\n            return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n        # Apply conversion and handle length mismatches\\n        for col in df1.columns:\\n            if col != \\'Date\\':\\n                df1[col] = convert_to_float(df1[col].tolist())\\n\\n        logging.info(\"Data preprocessing completed successfully.\")\\n\\n        # Extract and scale the selected indicators\\n        indicators = {indicator: StandardScaler().fit_transform(df1[indicator].values.reshape(-1, 1)).flatten() for indicator in selected_indicators}\\n        close = df1[\\'Close\\'].values.flatten()\\n\\n        # Save the scalers for future use\\n        joblib.dump({indicator: StandardScaler() for indicator in selected_indicators}, \\'output/scalers.pkl\\')\\n        logging.info(\"Scalers saved successfully.\")\\n\\n        # Initialize agent\\n        window_size, skip, initial_money = 30, 1, 10000\\n        agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money,\\n                                  indicators=indicators)\\n\\n        # Train the agent\\n        agent.fit(iterations=500, checkpoint=10)\\n        logging.info(\"Agent training completed successfully.\")\\n\\n        # Perform predictions\\n        states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n\\n        # Save the updated model\\n        joblib.dump(model, \\'output/model.pkl\\')\\n        logging.info(\"Model saved successfully.\")\\n\\n        # Plotting results\\n        plt.figure(figsize=(15, 5))\\n        plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n        plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n        plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n        plt.legend()\\n        plt.show()\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n    except pd.errors.EmptyDataError:\\n        logging.error(\"No data found in the CSV file.\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred: {e}\")\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        # Load the trained model\\n        model = joblib.load(\\'output/model.pkl\\')\\n        logging.info(\"Model loaded successfully.\")\\n        run_retrading_simulation(model)\\n    except FileNotFoundError as e:\\n        logging.error(f\"Model file not found: {e}\")\\n    except Exception as e:\\n        logging.error(f\"An error occurred while loading the model: {e}\")\\n', 'incorporate on the plt.plot the following metrics ticks_earnedT, max_drawdownT, start_idxT, end_idxT\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract and scale the selected indicators\\n    scalers = {}\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        scaler = StandardScaler()\\n        indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n        scalers[indicator] = scaler  # Save the fitted scaler\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n    # Define the step-by-step prediction function\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                logging.info(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                logging.info(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n    logging.info(f\\'Training completed. Total gains: {total_gains}, Total investment: {invest}%\\')\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract and scale the selected indicators for testing\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = scalers[indicator].transform(df2[indicator].values.reshape(-1, 1)).flatten()\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n    logging.info(f\\'Testing completed. Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n    logging.info(f\\'Loaded model testing completed. Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()', \"    plt.text(start_idxT, close_test[start_idxT], f'Ticks Earned: {ticks_earnedT}', fontsize=10, color='orange')\\n                         ~~~~~~~~~~^^^^^^^^^^^^\\nIndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\\n\", 'File \"C:\\\\PythonApps\\\\drl-trainer\\\\train.py\", line 177, in run_trading_simulation\\n    plt.text(start_idxT, close_test[start_idxT], f\\'Ticks Earned: {ticks_earnedT}\\', fontsize=10, color=\\'orange\\')\\n                         ~~~~~~~~~~^^^^^^^^^^^^\\nIndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'Volume\\', \\'rsi\\', \\'adx\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract and scale the selected indicators\\n    scalers = {}\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        scaler = StandardScaler()\\n        indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n        scalers[indicator] = scaler  # Save the fitted scaler\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n    # Define the step-by-step prediction function\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                logging.info(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                logging.info(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n    logging.info(f\\'Training completed. Total gains: {total_gains}, Total investment: {invest}%\\')\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract and scale the selected indicators for testing\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = scalers[indicator].transform(df2[indicator].values.reshape(-1, 1)).flatten()\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n    logging.info(f\\'Testing completed. Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results with metrics\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n\\n    # Annotate metrics on the plot\\n    if 0 <= start_idxT < len(close_test):\\n        plt.axvline(x=start_idxT, color=\\'orange\\', linestyle=\\'--\\', label=\\'Start of Max Drawdown\\')\\n        plt.text(start_idxT, close_test[start_idxT], f\\'Ticks Earned: {ticks_earnedT}\\', fontsize=10, color=\\'orange\\')\\n    if 0 <= end_idxT < len(close_test):\\n        plt.axvline(x=end_idxT, color=\\'purple\\', linestyle=\\'--\\', label=\\'End of Max Drawdown\\')\\n        plt.text(end_idxT, close_test[end_idxT], f\\'Max Drawdown: {max_drawdownT:.2f}\\', fontsize=10, color=\\'purple\\')\\n\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n    logging.info(f\\'Loaded model testing completed. Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()', 'Let\\'s incorporate the \"Volume\" in the following code so we can get a more reliable prediction when training:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n            self.experience_replay = []  # Initialize experience replay buffer\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n            holding_penalty = 0  # Penalty for holding positions too long\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n                    if rsi > 70:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                # Penalty for holding positions too long\\n                if position != 0:\\n                    holding_penalty += 1\\n                    if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                        earned_money -= 5  # Penalty for holding too long\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                # Store experience in replay buffer\\n                self.experience_replay.append((state, action, earned_money, next_state))\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return (earned_money / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n', 'Take out the scalers from the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'Volume\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract and scale the selected indicators\\n    scalers = {}\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        scaler = StandardScaler()\\n        indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n        scalers[indicator] = scaler  # Save the fitted scaler\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    # Save the scalers for future use\\n    joblib.dump(scalers, \\'output/scalers.pkl\\')\\n\\n    # Define the step-by-step prediction function\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                logging.info(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                logging.info(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n    logging.info(f\\'Training completed. Total gains: {total_gains}, Total investment: {invest}%\\')\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract and scale the selected indicators for testing\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = scalers[indicator].transform(df2[indicator].values.reshape(-1, 1)).flatten()\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n    logging.info(f\\'Testing completed. Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n    logging.info(f\\'Loaded model testing completed. Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()', 'Change the np.random.seed(42) to a Range of Seeds instead on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef run_trading_simulation():\\n    # Set random seed for reproducibility\\n    np.random.seed(42)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'Volume\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract the selected indicators without scaling\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        indicators[indicator] = df1[indicator].values.flatten()  # Use raw values directly\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                logging.info(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                logging.info(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n    logging.info(f\\'Training completed. Total gains: {total_gains}, Total investment: {invest}%\\')\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract the selected indicators for testing without scaling\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = df2[indicator].values.flatten()  # Use raw values directly\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n    logging.info(f\\'Testing completed. Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n    logging.info(f\\'Loaded model testing completed. Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation\\nrun_trading_simulation()', 'When reading the table to df1 is converting some columns to nan on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef run_trading_simulation(seed):\\n    # Set random seed for reproducibility\\n    np.random.seed(seed)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'Volume\\', \\'momentum\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[14:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    def convert_to_float(lst):\\n        return [float(i) if isinstance(i, (int, float, str)) and str(i).replace(\\'.\\', \\'\\', 1).isdigit() else np.nan for i in lst]\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract the selected indicators without scaling\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        indicators[indicator] = df1[indicator].values.flatten()  # Use raw values directly\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                logging.info(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                logging.info(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n    logging.info(f\\'Training completed. Total gains: {total_gains}, Total investment: {invest}%\\')\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df2[col].tolist())\\n            df2[col] = pd.Series(converted_values, index=df2.index)\\n\\n    # Extract the selected indicators for testing without scaling\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = df2[indicator].values.flatten()  # Use raw values directly\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n    logging.info(f\\'Testing completed. Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n    logging.info(f\\'Loaded model testing completed. Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation with a range of seeds\\nfor seed in range(42, 47):  # Example range of seeds\\n    logging.info(f\\'Running simulation with seed: {seed}\\')\\n    run_trading_simulation(seed)', '\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added the wrapped the entire code in order to allow reset all variables\\n[JSanchez]\\t24-Jun-2024:\\tRev. 1.02\\t    Added to ensure that the scalers are fitted with the training data before\\n                                            saving them.\\n------------------------------------------------------------------------------------------------------------------------\\n\\nImprovements Made:\\n\\nConsistent Use of convert_to_float: Ensured that the convert_to_float function is used consistently and handles\\npotential conversion errors by filling with NaN if necessary.\\nLength Check: Added length checks before assigning converted values back to the DataFrame to avoid length mismatches.\\nImplemented calculate_metrics Method: Ensured that the calculate_metrics method in the agent class is implemented\\ncorrectly and used in the testing phase.\\n\\nThe entire code is wrapped in the run_trading_simulation function. This allows you to reset all variables each time\\nyou call this function.\\n\\nFitting and Saving Scalers: The scalers are now fitted with the training data before being saved. This ensures that\\nwhen you load them later, they are ready to transform new data.\\nUsing Fitted Scalers for Testing: When extracting and scaling the indicators for testing, the code now uses the fitted\\nscalers from the training phase.\\n\\n\"\"\"\\n\\nimport joblib\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport logging\\nfrom modules import agent_function as agent_funct\\nfrom modules import model_function as model_funct\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef run_trading_simulation(seed):\\n    # Set random seed for reproducibility\\n    np.random.seed(seed)\\n    sns.set()\\n\\n    # Define the list of indicators to be used\\n    selected_indicators = [\\n        \\'rsi\\', \\'adx\\', \\'Volume\\', \\'ppoValue\\', \\'ppoSmoothed\\', \\'QQEDiff1\\', \\'QQEDiff2\\', \\'QQEDiff3\\'\\n    ]\\n\\n    # Load and preprocess data\\n    df1 = pd.read_csv(\\'data/dataFile.csv\\')\\n    rows_toTrain = int(len(df1) * 0.90)  # Calculate 90% for training\\n\\n    # Copy the records to df1 for training\\n    df1 = df1.iloc[14:rows_toTrain].copy()  # Start from 0\\n\\n    # Ensure \\'Date\\' column exists\\n    if \\'Date\\' not in df1.columns:\\n        df1[\\'Date\\'] = pd.date_range(start=\\'1/1/2000\\', periods=len(df1), freq=\\'D\\')\\n\\n    # Convert columns to float, excluding the \\'Date\\' column\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            # Use pd.to_numeric to convert and handle errors\\n            df1[col] = pd.to_numeric(df1[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n    # Log the number of NaNs in each column after conversion\\n    logging.info(f\\'NaN values after conversion:\\\\n{df1.isna().sum()}\\')\\n\\n    # Apply conversion and handle length mismatches\\n    for col in df1.columns:\\n        if col != \\'Date\\':\\n            converted_values = convert_to_float(df1[col].tolist())\\n            df1[col] = pd.Series(converted_values, index=df1.index)  # Fill with NaN if lengths differ\\n\\n    # Extract and scale the selected indicators\\n    scalers = {}\\n    indicators = {}\\n    for indicator in selected_indicators:\\n        scaler = StandardScaler()\\n        indicators[indicator] = scaler.fit_transform(df1[indicator].values.reshape(-1, 1)).flatten()\\n        scalers[indicator] = scaler  # Save the fitted scaler\\n\\n\\n    close = df1[\\'Close\\'].values.flatten()\\n\\n    def step_by_step_prediction(agent, trend):\\n        position, bought_price, earned_money = 0, 0, 0\\n\\n        for t in range(0, len(trend) - 1, agent.skip):\\n            action = agent.act(agent.get_state(t))\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = 1, trend[t] + 0.25\\n                logging.info(f\\'day {t}: buy 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position, bought_price = -1, trend[t] - 0.25\\n                logging.info(f\\'day {t}: sell 1 contract at price {trend[t]:.2f}, total balance {earned_money:.2f}\\')\\n\\n    # Initialize agent\\n    window_size, skip, initial_money = 30, 1, 10000\\n    model = model_funct.Model(input_size=window_size + len(selected_indicators), layer_size=500, output_size=3)\\n    agent = agent_funct.Agent(model=model, window_size=window_size, trend=close, skip=skip, initial_money=initial_money, indicators=indicators)\\n\\n    # Train the agent\\n    agent.fit(iterations=500, checkpoint=10)\\n\\n    # Perform predictions\\n    states_buy, states_sell, total_gains, invest = agent.buy_tick()\\n    logging.info(f\\'Training completed. Total gains: {total_gains}, Total investment: {invest}%\\')\\n\\n    # Save the model\\n    joblib.dump(model, \\'output/model.pkl\\')\\n\\n    # Plotting results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buy)\\n    plt.plot(close, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sell)\\n    plt.title(f\\'Total gains: {total_gains}, Total investment: {invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Testing\\n    df2 = pd.read_csv(\\'data/dataFile.csv\\').iloc[rows_toTrain:].copy()  # Ensure correct indexing\\n    df2.to_csv(\\'data/ApiTest.csv\\', index=False)\\n\\n    # Load the trained model\\n    model = joblib.load(\\'output/model.pkl\\')\\n\\n    # Convert test data\\n    for col in df2.columns:\\n        if col != \\'Date\\':\\n            df2[col] = pd.to_numeric(df2[col], errors=\\'coerce\\')  # Convert to float, set invalid parsing to NaN\\n\\n    # Extract the selected indicators for testing without scaling\\n    test_indicators = {}\\n    for indicator in selected_indicators:\\n        test_indicators[indicator] = df2[indicator].values.flatten()  # Use raw values directly\\n\\n    close_test = df2[\\'Close\\'].values.flatten()\\n\\n    # Initialize test agent\\n    agentTest = agent_funct.Agent(model=model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    states_buyT, states_sellT, total_gainsT, roiT = agentTest.buy_tick()\\n    logging.info(f\\'Testing completed. Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n\\n    # Aligning the indices for testing data\\n    test = pd.DataFrame({\\'Close\\': close_test, \\'Signal\\': 0})\\n    test.loc[test.index.intersection(states_buyT), \\'Signal\\'] = 1\\n    test.loc[test.index.intersection(states_sellT), \\'Signal\\'] = -1\\n    test[\\'Market_Returns\\'] = test[\\'Close\\'].pct_change()\\n    test[\\'Strategy_Returns\\'] = test[\\'Market_Returns\\'] * test[\\'Signal\\'].shift(1)\\n    test[\\'Cumulative_Profit\\'] = (test[\\'Strategy_Returns\\'].fillna(0) * initial_money).cumsum()\\n\\n    # Calculate metrics\\n    ticks_earnedT, max_drawdownT, start_idxT, end_idxT = agentTest.calculate_metrics(test[\\'Cumulative_Profit\\'])\\n\\n    # Plotting test results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=states_buyT)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=states_sellT)\\n    plt.title(f\\'Total gains: {total_gainsT}, ROI: {roiT}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Load and test the saved model\\n    loaded_model = joblib.load(\\'output/model.pkl\\')\\n    loaded_agent = agent_funct.Agent(model=loaded_model, window_size=window_size, trend=close_test, skip=skip, initial_money=initial_money, indicators=test_indicators)\\n    loaded_states_buy, loaded_states_sell, loaded_total_gains, loaded_invest = loaded_agent.buy_tick()\\n    logging.info(f\\'Loaded model testing completed. Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n\\n    # Plot the loaded model results\\n    plt.figure(figsize=(15, 5))\\n    plt.plot(close_test, color=\\'black\\', alpha=0.5, lw=1.)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'green\\', alpha=0.5, label=\\'buying signal\\', markevery=loaded_states_buy)\\n    plt.plot(close_test, \\'.\\', markersize=10, color=\\'red\\', alpha=0.5, label=\\'selling signal\\', markevery=loaded_states_sell)\\n    plt.title(f\\'Total gains: {loaded_total_gains}, Total investment: {loaded_invest}%\\')\\n    plt.legend()\\n    plt.show()\\n\\n    # Perform step-by-step predictions\\n    step_by_step_prediction(agentTest, close_test)\\n\\n# Call the function to run the simulation with a range of seeds\\nfor seed in range(42, 47):  # Example range of seeds\\n    logging.info(f\\'Running simulation with seed: {seed}\\')\\n    run_trading_simulation(seed)', 'The learning_rate is being pass from another model how do I initialize it on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.01\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators, learning_rate):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n            self.experience_replay = []  # Initialize experience replay buffer\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n            holding_penalty = 0  # Penalty for holding positions too long\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Penalty for holding positions too long\\n                if position != 0:\\n                    holding_penalty += 1\\n                    if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                        earned_money -= 5  # Penalty for holding too long\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                # Store experience in replay buffer\\n                self.experience_replay.append((state, action, earned_money, next_state))\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return (earned_money / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n', 'check for errors\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.01\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators, learning_rate):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n            self.experience_replay = []  # Initialize experience replay buffer\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n            holding_penalty = 0  # Penalty for holding positions too long\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n\\n                # Penalty for holding positions too long\\n                if position != 0:\\n                    holding_penalty += 1\\n                    if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                        earned_money -= 5  # Penalty for holding too long\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                # Store experience in replay buffer\\n                self.experience_replay.append((state, action, earned_money, next_state))\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return (earned_money / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise\\n', 'What is wrong with get_tick_reward on the following code:\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n[JSanchez]\\t23-Aug-2024:\\tRev. 1.03\\t    Added Meta-Learning and Relational Inductive Biases\\n------------------------------------------------------------------------------------------------------------------------\\n\\nSuggested Modifications\\n\\nIncorporate Indicators: Use technical indicators (like RSI, CCI, ADX) to inform the decision-making process in both\\nmethods.\\nContextual Information: Include historical price movements or trends to provide context for current actions.\\nReward Shaping: Modify the reward function to consider not just the profit/loss but also the risk associated with\\nthe actions taken.\\n\\nMeta-Learning: Implement a mechanism to adapt the agent\\'s learning strategy based on past performance.\\nRelational Inductive Biases: Use relationships between different market indicators and historical prices to inform\\ndecision-making.\\n\\nExplanation of Changes:\\n\\nMeta-Learning Adaptation:\\n\\nIn get_tick_reward, the agent adapts its exploration strategy based on the average of the last few rewards. If the\\naverage reward is negative, it increases exploration (e.g., by increasing SIGMA).\\nIn buy_tick, the agent tracks its past actions and can adjust its future actions based on the frequency of buys and\\nsells. If it has been buying more frequently, it may consider reducing buy actions.\\n\\nRelational Inductive Biases:\\n\\nBoth methods incorporate technical indicators (RSI, CCI, ADX) to inform decision-making.\\nReward shaping is based on these indicators, providing bonuses for actions taken under certain conditions\\n(e.g., buying in an oversold condition).\\n\\n\"\"\"\\n\\nimport numpy as np\\nimport modules.deep_evolution_strategy_function as deep\\nimport logging\\n\\nclass Agent():\\n    POPULATION_SIZE = 30\\n    SIGMA = 0.1\\n    LEARNING_RATE = 0.03\\n    NOISE_STD_DEV = 0.01  # Minimal Gaussian noise standard deviation\\n\\n    def __init__(self, model, window_size, trend, skip, initial_money, indicators):\\n        try:\\n            self.model = model\\n            self.window_size = window_size\\n            self.trend = np.array(trend)  # Ensure trend is a numpy array\\n            self.skip = skip\\n            self.initial_money = initial_money\\n            self.indicators = {k: np.array(v) for k, v in indicators.items()}  # Ensure indicators are numpy arrays\\n            self.es = deep.Deep_Evolution_Strategy(\\n                self.model.get_weights(),\\n                self.get_tick_reward,\\n                self.POPULATION_SIZE,\\n                self.SIGMA,\\n                self.LEARNING_RATE,\\n            )\\n            self.experience_replay = []  # Initialize experience replay buffer\\n        except Exception as e:\\n            logging.error(f\"Error initializing Agent: {e}\")\\n            raise\\n\\n    def act(self, sequence):\\n        try:\\n            decision = self.model.predict(np.array(sequence))\\n            action = np.argmax(decision[0])\\n\\n            # Add minimal Gaussian noise to the action for exploration\\n            noise = np.random.normal(0, self.NOISE_STD_DEV)  # Minimal Gaussian noise\\n            action += int(noise)  # Convert noise to integer action\\n            action = np.clip(action, 0, 2)  # Ensure action is within bounds\\n\\n            return action\\n        except Exception as e:\\n            logging.error(f\"Error in act method: {e}\")\\n            raise\\n\\n    def get_state(self, t):\\n        try:\\n            window_size = self.window_size + 1\\n            d = t - window_size + 1\\n\\n            # Ensure that block has the correct length\\n            if d >= 0:\\n                block = self.trend[d:t + 1]\\n            else:\\n                # Pad with the first value if there\\'s not enough data\\n                block = np.concatenate([self.trend[0] * np.ones(abs(d)), self.trend[0:t + 1]])\\n\\n            res = []\\n            for i in range(len(block) - 1):\\n                res.append(block[i + 1] - block[i])\\n\\n            # Ensure the length of res matches the expected window_size\\n            if len(res) < self.window_size:\\n                res = np.pad(res, (self.window_size - len(res), 0), \\'constant\\')\\n\\n            indicators_state = [self.indicators[indicator][t] for indicator in self.indicators]\\n            state = np.array([res + indicators_state])\\n\\n            return state\\n        except Exception as e:\\n            logging.error(f\"Error in get_state method: {e}\")\\n            raise\\n\\n    def predict(self):\\n        try:\\n            actions = []\\n            for t in range(len(self.trend) - self.window_size):\\n                state = self.get_state(t)\\n                action_index = self.act(state)\\n                if action_index == 0:\\n                    actions.append(\\'Hold\\')\\n                elif action_index == 1:\\n                    actions.append(\\'Buy\\')\\n                elif action_index == 2:\\n                    actions.append(\\'Sell\\')\\n            return actions\\n        except Exception as e:\\n            logging.error(f\"Error in predict method: {e}\")\\n            raise\\n\\n    def get_tick_reward(self, weights):\\n        try:\\n            initial_money = self.initial_money\\n            earned_money = 0\\n            self.model.weights = weights\\n            state = self.get_state(0)\\n            position = 0\\n            bought_price = 0\\n            past_rewards = []  # Store past rewards for meta-learning\\n            holding_penalty = 0  # Penalty for holding positions too long\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the reward calculation\\n                rsi = self.indicators[\\'rsi\\'][t]\\n                adx = self.indicators[\\'adx\\'][t]\\n\\n                # Modify reward based on indicators and past performance\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    bought_price = self.trend[t] + 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n                    if rsi < 30:  # Oversold condition\\n                        earned_money += 10  # Bonus for buying in an oversold condition\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    bought_price = self.trend[t] - 0.25\\n                    if adx > 25:  # Strong trend condition\\n                        earned_money += 5  # Bonus for selling in a strong trend\\n                    if rsi > 70:  # Overbought condition\\n                        earned_money += 10  # Bonus for selling in an overbought condition\\n\\n                # Penalty for holding positions too long\\n                if position != 0:\\n                    holding_penalty += 1\\n                    if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                        earned_money -= 5  # Penalty for holding too long\\n\\n                # Store the reward for meta-learning\\n                past_rewards.append(earned_money)\\n\\n                # Adapt the reward based on past performance\\n                if len(past_rewards) > 5:  # Use the last 5 rewards for adaptation\\n                    avg_reward = np.mean(past_rewards[-5:])\\n                    if avg_reward < 0:  # If the average reward is negative, adjust strategy\\n                        self.SIGMA *= 1.1  # Increase exploration\\n                    else:\\n                        self.SIGMA *= 0.9  # Decrease exploration\\n\\n                # Store experience in replay buffer\\n                self.experience_replay.append((state, action, earned_money, next_state))\\n\\n                state = next_state\\n            logging.debug(f\\'Earned money: {earned_money}\\')\\n            return (earned_money / initial_money) * 100\\n        except Exception as e:\\n            logging.error(f\"Error in get_tick_reward method: {e}\")\\n            raise\\n\\n    def fit(self, iterations, checkpoint):\\n        try:\\n            # Call the train method with appropriate parameters\\n            self.es.train(epoch=iterations, print_every=checkpoint, update_frequency=5, patience=10)\\n        except Exception as e:\\n            logging.error(f\"Error in fit method: {e}\")\\n            raise\\n\\n    def buy_tick(self):\\n        try:\\n            initial_money = self.initial_money\\n            state = self.get_state(0)\\n            states_sell = []\\n            states_buy = []\\n\\n            earned_money = 0\\n            position = 0\\n            bought_price = 0\\n            past_actions = []  # Store past actions for meta-learning\\n\\n            for t in range(0, len(self.trend) - 1, self.skip):\\n                action = self.act(state)\\n                next_state = self.get_state(t + 1)\\n\\n                # Incorporate indicators into the buy/sell decision\\n                rsi = self.indicators[\\'rsi\\'][t]\\n\\n                # Store the action for meta-learning\\n                past_actions.append(action)\\n\\n                if action == 1 and (position == 0 or position == -1):  # Buy\\n                    if position == -1:\\n                        earned_money += ((bought_price - self.trend[t]) * 4 * 12.5) - 5.8\\n                    position = 1\\n                    states_buy.append(t)\\n                    bought_price = self.trend[t] + 0.25\\n                    print(\\'bar %d: buy 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                elif action == 2 and (position == 0 or position == 1):  # Sell\\n                    if position == 1:\\n                        earned_money += ((self.trend[t] - bought_price) * 4 * 12.5) - 5.8\\n                    position = -1\\n                    states_sell.append(t)\\n                    bought_price = self.trend[t] - 0.25\\n                    print(\\'bar %d: sell 1 contract at price %f, total balance %f\\' % (t, self.trend[t], earned_money))\\n\\n                # Additional logic based on indicators\\n                if rsi > 70 and position == 1:  # Overbought condition\\n                    print(\\'bar %d: consider selling due to overbought condition\\' % t)\\n\\n                # Adapt future actions based on past actions\\n                if len(past_actions) > 5:  # Use the last 5 actions for adaptation\\n                    action_counts = np.bincount(past_actions[-5:], minlength=3)  # Count actions\\n                    if action_counts[1] > action_counts[2]:  # More buys than sells\\n                        print(\\'Consider reducing buy actions due to recent performance.\\')\\n\\n                state = next_state\\n\\n            roi = (earned_money / initial_money) * 100\\n            total_gains = earned_money\\n            return states_buy, states_sell, total_gains, roi\\n        except Exception as e:\\n            logging.error(f\"Error in buy_tick method: {e}\")\\n            raise\\n\\n    def calculate_metrics(self, cumulative_profit):\\n        try:\\n            ticks_earned = cumulative_profit.iloc[-1] / (12.5 * 4)\\n            max_drawdown, start_idx, end_idx = self.calculate_max_drawdown(cumulative_profit)\\n            return ticks_earned, max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_metrics method: {e}\")\\n            raise\\n\\n    @staticmethod\\n    def calculate_max_drawdown(cumulative_profit):\\n        try:\\n            peak = cumulative_profit.iloc[0]\\n            max_drawdown = 0\\n            start_idx, end_idx = 0, 0\\n\\n            for i in range(1, len(cumulative_profit)):\\n                if cumulative_profit.iloc[i] > peak:\\n                    peak = cumulative_profit.iloc[i]\\n                drawdown = peak - cumulative_profit.iloc[i]\\n                if drawdown > max_drawdown:\\n                    max_drawdown = drawdown\\n                    start_idx, end_idx = peak, cumulative_profit.iloc[i]\\n            return max_drawdown, start_idx, end_idx\\n        except Exception as e:\\n            logging.error(f\"Error in calculate_max_drawdown method: {e}\")\\n            raise', 'Separate the reward for the action taken from the profit/loss calculation. You could define a reward structure that considers both the profit/loss and the conditions under which the action was taken.\\n Make the holding penalty more dynamic. For example, you could increase the penalty based on how long the position has been held or the volatility of the market during that period.\\nEnsure that the state reflects the most current information available at the time of the action. You might want to update the state before making the action decision.\\n\\n\\ndef get_tick_reward(self, weights):\\n    try:\\n        initial_money = self.initial_money\\n        earned_money = 0\\n        self.model.weights = weights\\n        state = self.get_state(0)\\n        position = 0\\n        bought_price = 0\\n        holding_penalty = 0\\n\\n        for t in range(0, len(self.trend) - 1, self.skip):\\n            action = self.act(state)\\n            next_state = self.get_state(t + 1)\\n\\n            # Incorporate indicators into the reward calculation\\n            rsi = self.indicators[\\'rsi\\'][t]\\n            adx = self.indicators[\\'adx\\'][t]\\n\\n            # Calculate reward based on action taken\\n            reward = 0\\n            if action == 1 and (position == 0 or position == -1):  # Buy\\n                if position == -1:\\n                    earned_money += ((bought_price - self.trend[t] + 0.25) * 4 * 12.5) - 5.8\\n                position = 1\\n                bought_price = self.trend[t] + 0.25\\n                if adx > 25:  # Strong trend condition\\n                    reward += 5  # Bonus for buying in a strong trend\\n                if rsi < 30:  # Oversold condition\\n                    reward += 10  # Bonus for buying in an oversold condition\\n\\n            elif action == 2 and (position == 0 or position == 1):  # Sell\\n                if position == 1:\\n                    earned_money += ((self.trend[t] - bought_price - 0.25) * 4 * 12.5) - 5.8\\n                position = -1\\n                bought_price = self.trend[t] - 0.25\\n                if adx > 25:  # Strong trend condition\\n                    reward += 5  # Bonus for selling in a strong trend\\n                if rsi > 70:  # Overbought condition\\n                    reward += 10  # Bonus for selling in an overbought condition\\n\\n            # Penalty for holding positions too long\\n            if position != 0:\\n                holding_penalty += 1\\n                if holding_penalty > 10:  # Arbitrary threshold for holding too long\\n                    reward -= 5  # Penalty for holding too long\\n\\n            # Calculate total reward\\n            total_reward = (earned_money / initial_money) * 100 + reward\\n\\n            # Store experience in replay buffer\\n            self.experience_replay.append((state, action, total_reward, next_state))\\n\\n            state = next_state\\n\\n        logging.debug(f\\'Earned money: {earned_money}, Total Reward: {total_reward}\\')\\n        return total_reward\\n    except Exception as e:\\n        logging.error(f\"Error in get_tick_reward method: {e}\")\\n        raise', 'import gc\\nimport logging\\n\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import (\\n    get_state\\n)\\nfrom .utils import (\\n    format_currency,\\n    format_position\\n)\\n\\n\\ndef monitor_memory(threshold=80):\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'close\\'):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        if monitor_memory():\\n            agent.save()\\n            gc.collect()\\n            # break\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta  # max(delta, 0)\\n            total_profit += delta\\n\\n        # HOLD\\n        else:\\n            pass\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        agent.save()\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss))\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {}\".format(format_currency(data.iloc[t][adjcp_field])))\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta  # max(delta, 0)\\n            total_profit += delta\\n\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Position: {}\".format(\\n                    format_currency(data.iloc[t][adjcp_field]),\\n                    format_position(data.iloc[t][adjcp_field] - bought_price)))\\n        # HOLD\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action\\n', 'Please check the following code:\\n\\nimport gc\\nimport logging\\n\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import (\\n    get_state\\n)\\nfrom .utils import (\\n    format_currency,\\n    format_position\\n)\\n\\n\\ndef monitor_memory(threshold=80):\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'close\\'):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        if monitor_memory():\\n            agent.save()\\n            gc.collect()\\n            # break\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta  # max(delta, 0)\\n            total_profit += delta\\n\\n        # HOLD\\n        else:\\n            pass\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        agent.save()\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss))\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {}\".format(format_currency(data.iloc[t][adjcp_field])))\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta  # max(delta, 0)\\n            total_profit += delta\\n\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Position: {}\".format(\\n                    format_currency(data.iloc[t][adjcp_field]),\\n                    format_position(data.iloc[t][adjcp_field] - bought_price)))\\n        # HOLD\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action\\n', 'Check the following code:\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2023\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2023:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2023:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2023:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\"\"\"\\n\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n\\ndef main(train_stock, train_percent, date_field, adjcp_field, window_size, batch_size, ep_count,\\n         strategy=\"t-dqn\", model_name=\"model_debug\", debug=False):\\n    \"\"\" Trains the stock trading bot using Deep Q-Learning.\\n    Please see https://arxiv.org/abs/1312.5602 for more details.\\n\\n    Args: [python train.py --help]\\n    \"\"\"\\n    agent = Agent(window_size, strategy=strategy, pretrained=True, model_name=model_name)\\n    data = load_data(train_stock, date_field)\\n    take = int(len(data) * (train_percent / 100))\\n    train_data = data_split(data, 0, take, adjcp_field)\\n    val_data = data_split(data, take, int(len(data)), adjcp_field)\\n\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = -1000000000\\n    episode = 0\\n    episode_count = ep_count\\n    while True:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   window_size=window_size, prev_agent_profit=agent_profit, adjcp_field=adjcp_field)\\n        agent_profit = train_result[2]\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, strategy)\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        if episode >= episode_count:\\n            break\\n        episode += 1\\n\\n\\nif __name__ == \"__main__\":\\n    args = docopt(__doc__)\\n\\n    file_data = args[\"<file-data>\"]\\n    train_percent = args[\"<train-percent>\"]\\n\\n    date_field = args[\"--date-field\"]\\n    adjcp_field = args[\"--adjcp-field\"]\\n    window_size = int(args[\"--window-size\"])\\n    batch_size = int(args[\"--batch-size\"])\\n    ep_count = int(args[\"--episode-count\"])\\n    model_name = args[\"--model-name\"]\\n    debug = args[\"--debug\"]\\n\\n    coloredlogs.install(level=\"DEBUG\")\\n    switch_k_backend_device()\\n\\n    try:\\n        # dqn\\n        main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n             ep_count, \"dqn\", f\"{model_name}_dqn\", debug)\\n\\n        # # t-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"t-dqn\", f\"{model_name}_tdqn\", debug)\\n        #\\n        # # double-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"double-dqn\", f\"{model_name}_ddqn\", debug)\\n\\n    except KeyboardInterrupt:\\n        print(\"Aborted!\")\\n', \"how i make the following give a reward on a buy, and add a profit also on the following:\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field='Close'):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f'{agent.strategy}: Episode {episode}/{ep_count}'):\\n\\n        if monitor_memory():\\n            agent.save()\\n            gc.collect()\\n            # break\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta  # max(delta, 0)\\n            total_profit += delta\\n\\n        # HOLD\\n        else:\\n            pass\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        agent.save()\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss))\\n\\n\", \"how i make the following give a reward on a buy, and add a profit just like the way I have it on the sell on the following code:\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field='Close'):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f'{agent.strategy}: Episode {episode}/{ep_count}'):\\n\\n        if monitor_memory():\\n            agent.save()\\n            gc.collect()\\n            # break\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta  # max(delta, 0)\\n            total_profit += delta\\n\\n        # HOLD\\n        else:\\n            pass\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        agent.save()\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss))\", \"how i make the following give a reward on a buy, and add a the delta to total_profit just like the way I have it on the sell on the following code:\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0, adjcp_field='Close'): total_profit = 0 data_length = len(data) - 1\\n\\nagent.inventory = []\\navg_loss = []\\n\\nstate = get_state(data, 0, window_size + 1)\\n\\nfor t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n              desc=f'{agent.strategy}: Episode {episode}/{ep_count}'):\\n\\n    if monitor_memory():\\n        agent.save()\\n        gc.collect()\\n        # break\\n\\n    reward = 0\\n    next_state = get_state(data, t + 1, window_size + 1)\\n\\n    # select an action\\n    action = agent.act(state)\\n\\n    # BUY\\n    if action == 1:\\n        agent.inventory.append(data.iloc[t][adjcp_field])\\n\\n    # SELL\\n    elif action == 2 and len(agent.inventory) > 0:\\n        bought_price = agent.inventory.pop(0)\\n        delta = data.iloc[t][adjcp_field] - bought_price\\n        reward = delta  # max(delta, 0)\\n        total_profit += delta\\n\\n    # HOLD\\n    else:\\n        pass\\n\\n    done = (t == data_length - 1)\\n    agent.remember(state, action, reward, next_state, done)\\n\\n    if len(agent.memory) > batch_size:\\n        loss = agent.train_experience_replay(batch_size)\\n        avg_loss.append(loss)\\n\\n    state = next_state\\n\\nif total_profit > 0 and total_profit > prev_agent_profit:\\n    agent.save()\\n\\nreturn episode, ep_count, total_profit, np.mean(np.array(avg_loss))\", 'how i make the following give a reward on a buy, and add a the delta to total_profit just like the way I have it on the sell on the following code:\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {}\".format(format_currency(data.iloc[t][adjcp_field])))\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta  # max(delta, 0)\\n            total_profit += delta\\n\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Position: {}\".format(\\n                    format_currency(data.iloc[t][adjcp_field]),\\n                    format_position(data.iloc[t][adjcp_field] - bought_price)))\\n        # HOLD\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n', 'How can I make the following code better:\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\n\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\\n    \"\"\"\\n    error = y_true - y_pred\\n    cond = keras.backend.abs(error) <= clip_delta\\n    squared_loss = 0.5 * keras.backend.square(error)\\n    quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n    return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n\\n        # agent config\\n        self.state_size = state_size  # normalized previous days\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n        self.first_iter = True\\n\\n        # model config\\n        self.model_name = model_name\\n        self.gamma = 0.95  # affinity for long term reward\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}  # important for loading the model from memory\\n        # self.optimizer = tf.keras.optimizers.legacy.Adam(self.learning_rate)\\n        self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\\n        model_path = f\"models/{self.model_name}.keras\"\\n        if pretrained and os.path.exists(model_path) and self.model_name is not None:\\n            self.model = self.load()\\n        else:\\n            self.model = self._model()\\n\\n        # strategy config\\n        if self.strategy in [\"t-dqn\", \"double-dqn\"]:\\n            self.n_iter = 1\\n            self.reset_every = reset_every\\n\\n            # target network\\n            self.target_model = tf.keras.models.clone_model(self.model)\\n            self.target_model.set_weights(self.model.get_weights())\\n\\n    def _model(self):\\n        \"\"\"Creates the model\\n        \"\"\"\\n        model = keras.Sequential()\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=self.action_size))\\n\\n        model.compile(loss=self.loss, optimizer=self.optimizer)\\n        return model\\n\\n    def remember(self, state, action, reward, next_state, done):\\n        \"\"\"Adds relevant data to memory\\n        \"\"\"\\n        self.memory.append((state, action, reward, next_state, done))\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions\\n        \"\"\"\\n        # take random action in order to diversify experience at the beginning\\n        if not is_eval and random.random() <= self.epsilon:\\n            return random.randrange(self.action_size)\\n\\n        if self.first_iter:\\n            self.first_iter = False\\n            return 1  # make a definite buy on the first iter\\n\\n        action_probs = self.model.predict(state, verbose=0)\\n        return np.argmax(action_probs[0])\\n\\n    def train_experience_replay(self, batch_size):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        mini_batch = random.sample(self.memory, batch_size)\\n        X_train, y_train = [], []\\n\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation\\n                    target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # DQN with fixed targets\\n        elif self.strategy == \"t-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation with fixed targets\\n                    target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # Double DQN\\n        elif self.strategy == \"double-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate double deep q-learning equation\\n                    target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                        np.argmax(self.model.predict(next_state)[0])]\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        else:\\n            raise NotImplementedError()\\n\\n        # update q-function parameters based on huber loss gradient\\n        loss = self.model.fit(\\n            np.array(X_train), np.array(y_train),\\n            epochs=1, verbose=0\\n        ).history[\"loss\"][0]\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def save(self):\\n        path = pathlib.Path()\\n        self.model.save(path / \"models/{}.keras\".format(self.model_name), overwrite=True)\\n\\n    def load(self):\\n        return tf.keras.models.load_model(\"models/{}.keras\".format(self.model_name), custom_objects=self.custom_objects)\\n', 'How can I make the following code better:\\n\\n\"\"\"\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, June 15, 2024\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t15-Jun-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t17-Jun-2024:\\tRev. 1.01\\t    Added the following rsi, cci, and adx indicator.\\n[JSanchez]\\t21-Jun-2024:\\tRev. 1.02\\t    Added the requirements.txt file - pip freeze > requirements.txt\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\"\"\"\\n\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    show_train_result, switch_k_backend_device\\n)\\n\\n\\ndef main(train_stock, train_percent, date_field, adjcp_field, window_size, batch_size, ep_count,\\n         strategy=\"t-dqn\", model_name=\"model_debug\", debug=False):\\n    \"\"\" Trains the stock trading bot using Deep Q-Learning.\\n    Please see https://arxiv.org/abs/1312.5602 for more details.\\n\\n    Args: [python train.py --help]\\n    \"\"\"\\n    agent = Agent(window_size, strategy=strategy, pretrained=True, model_name=model_name)\\n    data = load_data(train_stock, date_field)\\n    take = int(len(data) * (train_percent / 100))\\n    train_data = data_split(data, 0, take, adjcp_field)\\n    val_data = data_split(data, take, int(len(data)), adjcp_field)\\n\\n    initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n    agent_profit = 0\\n    prev_agent_profit = -1000000000\\n    episode = 0\\n    episode_count = ep_count\\n    while True:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   window_size=window_size, prev_agent_profit=agent_profit, adjcp_field=adjcp_field)\\n        agent_profit = train_result[2]\\n        if agent_profit > 0:\\n            val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n            show_train_result(train_result, val_result, initial_offset, strategy)\\n        if agent_profit > prev_agent_profit:\\n            prev_agent_profit = agent_profit\\n        if episode >= episode_count:\\n            break\\n        episode += 1\\n\\n\\nif __name__ == \"__main__\":\\n    args = docopt(__doc__)\\n\\n    file_data = args[\"<file-data>\"]\\n    train_percent = args[\"<train-percent>\"]\\n\\n    date_field = args[\"--date-field\"]\\n    adjcp_field = args[\"--adjcp-field\"]\\n    window_size = int(args[\"--window-size\"])\\n    batch_size = int(args[\"--batch-size\"])\\n    ep_count = int(args[\"--episode-count\"])\\n    model_name = args[\"--model-name\"]\\n    debug = args[\"--debug\"]\\n\\n    coloredlogs.install(level=\"DEBUG\")\\n    switch_k_backend_device()\\n\\n    try:\\n        # dqn\\n        main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n             ep_count, \"dqn\", f\"{model_name}_dqn\", debug)\\n\\n        # # t-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"t-dqn\", f\"{model_name}_tdqn\", debug)\\n        #\\n        # # double-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"double-dqn\", f\"{model_name}_ddqn\", debug)\\n\\n    except KeyboardInterrupt:\\n        print(\"Aborted!\")\\n', 'The following code does not run the predict procedure when running:\\n\\n\\nimport os\\nfrom pathlib import Path\\n\\nimport pandas as pd\\nfrom flask import Flask, jsonify, request\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.ops import get_state\\nfrom trading_bot.utils import switch_k_backend_device, add_indicators\\n\\napp = Flask(__name__)\\napp.config[\\'WTF_CSRF_CHECK_DEFAULT\\'] = False\\napp.config[\\'WTF_CSRF_ENABLED\\'] = False\\n\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    global agent\\n    rq = request.json\\n\\n    # Extract the data\\n    data = rq.get(\\'Data\\', [])\\n\\n    # Check for the presence of the apiKey\\n    # api_key = rq.get(\\'apiKey\\', \\'\\')\\n\\n    # if not api_key or api_key != \"apiKey\":\\n    #     return jsonify({\"error\": \"Invalid or missing API key\"}), 401\\n\\n    # Convert the data to a DataFrame\\n    df = pd.DataFrame(data, columns=[\\'Date\\', \\'Open\\', \\'High\\', \\'Low\\', \\'Close\\', \\'Volume\\'])\\n    df = add_indicators(df)\\n\\n    # Convert the \\'date\\' column to datetime\\n    # df[\\'date\\'] = pd.to_datetime(df[\\'date\\'], format=\\'%m/%d/%Y %H:%M\\')\\n    t = len(df) - 1\\n    state = get_state(df, t, window_size)\\n    output = agent.act(state, True)\\n    if output == 1:\\n        return jsonify({\\'Res\\': \\'buy\\'}), 200\\n    if output == 2:\\n        return jsonify({\\'Res\\': \\'sell\\'}), 200\\n\\n    return jsonify({\\'Res\\': \\'hold\\'}), 200\\n\\n\\nif __name__ == \"__main__\":\\n    predict_data = None\\n    directory_path = os.path.realpath(__file__)\\n    folder = Path(directory_path)\\n    window_size = 60\\n    env = None\\n    env_test = None\\n    switch_k_backend_device()\\n\\n    agent = Agent(window_size, pretrained=True, model_name=\\'models/model_dqn\\')\\n    agent.first_iter = False\\n    app.run(host=\"0.0.0.0\", debug=False, port=5020)\\n', 'how do I print the payload values from the following:\\n\\n\\n\\n\\xa0 \\xa0 \\xa0 private object CreateJsonPayload(int _loopNo)\\n\\xa0 \\xa0 \\xa0 \\xa0 {\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\tvar date = new List<DateTime>(); \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\n\\t\\tvar open = new List<double>();\\n\\t\\tvar high = new List<double>();\\n\\t\\tvar low = new List<double>();\\n\\t\\tvar close = new List<double>();\\n\\t\\tvar volume = new List<double>();\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 for (int i = _loopNo; i >= 0; i--)\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 {\\n\\n\\t\\tdate.Add(Times[0][I]);\\n\\t\\topen.Add(Open[i]);\\n\\t\\thigh.Add(High[i]);\\n\\t\\tlow.Add(Low[i]);\\n\\t\\tclose.Add(Close[i]);\\n\\t\\tvolume.Add(Volume[i]);\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 }\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 var indicators = new Dictionary<string, List<double>>\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 {\\n// \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0{ \"Date\", date },\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 { \"Opem\", open },\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 { \"High\", high },\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 { \"Low\", low },\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 { \"Close\", close },\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 { \"Volume\", volume }\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 };\\n\\n\\t    var payload = new\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 {\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 indicators = indicators\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 };\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 return payload;\\n\\xa0 \\xa0 \\xa0 \\xa0 }\\xa0\\xa0\\n\\n', 'how do I add the date to the indicators part\\n\\nprivate object CreateJsonPayload(int _loopNo)\\n{\\n    var date = new List<DateTime>();          \\n    var open = new List<double>();\\n    var high = new List<double>();\\n    var low = new List<double>();\\n    var close = new List<double>();\\n    var volume = new List<double>();\\n\\n    for (int i = _loopNo; i >= 0; i--)\\n    {\\n        date.Add(Times[0][i]);\\n        open.Add(Open[i]);\\n        high.Add(High[i]);\\n        low.Add(Low[i]);\\n        close.Add(Close[i]);\\n        volume.Add(Volume[i]);\\n    }\\n\\n    var indicators = new Dictionary<string, List<double>>\\n    {\\n        // { \"Date\", date }, // Uncomment if you want to include date\\n        { \"Open\", open },\\n        { \"High\", high },\\n        { \"Low\", low },\\n        { \"Close\", close },\\n        { \"Volume\", volume }\\n    };\\n\\n    // Print the payload values\\n    Console.WriteLine(\"Payload Values:\");\\n    foreach (var indicator in indicators)\\n    {\\n        Console.WriteLine($\"{indicator.Key}: {string.Join(\", \", indicator.Value)}\");\\n    }\\n\\n    var payload = new\\n    {\\n        indicators = indicators\\n    };\\n\\n    return payload;\\n}', 'Please check the following :\\nimport os\\nfrom pathlib import Path\\nimport pandas as pd\\nfrom flask import Flask, jsonify, request\\nimport logging\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.ops import get_state\\nfrom trading_bot.utils import switch_k_backend_device, add_indicators\\n\\napp = Flask(__name__)\\napp.config[\\'WTF_CSRF_CHECK_DEFAULT\\'] = False\\napp.config[\\'WTF_CSRF_ENABLED\\'] = False\\n\\n# Set up logging\\nlogging.basicConfig(level=logging.INFO)\\n\\n@app.route(\\'/predict\\', methods=[\\'POST\\'])\\ndef predict():\\n    global agent\\n    rq = request.get_json()\\n\\n    # Extract the data\\n    data = rq.get(\\'data\\', [])\\n    logging.info(f\"Received data: {data}\")\\n\\n    # Convert the data to a DataFrame\\n    df = pd.DataFrame(data, columns=[\\'Date\\', \\'Open\\', \\'High\\', \\'Low\\', \\'Close\\', \\'Volume\\'])\\n    df = add_indicators(df)\\n\\n    # Check if DataFrame is empty or has insufficient rows\\n    if df.empty or len(df) < window_size:\\n        return jsonify({\\'error\\': \\'Insufficient data for prediction\\'}), 400\\n\\n    # Prepare the state for prediction\\n    t = len(df) - 1\\n    state = get_state(df, t, window_size)\\n    logging.info(f\"State for prediction: {state}\")\\n\\n    # Make a prediction\\n    output = agent.act(state, True)\\n    logging.info(f\"Predicted action: {output}\")\\n\\n    if output == 1:\\n        return jsonify({\\'Res\\': \\'buy\\'}), 200\\n    elif output == 2:\\n        return jsonify({\\'Res\\': \\'sell\\'}), 200\\n\\n    return jsonify({\\'Res\\': \\'hold\\'}), 200\\n\\nif __name__ == \"__main__\":\\n    directory_path = os.path.realpath(__file__)\\n    folder = Path(directory_path)\\n    window_size = 60\\n    adjcp-field = \"Close\"\\n    date-field = \"Date\"\\n    switch_k_backend_device()\\n\\n    # Initialize the agent\\n    agent = Agent(--date-field, adjcp-field, window_size, pretrained=True, model_name=\\'models/model_dqn\\')\\n    agent.first_iter = False\\n    app.run(host=\"0.0.0.0\", debug=False, port=5020)', 'How do I convert payload ToList() for the following code:\\n\\n private object CreateJsonPayload(int _loopNo)\\n{\\n   var date = new List<string>(); // Change to List<string> for JSON serialization\\n   var open = new List<double>();\\n   var high = new List<double>();\\n   var low = new List<double>();\\n   var close = new List<double>();\\n   var volume = new List<double>();\\n\\n   for (int i = _loopNo; i >= 0; i--)\\n   {\\n       date.Add(Times[0][i].ToString(\"yyyy-MM-dd\")); // Convert DateTime to string\\n       open.Add(Open[i]);\\n       high.Add(High[i]);\\n       low.Add(Low[i]);\\n       close.Add(Close[i]);\\n       volume.Add(Volume[i]);\\n   }\\n\\n   var indicators = new Dictionary<string, List<double>>\\n   {\\n       { \"Date\", date.Select(d => Convert.ToDouble((DateTime.Parse(d) - new DateTime(1970, 1, 1)).TotalSeconds)).ToList() }, // Convert to Unix timestamp\\n       { \"Open\", open },\\n       { \"High\", high },\\n       { \"Low\", low },\\n       { \"Close\", close },\\n       { \"Volume\", volume }\\n   };\\n\\n   // Print the payload values\\n   Print(\"Payload Values:\");\\n   foreach (var indicator in indicators)\\n   {\\n       Print($\"{indicator.Key}: {string.Join(\", \", indicator.Value)}\");\\n   }\\n\\n   var payload = new\\n   {\\n       indicators = indicators\\n   };\\n\\n   return payload;\\n}\\n', 'How do I convert payload  to a system.colletions.generic.list for the following code:\\n\\nprivate object CreateJsonPayload(int _loopNo)\\n{\\n    var date = new List<string>(); // Change to List<string> for JSON serialization\\n    var open = new List<double>();\\n    var high = new List<double>();\\n    var low = new List<double>();\\n    var close = new List<double>();\\n    var volume = new List<double>();\\n\\n    for (int i = _loopNo; i >= 0; i--)\\n    {\\n        date.Add(Times[0][i].ToString(\"yyyy-MM-dd\")); // Convert DateTime to string\\n        open.Add(Open[i]);\\n        high.Add(High[i]);\\n        low.Add(Low[i]);\\n        close.Add(Close[i]);\\n        volume.Add(Volume[i]);\\n    }\\n\\n    var indicators = new List<Dictionary<string, object>>\\n    {\\n        new Dictionary<string, object>\\n        {\\n            { \"Date\", date.Select(d => Convert.ToDouble((DateTime.Parse(d) - new DateTime(1970, 1, 1)).TotalSeconds)).ToList() }, // Convert to Unix timestamp\\n            { \"Open\", open },\\n            { \"High\", high },\\n            { \"Low\", low },\\n            { \"Close\", close },\\n            { \"Volume\", volume }\\n        }\\n    };\\n\\n    // Print the payload values\\n    Console.WriteLine(\"Payload Values:\");\\n    foreach (var indicator in indicators)\\n    {\\n        foreach (var kvp in indicator)\\n        {\\n            Console.WriteLine($\"{kvp.Key}: {string.Join(\", \", (List<double>)kvp.Value)}\");\\n        }\\n    }\\n\\n    return indicators; // Return the list of dictionaries\\n}', 'Check the following buy and sell for the following code:\\n\\nimport logging\\n\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import (\\n    get_state\\n)\\nfrom .utils import (\\n    format_currency,\\n    format_position\\n)\\n\\n\\ndef monitor_memory(threshold=80):\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'close\\'):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size - 5)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size - 5)\\n\\n        # select an action\\n        action = agent.act(state)\\n\\n        # BUY\\n        if action == 1:\\n            delta = data.iloc[t + 5][adjcp_field] - data.iloc[t][adjcp_field]\\n            if delta > 3:\\n                reward = delta\\n            else:\\n                reward = -abs(delta)\\n            total_profit += delta\\n\\n        # SELL\\n        elif action == 2:\\n            delta = data.iloc[t][adjcp_field] - data.iloc[t + 5][adjcp_field]\\n            if delta > 3:\\n                reward = delta\\n            else:\\n                reward = -abs(delta)\\n            total_profit += delta\\n\\n        # HOLD\\n        else:\\n            pass\\n\\n        done = (t == data_length - 5)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        agent.save()\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss))\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size - 5)\\n\\n    for t in range(data_length):\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size - 5)\\n\\n        # select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        # BUY\\n        if action == 1:\\n            delta = data.iloc[t + 5][adjcp_field] - data.iloc[t][adjcp_field]\\n            reward = delta  # max(delta, 0)\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {} | Profit: {}\".format(\\n                    format_currency(data.iloc[t][adjcp_field]),\\n                    format_position(data.iloc[t + 5][adjcp_field] - data.iloc[t][adjcp_field])))\\n\\n        # SELL\\n        elif action == 2:\\n            delta = data.iloc[t][adjcp_field] - data.iloc[t + 5][adjcp_field]\\n            reward = delta  # max(delta, 0)\\n            total_profit += delta\\n\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Profit: {}\".format(\\n                    format_currency(data.iloc[t][adjcp_field]),\\n                    format_position(data.iloc[t][adjcp_field] - data.iloc[t + 5][adjcp_field])))\\n        # HOLD\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action\\n', 'improve the buy and sell for the following code:\\n\\nimport gc\\nimport logging\\n\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import (\\n    get_state\\n)\\nfrom .utils import (\\n    format_currency,\\n    format_position\\n)\\n\\n\\ndef monitor_memory(threshold=80):\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0,\\n                adjcp_field=\\'Close\\'):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    agent.inventory = []\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False,\\n                  desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n\\n        if monitor_memory():\\n            agent.save()\\n            gc.collect()\\n            # break\\n\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state)\\n\\n        \"\"\"\\n        Key Changes:\\n        Reward on Buy: When the action is BUY, the delta is calculated as the difference between the current\\n        price and the previous price in the inventory (if there is more than one item in the inventory). \\n        This delta is assigned as the reward.\\n\\n        Add Delta to Total Profit on Buy: The delta is added to the total_profit when a buy action is taken.\\n        \\n        This approach ensures that the agent receives feedback for both buying and selling actions, helping it \\n        to learn more effectively.\\n        \"\"\"\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = delta  # Reward is the change in price\\n                total_profit += delta\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta  # Reward is the profit made\\n            total_profit += delta\\n\\n        # HOLD\\n        else:\\n            pass\\n\\n        done = (t == data_length - 1)\\n        agent.remember(state, action, reward, next_state, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay(batch_size)\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    if total_profit > 0 and total_profit > prev_agent_profit:\\n        agent.save()\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss))\\n\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    agent.inventory = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        reward = 0\\n        next_state = get_state(data, t + 1, window_size + 1)\\n\\n        # select an action\\n        action = agent.act(state, is_eval=True)\\n\\n        \"\"\"\\n        Key Changes:\\n        Reward on Buy: When the action is BUY, the delta is calculated as the difference between the current \\n        price and the previous price in the inventory (if there is more than one item in the inventory). \\n        This delta is assigned as the reward.\\n\\n        Add Delta to Total Profit on Buy: The delta is added to the total_profit when a buy action is taken.\\n\\n        This approach ensures that the agent receives feedback for both buying and selling actions, helping it \\n        to learn more effectively. \\n        \"\"\"\\n\\n        # BUY\\n        if action == 1:\\n            agent.inventory.append(data.iloc[t][adjcp_field])\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {}\".format(format_currency(data.iloc[t][adjcp_field])))\\n\\n            # Calculate delta and reward for buy action\\n            if len(agent.inventory) > 1:\\n                previous_price = agent.inventory[-2]\\n                delta = data.iloc[t][adjcp_field] - previous_price\\n                reward = delta\\n                total_profit += delta\\n\\n        # SELL\\n        elif action == 2 and len(agent.inventory) > 0:\\n            bought_price = agent.inventory.pop(0)\\n            delta = data.iloc[t][adjcp_field] - bought_price\\n            reward = delta  # Reward is the profit made\\n            total_profit += delta\\n\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Position: {}\".format(\\n                    format_currency(data.iloc[t][adjcp_field]),\\n                    format_position(data.iloc[t][adjcp_field] - bought_price)))\\n\\n        # HOLD\\n        else:\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        done = (t == data_length - 1)\\n        agent.memory.append((state, action, reward, next_state, done))\\n\\n        state = next_state\\n        if done:\\n            return total_profit, history\\n\\n\\ndef predict_model(agent, data, window):\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action\\n', 'Check for errors\\n\\nimport logging\\nimport os\\n\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\n# Formats Position\\nformat_position = lambda price: (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n# Formats Currency\\nformat_currency = lambda price: \\'${0:.2f}\\'.format(abs(price))\\n\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\"Displays training results\"\"\"\\n    try:\\n        if val_position == initial_offset or val_position == 0.0:\\n            logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: USELESS  Train Loss: {:.4f}\\'\\n                         .format(strategy, result[0], result[1], format_position(result[2]), result[3]))\\n            return False\\n        else:\\n            logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: {}  Train Loss: {:.4f}\\'\\n                         .format(strategy, result[0], result[1], format_position(result[2]),\\n                                 format_position(val_position), result[3]))\\n            return True\\n    except Exception as e:\\n        logging.error(f\"Error in show_train_result: {e}\")\\n        return False\\n\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\"Displays eval results\"\"\"\\n    try:\\n        if profit == initial_offset or profit == 0.0:\\n            logging.info(\\'{}: USELESS\\\\n\\'.format(model_name))\\n        else:\\n            logging.info(\\'{}: {}\\\\n\\'.format(model_name, format_position(profit)))\\n    except Exception as e:\\n        logging.error(f\"Error in show_eval_result: {e}\")\\n\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\"Reads stock data from csv file\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        if date_field != \"Date\":\\n            df[\\'Date\\'] = df[date_field]\\n        df[\\'adjcp\\'] = df[adjcp_field]\\n        df = df.sort_values([date_field], ascending=True)\\n        df = add_indicators(df)\\n        return df\\n    except Exception as e:\\n        logging.error(f\"Error in load_data: {e}\")\\n        return pd.DataFrame()\\n\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\"Reads stock data from csv file\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        df = df.sort_values([date_field])\\n        return list(df[adjcp_field])\\n    except Exception as e:\\n        logging.error(f\"Error in get_stock_data: {e}\")\\n        return []\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\"Splits the dataset into training or testing using date\"\"\"\\n    try:\\n        data = df[(df.index >= start) & (df.index < end)]\\n        data.reset_index(drop=True, inplace=True)\\n        return data\\n    except Exception as e:\\n        logging.error(f\"Error in data_split: {e}\")\\n        return pd.DataFrame()\\n\\ndef switch_k_backend_device():\\n    \"\"\"Switches `keras` backend from GPU to CPU if required.\"\"\"\\n    try:\\n        if keras.backend == \"tensorflow\":\\n            logging.debug(\"Switching to TensorFlow for CPU\")\\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n    except Exception as e:\\n        logging.error(f\"Error in switch_k_backend_device: {e}\")\\n\\n\\ndef get_features():\\n    all_feature_name = [\\n        # \\'CDL2CROWS\\',\\n        # \\'CDL3BLACKCROWS\\',\\n        # \\'CDL3INSIDE\\',\\n        # \\'CDL3LINESTRIKE\\',\\n        # \\'CDL3OUTSIDE\\',\\n        # \\'CDL3STARSINSOUTH\\',\\n        # \\'CDL3WHITESOLDIERS\\',\\n        # \\'CDLABANDONEDBABY\\',\\n        # \\'CDLADVANCEBLOCK\\',\\n        # \\'CDLBELTHOLD\\',\\n        # \\'CDLBREAKAWAY\\',\\n        # \\'CDLCLOSINGMARUBOZU\\',\\n        # \\'CDLCONCEALBABYSWALL\\',\\n        # \\'CDLCOUNTERATTACK\\',\\n        \\'CDLDARKCLOUDCOVER\\',\\n        # \\'CDLDOJI\\',\\n        # \\'CDLDOJISTAR\\',\\n        \\'CDLDRAGONFLYDOJI\\',\\n        \\'CDLENGULFING\\',\\n        # \\'CDLEVENINGDOJISTAR\\',\\n        \\'CDLEVENINGSTAR\\',\\n        # \\'CDLGAPSIDESIDEWHITE\\',\\n        \\'CDLGRAVESTONEDOJI\\',\\n        \\'CDLHAMMER\\',\\n        \\'CDLHANGINGMAN\\',\\n        # \\'CDLHARAMI\\',\\n        # \\'CDLHARAMICROSS\\',\\n        # \\'CDLHIGHWAVE\\',\\n        # \\'CDLHIKKAKE\\',\\n        # \\'CDLHIKKAKEMOD\\',\\n        # \\'CDLHOMINGPIGEON\\',\\n        # \\'CDLIDENTICAL3CROWS\\',\\n        # \\'CDLINNECK\\',\\n        # \\'CDLINVERTEDHAMMER\\',\\n        # \\'CDLKICKING\\',\\n        # \\'CDLKICKINGBYLENGTH\\',\\n        # \\'CDLLADDERBOTTOM\\',\\n        # \\'CDLLONGLEGGEDDOJI\\',\\n        # \\'CDLLONGLINE\\',\\n        # \\'CDLMARUBOZU\\',\\n        # \\'CDLMATCHINGLOW\\',\\n        # \\'CDLMATHOLD\\',\\n        # \\'CDLMORNINGDOJISTAR\\',\\n        \\'CDLMORNINGSTAR\\',\\n        # \\'CDLONNECK\\',\\n        # \\'CDLPIERCING\\',\\n        # \\'CDLRICKSHAWMAN\\',\\n        # \\'CDLRISEFALL3METHODS\\',\\n        # \\'CDLSEPARATINGLINES\\',\\n        # \\'CDLSHOOTINGSTAR\\',\\n        # \\'CDLSHORTLINE\\',\\n        # \\'CDLSPINNINGTOP\\',\\n        # \\'CDLSTALLEDPATTERN\\',\\n        # \\'CDLSTICKSANDWICH\\',\\n        # \\'CDLTAKURI\\',\\n        # \\'CDLTASUKIGAP\\',\\n        # \\'CDLTHRUSTING\\',\\n        # \\'CDLTRISTAR\\',\\n        # \\'CDLUNIQUE3RIVER\\',\\n        # \\'CDLUPSIDEGAP2CROWS\\',\\n        # \\'CDLXSIDEGAP3METHODS\\',\\n        \\'MOMENTUM\\',\\n        \\'DX\\',\\n        \\'FASTK\\',\\n        \\'FASTD\\',\\n        \\'DX\\',\\n        \\'CCI\\',\\n        \\'WILLR\\',\\n        \\'MFI\\',\\n        \\'ROC\\',\\n        \\'ADX\\',\\n        \\'PPO\\',\\n        \\'RSI\\',\\n        \\'STDDEV\\',\\n        \\'SIN\\',\\n        \\'COS\\',\\n        \\'TAN\\'\\n    ]\\n    return all_feature_name\\n\\n\\ndef add_indicators(df):\\n    # Pattern Recognition Functions\\n    # df[\\'CDL2CROWS\\'] = talib.CDL2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3BLACKCROWS\\'] = talib.CDL3BLACKCROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3INSIDE\\'] = talib.CDL3INSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3LINESTRIKE\\'] = talib.CDL3LINESTRIKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3OUTSIDE\\'] = talib.CDL3OUTSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3STARSINSOUTH\\'] = talib.CDL3STARSINSOUTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3WHITESOLDIERS\\'] = talib.CDL3WHITESOLDIERS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLABANDONEDBABY\\'] = talib.CDLABANDONEDBABY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLADVANCEBLOCK\\'] = talib.CDLADVANCEBLOCK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLBELTHOLD\\'] = talib.CDLBELTHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLBREAKAWAY\\'] = talib.CDLBREAKAWAY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCLOSINGMARUBOZU\\'] = talib.CDLCLOSINGMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCONCEALBABYSWALL\\'] = talib.CDLCONCEALBABYSWALL(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCOUNTERATTACK\\'] = talib.CDLCOUNTERATTACK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDARKCLOUDCOVER\\'] = talib.CDLDARKCLOUDCOVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLDOJI\\'] = talib.CDLDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLDOJISTAR\\'] = talib.CDLDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDRAGONFLYDOJI\\'] = talib.CDLDRAGONFLYDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLENGULFING\\'] = talib.CDLENGULFING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLEVENINGDOJISTAR\\'] = talib.CDLEVENINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLEVENINGSTAR\\'] = talib.CDLEVENINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLGAPSIDESIDEWHITE\\'] = talib.CDLGAPSIDESIDEWHITE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLGRAVESTONEDOJI\\'] = talib.CDLGRAVESTONEDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHAMMER\\'] = talib.CDLHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHANGINGMAN\\'] = talib.CDLHANGINGMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHARAMI\\'] = talib.CDLHARAMI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHARAMICROSS\\'] = talib.CDLHARAMICROSS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIGHWAVE\\'] = talib.CDLHIGHWAVE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIKKAKE\\'] = talib.CDLHIKKAKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIKKAKEMOD\\'] = talib.CDLHIKKAKEMOD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHOMINGPIGEON\\'] = talib.CDLHOMINGPIGEON(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLIDENTICAL3CROWS\\'] = talib.CDLIDENTICAL3CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLINNECK\\'] = talib.CDLINNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLINVERTEDHAMMER\\'] = talib.CDLINVERTEDHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLKICKING\\'] = talib.CDLKICKING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLKICKINGBYLENGTH\\'] = talib.CDLKICKINGBYLENGTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLADDERBOTTOM\\'] = talib.CDLLADDERBOTTOM(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLONGLEGGEDDOJI\\'] = talib.CDLLONGLEGGEDDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLONGLINE\\'] = talib.CDLLONGLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMARUBOZU\\'] = talib.CDLMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMATCHINGLOW\\'] = talib.CDLMATCHINGLOW(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMATHOLD\\'] = talib.CDLMATHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLMORNINGDOJISTAR\\'] = talib.CDLMORNINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLMORNINGSTAR\\'] = talib.CDLMORNINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLONNECK\\'] = talib.CDLONNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLPIERCING\\'] = talib.CDLPIERCING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLRICKSHAWMAN\\'] = talib.CDLRICKSHAWMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLRISEFALL3METHODS\\'] = talib.CDLRISEFALL3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSEPARATINGLINES\\'] = talib.CDLSEPARATINGLINES(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSHOOTINGSTAR\\'] = talib.CDLSHOOTINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSHORTLINE\\'] = talib.CDLSHORTLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSPINNINGTOP\\'] = talib.CDLSPINNINGTOP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSTALLEDPATTERN\\'] = talib.CDLSTALLEDPATTERN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSTICKSANDWICH\\'] = talib.CDLSTICKSANDWICH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTAKURI\\'] = talib.CDLTAKURI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTASUKIGAP\\'] = talib.CDLTASUKIGAP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTHRUSTING\\'] = talib.CDLTHRUSTING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTRISTAR\\'] = talib.CDLTRISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLUNIQUE3RIVER\\'] = talib.CDLUNIQUE3RIVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLUPSIDEGAP2CROWS\\'] = talib.CDLUPSIDEGAP2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLXSIDEGAP3METHODS\\'] = talib.CDLXSIDEGAP3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # Price action\\n    # https://ta-lib.github.io/ta-lib-python/func_groups/momentum_indicators.html\\n    # rsi,stochRsi,cci,williamsr,mfi,roc,atr,adx\\n    df[\\'MOMENTUM\\'] = talib.MOM(df[\\'Close\\'], timeperiod=10)\\n    df[\\'DX\\'] = talib.stream_DX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'FASTK\\'], df[\\'FASTD\\'] = talib.STOCHRSI(df[\\'Close\\'], timeperiod=14, fastk_period=5, fastd_period=3,\\n                                              fastd_matype=0)\\n    df[\\'CCI\\'] = talib.CCI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'WILLR\\'] = talib.WILLR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'MFI\\'] = talib.MFI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'], timeperiod=14)\\n    df[\\'ROC\\'] = talib.ROC(df[\\'Close\\'], timeperiod=10)\\n    # df[\\'ATR\\'] = talib.ATR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'ADX\\'] = talib.ADX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'PPO\\'] = talib.PPO(df[\\'Close\\'], fastperiod=12, slowperiod=26, matype=0)\\n    df[\\'RSI\\'] = talib.RSI(df[\\'Close\\'], timeperiod=14)\\n    df[\\'STDDEV\\'] = talib.STDDEV(df[\\'Close\\'], timeperiod=14, nbdev=1)\\n    df[\\'SIN\\'] = talib.SIN(df[\\'Close\\'])\\n    df[\\'COS\\'] = talib.COS(df[\\'Close\\'])\\n    df[\\'TAN\\'] = talib.TAN(df[\\'Close\\'])\\n\\n    df = df.fillna(0)\\n\\n    return df\\n', 'ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 24), found shape=(None, 22)', 'ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 24), found shape=(None, 22)\\n\\n\"\"\"\\nScript for training Stock Trading Bot.\\n\\nUsage:\\n  train.py <file-data> <train-percent>\\n    [--date-field=<date-field>] [--adjcp-field=<adjcp-field>]\\n    [--window-size=<window-size>] [--batch-size=<batch-size>]\\n    [--episode-count=<episode-count>] [--model-name=<model-name>]\\n    [--debug]\\n\\nOptions:\\n  --date-field=<date-field>         Specifies name of the date field [default: Date].\\n  --adjcp-field=<adjcp-field>       Specifies name of the adjusted close price field [default: Adj Close].\\n  --window-size=<window-size>       Size of the n-day window stock data representation\\n                                    used as the feature vector. [default: 10]\\n  --batch-size=<batch-size>         Number of samples to train on in one mini-batch\\n                                    during training. [default: 32]\\n  --episode-count=<episode-count>   Number of trading episodes to use for training. [default: 50]\\n  --model-name=<model-name>         Name of the pretrained model to use. [default: model_debug]\\n  --debug                           Specifies whether to use verbose logs during eval operation.\\n\\nKey Changes:\\n    Introduced max_agent_profit: This variable keeps track of the maximum profit observed across all episodes.\\n    Updated prev_agent_profit: This variable is updated within each episode to reflect the profit of the current\\n    episode.\\n    Printed max_agent_profit: At the end of the training, the maximum profit observed is printed.\\n\\n\"\"\"\\n\\nimport logging\\n\\nimport coloredlogs\\nfrom docopt import docopt\\n\\nfrom trading_bot.agent import Agent\\nfrom trading_bot.methods import train_model, evaluate_model\\nfrom trading_bot.utils import (\\n    load_data,\\n    data_split,\\n    switch_k_backend_device,\\n    format_currency,\\n    format_position, show_train_result, )\\n\\n\\ndef main(train_stock, train_percent, date_field, adjcp_field, window_size, batch_size, ep_count,\\n         strategy=\"t-dqn\", model_name=\"model_debug\", debug=False):\\n    \"\"\" Trains the stock trading bot using Deep Q-Learning. \"\"\"\\n    agent = Agent(window_size, strategy=strategy, pretrained=True, model_name=model_name)\\n    data = load_data(train_stock, date_field)\\n    take = int(len(data) * (train_percent / 100))\\n    train_data = data_split(data, 0, take, adjcp_field)\\n    agent_profit = 0\\n    prev_agent_profit = -1000000000\\n    max_agent_profit = -1000000000  # New variable to track the maximum profit\\n    episode = 0\\n    episode_count = ep_count\\n    while True:\\n        train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                                   window_size=window_size, prev_agent_profit=agent_profit, adjcp_field=adjcp_field)\\n\\n        agent_profit = train_result[2]\\n        ave_loss = train_result[3]\\n        logging.debug(\"Current Profit at: {} Ave Loss: {} | Previous Profit: {}\".format(\\n            format_currency(agent_profit),\\n            format_currency(ave_loss),\\n            format_position(prev_agent_profit)))\\n\\n        # Update the maximum profit observed so far\\n        if agent_profit > max_agent_profit:\\n            max_agent_profit = agent_profit\\n\\n        # Update the previous profit for the next episode\\n        prev_agent_profit = agent_profit\\n\\n        if episode >= episode_count:\\n            break\\n        episode += 1\\n\\n    # Show Validation after finishing\\n    if agent_profit > 0:\\n        val_data = data_split(data, take, int(len(data)), adjcp_field)\\n        initial_offset = val_data.iloc[1][adjcp_field] - val_data.iloc[0][adjcp_field]\\n        val_result, _ = evaluate_model(agent, val_data, window_size, debug, adjcp_field)\\n        show_train_result(train_result, val_result, initial_offset, strategy)\\n\\n    # Print the maximum profit observed\\n    print(\"Maximum Profit Observed: {}\".format(format_currency(max_agent_profit)))\\n\\n\\nif __name__ == \"__main__\":\\n    args = docopt(__doc__)\\n\\n    file_data = args[\"<file-data>\"]\\n    train_percent = args[\"<train-percent>\"]\\n\\n    date_field = args[\"--date-field\"]\\n    adjcp_field = args[\"--adjcp-field\"]\\n    window_size = int(args[\"--window-size\"])\\n    batch_size = int(args[\"--batch-size\"])\\n    ep_count = int(args[\"--episode-count\"])\\n    model_name = args[\"--model-name\"]\\n    # strategy = args[\"--strategy\"]\\n    debug = args[\"--debug\"]\\n\\n    coloredlogs.install(level=\"DEBUG\")\\n    switch_k_backend_device()\\n\\n    try:\\n        # dqn\\n        main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n             ep_count, \"dqn\", f\"{model_name}_dqn\", debug)\\n\\n        # # t-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"t-dqn\", f\"{model_name}_tdqn\", debug)\\n        #\\n        # # double-dqn\\n        # main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n        #      ep_count, \"double-dqn\", f\"{model_name}_ddqn\", debug)\\n\\n    except KeyboardInterrupt:\\n        print(\"Aborted!\")\\n\\nimport logging\\nimport os\\n\\nimport pandas as pd\\nimport talib\\nimport tensorflow.keras as keras\\n\\n# Formats Position\\nformat_position = lambda price: (\\'-$\\' if price < 0 else \\'+$\\') + \\'{0:.2f}\\'.format(abs(price))\\n\\n# Formats Currency\\nformat_currency = lambda price: \\'${0:.2f}\\'.format(abs(price))\\n\\n\\ndef show_train_result(result, val_position, initial_offset, strategy):\\n    \"\"\"Displays training results\"\"\"\\n    try:\\n        if val_position == initial_offset or val_position == 0.0:\\n            logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: USELESS  Train Loss: {:.4f}\\'\\n                         .format(strategy, result[0], result[1], format_position(result[2]), result[3]))\\n            return False\\n        else:\\n            logging.info(\\'{}: Episode {}/{} - Train Position: {}  Val Position: {}  Train Loss: {:.4f}\\'\\n                         .format(strategy, result[0], result[1], format_position(result[2]),\\n                                 format_position(val_position), result[3]))\\n            return True\\n    except Exception as e:\\n        logging.error(f\"Error in show_train_result: {e}\")\\n        return False\\n\\n\\ndef show_eval_result(model_name, profit, initial_offset):\\n    \"\"\"Displays eval results\"\"\"\\n    try:\\n        if profit == initial_offset or profit == 0.0:\\n            logging.info(\\'{}: USELESS\\\\n\\'.format(model_name))\\n        else:\\n            logging.info(\\'{}: {}\\\\n\\'.format(model_name, format_position(profit)))\\n    except Exception as e:\\n        logging.error(f\"Error in show_eval_result: {e}\")\\n\\n\\ndef load_data(stock_file, date_field=\"Date\", adjcp_field=\"Close\"):\\n    \"\"\"Reads stock data from csv file\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        if date_field != \"Date\":\\n            df[\\'Date\\'] = df[date_field]\\n        df[\\'adjcp\\'] = df[adjcp_field]\\n        df = df.sort_values([date_field], ascending=True)\\n        df = add_indicators(df)\\n        return df\\n    except Exception as e:\\n        logging.error(f\"Error in load_data: {e}\")\\n        return pd.DataFrame()\\n\\n\\ndef get_stock_data(stock_file, date_field=\"Date\", adjcp_field=\"Adj Close\"):\\n    \"\"\"Reads stock data from csv file\"\"\"\\n    try:\\n        df = pd.read_csv(stock_file)\\n        df = df.sort_values([date_field])\\n        return list(df[adjcp_field])\\n    except Exception as e:\\n        logging.error(f\"Error in get_stock_data: {e}\")\\n        return []\\n\\ndef data_split(df, start, end, adjcp_field=\"Adj Close\"):\\n    \"\"\"Splits the dataset into training or testing using date\"\"\"\\n    try:\\n        data = df[(df.index >= start) & (df.index < end)]\\n        data.reset_index(drop=True, inplace=True)\\n        return data\\n    except Exception as e:\\n        logging.error(f\"Error in data_split: {e}\")\\n        return pd.DataFrame()\\n\\ndef switch_k_backend_device():\\n    \"\"\"Switches `keras` backend from GPU to CPU if required.\"\"\"\\n    try:\\n        if keras.backend == \"tensorflow\":\\n            logging.debug(\"Switching to TensorFlow for CPU\")\\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\\n    except Exception as e:\\n        logging.error(f\"Error in switch_k_backend_device: {e}\")\\n\\n\\ndef get_features():\\n    all_feature_name = [\\n        # \\'CDL2CROWS\\',\\n        # \\'CDL3BLACKCROWS\\',\\n        # \\'CDL3INSIDE\\',\\n        # \\'CDL3LINESTRIKE\\',\\n        # \\'CDL3OUTSIDE\\',\\n        # \\'CDL3STARSINSOUTH\\',\\n        # \\'CDL3WHITESOLDIERS\\',\\n        # \\'CDLABANDONEDBABY\\',\\n        # \\'CDLADVANCEBLOCK\\',\\n        # \\'CDLBELTHOLD\\',\\n        # \\'CDLBREAKAWAY\\',\\n        # \\'CDLCLOSINGMARUBOZU\\',\\n        # \\'CDLCONCEALBABYSWALL\\',\\n        # \\'CDLCOUNTERATTACK\\',\\n        \\'CDLDARKCLOUDCOVER\\',\\n        # \\'CDLDOJI\\',\\n        # \\'CDLDOJISTAR\\',\\n        \\'CDLDRAGONFLYDOJI\\',\\n        \\'CDLENGULFING\\',\\n        # \\'CDLEVENINGDOJISTAR\\',\\n        \\'CDLEVENINGSTAR\\',\\n        # \\'CDLGAPSIDESIDEWHITE\\',\\n        \\'CDLGRAVESTONEDOJI\\',\\n        \\'CDLHAMMER\\',\\n        \\'CDLHANGINGMAN\\',\\n        # \\'CDLHARAMI\\',\\n        # \\'CDLHARAMICROSS\\',\\n        # \\'CDLHIGHWAVE\\',\\n        # \\'CDLHIKKAKE\\',\\n        # \\'CDLHIKKAKEMOD\\',\\n        # \\'CDLHOMINGPIGEON\\',\\n        # \\'CDLIDENTICAL3CROWS\\',\\n        # \\'CDLINNECK\\',\\n        # \\'CDLINVERTEDHAMMER\\',\\n        # \\'CDLKICKING\\',\\n        # \\'CDLKICKINGBYLENGTH\\',\\n        # \\'CDLLADDERBOTTOM\\',\\n        # \\'CDLLONGLEGGEDDOJI\\',\\n        # \\'CDLLONGLINE\\',\\n        # \\'CDLMARUBOZU\\',\\n        # \\'CDLMATCHINGLOW\\',\\n        # \\'CDLMATHOLD\\',\\n        # \\'CDLMORNINGDOJISTAR\\',\\n        \\'CDLMORNINGSTAR\\',\\n        # \\'CDLONNECK\\',\\n        # \\'CDLPIERCING\\',\\n        # \\'CDLRICKSHAWMAN\\',\\n        # \\'CDLRISEFALL3METHODS\\',\\n        # \\'CDLSEPARATINGLINES\\',\\n        # \\'CDLSHOOTINGSTAR\\',\\n        # \\'CDLSHORTLINE\\',\\n        # \\'CDLSPINNINGTOP\\',\\n        # \\'CDLSTALLEDPATTERN\\',\\n        # \\'CDLSTICKSANDWICH\\',\\n        # \\'CDLTAKURI\\',\\n        # \\'CDLTASUKIGAP\\',\\n        # \\'CDLTHRUSTING\\',\\n        # \\'CDLTRISTAR\\',\\n        # \\'CDLUNIQUE3RIVER\\',\\n        # \\'CDLUPSIDEGAP2CROWS\\',\\n        # \\'CDLXSIDEGAP3METHODS\\',\\n        \\'MOMENTUM\\',\\n        # \\'DX\\',\\n        \\'FASTK\\',\\n        \\'FASTD\\',\\n        # \\'DX\\',\\n        \\'CCI\\',\\n        \\'WILLR\\',\\n        \\'MFI\\',\\n        \\'ROC\\',\\n        \\'ADX\\',\\n        \\'PPO\\',\\n        \\'RSI\\',\\n        \\'STDDEV\\',\\n        \\'SIN\\',\\n        \\'COS\\',\\n        \\'TAN\\'\\n    ]\\n    return all_feature_name\\n\\n\\ndef add_indicators(df):\\n    # Pattern Recognition Functions\\n    # df[\\'CDL2CROWS\\'] = talib.CDL2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3BLACKCROWS\\'] = talib.CDL3BLACKCROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3INSIDE\\'] = talib.CDL3INSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3LINESTRIKE\\'] = talib.CDL3LINESTRIKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3OUTSIDE\\'] = talib.CDL3OUTSIDE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3STARSINSOUTH\\'] = talib.CDL3STARSINSOUTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDL3WHITESOLDIERS\\'] = talib.CDL3WHITESOLDIERS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLABANDONEDBABY\\'] = talib.CDLABANDONEDBABY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLADVANCEBLOCK\\'] = talib.CDLADVANCEBLOCK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLBELTHOLD\\'] = talib.CDLBELTHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLBREAKAWAY\\'] = talib.CDLBREAKAWAY(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCLOSINGMARUBOZU\\'] = talib.CDLCLOSINGMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCONCEALBABYSWALL\\'] = talib.CDLCONCEALBABYSWALL(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLCOUNTERATTACK\\'] = talib.CDLCOUNTERATTACK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDARKCLOUDCOVER\\'] = talib.CDLDARKCLOUDCOVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLDOJI\\'] = talib.CDLDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLDOJISTAR\\'] = talib.CDLDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLDRAGONFLYDOJI\\'] = talib.CDLDRAGONFLYDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLENGULFING\\'] = talib.CDLENGULFING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLEVENINGDOJISTAR\\'] = talib.CDLEVENINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLEVENINGSTAR\\'] = talib.CDLEVENINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLGAPSIDESIDEWHITE\\'] = talib.CDLGAPSIDESIDEWHITE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLGRAVESTONEDOJI\\'] = talib.CDLGRAVESTONEDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHAMMER\\'] = talib.CDLHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    df[\\'CDLHANGINGMAN\\'] = talib.CDLHANGINGMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHARAMI\\'] = talib.CDLHARAMI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHARAMICROSS\\'] = talib.CDLHARAMICROSS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIGHWAVE\\'] = talib.CDLHIGHWAVE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIKKAKE\\'] = talib.CDLHIKKAKE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHIKKAKEMOD\\'] = talib.CDLHIKKAKEMOD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLHOMINGPIGEON\\'] = talib.CDLHOMINGPIGEON(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLIDENTICAL3CROWS\\'] = talib.CDLIDENTICAL3CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLINNECK\\'] = talib.CDLINNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLINVERTEDHAMMER\\'] = talib.CDLINVERTEDHAMMER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLKICKING\\'] = talib.CDLKICKING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLKICKINGBYLENGTH\\'] = talib.CDLKICKINGBYLENGTH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLADDERBOTTOM\\'] = talib.CDLLADDERBOTTOM(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLONGLEGGEDDOJI\\'] = talib.CDLLONGLEGGEDDOJI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLLONGLINE\\'] = talib.CDLLONGLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMARUBOZU\\'] = talib.CDLMARUBOZU(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMATCHINGLOW\\'] = talib.CDLMATCHINGLOW(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLMATHOLD\\'] = talib.CDLMATHOLD(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLMORNINGDOJISTAR\\'] = talib.CDLMORNINGDOJISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    df[\\'CDLMORNINGSTAR\\'] = talib.CDLMORNINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], penetration=0)\\n    # df[\\'CDLONNECK\\'] = talib.CDLONNECK(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLPIERCING\\'] = talib.CDLPIERCING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLRICKSHAWMAN\\'] = talib.CDLRICKSHAWMAN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLRISEFALL3METHODS\\'] = talib.CDLRISEFALL3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSEPARATINGLINES\\'] = talib.CDLSEPARATINGLINES(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSHOOTINGSTAR\\'] = talib.CDLSHOOTINGSTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSHORTLINE\\'] = talib.CDLSHORTLINE(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSPINNINGTOP\\'] = talib.CDLSPINNINGTOP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSTALLEDPATTERN\\'] = talib.CDLSTALLEDPATTERN(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLSTICKSANDWICH\\'] = talib.CDLSTICKSANDWICH(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTAKURI\\'] = talib.CDLTAKURI(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTASUKIGAP\\'] = talib.CDLTASUKIGAP(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTHRUSTING\\'] = talib.CDLTHRUSTING(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLTRISTAR\\'] = talib.CDLTRISTAR(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLUNIQUE3RIVER\\'] = talib.CDLUNIQUE3RIVER(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLUPSIDEGAP2CROWS\\'] = talib.CDLUPSIDEGAP2CROWS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # df[\\'CDLXSIDEGAP3METHODS\\'] = talib.CDLXSIDEGAP3METHODS(df[\\'Open\\'], df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'])\\n    # Price action\\n    # https://ta-lib.github.io/ta-lib-python/func_groups/momentum_indicators.html\\n    # rsi,stochRsi,cci,williamsr,mfi,roc,atr,adx\\n    df[\\'MOMENTUM\\'] = talib.MOM(df[\\'Close\\'], timeperiod=10)\\n    # df[\\'DX\\'] = talib.stream_DX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'FASTK\\'], df[\\'FASTD\\'] = talib.STOCHRSI(df[\\'Close\\'], timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)\\n    df[\\'CCI\\'] = talib.CCI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'WILLR\\'] = talib.WILLR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'MFI\\'] = talib.MFI(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], df[\\'Volume\\'], timeperiod=14)\\n    df[\\'ROC\\'] = talib.ROC(df[\\'Close\\'], timeperiod=10)\\n    # df[\\'ATR\\'] = talib.ATR(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'ADX\\'] = talib.ADX(df[\\'High\\'], df[\\'Low\\'], df[\\'Close\\'], timeperiod=14)\\n    df[\\'PPO\\'] = talib.PPO(df[\\'Close\\'], fastperiod=12, slowperiod=26, matype=0)\\n    df[\\'RSI\\'] = talib.RSI(df[\\'Close\\'], timeperiod=14)\\n    df[\\'STDDEV\\'] = talib.STDDEV(df[\\'Close\\'], timeperiod=14, nbdev=1)\\n    df[\\'SIN\\'] = talib.SIN(df[\\'Close\\'])\\n    df[\\'COS\\'] = talib.COS(df[\\'Close\\'])\\n    df[\\'TAN\\'] = talib.TAN(df[\\'Close\\'])\\n\\n    df = df.fillna(0)\\n\\n    return df\\n\\n\\n\"\"\"\\n------------------------------------------------------------------------------------------------------------------------\\nVersion 1.01\\nCreated by Javier Sanchez, August 28, 2024\\n------------------------------------------------------------------------------------------------------------------------\\nRevisor:\\tRev date:\\t    Revision:\\t    Changes:\\n------------------------------------------------------------------------------------------------------------------------\\n[JSanchez]\\t28-Aug-2024:\\tRev. 1.00\\t    First release.\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a try and except block to catch any exceptions that may\\n                                            occur during the calculation of the sigmoid function\\n[JSanchez]\\t31-Aug-2024:\\tRev. 1.01\\t    Added a try and except block to handle exceptions in the get_state\\n                                            function. If an error occurs, it prints the error message and returns None\\n------------------------------------------------------------------------------------------------------------------------\\nChanges Made:\\n    Error Handling in sigmoid: Added a try and except block to catch any exceptions that may\\n    occur during the calculation of the sigmoid function. If an error occurs, it prints the\\n    error message and returns None.\\n\\n    Error Handling in get_state: Similarly, added a try and except block to handle exceptions\\n    in the get_state function. If an error occurs, it prints the error message and returns None.\\n\\n\"\"\"\\n\\nimport math\\n\\nimport numpy as np\\n\\nfrom trading_bot.utils import get_features\\n\\n\\ndef sigmoid(x):\\n    \"\"\"Performs sigmoid operation\"\"\"\\n    try:\\n        if x < 0:\\n            return 1 - 1 / (1 + math.exp(x))\\n        return 1 / (1 + math.exp(-x))\\n    except Exception as err:\\n        print(\"Error in sigmoid: \" + str(err))\\n        return None  # Return None or a default value in case of an error\\n\\n\\ndef get_state(data, t, n_days):\\n    \"\"\"Returns an n-day state representation ending at time t\"\"\"\\n    try:\\n        all_feature_name = get_features()\\n        d = t - n_days + 1\\n        all_feature = np.column_stack([data[k] for k in all_feature_name])\\n        block = all_feature[d:t + 1] if d >= 0 else np.vstack([-d * [all_feature[0]], all_feature[0:t + 1]])\\n        return block\\n    except Exception as err:\\n        print(\"Error in get_state: \" + str(err))\\n        return None  # Return None or an empty array in case of an error\\n\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\n\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\\n\\n    Links: \\thttps://en.wikipedia.org/wiki/Huber_loss\\n            https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/\\n    \"\"\"\\n    error = y_true - y_pred\\n    cond = keras.backend.abs(error) <= clip_delta\\n    squared_loss = 0.5 * keras.backend.square(error)\\n    quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n    return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n\\n        # agent config\\n        self.state_size = state_size  # normalized previous days\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # model config\\n        self.model_name = model_name\\n        self.gamma = 0.95  # affinity for long term reward\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}  # important for loading the model from memory\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n        model_path = f\"models/{self.model_name}.keras\"\\n        if pretrained and os.path.exists(model_path) and self.model_name is not None:\\n            self.model = self.load()\\n        else:\\n            self.model = self._model()\\n\\n        # strategy config\\n        if self.strategy in [\"t-dqn\", \"double-dqn\"]:\\n            self.n_iter = 1\\n            self.reset_every = reset_every\\n\\n            # target network\\n            self.target_model = tf.keras.models.clone_model(self.model)\\n            self.target_model.set_weights(self.model.get_weights())\\n\\n    def _model(self):\\n        \"\"\"Creates the model\\n        \"\"\"\\n        model = keras.Sequential()\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=self.action_size))\\n\\n        model.compile(loss=self.loss, optimizer=self.optimizer)\\n        return model\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory\\n        \"\"\"\\n        self.memory.append((state, action, reward, done))\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions\\n        \"\"\"\\n        # take random action in order to diversify experience at the beginning\\n        if not is_eval and random.random() <= self.epsilon:\\n            return random.randrange(self.action_size)\\n\\n        action_probs = self.model.predict(state, verbose=0)\\n        return np.argmax(action_probs[0])\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            while len(self.memory) > 0:\\n                state, action, reward, done = self.memory.pop()\\n                # Here, target is directly set to the immediate reward\\n                target = reward\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on the immediate reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n                # update q-function parameters based on huber loss gradient\\n                loss += self.model.fit(\\n                    np.array(X_train), np.array(y_train),\\n                    epochs=1, verbose=0\\n                ).history[\"loss\"][0]\\n\\n        # DQN with fixed targets\\n        # elif self.strategy == \"t-dqn\":\\n        #     if self.n_iter % self.reset_every == 0:\\n        #         # reset target model weights\\n        #         self.target_model.set_weights(self.model.get_weights())\\n        #\\n        #     for state, action, reward, next_state, done in mini_batch:\\n        #         if done:\\n        #             target = reward\\n        #         else:\\n        #             # approximate deep q-learning equation with fixed targets\\n        #             target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n        #\\n        #         # estimate q-values based on current state\\n        #         q_values = self.model.predict(state)\\n        #         # update the target for current action based on discounted reward\\n        #         q_values[0][action] = target\\n        #\\n        #         X_train.append(state[0])\\n        #         y_train.append(q_values[0])\\n        #\\n        # # Double DQN\\n        # elif self.strategy == \"double-dqn\":\\n        #     if self.n_iter % self.reset_every == 0:\\n        #         # reset target model weights\\n        #         self.target_model.set_weights(self.model.get_weights())\\n        #\\n        #     for state, action, reward, next_state, done in mini_batch:\\n        #         if done:\\n        #             target = reward\\n        #         else:\\n        #             # approximate double deep q-learning equation\\n        #             target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n        #                 np.argmax(self.model.predict(next_state)[0])]\\n        #\\n        #         # estimate q-values based on current state\\n        #         q_values = self.model.predict(state)\\n        #         # update the target for current action based on discounted reward\\n        #         q_values[0][action] = target\\n        #\\n        #         X_train.append(state[0])\\n        #         y_train.append(q_values[0])\\n        #\\n        # else:\\n        #     raise NotImplementedError()\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def train_experience_replay(self, batch_size):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        mini_batch = random.sample(self.memory, batch_size)\\n        X_train, y_train = [], []\\n\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            for state, action, reward, done in mini_batch:\\n                # Here, target is directly set to the immediate reward\\n                target = reward\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on the immediate reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # DQN with fixed targets\\n        elif self.strategy == \"t-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation with fixed targets\\n                    target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # Double DQN\\n        elif self.strategy == \"double-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate double deep q-learning equation\\n                    target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                        np.argmax(self.model.predict(next_state)[0])]\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        else:\\n            raise NotImplementedError()\\n\\n        # update q-function parameters based on huber loss gradient\\n        loss = self.model.fit(\\n            np.array(X_train), np.array(y_train),\\n            epochs=1, verbose=0\\n        ).history[\"loss\"][0]\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def save(self):\\n        path = pathlib.Path()\\n        self.model.save(path / \"models/{}.keras\".format(self.model_name), overwrite=True)\\n\\n    def load(self):\\n        return tf.keras.models.load_model(\"models/{}.keras\".format(self.model_name), custom_objects=self.custom_objects)\\n\\n\\n\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0, adjcp_field=\\'close\\'):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False, desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n        reward = 0\\n        is_done = False\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state)  # Use agent\\'s action selection\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            reward = delta\\n            total_profit += delta\\n        elif action == 2:  # SELL\\n            reward = -delta\\n            total_profit -= delta\\n        else:  # HOLD\\n            reward = 0\\n\\n        done = (t == data_length - 1) or is_done\\n        agent.remember(state, action, reward, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay2()\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit >= prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved successfully after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Update prev_agent_profit to retain the highest profit observed\\n    prev_agent_profit = max(prev_agent_profit, total_profit)\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss)), prev_agent_profit\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state, is_eval=True)\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n\\n        if action == 1:  # BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_fi', '   ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 24), found shape=(None, 23)\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\n\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\\n\\n    Links: \\thttps://en.wikipedia.org/wiki/Huber_loss\\n            https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/\\n    \"\"\"\\n    error = y_true - y_pred\\n    cond = keras.backend.abs(error) <= clip_delta\\n    squared_loss = 0.5 * keras.backend.square(error)\\n    quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n    return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n\\n        # agent config\\n        self.state_size = state_size  # normalized previous days\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # model config\\n        self.model_name = model_name\\n        self.gamma = 0.95  # affinity for long term reward\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}  # important for loading the model from memory\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n        model_path = f\"models/{self.model_name}.keras\"\\n        if pretrained and os.path.exists(model_path) and self.model_name is not None:\\n            self.model = self.load()\\n        else:\\n            self.model = self._model()\\n\\n        # strategy config\\n        if self.strategy in [\"t-dqn\", \"double-dqn\"]:\\n            self.n_iter = 1\\n            self.reset_every = reset_every\\n\\n            # target network\\n            self.target_model = tf.keras.models.clone_model(self.model)\\n            self.target_model.set_weights(self.model.get_weights())\\n\\n    def _model(self):\\n        \"\"\"Creates the model\\n        \"\"\"\\n        model = keras.Sequential()\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=64, activation=\\'relu\\'))\\n        model.add(tf.keras.layers.Dense(units=self.action_size))\\n\\n        model.compile(loss=self.loss, optimizer=self.optimizer)\\n        return model\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory\\n        \"\"\"\\n        self.memory.append((state, action, reward, done))\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions\\n        \"\"\"\\n        # take random action in order to diversify experience at the beginning\\n        if not is_eval and random.random() <= self.epsilon:\\n            return random.randrange(self.action_size)\\n\\n        action_probs = self.model.predict(state, verbose=0)\\n        return np.argmax(action_probs[0])\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            while len(self.memory) > 0:\\n                state, action, reward, done = self.memory.pop()\\n                # Here, target is directly set to the immediate reward\\n                target = reward\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on the immediate reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n                # update q-function parameters based on huber loss gradient\\n                loss += self.model.fit(\\n                    np.array(X_train), np.array(y_train),\\n                    epochs=1, verbose=0\\n                ).history[\"loss\"][0]\\n\\n        # DQN with fixed targets\\n        # elif self.strategy == \"t-dqn\":\\n        #     if self.n_iter % self.reset_every == 0:\\n        #         # reset target model weights\\n        #         self.target_model.set_weights(self.model.get_weights())\\n        #\\n        #     for state, action, reward, next_state, done in mini_batch:\\n        #         if done:\\n        #             target = reward\\n        #         else:\\n        #             # approximate deep q-learning equation with fixed targets\\n        #             target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n        #\\n        #         # estimate q-values based on current state\\n        #         q_values = self.model.predict(state)\\n        #         # update the target for current action based on discounted reward\\n        #         q_values[0][action] = target\\n        #\\n        #         X_train.append(state[0])\\n        #         y_train.append(q_values[0])\\n        #\\n        # # Double DQN\\n        # elif self.strategy == \"double-dqn\":\\n        #     if self.n_iter % self.reset_every == 0:\\n        #         # reset target model weights\\n        #         self.target_model.set_weights(self.model.get_weights())\\n        #\\n        #     for state, action, reward, next_state, done in mini_batch:\\n        #         if done:\\n        #             target = reward\\n        #         else:\\n        #             # approximate double deep q-learning equation\\n        #             target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n        #                 np.argmax(self.model.predict(next_state)[0])]\\n        #\\n        #         # estimate q-values based on current state\\n        #         q_values = self.model.predict(state)\\n        #         # update the target for current action based on discounted reward\\n        #         q_values[0][action] = target\\n        #\\n        #         X_train.append(state[0])\\n        #         y_train.append(q_values[0])\\n        #\\n        # else:\\n        #     raise NotImplementedError()\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def train_experience_replay(self, batch_size):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        mini_batch = random.sample(self.memory, batch_size)\\n        X_train, y_train = [], []\\n\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            for state, action, reward, done in mini_batch:\\n                # Here, target is directly set to the immediate reward\\n                target = reward\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on the immediate reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # DQN with fixed targets\\n        elif self.strategy == \"t-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation with fixed targets\\n                    target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # Double DQN\\n        elif self.strategy == \"double-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate double deep q-learning equation\\n                    target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                        np.argmax(self.model.predict(next_state)[0])]\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        else:\\n            raise NotImplementedError()\\n\\n        # update q-function parameters based on huber loss gradient\\n        loss = self.model.fit(\\n            np.array(X_train), np.array(y_train),\\n            epochs=1, verbose=0\\n        ).history[\"loss\"][0]\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def save(self):\\n        path = pathlib.Path()\\n        self.model.save(path / \"models/{}.keras\".format(self.model_name), overwrite=True)\\n\\n    def load(self):\\n        return tf.keras.models.load_model(\"models/{}.keras\".format(self.model_name), custom_objects=self.custom_objects)\\n', 'If we Detect market reversal and the agent is currently on a Buy we close the Buy and add a condition to be a 3, and the agent is currently on a Sell we close the Sell and add a condition to be a 4\\n\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"\\n    Monitor system memory usage and print a warning if it exceeds the threshold.\\n\\n    Parameters:\\n    threshold (int): Memory usage threshold percentage.\\n\\n    Returns:\\n    bool: True if memory usage exceeds the threshold, False otherwise.\\n    \"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef detect_market_reversal(data, t, window_size, adjcp_field=\\'close\\'):\\n    \"\"\"\\n    Detect market reversal using a simple moving average crossover strategy.\\n\\n    Parameters:\\n    data (DataFrame): The historical stock data.\\n    t (int): The current time step.\\n    window_size (int): The size of the window for state representation.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    bool: True if a market reversal is detected, False otherwise.\\n    \"\"\"\\n    if t < window_size:\\n        return False\\n\\n    short_window = 5\\n    long_window = 20\\n\\n    short_mavg = data[adjcp_field].rolling(window=short_window).mean()\\n    long_mavg = data[adjcp_field].rolling(window=long_window).mean()\\n\\n    if short_mavg.iloc[t] > long_mavg.iloc[t] and short_mavg.iloc[t - 1] <= long_mavg.iloc[t - 1]:\\n        return True  # Bullish crossover (buy signal)\\n    elif short_mavg.iloc[t] < long_mavg.iloc[t] and short_mavg.iloc[t - 1] >= long_mavg.iloc[t - 1]:\\n        return True  # Bearish crossover (sell signal)\\n\\n    return False\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0, adjcp_field=\\'close\\'):\\n    \"\"\"\\n    Train the trading agent over a specified number of episodes.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    episode (int): The current episode number.\\n    data (DataFrame): The historical stock data.\\n    ep_count (int): Total number of episodes for training.\\n    batch_size (int): Size of the mini-batch for experience replay.\\n    window_size (int): The size of the window for state representation.\\n    prev_agent_profit (float): The highest profit observed so far.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (episode, ep_count, total_profit, avg_loss, prev_agent_profit, avg_profit_per_trade, win_loss_ratio)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n    trades = 0\\n    wins = 0\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False, desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n        reward = 0\\n        is_done = False\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state)  # Use agent\\'s action selection\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n\\n        # Detect market reversal\\n        if detect_market_reversal(data, t, window_size, adjcp_field):\\n            if action == 1:  # If the agent wants to BUY, we SELL\\n                action = 2\\n            elif action == 2:  # If the agent wants to SELL, we BUY\\n                action = 1\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            reward = delta\\n            total_profit += delta\\n            trades += 1\\n            if delta > 0:\\n                wins += 1\\n        elif action == 2:  # SELL\\n            reward = -delta\\n            total_profit -= delta\\n            trades += 1\\n            if delta < 0:\\n                wins += 1\\n        else:  # HOLD\\n            reward = 0\\n\\n        done = (t == data_length - 1) or is_done\\n        agent.remember(state, action, reward, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay2()\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit >= prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved successfully after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Update prev_agent_profit to retain the highest profit observed\\n    prev_agent_profit = max(prev_agent_profit, total_profit)\\n\\n    avg_profit_per_trade = total_profit / trades if trades > 0 else 0\\n    win_loss_ratio = wins / trades if trades > 0 else 0\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss)), prev_agent_profit, avg_profit_per_trade, win_loss_ratio\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    \"\"\"\\n    Evaluate the trading agent on historical stock data.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window_size (int): The size of the window for state representation.\\n    debug (bool): Flag to enable/disable debug logging.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (total_profit, history)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state, is_eval=True)\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n\\n        # Detect market reversal\\n        if detect_market_reversal(data, t, window_size, adjcp_field):\\n            if action == 1:  # If the agent wants to BUY, we SELL\\n                action = 2\\n            elif action == 2:  # If the agent wants to SELL, we BUY\\n                action = 1\\n\\n        if action == 1:  # BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 2:  # SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        else:  # HOLD\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        state = next_state\\n        if t == data_length - 1:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    \"\"\"\\n    Predict the next action using the trained trading agent.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window (int): The size of the window for state representation.\\n\\n    Returns:\\n    int: The predicted action (0: HOLD, 1: BUY, 2: SELL).\\n    \"\"\"\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'In both train_model and evaluate_model, if a market reversal is detected, the agent\\'s action is adjusted:\\nIf the agent is currently on a BUY (action 1), it closes the BUY and sets the action to 3.\\nIf the agent is currently on a SELL (action 2), it closes the SELL and sets the action to 4.\\nMake sure is add it to the prediction\\n\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"\\n    Monitor system memory usage and print a warning if it exceeds the threshold.\\n\\n    Parameters:\\n    threshold (int): Memory usage threshold percentage.\\n\\n    Returns:\\n    bool: True if memory usage exceeds the threshold, False otherwise.\\n    \"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef detect_market_reversal(data, t, window_size, adjcp_field=\\'close\\'):\\n    \"\"\"\\n    Detect market reversal using a simple moving average crossover strategy.\\n\\n    Parameters:\\n    data (DataFrame): The historical stock data.\\n    t (int): The current time step.\\n    window_size (int): The size of the window for state representation.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    bool: True if a market reversal is detected, False otherwise.\\n    \"\"\"\\n    if t < window_size:\\n        return False\\n\\n    short_window = 5\\n    long_window = 20\\n\\n    short_mavg = data[adjcp_field].rolling(window=short_window).mean()\\n    long_mavg = data[adjcp_field].rolling(window=long_window).mean()\\n\\n    if short_mavg.iloc[t] > long_mavg.iloc[t] and short_mavg.iloc[t - 1] <= long_mavg.iloc[t - 1]:\\n        return True  # Bullish crossover (buy signal)\\n    elif short_mavg.iloc[t] < long_mavg.iloc[t] and short_mavg.iloc[t - 1] >= long_mavg.iloc[t - 1]:\\n        return True  # Bearish crossover (sell signal)\\n\\n    return False\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0, adjcp_field=\\'close\\'):\\n    \"\"\"\\n    Train the trading agent over a specified number of episodes.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    episode (int): The current episode number.\\n    data (DataFrame): The historical stock data.\\n    ep_count (int): Total number of episodes for training.\\n    batch_size (int): Size of the mini-batch for experience replay.\\n    window_size (int): The size of the window for state representation.\\n    prev_agent_profit (float): The highest profit observed so far.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (episode, ep_count, total_profit, avg_loss, prev_agent_profit, avg_profit_per_trade, win_loss_ratio)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n    trades = 0\\n    wins = 0\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False, desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n        reward = 0\\n        is_done = False\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state)  # Use agent\\'s action selection\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n\\n        # Detect market reversal\\n        if detect_market_reversal(data, t, window_size, adjcp_field):\\n            if action == 1:  # If the agent is currently on a BUY, we close the BUY and set action to 3\\n                action = 3\\n            elif action == 2:  # If the agent is currently on a SELL, we close the SELL and set action to 4\\n                action = 4\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            reward = delta\\n            total_profit += delta\\n            trades += 1\\n            if delta > 0:\\n                wins += 1\\n        elif action == 2:  # SELL\\n            reward = -delta\\n            total_profit -= delta\\n            trades += 1\\n            if delta < 0:\\n                wins += 1\\n        elif action == 3:  # Close BUY\\n            reward = delta\\n            total_profit += delta\\n            trades += 1\\n            if delta > 0:\\n                wins += 1\\n        elif action == 4:  # Close SELL\\n            reward = -delta\\n            total_profit -= delta\\n            trades += 1\\n            if delta < 0:\\n                wins += 1\\n        else:  # HOLD\\n            reward = 0\\n\\n        done = (t == data_length - 1) or is_done\\n        agent.remember(state, action, reward, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay2()\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit >= prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved successfully after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Update prev_agent_profit to retain the highest profit observed\\n    prev_agent_profit = max(prev_agent_profit, total_profit)\\n\\n    avg_profit_per_trade = total_profit / trades if trades > 0 else 0\\n    win_loss_ratio = wins / trades if trades > 0 else 0\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss)), prev_agent_profit, avg_profit_per_trade, win_loss_ratio\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    \"\"\"\\n    Evaluate the trading agent on historical stock data.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window_size (int): The size of the window for state representation.\\n    debug (bool): Flag to enable/disable debug logging.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (total_profit, history)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state, is_eval=True)\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n\\n        # Detect market reversal\\n        if detect_market_reversal(data, t, window_size, adjcp_field):\\n            if action == 1:  # If the agent is currently on a BUY, we close the BUY and set action to 3\\n                action = 3\\n            elif action == 2:  # If the agent is currently on a SELL, we close the SELL and set action to 4\\n                action = 4\\n\\n        if action == 1:  # BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 2:  # SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 3:  # Close BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"CLOSE BUY\"))\\n            if debug:\\n                logging.debug(\"Close Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 4:  # Close SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"CLOSE SELL\"))\\n            if debug:\\n                logging.debug(\"Close Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        else:  # HOLD\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        state = next_state\\n        if t == data_length - 1:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    \"\"\"\\n    Predict the next action using the trained trading agent.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window (int): The size of the window for state representation.\\n\\n    Returns:\\n    int: The predicted action (0: HOLD, 1: BUY, 2: SELL).\\n    \"\"\"\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'is the following would have it on the model we save when we do a prediction?\\n\\nIn both train_model and evaluate_model, if a market reversal is detected, the agent\\'s action is adjusted:\\nIf the agent is currently on a BUY (action 1), it closes the BUY and sets the action to 3.\\nIf the agent is currently on a SELL (action 2), it closes the SELL and sets the action to 4.\\n\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"\\n    Monitor system memory usage and print a warning if it exceeds the threshold.\\n\\n    Parameters:\\n    threshold (int): Memory usage threshold percentage.\\n\\n    Returns:\\n    bool: True if memory usage exceeds the threshold, False otherwise.\\n    \"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef detect_market_reversal(data, t, window_size, adjcp_field=\\'close\\'):\\n    \"\"\"\\n    Detect market reversal using a simple moving average crossover strategy.\\n\\n    Parameters:\\n    data (DataFrame): The historical stock data.\\n    t (int): The current time step.\\n    window_size (int): The size of the window for state representation.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    bool: True if a market reversal is detected, False otherwise.\\n    \"\"\"\\n    if t < window_size:\\n        return False\\n\\n    short_window = 5\\n    long_window = 20\\n\\n    short_mavg = data[adjcp_field].rolling(window=short_window).mean()\\n    long_mavg = data[adjcp_field].rolling(window=long_window).mean()\\n\\n    if short_mavg.iloc[t] > long_mavg.iloc[t] and short_mavg.iloc[t - 1] <= long_mavg.iloc[t - 1]:\\n        return True  # Bullish crossover (buy signal)\\n    elif short_mavg.iloc[t] < long_mavg.iloc[t] and short_mavg.iloc[t - 1] >= long_mavg.iloc[t - 1]:\\n        return True  # Bearish crossover (sell signal)\\n\\n    return False\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0, adjcp_field=\\'close\\'):\\n    \"\"\"\\n    Train the trading agent over a specified number of episodes.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    episode (int): The current episode number.\\n    data (DataFrame): The historical stock data.\\n    ep_count (int): Total number of episodes for training.\\n    batch_size (int): Size of the mini-batch for experience replay.\\n    window_size (int): The size of the window for state representation.\\n    prev_agent_profit (float): The highest profit observed so far.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (episode, ep_count, total_profit, avg_loss, prev_agent_profit, avg_profit_per_trade, win_loss_ratio)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n    trades = 0\\n    wins = 0\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False, desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n        reward = 0\\n        is_done = False\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state)  # Use agent\\'s action selection\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n\\n        # Detect market reversal\\n        if detect_market_reversal(data, t, window_size, adjcp_field):\\n            if action == 1:  # If the agent is currently on a BUY, we close the BUY and set action to 3\\n                action = 3\\n            elif action == 2:  # If the agent is currently on a SELL, we close the SELL and set action to 4\\n                action = 4\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            reward = delta\\n            total_profit += delta\\n            trades += 1\\n            if delta > 0:\\n                wins += 1\\n        elif action == 2:  # SELL\\n            reward = -delta\\n            total_profit -= delta\\n            trades += 1\\n            if delta < 0:\\n                wins += 1\\n        elif action == 3:  # Close BUY\\n            reward = delta\\n            total_profit += delta\\n            trades += 1\\n            if delta > 0:\\n                wins += 1\\n        elif action == 4:  # Close SELL\\n            reward = -delta\\n            total_profit -= delta\\n            trades += 1\\n            if delta < 0:\\n                wins += 1\\n        else:  # HOLD\\n            reward = 0\\n\\n        done = (t == data_length - 1) or is_done\\n        agent.remember(state, action, reward, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay2()\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit >= prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved successfully after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Update prev_agent_profit to retain the highest profit observed\\n    prev_agent_profit = max(prev_agent_profit, total_profit)\\n\\n    avg_profit_per_trade = total_profit / trades if trades > 0 else 0\\n    win_loss_ratio = wins / trades if trades > 0 else 0\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss)), prev_agent_profit, avg_profit_per_trade, win_loss_ratio\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    \"\"\"\\n    Evaluate the trading agent on historical stock data.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window_size (int): The size of the window for state representation.\\n    debug (bool): Flag to enable/disable debug logging.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (total_profit, history)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state, is_eval=True)\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n\\n        # Detect market reversal\\n        if detect_market_reversal(data, t, window_size, adjcp_field):\\n            if action == 1:  # If the agent is currently on a BUY, we close the BUY and set action to 3\\n                action = 3\\n            elif action == 2:  # If the agent is currently on a SELL, we close the SELL and set action to 4\\n                action = 4\\n\\n        if action == 1:  # BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 2:  # SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 3:  # Close BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"CLOSE BUY\"))\\n            if debug:\\n                logging.debug(\"Close Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 4:  # Close SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"CLOSE SELL\"))\\n            if debug:\\n                logging.debug(\"Close Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        else:  # HOLD\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        state = next_state\\n        if t == data_length - 1:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    \"\"\"\\n    Predict the next action using the trained trading agent.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window (int): The size of the window for state representation.\\n\\n    Returns:\\n    int: The predicted action (0: HOLD, 1: BUY, 2: SELL).\\n    \"\"\"\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n    return action', 'IndentationError: expected an indented block after \\'elif\\' statement on line 536\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"\\n    Monitor system memory usage and print a warning if it exceeds the threshold.\\n\\n    Parameters:\\n    threshold (int): Memory usage threshold percentage.\\n\\n    Returns:\\n    bool: True if memory usage exceeds the threshold, False otherwise.\\n    \"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef detect_market_reversal(data, t, window_size, adjcp_field=\\'close\\'):\\n    \"\"\"\\n    Detect market reversal using a simple moving average crossover strategy.\\n\\n    Parameters:\\n    data (DataFrame): The historical stock data.\\n    t (int): The current time step.\\n    window_size (int): The size of the window for state representation.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    bool: True if a market reversal is detected, False otherwise.\\n    \"\"\"\\n    if t < window_size:\\n        return False\\n\\n    short_window = 5\\n    long_window = 20\\n\\n    short_mavg = data[adjcp_field].rolling(window=short_window).mean()\\n    long_mavg = data[adjcp_field].rolling(window=long_window).mean()\\n\\n    if short_mavg.iloc[t] > long_mavg.iloc[t] and short_mavg.iloc[t - 1] <= long_mavg.iloc[t - 1]:\\n        return True  # Bullish crossover (buy signal)\\n    elif short_mavg.iloc[t] < long_mavg.iloc[t] and short_mavg.iloc[t - 1] >= long_mavg.iloc[t - 1]:\\n        return True  # Bearish crossover (sell signal)\\n\\n    return False\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0, adjcp_field=\\'close\\'):\\n    \"\"\"\\n    Train the trading agent over a specified number of episodes.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    episode (int): The current episode number.\\n    data (DataFrame): The historical stock data.\\n    ep_count (int): Total number of episodes for training.\\n    batch_size (int): Size of the mini-batch for experience replay.\\n    window_size (int): The size of the window for state representation.\\n    prev_agent_profit (float): The highest profit observed so far.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (episode, ep_count, total_profit, avg_loss, prev_agent_profit, avg_profit_per_trade, win_loss_ratio)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n    trades = 0\\n    wins = 0\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False, desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n        reward = 0\\n        is_done = False\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state)  # Use agent\\'s action selection\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n\\n        # Detect market reversal\\n        if detect_market_reversal(data, t, window_size, adjcp_field):\\n            if action == 1:  # If the agent is currently on a BUY, we close the BUY and set action to 3\\n                action = 3\\n            elif action == 2:  # If the agent is currently on a SELL, we close the SELL and set action to 4\\n                action = 4\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            reward = delta\\n            total_profit += delta\\n            trades += 1\\n            if delta > 0:\\n                wins += 1\\n        elif action == 2:  # SELL\\n            reward = -delta\\n            total_profit -= delta\\n            trades += 1\\n            if delta < 0:\\n                wins += 1\\n        elif action == 3:  # Close BUY\\n            reward = delta\\n            total_profit += delta\\n            trades += 1\\n            if delta > 0:\\n                wins += 1\\n        elif action == 4:  # Close SELL\\n            reward = -delta\\n            total_profit -= delta\\n            trades += 1\\n            if delta < 0:\\n                wins += 1\\n        else:  # HOLD\\n            reward = 0\\n\\n        done = (t == data_length - 1) or is_done\\n        agent.remember(state, action, reward, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay2()\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit >= prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved successfully after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Update prev_agent_profit to retain the highest profit observed\\n    prev_agent_profit = max(prev_agent_profit, total_profit)\\n\\n    avg_profit_per_trade = total_profit / trades if trades > 0 else 0\\n    win_loss_ratio = wins / trades if trades > 0 else 0\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss)), prev_agent_profit, avg_profit_per_trade, win_loss_ratio\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    \"\"\"\\n    Evaluate the trading agent on historical stock data.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window_size (int): The size of the window for state representation.\\n    debug (bool): Flag to enable/disable debug logging.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (total_profit, history)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state, is_eval=True)\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n\\n        # Detect market reversal\\n        if detect_market_reversal(data, t, window_size, adjcp_field):\\n            if action == 1:  # If the agent is currently on a BUY, we close the BUY and set action to 3\\n                action = 3\\n            elif action == 2:  # If the agent is currently on a SELL, we close the SELL and set action to 4\\n\\n                if action == 1:  # BUY\\n                    total_profit += delta\\n                    history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n                    if debug:\\n                        logging.debug(\"Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n                elif action == 2:  # SELL\\n                    total_profit -= delta\\n                    history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n                    if debug:\\n                        logging.debug(\"Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n                elif action == 3:  # Close BUY\\n                    total_profit += delta\\n                    history.append((data.iloc[t][adjcp_field], \"CLOSE BUY\"))\\n                    if debug:\\n                        logging.debug(\"Close Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n                elif action == 4:  # Close SELL\\n                    total_profit -= delta\\n                    history.append((data.iloc[t][adjcp_field], \"CLOSE SELL\"))\\n                    if debug:\\n                        logging.debug(\"Close Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n                else:  # HOLD\\n                    history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        state = next_state\\n        if t == data_length - 1:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    \"\"\"\\n    Predict the next action using the trained trading agent.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window (int): The size of the window for state representation.\\n\\n    Returns:\\n    int: The predicted action (0: HOLD, 1: BUY, 2: SELL, 3: CLOSE BUY, 4: CLOSE SELL).\\n    \"\"\"\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n\\n    # Detect market reversal\\n    if detect_market_reversal(data, data_length, window, adjcp_field=\\'close\\'):\\n        if action == 1:  # If the agent is currently on a BUY, we close the BUY and set action to 3\\n            action = 3\\n        elif action == 2:  # If the agent is currently on a SELL, we close the SELL and set action to 4\\n\\n    return action', 'File \"/Users/javiersanchez/PycharmProjects/trading-bot/trading_bot/agent.py\", line 526, in train_experience_replay2\\n    q_values[0][action] = target\\n    ~~~~~~~~~~~^^^^^^^^\\nIndexError: index 4 is out of bounds for axis 0 with size 3\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\n\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\\n\\n    Links: \\thttps://en.wikipedia.org/wiki/Huber_loss\\n            https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/\\n    \"\"\"\\n    error = y_true - y_pred\\n    cond = keras.backend.abs(error) <= clip_delta\\n    squared_loss = 0.5 * keras.backend.square(error)\\n    quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n    return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n\\n        # agent config\\n        self.state_size = state_size  # normalized previous days\\n        self.action_size = 3  # [sit, buy, sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # model config\\n        self.model_name = model_name\\n        self.gamma = 0.95  # affinity for long term reward\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}  # important for loading the model from memory\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n        model_path = f\"models/{self.model_name}.keras\"\\n        if pretrained and os.path.exists(model_path) and self.model_name is not None:\\n            self.model = self.load()\\n        else:\\n            self.model = self._model()\\n\\n        # strategy config\\n        if self.strategy in [\"t-dqn\", \"double-dqn\"]:\\n            self.n_iter = 1\\n            self.reset_every = reset_every\\n\\n            # target network\\n            self.target_model = tf.keras.models.clone_model(self.model)\\n            self.target_model.set_weights(self.model.get_weights())\\n\\n    def _model(self):\\n        \"\"\"Creates the model\\n        \"\"\"\\n        model = keras.Sequential()\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=self.action_size))\\n\\n        model.compile(loss=self.loss, optimizer=self.optimizer)\\n        return model\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory\\n        \"\"\"\\n        self.memory.append((state, action, reward, done))\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions\\n        \"\"\"\\n        # take random action in order to diversify experience at the beginning\\n        if not is_eval and random.random() <= self.epsilon:\\n            return random.randrange(self.action_size)\\n\\n        action_probs = self.model.predict(state, verbose=0)\\n        return np.argmax(action_probs[0])\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            while len(self.memory) > 0:\\n                state, action, reward, done = self.memory.pop()\\n                # Here, target is directly set to the immediate reward\\n                target = reward\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on the immediate reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n                # update q-function parameters based on huber loss gradient\\n                loss += self.model.fit(\\n                    np.array(X_train), np.array(y_train),\\n                    epochs=1, verbose=0\\n                ).history[\"loss\"][0]\\n\\n        # DQN with fixed targets\\n        # elif self.strategy == \"t-dqn\":\\n        #     if self.n_iter % self.reset_every == 0:\\n        #         # reset target model weights\\n        #         self.target_model.set_weights(self.model.get_weights())\\n        #\\n        #     for state, action, reward, next_state, done in mini_batch:\\n        #         if done:\\n        #             target = reward\\n        #         else:\\n        #             # approximate deep q-learning equation with fixed targets\\n        #             target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n        #\\n        #         # estimate q-values based on current state\\n        #         q_values = self.model.predict(state)\\n        #         # update the target for current action based on discounted reward\\n        #         q_values[0][action] = target\\n        #\\n        #         X_train.append(state[0])\\n        #         y_train.append(q_values[0])\\n        #\\n        # # Double DQN\\n        # elif self.strategy == \"double-dqn\":\\n        #     if self.n_iter % self.reset_every == 0:\\n        #         # reset target model weights\\n        #         self.target_model.set_weights(self.model.get_weights())\\n        #\\n        #     for state, action, reward, next_state, done in mini_batch:\\n        #         if done:\\n        #             target = reward\\n        #         else:\\n        #             # approximate double deep q-learning equation\\n        #             target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n        #                 np.argmax(self.model.predict(next_state)[0])]\\n        #\\n        #         # estimate q-values based on current state\\n        #         q_values = self.model.predict(state)\\n        #         # update the target for current action based on discounted reward\\n        #         q_values[0][action] = target\\n        #\\n        #         X_train.append(state[0])\\n        #         y_train.append(q_values[0])\\n        #\\n        # else:\\n        #     raise NotImplementedError()\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def train_experience_replay(self, batch_size):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        mini_batch = random.sample(self.memory, batch_size)\\n        X_train, y_train = [], []\\n\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            for state, action, reward, done in mini_batch:\\n                # Here, target is directly set to the immediate reward\\n                target = reward\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on the immediate reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # DQN with fixed targets\\n        elif self.strategy == \"t-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation with fixed targets\\n                    target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # Double DQN\\n        elif self.strategy == \"double-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate double deep q-learning equation\\n                    target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                        np.argmax(self.model.predict(next_state)[0])]\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        else:\\n            raise NotImplementedError()\\n\\n        # update q-function parameters based on huber loss gradient\\n        loss = self.model.fit(\\n            np.array(X_train), np.array(y_train),\\n            epochs=1, verbose=0\\n        ).history[\"loss\"][0]\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def save(self):\\n        path = pathlib.Path()\\n        self.model.save(path / \"models/{}.keras\".format(self.model_name), overwrite=True)\\n\\n    def load(self):\\n        return tf.keras.models.load_model(\"models/{}.keras\".format(self.model_name), custom_objects=self.custom_objects)\\n\\n', 'Traceback (most recent call last):\\n  File \"/Users/javiersanchez/PycharmProjects/trading-bot/train.py\", line 110, in <module>\\n    main(file_data, float(train_percent), date_field, adjcp_field, window_size, batch_size,\\n  File \"/Users/javiersanchez/PycharmProjects/trading-bot/train.py\", line 58, in main\\n    train_result = train_model(agent, episode, train_data, ep_count=episode_count, batch_size=batch_size,\\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/javiersanchez/PycharmProjects/trading-bot/trading_bot/methods.py\", line 433, in train_model\\n    loss = agent.train_experience_replay2()\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/javiersanchez/PycharmProjects/trading-bot/trading_bot/agent.py\", line 526, in train_experience_replay2\\n    q_values[0][action] = target\\n    ~~~~~~~~~~~^^^^^^^^\\nIndexError: index 3 is out of bounds for axis 0 with size 3\\n\\nimport os\\nimport pathlib\\nimport random\\nfrom collections import deque\\n\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\n\\n\\ndef huber_loss(y_true, y_pred, clip_delta=1.0):\\n    \"\"\"Huber loss - Custom Loss Function for Q Learning\\n\\n    Links: \\thttps://en.wikipedia.org/wiki/Huber_loss\\n            https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/\\n    \"\"\"\\n    error = y_true - y_pred\\n    cond = keras.backend.abs(error) <= clip_delta\\n    squared_loss = 0.5 * keras.backend.square(error)\\n    quadratic_loss = 0.5 * keras.backend.square(clip_delta) + clip_delta * (keras.backend.abs(error) - clip_delta)\\n    return keras.backend.mean(tf.where(cond, squared_loss, quadratic_loss))\\n\\n\\nclass Agent:\\n    \"\"\" Stock Trading Bot \"\"\"\\n\\n    def __init__(self, state_size, strategy=\"t-dqn\", reset_every=1000, pretrained=False, model_name=None):\\n        self.strategy = strategy\\n\\n        # agent config\\n        self.state_size = state_size  # normalized previous days\\n        self.action_size = 5  # [sit, buy, sell, close buy, close sell]\\n        self.model_name = model_name\\n        self.inventory = []\\n        self.memory = deque(maxlen=10000)\\n\\n        # model config\\n        self.model_name = model_name\\n        self.gamma = 0.95  # affinity for long term reward\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n        self.learning_rate = 0.001\\n        self.loss = huber_loss\\n        self.custom_objects = {\"huber_loss\": huber_loss}  # important for loading the model from memory\\n        self.optimizer = tf.optimizers.Adam(self.learning_rate)\\n        model_path = f\"models/{self.model_name}.keras\"\\n        if pretrained and os.path.exists(model_path) and self.model_name is not None:\\n            self.model = self.load()\\n        else:\\n            self.model = self._model()\\n\\n        # strategy config\\n        if self.strategy in [\"t-dqn\", \"double-dqn\"]:\\n            self.n_iter = 1\\n            self.reset_every = reset_every\\n\\n            # target network\\n            self.target_model = tf.keras.models.clone_model(self.model)\\n            self.target_model.set_weights(self.model.get_weights())\\n\\n    def _model(self):\\n        \"\"\"Creates the model\\n        \"\"\"\\n        model = keras.Sequential()\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\\n        model.add(tf.keras.layers.Dense(units=self.action_size))\\n\\n        model.compile(loss=self.loss, optimizer=self.optimizer)\\n        return model\\n\\n    def remember(self, state, action, reward, done):\\n        \"\"\"Adds relevant data to memory\\n        \"\"\"\\n        self.memory.append((state, action, reward, done))\\n\\n    def act(self, state, is_eval=False):\\n        \"\"\"Take action from given possible set of actions\\n        \"\"\"\\n        # take random action in order to diversify experience at the beginning\\n        if not is_eval and random.random() <= self.epsilon:\\n            return random.randrange(self.action_size)\\n\\n        action_probs = self.model.predict(state, verbose=0)\\n        return np.argmax(action_probs[0])\\n\\n    def train_experience_replay2(self):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        X_train, y_train = [], []\\n        loss = 0\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            while len(self.memory) > 0:\\n                state, action, reward, done = self.memory.pop()\\n                # Here, target is directly set to the immediate reward\\n                target = reward\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on the immediate reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n                # update q-function parameters based on huber loss gradient\\n                loss += self.model.fit(\\n                    np.array(X_train), np.array(y_train),\\n                    epochs=1, verbose=0\\n                ).history[\"loss\"][0]\\n\\n        # DQN with fixed targets\\n        # elif self.strategy == \"t-dqn\":\\n        #     if self.n_iter % self.reset_every == 0:\\n        #         # reset target model weights\\n        #         self.target_model.set_weights(self.model.get_weights())\\n        #\\n        #     for state, action, reward, next_state, done in mini_batch:\\n        #         if done:\\n        #             target = reward\\n        #         else:\\n        #             # approximate deep q-learning equation with fixed targets\\n        #             target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n        #\\n        #         # estimate q-values based on current state\\n        #         q_values = self.model.predict(state)\\n        #         # update the target for current action based on discounted reward\\n        #         q_values[0][action] = target\\n        #\\n        #         X_train.append(state[0])\\n        #         y_train.append(q_values[0])\\n        #\\n        # # Double DQN\\n        # elif self.strategy == \"double-dqn\":\\n        #     if self.n_iter % self.reset_every == 0:\\n        #         # reset target model weights\\n        #         self.target_model.set_weights(self.model.get_weights())\\n        #\\n        #     for state, action, reward, next_state, done in mini_batch:\\n        #         if done:\\n        #             target = reward\\n        #         else:\\n        #             # approximate double deep q-learning equation\\n        #             target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n        #                 np.argmax(self.model.predict(next_state)[0])]\\n        #\\n        #         # estimate q-values based on current state\\n        #         q_values = self.model.predict(state)\\n        #         # update the target for current action based on discounted reward\\n        #         q_values[0][action] = target\\n        #\\n        #         X_train.append(state[0])\\n        #         y_train.append(q_values[0])\\n        #\\n        # else:\\n        #     raise NotImplementedError()\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def train_experience_replay(self, batch_size):\\n        \"\"\"Train on previous experiences in memory\\n        \"\"\"\\n        mini_batch = random.sample(self.memory, batch_size)\\n        X_train, y_train = [], []\\n\\n        # DQN\\n        if self.strategy == \"dqn\":\\n            for state, action, reward, done in mini_batch:\\n                # Here, target is directly set to the immediate reward\\n                target = reward\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state, verbose=0)\\n                # update the target for current action based on the immediate reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # DQN with fixed targets\\n        elif self.strategy == \"t-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate deep q-learning equation with fixed targets\\n                    target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        # Double DQN\\n        elif self.strategy == \"double-dqn\":\\n            if self.n_iter % self.reset_every == 0:\\n                # reset target model weights\\n                self.target_model.set_weights(self.model.get_weights())\\n\\n            for state, action, reward, next_state, done in mini_batch:\\n                if done:\\n                    target = reward\\n                else:\\n                    # approximate double deep q-learning equation\\n                    target = reward + self.gamma * self.target_model.predict(next_state)[0][\\n                        np.argmax(self.model.predict(next_state)[0])]\\n\\n                # estimate q-values based on current state\\n                q_values = self.model.predict(state)\\n                # update the target for current action based on discounted reward\\n                q_values[0][action] = target\\n\\n                X_train.append(state[0])\\n                y_train.append(q_values[0])\\n\\n        else:\\n            raise NotImplementedError()\\n\\n        # update q-function parameters based on huber loss gradient\\n        loss = self.model.fit(\\n            np.array(X_train), np.array(y_train),\\n            epochs=1, verbose=0\\n        ).history[\"loss\"][0]\\n\\n        # as the training goes on we want the agent to\\n        # make less random and more optimal decisions\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n        return loss\\n\\n    def save(self):\\n        path = pathlib.Path()\\n        self.model.save(path / \"models/{}.keras\".format(self.model_name), overwrite=True)\\n\\n    def load(self):\\n        return tf.keras.models.load_model(\"models/{}.keras\".format(self.model_name), custom_objects=self.custom_objects)\\n', 'Make sure the Current Profit doesn\\'t overwrite the Previous Profit if the current Profit is smaller\\n\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"\\n    Monitor system memory usage and print a warning if it exceeds the threshold.\\n\\n    Parameters:\\n    threshold (int): Memory usage threshold percentage.\\n\\n    Returns:\\n    bool: True if memory usage exceeds the threshold, False otherwise.\\n    \"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef detect_market_reversal(data, t, window_size, adjcp_field=\\'close\\'):\\n    \"\"\"\\n    Detect market reversal using a simple moving average crossover strategy.\\n\\n    Parameters:\\n    data (DataFrame): The historical stock data.\\n    t (int): The current time step.\\n    window_size (int): The size of the window for state representation.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    bool: True if a market reversal is detected, False otherwise.\\n    \"\"\"\\n    if t < window_size:\\n        return False\\n\\n    short_window = 5\\n    long_window = 20\\n\\n    short_mavg = data[adjcp_field].rolling(window=short_window).mean()\\n    long_mavg = data[adjcp_field].rolling(window=long_window).mean()\\n\\n    if short_mavg.iloc[t] > long_mavg.iloc[t] and short_mavg.iloc[t - 1] <= long_mavg.iloc[t - 1]:\\n        return True  # Bullish crossover (buy signal)\\n    elif short_mavg.iloc[t] < long_mavg.iloc[t] and short_mavg.iloc[t - 1] >= long_mavg.iloc[t - 1]:\\n        return True  # Bearish crossover (sell signal)\\n\\n    return False\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0, adjcp_field=\\'close\\'):\\n    \"\"\"\\n    Train the trading agent over a specified number of episodes.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    episode (int): The current episode number.\\n    data (DataFrame): The historical stock data.\\n    ep_count (int): Total number of episodes for training.\\n    batch_size (int): Size of the mini-batch for experience replay.\\n    window_size (int): The size of the window for state representation.\\n    prev_agent_profit (float): The highest profit observed so far.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (episode, ep_count, total_profit, avg_loss, prev_agent_profit, avg_profit_per_trade, win_loss_ratio)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n    trades = 0\\n    wins = 0\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False, desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n        reward = 0\\n        is_done = False\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state)  # Use agent\\'s action selection\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n\\n        # Detect market reversal\\n        if detect_market_reversal(data, t, window_size, adjcp_field):\\n            if action == 1:  # If the agent is currently on a BUY, we close the BUY and set action to 3\\n                action = 3\\n            elif action == 2:  # If the agent is currently on a SELL, we close the SELL and set action to 4\\n                action = 4\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            reward = delta\\n            total_profit += delta\\n            trades += 1\\n            if delta > 0:\\n                wins += 1\\n        elif action == 2:  # SELL\\n            reward = -delta\\n            total_profit -= delta\\n            trades += 1\\n            if delta < 0:\\n                wins += 1\\n        elif action == 3:  # Close BUY\\n            reward = delta\\n            total_profit += delta\\n            trades += 1\\n            if delta > 0:\\n                wins += 1\\n        elif action == 4:  # Close SELL\\n            reward = -delta\\n            total_profit -= delta\\n            trades += 1\\n            if delta < 0:\\n                wins += 1\\n        else:  # HOLD\\n            reward = 0\\n\\n        done = (t == data_length - 1) or is_done\\n        agent.remember(state, action, reward, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay2()\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit >= prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved successfully after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Update prev_agent_profit to retain the highest profit observed\\n    prev_agent_profit = max(prev_agent_profit, total_profit)\\n\\n    avg_profit_per_trade = total_profit / trades if trades > 0 else 0\\n    win_loss_ratio = wins / trades if trades > 0 else 0\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss)), prev_agent_profit, avg_profit_per_trade, win_loss_ratio\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    \"\"\"\\n    Evaluate the trading agent on historical stock data.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window_size (int): The size of the window for state representation.\\n    debug (bool): Flag to enable/disable debug logging.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (total_profit, history)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state, is_eval=True)\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n\\n        # Detect market reversal\\n        if detect_market_reversal(data, t, window_size, adjcp_field):\\n            if action == 1:  # If the agent is currently on a BUY, we close the BUY and set action to 3\\n                action = 3\\n            elif action == 2:  # If the agent is currently on a SELL, we close the SELL and set action to 4\\n                action = 4\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 2:  # SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 3:  # Close BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"CLOSE BUY\"))\\n            if debug:\\n                logging.debug(\"Close Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 4:  # Close SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"CLOSE SELL\"))\\n            if debug:\\n                logging.debug(\"Close Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        else:  # HOLD\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        state = next_state\\n        if t == data_length - 1:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    \"\"\"\\n    Predict the next action using the trained trading agent.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window (int): The size of the window for state representation.\\n\\n    Returns:\\n    int: The predicted action (0: HOLD, 1: BUY, 2: SELL, 3: CLOSE BUY, 4: CLOSE SELL).\\n    \"\"\"\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n\\n    # Detect market reversal\\n    if detect_market_reversal(data, data_length, window, adjcp_field=\\'close\\'):\\n        if action == 1:  # If the agent is currently on a BUY, we close the BUY and set action to 3\\n            action = 3\\n        elif action == 2:  # If the agent is currently on a SELL, we close the SELL and set action to 4\\n            action = 4\\n\\n    return action', 'The prev_agent_profit is not holding the largest Profit made during training and is affecting the when we save the model for prediction.\\n\\n\\nimport logging\\nimport numpy as np\\nimport psutil\\nfrom tqdm import tqdm\\n\\nfrom .ops import get_state\\nfrom .utils import format_currency, format_position\\n\\ndef monitor_memory(threshold=80):\\n    \"\"\"\\n    Monitor system memory usage and print a warning if it exceeds the threshold.\\n\\n    Parameters:\\n    threshold (int): Memory usage threshold percentage.\\n\\n    Returns:\\n    bool: True if memory usage exceeds the threshold, False otherwise.\\n    \"\"\"\\n    memory = psutil.virtual_memory()\\n    if memory.percent > threshold:\\n        print(f\"Memory usage is at {memory.percent}% which is above {threshold}%.\")\\n        return True\\n    return False\\n\\ndef detect_market_reversal(data, t, window_size, adjcp_field=\\'close\\'):\\n    \"\"\"\\n    Detect market reversal using a simple moving average crossover strategy.\\n\\n    Parameters:\\n    data (DataFrame): The historical stock data.\\n    t (int): The current time step.\\n    window_size (int): The size of the window for state representation.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    bool: True if a market reversal is detected, False otherwise.\\n    \"\"\"\\n    if t < window_size:\\n        return False\\n\\n    short_window = 5\\n    long_window = 20\\n\\n    short_mavg = data[adjcp_field].rolling(window=short_window).mean()\\n    long_mavg = data[adjcp_field].rolling(window=long_window).mean()\\n\\n    if short_mavg.iloc[t] > long_mavg.iloc[t] and short_mavg.iloc[t - 1] <= long_mavg.iloc[t - 1]:\\n        return True  # Bullish crossover (buy signal)\\n    elif short_mavg.iloc[t] < long_mavg.iloc[t] and short_mavg.iloc[t - 1] >= long_mavg.iloc[t - 1]:\\n        return True  # Bearish crossover (sell signal)\\n\\n    return False\\n\\ndef train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10, prev_agent_profit=0, adjcp_field=\\'close\\'):\\n    \"\"\"\\n    Train the trading agent over a specified number of episodes.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    episode (int): The current episode number.\\n    data (DataFrame): The historical stock data.\\n    ep_count (int): Total number of episodes for training.\\n    batch_size (int): Size of the mini-batch for experience replay.\\n    window_size (int): The size of the window for state representation.\\n    prev_agent_profit (float): The highest profit observed so far.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (episode, ep_count, total_profit, avg_loss, prev_agent_profit, avg_profit_per_trade, win_loss_ratio)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n    avg_loss = []\\n    trades = 0\\n    wins = 0\\n\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in tqdm(range(data_length), total=data_length, leave=True, disable=False, desc=f\\'{agent.strategy}: Episode {episode}/{ep_count}\\'):\\n        reward = 0\\n        is_done = False\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state)  # Use agent\\'s action selection\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n            is_done = True\\n\\n        # Detect market reversal\\n        if detect_market_reversal(data, t, window_size, adjcp_field):\\n            if action == 1:  # If the agent is currently on a BUY, we close the BUY and set action to 3\\n                action = 3\\n            elif action == 2:  # If the agent is currently on a SELL, we close the SELL and set action to 4\\n                action = 4\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            reward = delta\\n            total_profit += delta\\n            trades += 1\\n            if delta > 0:\\n                wins += 1\\n        elif action == 2:  # SELL\\n            reward = -delta\\n            total_profit -= delta\\n            trades += 1\\n            if delta < 0:\\n                wins += 1\\n        elif action == 3:  # Close BUY\\n            reward = delta\\n            total_profit += delta\\n            trades += 1\\n            if delta > 0:\\n                wins += 1\\n        elif action == 4:  # Close SELL\\n            reward = -delta\\n            total_profit -= delta\\n            trades += 1\\n            if delta < 0:\\n                wins += 1\\n        else:  # HOLD\\n            reward = 0\\n\\n        done = (t == data_length - 1) or is_done\\n        agent.remember(state, action, reward, done)\\n\\n        if len(agent.memory) > batch_size:\\n            loss = agent.train_experience_replay2()\\n            avg_loss.append(loss)\\n\\n        state = next_state\\n\\n    # Save the model if profitable\\n    if total_profit > 0 and total_profit >= prev_agent_profit:\\n        try:\\n            agent.save()  # Save the model\\n            logging.info(\"Model saved successfully after profitable episode.\")\\n        except Exception as e:\\n            logging.error(f\"Error saving model after training: {e}\")\\n\\n    # Update prev_agent_profit to retain the highest profit observed\\n    if total_profit > prev_agent_profit:\\n        prev_agent_profit = total_profit\\n\\n    avg_profit_per_trade = total_profit / trades if trades > 0 else 0\\n    win_loss_ratio = wins / trades if trades > 0 else 0\\n\\n    return episode, ep_count, total_profit, np.mean(np.array(avg_loss)), prev_agent_profit, avg_profit_per_trade, win_loss_ratio\\n\\ndef evaluate_model(agent, data, window_size, debug, adjcp_field):\\n    \"\"\"\\n    Evaluate the trading agent on historical stock data.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window_size (int): The size of the window for state representation.\\n    debug (bool): Flag to enable/disable debug logging.\\n    adjcp_field (str): The field name for adjusted closing price.\\n\\n    Returns:\\n    tuple: (total_profit, history)\\n    \"\"\"\\n    total_profit = 0\\n    data_length = len(data) - 1\\n\\n    history = []\\n    state = get_state(data, 0, window_size + 1)\\n\\n    for t in range(data_length):\\n        next_state = get_state(data, t + 1, window_size + 1)\\n        action = agent.act(state, is_eval=True)\\n\\n        try:\\n            delta = data.iloc[t + window_size][adjcp_field] - data.iloc[t][adjcp_field]\\n        except IndexError:\\n            delta = 0\\n\\n        # Detect market reversal\\n        if detect_market_reversal(data, t, window_size, adjcp_field):\\n            if action == 1:  # If the agent is currently on a BUY, we close the BUY and set action to 3\\n                action = 3\\n            elif action == 2:  # If the agent is currently on a SELL, we close the SELL and set action to 4\\n                action = 4\\n\\n        # select an action based on agent\\'s decision\\n        if action == 1:  # BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"BUY\"))\\n            if debug:\\n                logging.debug(\"Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 2:  # SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"SELL\"))\\n            if debug:\\n                logging.debug(\"Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 3:  # Close BUY\\n            total_profit += delta\\n            history.append((data.iloc[t][adjcp_field], \"CLOSE BUY\"))\\n            if debug:\\n                logging.debug(\"Close Buy at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        elif action == 4:  # Close SELL\\n            total_profit -= delta\\n            history.append((data.iloc[t][adjcp_field], \"CLOSE SELL\"))\\n            if debug:\\n                logging.debug(\"Close Sell at: {} | Profit: {}\".format(format_currency(data.iloc[t][adjcp_field]), format_position(delta)))\\n        else:  # HOLD\\n            history.append((data.iloc[t][adjcp_field], \"HOLD\"))\\n\\n        state = next_state\\n        if t == data_length - 1:\\n            return total_profit, history\\n\\ndef predict_model(agent, data, window):\\n    \"\"\"\\n    Predict the next action using the trained trading agent.\\n\\n    Parameters:\\n    agent (object): The trading agent.\\n    data (DataFrame): The historical stock data.\\n    window (int): The size of the window for state representation.\\n\\n    Returns:\\n    int: The predicted action (0: HOLD, 1: BUY, 2: SELL, 3: CLOSE BUY, 4: CLOSE SELL).\\n    \"\"\"\\n    data_length = len(data) - 1\\n    state = get_state(data, data_length, window)\\n    action = agent.act(state, is_eval=True)\\n\\n    # Detect market reversal\\n    if detect_market_reversal(data, data_length, window, adjcp_field=\\'close\\'):\\n        if action == 1:  # If the agent is currently on a BUY, we close the BUY and set action to 3\\n            action = 3\\n        elif action == 2:  # If the agent is currently on a SELL, we close the SELL and set action to 4\\n            action = 4\\n            \\n    return action']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 20460/35267 [15:39<11:04, 22.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 18716 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Failed ['time\\topen\\thigh\\tlow\\tclose\\tShort EMA\\tLong EMA\\tShapes\\tShapes\\tBuy Signal\\tStop Loss\\tShapes\\tShapes\\tShapes\\tShapes\\tPivot High\\tPivot Low 1684762200\\t55.464\\t55.872\\t52.8\\t53.856\\t55.23721481155627\\t56.44687439182716\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\t0\\tNaN\\tNaN\\tNaN 1684848600\\t53.928\\t58.176\\t53.52\\t55.944\\t55.37857184924502\\t56.40115853802469\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1684935000\\t55.584\\t55.656\\t50.4\\t51.768\\t54.65645747939602\\t55.979962307295175\\t0\\t0\\tNaN\\tNaN\\t0\\t1\\tNaN\\tNaN\\tNaN\\tNaN 1685021400\\t51.24\\t52.416\\t43.2\\t49.632\\t53.65156598351681\\t55.40287482481379\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\t43.2 1685107800\\t52.224\\t55.176\\t50.064\\t52.8\\t53.48125278681345\\t55.16624984073981\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1685453400\\t55.968\\t60.888\\t54.72\\t59.928\\t54.77060222945076\\t55.59913621885437\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1685539800\\t65.088\\t65.52\\t55.2\\t59.184\\t55.65328178356061\\t55.925032926231246\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1685626200\\t55.2\\t57.24\\t51.36\\t52.56\\t55.034625426848486\\t55.61912084202841\\t0\\t0\\tNaN\\tNaN\\t0\\t1\\tNaN\\tNaN\\tNaN\\tNaN 1685712600\\t54\\t59.52\\t53.544\\t57.6\\t55.54770034147879\\t55.79920076548037\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\t0\\tNaN\\tNaN 1685971800\\t59.4\\t66.96\\t56.88\\t65.304\\t57.49896027318303\\t56.66327342316397\\t1\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1686058200\\t66.48\\t89.304\\t61.68\\t85.848\\t63.168768218546425\\t59.31643038469452\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1686144600\\t95.088\\t98.4\\t76.8\\t80.208\\t66.57661457483714\\t61.21566398608593\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\t98.4\\tNaN 1686231000\\t81.6\\t81.648\\t68.76\\t74.112\\t68.08369165986971\\t62.38805816916903\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1686317400\\t73.8\\t80.4\\t72.024\\t78.672\\t70.20135332789577\\t63.86841651742639\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1686576600\\t82.704\\t84.336\\t73.944\\t75.912\\t71.34348266231662\\t64.9632877431149\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1686663000\\t74.544\\t81.336\\t72.552\\t79.2\\t72.9147861298533\\t66.25753431192264\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1686749400\\t79.2\\t83.952\\t76.848\\t83.76\\t75.08382890388263\\t67.84866755629331\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\t0\\tNaN\\tNaN\\tNaN 1686835800\\t83.64\\t100.272\\t83.328\\t95.976\\t79.26226312310611\\t70.40569777844846\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1686922200\\t97.272\\t117.36\\t96.12\\t107.952\\t85.00021049848489\\t73.81899798040769\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\t117.36\\tNaN 1687267800\\t69.6\\t79.992\\t64.08\\t67.392\\t81.47856839878791\\t73.23472543673427\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1687354200\\t67.248\\t67.248\\t52.776\\t58.728\\t76.92845471903033\\t71.91593221521296\\t0\\t0\\tNaN\\tNaN\\t0\\t1\\tNaN\\tNaN\\tNaN\\tNaN 1687440600\\t56.784\\t57.936\\t53.04\\t56.016\\t72.74596377522425\\t70.47048383201178\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1687527000\\t54.48\\t58.536\\t53.64\\t54.456\\t69.08797102017941\\t69.01462166546526\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1687786200\\t54.864\\t62.88\\t54.84\\t56.568\\t66.58397681614353\\t67.88311060496841\\t0\\t1\\tNaN\\tNaN\\t0\\t0\\t0\\tNaN\\tNaN\\tNaN 1687872600\\t59.136\\t60.48\\t56.352\\t56.904\\t64.64798145291482\\t66.88500964088037\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1687959000\\t57.096\\t58.8\\t52.2\\t52.728\\t62.263985162331856\\t65.5980087644367\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1688045400\\t52.44\\t52.728\\t49.488\\t50.4\\t59.89118812986548\\t64.21637160403336\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1688131800\\t49.416\\t50.472\\t47.04\\t48\\t57.512950503892384\\t62.742156003666686\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1688391000\\t48.432\\t52.32\\t48\\t50.4\\t56.090360403113905\\t61.62014182151517\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1688563800\\t51.192\\t51.672\\t47.52\\t48.24\\t54.520288322491126\\t60.403765292286515\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1688650200\\t46.392\\t48.144\\t44.4\\t48\\t53.2162306579929\\t59.27615026571501\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\t44.4 1688736600\\t46.608\\t47.352\\t45.6\\t46.8\\t51.93298452639432\\t58.141954787013646\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1688995800\\t46.152\\t48.192\\t45.648\\t48\\t51.14638762111546\\t57.21995889728513\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1689082200\\t47.52\\t56.16\\t47.496\\t54\\t51.71711009689237\\t56.9272353611683\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1689168600\\t52.8\\t58.31976\\t52.8\\t54.84\\t52.34168807751389\\t56.737486691971185\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1689255000\\t55.176\\t58.08\\t53.976\\t56.52\\t53.17735046201111\\t56.717715174519256\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\t0\\tNaN\\tNaN 1689341400\\t55.872\\t62.064\\t54.168\\t60.984\\t54.73868036960889\\t57.10555924956296\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1689600600\\t62.4\\t69\\t60.648\\t68.928\\t57.576544295687114\\t58.18032659051178\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1689687000\\t68.88\\t79.68\\t66.24\\t75.528\\t61.16683543654969\\t59.757387809556164\\t1\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1689773400\\t77.736\\t82.728\\t67.992\\t70.56\\t63.045468349239755\\t60.739443463232874\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\t82.728\\tNaN 1689859800\\t69.84\\t73.776\\t66.384\\t68.64\\t64.1643746793918\\t61.45767587566625\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1689946200\\t69.12\\t70.92\\t63.6\\t69.576\\t65.24669974351343\\t62.19570534151477\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1690205400\\t67.2\\t70.032\\t64.8\\t69.84\\t66.16535979481074\\t62.89064121955888\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1690291800\\t70.368\\t74.4\\t62.76\\t64.824\\t65.8970878358486\\t63.06640110868989\\t0\\t0\\tNaN\\tNaN\\t0\\t1\\tNaN\\tNaN\\tNaN\\tNaN 1690378200\\t63.696\\t68.4\\t63.6\\t66.12\\t65.94167026867888\\t63.3440010078999\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\t0\\tNaN\\tNaN\\tNaN 1690464600\\t65.52\\t68.712\\t61.32\\t62.112\\t65.1757362149431\\t63.23200091627263\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1690551000\\t62.952\\t65.64\\t59.52\\t61.608\\t64.46218897195448\\t63.08436446933876\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\t59.52 1690810200\\t67.224\\t72.96\\t65.04\\t72.576\\t66.08495117756358\\t63.9472404266716\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1690896600\\t76.8\\t82.656\\t73.584\\t78.24\\t68.51596094205087\\t65.24658220606509\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1690983000\\t76.56\\t80.112\\t70.32\\t79.2\\t70.6527687536407\\t66.51507473278645\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1691069400\\t76.416\\t83.64\\t75.12\\t80.136\\t72.54941500291255\\t67.75334066616949\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\t83.64\\tNaN 1691155800\\t81.456\\t83.184\\t77.28\\t78.168\\t73.67313200233005\\t68.7001278783359\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\t0\\tNaN\\tNaN 1691415000\\t77.76\\t78.72\\t68.64\\t69.6\\t72.85850560186404\\t68.78193443485083\\t0\\t0\\tNaN\\tNaN\\t0\\t1\\tNaN\\tNaN\\tNaN\\tNaN 1691501400\\t69.408\\t69.408\\t65.04\\t65.568\\t71.40040448149124\\t68.48975857713711\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1691587800\\t66.72\\t67.2\\t60.96\\t66\\t70.320323585193\\t68.26341688830647\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1691674200\\t70.8\\t70.8\\t61.92\\t63.12\\t68.8802588681544\\t67.79583353482406\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\t0\\tNaN\\tNaN\\tNaN 1691760600\\t60.96\\t66\\t60.72\\t63.864\\t67.87700709452352\\t67.43839412256732\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1692019800\\t67.344\\t67.92\\t62.424\\t64.344\\t67.17040567561881\\t67.15708556597029\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1692106200\\t64.488\\t64.776\\t56.664\\t58.032\\t65.34272454049504\\t66.32753233270026\\t0\\t1\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1692192600\\t58.08\\t59.976\\t52.176\\t52.56\\t62.786179632396035\\t65.07593848427297\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1692279000\\t53.088\\t66\\t53.088\\t66\\t63.42894370591683\\t65.1599440766118\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1692365400\\t59.808\\t64.704\\t57.84\\t59.712\\t62.68555496473346\\t64.66467643328345\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1692624600\\t61.2\\t61.2\\t47.952\\t49.2\\t59.98844397178677\\t63.258796757530405\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1692711000\\t50.88\\t53.328\\t42.72\\t44.4\\t56.870755177429416\\t61.54436068866401\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1692797400\\t45.36\\t46.008\\t40.824\\t43.2\\t54.136604141943536\\t59.8766915351491\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1692883800\\t42.48\\t45.12\\t40.368\\t44.4\\t52.18928331355483\\t58.46971957740827\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1692970200\\t43.2\\t48.624\\t36.264\\t37.32\\t49.21542665084387\\t56.54701779764388\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1693229400\\t31.77\\t33.6\\t27.822\\t28.44\\t45.06034132067509\\t53.99183436149444\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1693315800\\t28.35\\t29.13\\t24.99\\t25.32\\t41.112273056540076\\t51.38530396499495\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1693402200\\t24.54\\t25.5\\t21.54\\t25.38\\t37.96581844523206\\t49.02118542272268\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1693488600\\t25.23\\t27.51\\t22.5\\t22.575\\t34.88765475618565\\t46.61698674792971\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1693575000\\t22.71\\t22.74\\t21.06\\t21.795\\t32.26912380494852\\t44.36044249811792\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1693920600\\t21.72\\t21.72\\t18.6\\t18.78\\t29.571299043958817\\t42.03494772556174\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1694007000\\t17.91\\t18.15\\t15.21\\t15.3\\t26.717039235167054\\t39.604497932328854\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1694093400\\t14.085\\t14.13\\t11.5833\\t11.67\\t23.707631388133642\\t37.06499812029896\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1694179800\\t12.54\\t13.14\\t10.41\\t11.28\\t21.222105110506913\\t34.72090738208996\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1694439000\\t11.85\\t11.88\\t10.5\\t10.665\\t19.11068408840553\\t32.53400671099087\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1694525400\\t10.53\\t13.8\\t10.1382\\t11.64\\t17.616547270724425\\t30.634551555446247\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\t10.1382 1694611800\\t11.64\\t16.35\\t11.52\\t15.36\\t17.16523781657954\\t29.245955959496587\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1694698200\\t15.24\\t15.69\\t13.53\\t14.085\\t16.54919025326363\\t27.86768723590599\\t0\\t0\\tNaN\\tNaN\\t0\\t1\\tNaN\\tNaN\\tNaN\\tNaN 1694784600\\t14.16\\t17.34\\t13.92\\t15.33\\t16.305352202610905\\t26.727897487187263\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\t17.34\\tNaN 1695043800\\t15.24\\t15.6\\t13.77\\t14.04\\t15.852281762088724\\t25.57445226107933\\t0\\t0\\tNaN\\tNaN\\t0\\t1\\tNaN\\tNaN\\tNaN\\tNaN 1695130200\\t12.51\\t13.65\\t12\\t12.3\\t15.141825409670979\\t24.367683873708483\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\t0\\tNaN\\tNaN 1695216600\\t12.15\\t13.17\\t11.55\\t11.7\\t14.453460327736783\\t23.216076248825892\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1695303000\\t11.46\\t12.09\\t10.95\\t10.98\\t13.758768262189427\\t22.10370568075081\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1695389400\\t11.61\\t13.02\\t11.13\\t11.76\\t13.359014609751542\\t21.163368800682555\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\t0\\tNaN\\tNaN\\tNaN 1695648600\\t11.64\\t11.64\\t8.835\\t8.91\\t12.469211687801234\\t20.049426182438687\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1695735000\\t8.46\\t9.5391\\t7.35\\t7.38\\t11.451369350240988\\t18.897660165853353\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1695821400\\t5.46\\t6.48\\t3.93\\t4.11\\t9.983095480192791\\t17.55332742350305\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1695907800\\t4.26\\t4.56\\t3.885\\t3.9\\t8.766476384154233\\t16.312115839548227\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1695994200\\t3.9\\t4.26\\t3.72\\t3.99\\t7.811181107323386\\t15.191923490498388\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1696253400\\t4.05\\t4.05\\t3.51\\t3.585\\t6.9659448858587085\\t14.136748627725806\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1696339800\\t3.6\\t3.63\\t3.03\\t3.57\\t6.286755908686967\\t13.17613511611437\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\t3.03 1696426200\\t3.48\\t3.96\\t3.45\\t3.54\\t5.737404726949573\\t12.300122832831246\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1696512600\\t3.51\\t3.72\\t3.39\\t3.42\\t5.273923781559659\\t11.492838938937496\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1696599000\\t3.36\\t3.48\\t3.24\\t3.39\\t4.897139025247727\\t10.756217217215905\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1696858200\\t3.33\\t3.39\\t3.27\\t3.33\\t4.583711220198182\\t10.081106561105369\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1696944600\\t3.42\\t3.66\\t3.3\\t3.63\\t4.392968976158545\\t9.494642328277608\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\t0\\tNaN\\tNaN 1697031000\\t3.66\\t3.687\\t3.24\\t3.39\\t4.192375180926836\\t8.939674843888735\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1697117400\\t3.45\\t3.45\\t3.27\\t3.3\\t4.013900144741469\\t8.426977130807941\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1697203800\\t3.24\\t3.3\\t3.09\\t3.15\\t3.841120115793175\\t7.9472519370981285\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\t3.09 1697463000\\t3.33\\t3.48\\t3.18\\t3.24\\t3.7208960926345402\\t7.51931994281648\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1697549400\\t3.33\\t4.17\\t3.3\\t4.14\\t3.804716874107632\\t7.212109038924073\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1697635800\\t3.84\\t5.25\\t3.72\\t5.22\\t4.087773499286105\\t7.031008217203702\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\t5.25\\tNaN 1697722200\\t4.86\\t5.16\\t3.45\\t3.54\\t3.978218799428884\\t6.713643833821547\\t0\\t0\\tNaN\\tNaN\\t0\\t1\\tNaN\\tNaN\\tNaN\\tNaN 1697808600\\t3.6\\t3.72\\t3.18\\t3.51\\t3.8845750395431073\\t6.422403485292316\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\t0\\tNaN\\tNaN 1698067800\\t3.36\\t3.465\\t3.3\\t3.315\\t3.770660031634486\\t6.139912259356651\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1698154200\\t3.45\\t3.48\\t3.09\\t3.15\\t3.646528025307589\\t5.868102053960591\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1698240600\\t3.18\\t3.255\\t3.09\\t3.165\\t3.550222420246071\\t5.6223655036005376\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\t0\\tNaN\\tNaN\\tNaN 1698327000\\t3.21\\t3.36\\t3.09\\t3.21\\t3.482177936196857\\t5.403059548727762\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1698413400\\t3.24\\t3.255\\t3.09\\t3.15\\t3.4157423489574854\\t5.198235953388874\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1698672600\\t3.27\\t3.33\\t3.09\\t3.21\\t3.3745938791659884\\t5.017487230353522\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1698759000\\t3.24\\t3.27\\t3.15\\t3.18\\t3.335675103332791\\t4.85044293668502\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1698845400\\t3.18\\t3.21\\t3.12\\t3.18\\t3.304540082666233\\t4.698584487895473\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1698931800\\t3.21\\t3.36\\t3.1956\\t3.33\\t3.3096320661329863\\t4.574167716268612\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1699018200\\t3.33\\t3.54\\t3.3003\\t3.36\\t3.319705652906389\\t4.463788832971465\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\t3.54\\tNaN 1699281000\\t3.51\\t3.51\\t3.03\\t3.075\\t3.270764522325111\\t4.337535302701332\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1699367400\\t3.12\\t3.27\\t3.03\\t3.195\\t3.255611617860089\\t4.233668457001211\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1699453800\\t3.18\\t3.36\\t3.1497\\t3.18\\t3.240489294288071\\t4.137880415455646\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1699540200\\t3.21\\t3.3\\t2.8002\\t2.8263\\t3.1576514354304566\\t4.018645832232405\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1699626600\\t2.91\\t2.925\\t2.3103\\t2.37\\t3.0001211483443653\\t3.8687689383930954\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\t0\\tNaN\\tNaN\\tNaN 1699885800\\t2.37\\t2.6505\\t2.37\\t2.6283\\t2.9257569186754924\\t3.755999034902814\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1699972200\\t2.5899\\t2.6997\\t2.34\\t2.4858\\t2.837765534940394\\t3.6405263953661944\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1700058600\\t2.475\\t2.475\\t1.9902\\t2.025\\t2.675212427952315\\t3.493660359423813\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1700145000\\t1.98\\t1.9803\\t1.5603\\t1.5663\\t2.453429942361852\\t3.3184457812943755\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1700231400\\t1.5807\\t1.908\\t1.5663\\t1.7034\\t2.3034239538894816\\t3.171623437540341\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1700490600\\t1.74\\t1.7592\\t1.6296\\t1.6707\\t2.176879163111585\\t3.035175852309401\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1700577000\\t1.6581\\t1.6944\\t1.41\\t1.53\\t2.0475033304892682\\t2.8983416839176375\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1700663400\\t1.533\\t1.558797\\t1.4553\\t1.4703\\t1.9320626643914145\\t2.7685197126523975\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1700836200\\t1.5885\\t1.635\\t1.485\\t1.4892\\t1.8434901315131316\\t2.6522179205930887\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1701095400\\t1.497\\t1.497\\t1.32\\t1.3323\\t1.7412521052105052\\t2.5322253823573533\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1701181800\\t1.41\\t1.4175\\t1.3485\\t1.353\\t1.6636016841684043\\t2.425023074870321\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1701268200\\t1.32\\t1.3995\\t1.32\\t1.35\\t1.6008813473347234\\t2.3272937044275643\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1701354600\\t1.3722\\t1.3725\\t1.125\\t1.161\\t1.5129050778677788\\t2.2212670040250586\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1701441000\\t1.1463\\t1.1469\\t1.02\\t1.1382\\t1.4379640622942231\\t2.122806367295508\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1701700200\\t1.1538\\t1.1802\\t1.0236\\t1.0689\\t1.3641512498353785\\t2.026996697541371\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1701786600\\t1.0653\\t1.08\\t0.93\\t0.9303\\t1.2773809998683028\\t1.9272969977648826\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1701873000\\t0.915\\t0.9447\\t0.7923\\t0.798\\t1.1815047998946422\\t1.8246336343317116\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1701959400\\t0.783\\t0.8094\\t0.75\\t0.7581\\t1.0968238399157137\\t1.727676031210647\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1702045800\\t0.75\\t1.095\\t0.7233\\t0.888\\t1.055059071932571\\t1.6513418465551335\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1702305000\\t0.93\\t0.996\\t0.7686\\t0.7725\\t0.9985472575460568\\t1.5714471332319395\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1702391400\\t0.7644\\t0.7941\\t0.672\\t0.762\\t0.9512378060368454\\t1.4978610302108541\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\t0.672 1702477800\\t0.7512\\t0.855\\t0.711\\t0.7905\\t0.9190902448294763\\t1.4335554820098675\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1702564200\\t0.723\\t0.849\\t0.723\\t0.7857\\t0.892412195863581\\t1.3746595290998795\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1702650600\\t0.807\\t0.87\\t0.789\\t0.801\\t0.8741297566908648\\t1.3225086628180722\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1702909800\\t0.8121\\t0.858\\t0.7929\\t0.825\\t0.8643038053526918\\t1.277280602561884\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1702996200\\t0.825\\t0.888\\t0.8106\\t0.849\\t0.8612430442821535\\t1.2383460023289854\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\t0\\tNaN\\tNaN 1703082600\\t0.8424\\t0.891\\t0.7614\\t0.7671\\t0.8424144354257228\\t1.195505456662714\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1703169000\\t0.7683\\t1.1499\\t0.768\\t1.0926\\t0.8924515483405783\\t1.1861504151479216\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1703255400\\t1.0734\\t1.179\\t0.9795\\t1.0377\\t0.9215012386724626\\t1.1726549228617469\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1703601000\\t1.0308\\t1.3332\\t0.9795\\t1.0182\\t0.94084099093797\\t1.1586135662379518\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\t1.3332\\tNaN 1703687400\\t1.0257\\t1.125\\t0.9909\\t1.005\\t0.953672792750376\\t1.144648696579956\\t0\\t0\\tNaN\\tNaN\\t0\\t1\\tNaN\\tNaN\\tNaN\\tNaN 1703773800\\t0.9\\t0.9513\\t0.7539\\t0.7773\\t0.9183982342003008\\t1.1112533605272328\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1703860200\\t0.7932\\t0.7947\\t0.69\\t0.6927\\t0.8732585873602406\\t1.073203055024757\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1704205800\\t0.7245\\t0.7416\\t0.6\\t0.6012\\t0.8188468698881926\\t1.0302936863861427\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1704292200\\t0.6015\\t0.606\\t0.51\\t0.5301\\t0.7610974959105541\\t0.9848215330783116\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\t0\\tNaN\\tNaN\\tNaN 1704378600\\t0.5754\\t0.615\\t0.5166\\t0.5193\\t0.7127379967284433\\t0.9425013937075559\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1704465000\\t0.522\\t0.5277\\t0.465\\t0.4773\\t0.6656503973827547\\t0.9002103579159599\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\t0.465 1704724200\\t0.4938\\t0.5751\\t0.48\\t0.5664\\t0.6458003179062037\\t0.8698639617417817\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1704810600\\t0.57\\t0.6165\\t0.5253\\t0.5328\\t0.623200254324963\\t0.8392217834016198\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1704897000\\t0.5697\\t0.5829\\t0.486\\t0.5055\\t0.5996602034599704\\t0.808883439456018\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1704983400\\t0.5028\\t0.5298\\t0.48\\t0.5133\\t0.5823881627679763\\t0.7820122176872891\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1705069800\\t0.5142\\t0.5352\\t0.5028\\t0.513\\t0.568510530214381\\t0.7575565615338992\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\t0\\tNaN\\tNaN 1705415400\\t0.516\\t0.5625\\t0.495\\t0.4986\\t0.5545284241715048\\t0.7340150559399083\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1705501800\\t0.4863\\t0.5034\\t0.477\\t0.4875\\t0.5411227393372039\\t0.7116045963090076\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1705588200\\t0.4899\\t0.5208\\t0.4803\\t0.4833\\t0.5295581914697631\\t0.6908496330081888\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1705674600\\t0.4944\\t0.4944\\t0.468\\t0.4686\\t0.5173665531758105\\t0.6706451209165353\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1705933800\\t0.471\\t0.495\\t0.435\\t0.4416\\t0.5022132425406484\\t0.6498228371968503\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1706020200\\t0.4455\\t0.45\\t0.399\\t0.441\\t0.4899705940325187\\t0.6308389429062276\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1706106600\\t0.4425\\t0.464997\\t0.4302\\t0.4356\\t0.47909647522601495\\t0.6130899480965705\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1706193000\\t0.4332\\t0.4356\\t0.3939\\t0.4086\\t0.464997180180812\\t0.5944999528150641\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1706279400\\t0.4089\\t0.438\\t0.4044\\t0.414\\t0.4547977441446496\\t0.5780908661955128\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1706538600\\t0.4242\\t0.462\\t0.42\\t0.4395\\t0.4517381953157197\\t0.5654916965413752\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1706625000\\t0.45\\t0.45\\t0.42\\t0.4239\\t0.44617055625257573\\t0.5526197241285229\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1706711400\\t0.4161\\t0.429\\t0.402\\t0.4053\\t0.4379964450020606\\t0.5392270219350208\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1706797800\\t0.4074\\t0.4074\\t0.3753\\t0.3816\\t0.42671715600164845\\t0.5248972926682007\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1706884200\\t0.3816\\t0.3825\\t0.3\\t0.3525\\t0.41187372480131873\\t0.5092248115165461\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1707143400\\t0.3471\\t0.3477\\t0.3093\\t0.3204\\t0.393578979841055\\t0.49205891956049647\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1707229800\\t0.327\\t0.340758\\t0.2883\\t0.312\\t0.377263183872844\\t0.4756899268731786\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1707316200\\t0.3072\\t0.3084\\t0.27\\t0.2715\\t0.3561105470982752\\t0.4571272062483442\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1707402600\\t0.2784\\t0.282\\t0.2523\\t0.267\\t0.33828843767862016\\t0.439842914771222\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1707489000\\t0.2664\\t0.279\\t0.261\\t0.2655\\t0.32373075014289615\\t0.4239935588829291\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1707748200\\t0.2808\\t0.285\\t0.2406\\t0.2475\\t0.30848460011431694\\t0.4079486898935719\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1707834600\\t0.2475\\t0.24885\\t0.2109\\t0.2172\\t0.29022768009145355\\t0.3906078999032472\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1707921000\\t0.2196\\t0.2205\\t0.186\\t0.192\\t0.27058214407316283\\t0.37255263627567925\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\t0.186 1708007400\\t0.1956\\t0.4485\\t0.1935\\t0.3273\\t0.2819257152585303\\t0.3684387602506175\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\t0.4485\\tNaN 1708093800\\t0.324\\t0.3651\\t0.2778\\t0.2811\\t0.2817605722068242\\t0.3604988729551068\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1708439400\\t0.2865\\t0.315\\t0.2088\\t0.2415\\t0.2737084577654594\\t0.34968079359555165\\t0\\t0\\tNaN\\tNaN\\t0\\t1\\tNaN\\tNaN\\tNaN\\tNaN 1708525800\\t0.2403\\t0.2466\\t0.2103\\t0.2205\\t0.2630667662123675\\t0.33793708508686515\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1708612200\\t0.231\\t0.2649\\t0.2286\\t0.24\\t0.258453412969894\\t0.32903371371533197\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\t0\\tNaN\\tNaN 1708698600\\t0.231\\t0.258\\t0.2208\\t0.258\\t0.2583627303759152\\t0.32257610337757453\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\t0\\tNaN\\tNaN\\tNaN 1708957800\\t0.258\\t0.2817\\t0.219\\t0.2331\\t0.25331018430073216\\t0.3144419121614314\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1709044200\\t0.2391\\t0.2463\\t0.2253\\t0.246\\t0.2518481474405857\\t0.3082199201467558\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1709130600\\t0.246\\t0.246\\t0.1857\\t0.1878\\t0.23903851795246855\\t0.29727265467886893\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1709217000\\t0.201\\t0.2094\\t0.18\\t0.2094\\t0.23311081436197484\\t0.2892842315262445\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1709303400\\t0.19\\t0.235\\t0.171\\t0.198\\t0.22608865148957988\\t0.28098566502385863\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1709562600\\t0.2043\\t0.206\\t0.1807\\t0.1841\\t0.21769092119166392\\t0.27217787729441695\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1709649000\\t0.1801\\t0.18095\\t0.16\\t0.1751\\t0.20917273695333113\\t0.2633526157221972\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1709735400\\t0.179\\t0.1824\\t0.1602\\t0.1632\\t0.19997818956266492\\t0.2542478324747247\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1709821800\\t0.1602\\t0.16135\\t0.1466\\t0.1478\\t0.18954255165013192\\t0.2445707567952043\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1709908200\\t0.146\\t0.147\\t0.13\\t0.133\\t0.17823404132010554\\t0.234427960722913\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1710163800\\t0.135\\t0.1368\\t0.123\\t0.129\\t0.16838723305608444\\t0.22484360065719364\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1710250200\\t0.128\\t0.13\\t0.0955\\t0.101\\t0.15490978644486755\\t0.21358509150653968\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1710336600\\t0.1\\t0.1371\\t0.0998\\t0.108\\t0.14552782915589405\\t0.20398644682412698\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1710423000\\t0.11\\t0.1119\\t0.1007\\t0.1067\\t0.13776226332471525\\t0.19514222438556997\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1710509400\\t0.1067\\t0.11\\t0.096\\t0.0969\\t0.1295898106597722\\t0.18621111307779087\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1710768600\\t0.1\\t0.119\\t0.0976\\t0.105\\t0.12467184852781776\\t0.1788282846161735\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1710855000\\t0.1066\\t0.1081\\t0.0902\\t0.0921\\t0.1181574788222542\\t0.17094389510561228\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1710941400\\t0.0941\\t0.095\\t0.086\\t0.0915\\t0.11282598305780336\\t0.1637217228232839\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\t0.086 1711027800\\t0.092\\t0.1499\\t0.089\\t0.136\\t0.1174607864462427\\t0.16120156620298537\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1711114200\\t0.1719\\t0.1729\\t0.1273\\t0.1317\\t0.12030862915699415\\t0.1585196056390776\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\t0.1729\\tNaN 1711373400\\t0.1313\\t0.1347\\t0.1014\\t0.1141\\t0.11906690332559532\\t0.15448145967188873\\t0\\t0\\tNaN\\tNaN\\t0\\t1\\tNaN\\tNaN\\tNaN\\tNaN 1711459800\\t0.1139\\t0.12\\t0.1\\t0.1001\\t0.11527352266047625\\t0.14953769061080793\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1711546200\\t0.1\\t0.1039\\t0.0875\\t0.0952\\t0.111258818128381\\t0.14459790055527993\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\t0\\tNaN\\tNaN 1711632600\\t0.0947\\t0.1197\\t0.088\\t0.1\\t0.1090070545027048\\t0.1405435459593454\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1711978200\\t0.0999\\t0.1\\t0.09\\t0.0948\\t0.10616564360216384\\t0.1363850417812231\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\t0\\tNaN\\tNaN\\tNaN 1712064600\\t0.096\\t0.096\\t0.087\\t0.09\\t0.10293251488173107\\t0.1321682198011119\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1712151000\\t0.091\\t0.09565\\t0.089\\t0.09\\t0.10034601190538485\\t0.1283347452737381\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1712237400\\t0.0899\\t0.0909\\t0.082\\t0.0855\\t0.09737680952430788\\t0.12444067752158008\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1712323800\\t0.0825\\t0.0848\\t0.078\\t0.0784\\t0.09358144761944631\\t0.12025516138325462\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\t0.078 1712583000\\t0.084\\t0.105\\t0.081\\t0.09\\t0.09286515809555705\\t0.1175046921665951\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1712669400\\t0.1\\t0.1149\\t0.0924\\t0.1118\\t0.09665212647644564\\t0.11698608378781374\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\t0.1149\\tNaN 1712755800\\t0.1071\\t0.1092\\t0.09\\t0.0908\\t0.09548170118115651\\t0.11460553071619431\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1712842200\\t0.0908\\t0.0908\\t0.0801\\t0.083\\t0.09298536094492521\\t0.11173230065108573\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1712928600\\t0.0851\\t0.0896\\t0.079\\t0.08\\t0.09038828875594017\\t0.10884754604644158\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\t0\\tNaN\\tNaN 1713187800\\t0.08\\t0.08\\t0.07\\t0.072\\t0.08671063100475214\\t0.10549776913312871\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1713274200\\t0.07\\t0.0714\\t0.065\\t0.0659\\t0.08254850480380171\\t0.10189797193920791\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\t0\\tNaN\\tNaN\\tNaN 1713360600\\t0.0685\\t0.0685\\t0.061\\t0.0611\\t0.07825880384304137\\t0.09818906539927992\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1713447000\\t0.0621\\t0.0712\\t0.061\\t0.0615\\t0.0749070430744331\\t0.0948536958175272\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1713533400\\t0.0633\\t0.0638\\t0.056\\t0.0587\\t0.07166563445954649\\t0.091566996197752\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1713792600\\t0.0585\\t0.065\\t0.051\\t0.0555\\t0.06843250756763719\\t0.08828817836159274\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1713879000\\t0.0569\\t0.0615\\t0.05\\t0.054\\t0.06554600605410975\\t0.08517107123781158\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1713965400\\t0.055\\t0.055\\t0.048\\t0.0499\\t0.0624168048432878\\t0.08196461021619235\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1714051800\\t0.0498\\t0.0498\\t0.0451\\t0.049\\t0.05973344387463024\\t0.07896782746926577\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1714138200\\t0.05\\t0.052\\t0.046\\t0.047\\t0.057186755099704195\\t0.07606166133569615\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1714397400\\t0.0445\\t0.0445\\t0.038\\t0.0418\\t0.054109404079763355\\t0.07294696485063287\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\t0.038 1714483800\\t0.042\\t0.049\\t0.0395\\t0.0441\\t0.052107523263810684\\t0.07032451350057534\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1714570200\\t0.049\\t0.049\\t0.0411\\t0.0461\\t0.05090601861104855\\t0.06812228500052303\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1714656600\\t0.0473\\t0.0494\\t0.04\\t0.0401\\t0.04874481488883884\\t0.06557480454593002\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1714743000\\t0.0409\\t0.0436\\t0.0381\\t0.0409\\t0.04717585191107107\\t0.06333164049630002\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1715002200\\t0.041\\t0.0438\\t0.0401\\t0.0413\\t0.046000681528856856\\t0.06132876408754548\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\t0\\tNaN\\tNaN 1715088600\\t0.048\\t0.052\\t0.0418\\t0.0437\\t0.04554054522308548\\t0.05972614917049589\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1715175000\\t0.0415\\t0.0433\\t0.0402\\t0.0418\\t0.044792436178468384\\t0.058096499245905354\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1715261400\\t0.0416\\t0.0416\\t0.0391\\t0.0406\\t0.043953948942774705\\t0.0565059084053685\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1715347800\\t0.042\\t0.0484\\t0.04\\t0.0461\\t0.04438315915421977\\t0.055559916732153186\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1715607000\\t0.048\\t0.07\\t0.0451\\t0.061\\t0.04770652732337581\\t0.0560544697565029\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1715693400\\t0.0997\\t0.3598\\t0.09\\t0.2852\\t0.09520522185870064\\t0.07688588159682082\\t1\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1715779800\\t0.599\\t0.98\\t0.3883\\t0.705\\t0.2171641774869605\\t0.13398716508801892\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1715866200\\t0.8122\\t2.35\\t0.7554\\t1.65\\t0.5037313419895684\\t0.27180651371638087\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1715952600\\t2.7\\t3.9\\t0.9\\t1.03\\t0.6089850735916547\\t0.34073319428761895\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\t3.9\\tNaN 1716211800\\t1.96\\t2.25\\t1.2\\t1.8\\t0.8471880588733237\\t0.4733938129887445\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1716298200\\t1.48\\t1.66\\t1.24\\t1.39\\t0.955750447098659\\t0.5567216481715859\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1716384600\\t1.27\\t1.33\\t1\\t1.12\\t0.9886003576789272\\t0.607928771065078\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1716471000\\t1.185\\t1.47\\t1.13\\t1.19\\t1.0288802861431416\\t0.6608443373318891\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1716557400\\t1.36\\t1.37\\t1.13\\t1.16\\t1.0551042289145134\\t0.7062221248471718\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\t0\\tNaN\\tNaN\\tNaN 1716903000\\t1.25\\t1.25\\t1.13\\t1.17\\t1.0780833831316108\\t0.7483837498610653\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1716989400\\t0.8195\\t0.8777\\t0.3906\\t0.4442\\t0.9513067065052886\\t0.7207306816918775\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\t0.3906 1717075800\\t0.4415\\t0.7642\\t0.4279\\t0.5653\\t0.8741053652042309\\t0.7066006197198886\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1717162200\\t0.66\\t0.692\\t0.5056\\t0.5834\\t0.8159642921633847\\t0.695400563381717\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1717421400\\t0.72\\t0.7435\\t0.61\\t0.6248\\t0.7777314337307077\\t0.6889823303470154\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1717507800\\t0.591\\t0.5913\\t0.51\\t0.5571\\t0.7336051469845662\\t0.6769930275881958\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1717594200\\t0.5031\\t0.595\\t0.4544\\t0.5315\\t0.693184117587653\\t0.6637663887165416\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\t0\\tNaN\\tNaN 1717680600\\t0.5499\\t0.5775\\t0.5001\\t0.544\\t0.6633472940701224\\t0.652878535196856\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1717767000\\t0.61\\t0.7849\\t0.5856\\t0.6106\\t0.6527978352560979\\t0.6490350319971419\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\t0.7849\\tNaN 1718026200\\t0.6371\\t0.647\\t0.5256\\t0.56\\t0.6342382682048784\\t0.6409409381792198\\t0\\t1\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1718112600\\t0.546\\t0.576\\t0.5117\\t0.5595\\t0.6192906145639027\\t0.6335372165265635\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1718199000\\t0.5446\\t0.5605\\t0.519\\t0.5213\\t0.5996924916511222\\t0.6233338332059668\\t0\\t0\\tNaN\\tNaN\\t0\\t1\\tNaN\\tNaN\\tNaN\\tNaN 1718285400\\t0.5259\\t0.5278\\t0.4819\\t0.5087\\t0.5814939933208977\\t0.612912575641788\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1718371800\\t0.5008\\t0.5775\\t0.49\\t0.515\\t0.5681951946567182\\t0.6040114324016255\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\t0\\tNaN\\tNaN\\tNaN 1718631000\\t0.52\\t0.5518\\t0.505\\t0.5123\\t0.5570161557253746\\t0.5956740294560231\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1718717400\\t0.496\\t0.5105\\t0.465\\t0.4735\\t0.5403129245802997\\t0.5845672995054756\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1718890200\\t0.4687\\t0.47\\t0.4\\t0.4067\\t0.5135903396642397\\t0.5683975450049779\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1718976600\\t0.4134\\t0.4279\\t0.3814\\t0.3962\\t0.49011227173139177\\t0.5527432227317981\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1719235800\\t0.382\\t0.3833\\t0.2747\\t0.326\\t0.45728981738511343\\t0.5321302024834528\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1719322200\\t0.2187\\t0.2949\\t0.215\\t0.265\\t0.41883185390809075\\t0.5078456386213207\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\t0.215 1719408600\\t0.303\\t0.6\\t0.2953\\t0.4585\\t0.4267654831264726\\t0.5033596714739279\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1719495000\\t0.6153\\t0.69\\t0.5\\t0.6\\t0.4614123865011781\\t0.512145155885389\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\t0.69\\tNaN 1719581400\\t0.5315\\t0.5974\\t0.458\\t0.5066\\t0.4704499092009425\\t0.5116410508048991\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1719840600\\t0.491\\t0.5377\\t0.45\\t0.4599\\t0.46833992736075397\\t0.5069373189135447\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1719927000\\t0.4451\\t0.4451\\t0.3839\\t0.4342\\t0.46151194188860317\\t0.5003248353759497\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\t0\\tNaN\\tNaN 1720013400\\t0.4181\\t0.469\\t0.4146\\t0.424\\t0.4540095535108825\\t0.49338621397813615\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1720186200\\t0.4068\\t0.4183\\t0.3865\\t0.4\\t0.443207642808706\\t0.48489655816194194\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\t0\\tNaN\\tNaN\\tNaN 1720445400\\t0.3945\\t0.423\\t0.3601\\t0.415\\t0.4375661142469648\\t0.4785423256017654\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\t0.3601 1720531800\\t0.4\\t0.42\\t0.3872\\t0.39\\t0.42805289139757186\\t0.4704930232743322\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1720618200\\t0.389\\t0.439\\t0.38\\t0.3985\\t0.4221423131180575\\t0.4639482029766656\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1720704600\\t0.4\\t0.43\\t0.4\\t0.4048\\t0.418673850494446\\t0.4585710936151506\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1720791000\\t0.4029\\t0.44\\t0.4\\t0.422\\t0.4193390803955568\\t0.455246448741046\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1721050200\\t0.4131\\t0.5472\\t0.411\\t0.5152\\t0.43851126431644544\\t0.46069677158276906\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\t0\\t0.5472\\tNaN 1721136600\\t0.5185\\t0.545\\t0.4837\\t0.4894\\t0.44868901145315637\\t0.4633061559843355\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1721223000\\t0.4696\\t0.4979\\t0.4555\\t0.4612\\t0.4511912091625251\\t0.4631146872584868\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1721309400\\t0.4735\\t0.477\\t0.41\\t0.4172\\t0.4443929673300201\\t0.45894062478044256\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1721395800\\t0.421\\t0.421\\t0.39\\t0.4011\\t0.4357343738640161\\t0.4536823861640387\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1721655000\\t0.4068\\t0.4068\\t0.3623\\t0.3964\\t0.42786749909121286\\t0.44847489651276246\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\t0\\tNaN\\tNaN\\tNaN 1721741400\\t0.39\\t0.41\\t0.375\\t0.387\\t0.4196939992729703\\t0.4428862695570568\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1721827800\\t0.3787\\t0.3787\\t0.347\\t0.357\\t0.40715519941837625\\t0.4350784268700516\\t0\\t0\\tNaN\\tNaN\\t0\\t1\\tNaN\\tNaN\\tNaN\\tNaN 1721914200\\t0.3428\\t0.3724\\t0.3256\\t0.36\\t0.39772415953470097\\t0.42825311533641053\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1722000600\\t0.35\\t0.372\\t0.343\\t0.3567\\t0.38951932762776076\\t0.4217482866694641\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1722259800\\t0.3528\\t0.3567\\t0.31\\t0.3395\\t0.3795154621022086\\t0.41427116969951283\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1722346200\\t0.3221\\t0.3244\\t0.306\\t0.316\\t0.3668123696817669\\t0.4053374269995571\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN 1722432600\\t0.365\\t0.4069\\t0.33\\t0.35\\t0.36344989574541353\\t0.4003067518177792\\t0\\t0\\tNaN\\tNaN\\t0\\t0\\tNaN\\tNaN\\tNaN\\tNaN', 'Its this solid enough to start investing with 200 capital', 'What stock would be best for this aproach to use today', 'I meed entry and exit for each stock for today', 'Check the news articles to see if we have any tremendous change in any stocks for the best games for beers or bullish trying to make the most games in today', 'Are there any better stocks that I could pick that are said to have a greater gain or a gay greater loss that I can gain from the Bears trade or a bullet train? I\\x92m looking for the most profits possible that we know to be either moving how about any technology stocks, or anything that we can use that will definitely make a large dried in a day or so.', 'How to set the alert for today for ffie in tradingview', 'No how should i set it up using the data csv info to set a good alert for today', 'Analyze the data that I submitted to you and give me the price that I should be looking for and how to set it up which way for it to make the alert for the Bears or the bullish and what time should I do so', 'Igave you the data already', 'What could i potentally make with 200 dollar captial today', 'Yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 26827/35267 [20:20<05:35, 25.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 26673 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Failed ['What public companies produce top Alzheimer and prostate cancer ', 'which public company in the US produces isomex 30 mg', 'Barron is talking about Lumen ticker LUMN as a AI rocket that might explode. Elaborate on what they mean by that and analyze LUMN.', 'in the below transcript read between the lines to identify pros and cons of owning Lumens stock. Also determine the best price to acquire LUMN share and find out where it could be trading if the company acquires more contracts.', \"In the below transcript read between lines to identify pros and cons of owning lumens stock. Also determine the best price to acquire LUMN share and find out where it could be trading if the company acquires additional contracts.\\n\\nOperator\\n\\nGreetings, and welcome to Lumen Technologies' second quarter 2024 earnings call. [Operator instructions] And as a reminder, this conference is being recorded. I would now like to turn the conference over to Jim Breen, senior vice president, investor relations. Jim, please go ahead.\\n\\nJim Breen -- Senior Vice President, Investor Relations\\n\\nGood afternoon, everyone, and thank you for joining Lumen Technologies' second quarter 2024 earnings call. On the call today are: Kate Johnson, president and chief executive officer; and Chris Stansbury, executive vice president and chief financial officer. Before we begin, I need to call your attention to our safe harbor statement on Slide 1 of our second quarter 2024 presentation, which notes that this conference call may include forward-looking statements subject to certain risks and uncertainties. All forward-looking statements should be considered in conjunction with the cautionary statements and the risk factors in our SEC filings.\\n\\nWe will be referring to certain non-GAAP financial measures reconciled to the most comparable GAAP measures, which can be found in our earnings press release. In addition, certain metrics discussed today exclude costs for special items as detailed in our earnings materials, which can be found on our investor relations section of our Lumen website. With that, I'll turn the call over to Kate.\\n\\nKate Johnson -- President and Chief Executive Officer\\n\\nThanks, Jim. Good afternoon, everyone. Thanks for joining. I'm cognizant of the timing of this call because over the past two days, the markets have been a bit noisy with lots of uncertainty about the health of the economy in the next six to 12 to 18 months.\\n\\nIn contrast, the announcement we made last night about Lumen's pivot to growth is all about building critical infrastructure to support the AI economy for the next several decades. And to net it out, there are three key takeaways from our call today. First, Lumen's enterprise operational turnaround is progressing well. With continued sales momentum across our growth portfolio and further improvement in customer satisfaction.\\n\\nWe are also executing extremely well in our quantum fiber business. Second, Lumen has been anointed as the trusted network for AI by some of the most important technology companies on earth with over $5 billion in major partnerships in to date and visibility to nearly $7 billion more in opportunities, we see the market for Lumen's private connectivity fabric as providing a major positive momentum shift for this company. Third, given our success in forging these partnerships, we're seeing a significant improvement in our overall liquidity profile, further securing our ability to transform the company and pivot to growth. Let me give some detail on the operational turnaround part first.\\n\\nAs I've described on prior calls, we're focusing on delivering dramatically improved customer experiences from quote to cash, giving customers a reason to choose Lumen for core network services. The best way to measure that progress is to look at three areas: sales, customer SAP and securing the base. I'm delighted to share our progress across the fundamentals. After a blockbuster Q1, we continue to see strong sales performance in the second quarter, with North American large enterprise and mid-market sales up nearly 26% year over year.\\n\\nAdditionally, large and mid-market new logo sales increased 10% and net total contract value for all channels was up nearly 40% year over year. Two notable wins are Uber, who is leveraging custom fiber waves from Lumen to ensure unparalleled activity between their data centers and the State of New Mexico who is using Lumen to build its first statewide education network. To complement these sales results, we saw another step function improvement in customer SAP in our service delivery process with year-over-year transaction Net Promoter Scores rising 10 points for large enterprise, 35 points for wholesale, 37 points for mid-market and a whopping 42 points for public sector. Once again, every one of our enterprise customer channels showed significant year-over-year improvement, which should manifest in lower churn, higher growth sales and improved overall revenue growth over time.\\n\\nFinally, we're making progress securing the base with our relentless focus on five key levers: installs, renewals, migration, usage and disconnect. We think the best way to measure our progress here is to compare ourselves to market trends. And once again, we saw less revenue declines this quarter than our industry peers. We continue to fine tune in our motions, developing and launching new product bundles and educating our customers on the best migration path from legacy to modern technologies.\\n\\nAnd while we're excited by the progress of our operational turnaround in legacy core network services, the real breakthrough to share with you is how we're repositioning the company for the future in the growing market of AI. Two ways that we're repositioning Lumen. First, we're cloudifying telecom by delivering a digital platform to enable enterprise customers to digitally design, price, order and consume secure network services. quickly, securely and effortlessly.\\n\\nWe're thrilled with our progress driving adoption of our Lumen digital flagship network as a service offering with companies like Versa, T Marzetti and DXC Technologies, as well as many other companies across the industry. OK. The second way we're repositioning the company for growth is with Lumen's private connectivity fabric. To summarize what's happening.\\n\\nThe dramatic rise in AI innovation spring explosive growth in data center build-outs, and data centers simply have to be connected. We're honored that technology powerhouses like Microsoft and several other big technology firms are choosing Lumen to build their AI backbone, and they're choosing us for two reasons. First, our world-class fiber network with its unique route vast coverage and state-of-the-art fiber solutions from our strategic partnership with Corning. And second, the digital platform we're building that makes consumption quick, secure and effortless.\\n\\nWith $5 billion in closed deals so far and the active discussions we're having with a long list of additional customers, we believe Lumen is becoming known as the trusted network for AI. The growth in this type of sale will be meaningful and accretive to our cash position in the short term and positions us for long-term predictable revenue growth in the future. Looking ahead, I'm sure you, like everybody else on Planet Earth, is wondering how big are these networking deals gonna be for AI and what's the market look like, so I'm going to share our early hypothesis with you. We think there are likely to be three distinct phases.\\n\\nThe first phase as evidenced by our closed deals is with huge technology companies, cloud providers, social platforms, etc., who are AI thought leaders and are building and training AI models responsible for the explosive growth in data center build-outs. They were the first to recognize that today's Internet simply won't serve tomorrow's AI economy, and they're partnering with Lumen to massively expand their connectivity infrastructure. We think the next tranche of demand is likely to come from the AI model inference phase, probably with forward-thinking enterprises who see AI as a way to transform their businesses, think financial services, healthcare and retailers to start. And finally, in the third phase, we suspect breakout growth in demand for connectivity and digital on-demand network services will come when AI starts talking to AI in rings and exchanges.\\n\\nWe're in very early discussions with strategic partners who are helping shape our view in this space. Please note that these recent announcements, which were not included in our 2024 guidance, fund the necessary upfront opex and capex to ramp and scale these new AI workloads. Additionally, these deals provide funding for continued innovation and strategic cost takeouts. And that leads me to my next important piece of news.\\n\\nToday, we're announcing that we see a path to creating $1 billion in cost takeout by the end of 2027. This next cost wave of efficiency will come from deeply strategic infrastructure simplification in three major areas: network, product portfolio and IT. These infrastructure projects are rooted in network standardization. We're now integrating the network from four different architectures, engineering them into one simplified standardized and unified network fabric.\\n\\nThis move provides a step function change in the level of simplification that we can drive inside the company, providing breakthrough improvements in our customer and employee experiences. Let me provide just a little bit more color on the impact of the plan. Our target is to ensure that the majority of our net new services are on this unified network fabric by the end of 2025. This will enable massive simplification in our product portfolio, enabling us to significantly reduce our product count from thousands of product codes to a target of around 300, of massive simplification enabler across Lumen and our ecosystem.\\n\\nOnce we unify the network and simplify the product portfolio in our enterprise business, we'll go after technical cost savings in IT. For example, we'd like to compress our 24 order management systems to a target number of one and reduce our 17 billing systems to, well, you guessed it a target of one, this work is going to take a few years to complete, but it will yield material and enduring bottom-line benefits. To reemphasize, the work wouldn't be possible without the additional liquidity gained from our private connectivity fabric sales, which also allows us to self-fund a spending increase in key areas to drive out these costs for the long term. To summarize our enterprise business transformation efforts, we've got the cash, we've got the assets.\\n\\nAnd we've got a world-class leadership team needed to execute on the next phase of our transformation, unlocking breakthrough growth opportunities and strategic cost savings moving forward. And finally, I'm really delighted to share that our mass market segment is showing steady results improvement. We continue to opportunistically deploy capital, enabling 136,000 locations in Q2, on track to deliver 500,000 new fiber-enabled locations this year. We also continued our strong fiber sales momentum from first quarter '24 as indicated by our record level of 2Q fiber net adds of 40,000.\\n\\nAnd we're happy to announce we've reached over 1 million fiber subscribers in July. This is a significant milestone and reinforces the value of the product we're delivering to the consumer. And it also shows our mass markets team who really knows how to execute well. With that, I'll turn the call over to Chris.\\n\\nChristopher David Stansbury -- Executive Vice President and Chief Financial Officer\\n\\nThanks, Kate. Before I discuss the quarter, I want to take a moment to reflect back to Q2 earnings last year. Since that time, we successfully completed a refinancing that addressed over $15 billion of our debt and extended over $10 billion of our maturities, and we secured access to over $2.3 billion in new liquidity. And we launched our PCS solutions, as well as our suite of new digital offerings.\\n\\nAnd we've generated early growth in our public sector in the growth segment of our large enterprise business. And as of yesterday, we announced the largest sales in the company's history, totaling nearly $5 billion fueled by our AI hyperscaler customers. This is all as we drive in network unification from four discrete enterprise networks to one, resulting in over $1 billion in cost efficiencies. None of this would be possible without a world-class management team who's executing on our vision.\\n\\nWe're moving with pace and we're not done. The recent developments in our business reflect major proof points in terms of early and material execution on women's transformation path forward, and we are pleased the market is starting to value this opportunity. We believe we're in the first inning of the AI growth opportunity for our fiber infrastructure and Lumen digital services. Accordingly, the positive impact these private connectivity fabric sales will have on our financials are powerful and clear.\\n\\nFirst, we believe the progress we've made on driving PCF sales these past few months is just the beginning of a vast new TAM, which brings long-term sticky revenue offsetting higher churn legacy declines. Second, we estimate that the cash received from PCF sales will close any free cash flow deficit between now and when we reach sustainable positive free cash flow growth. Third, we will have ample free cash flow to invest in our transformation and reduce debt. And finally, in our view, PCF sales are significant and incremental to the overall value of Lumen's business.\\n\\nThe building blocks of our value creation are clear, starting with our nationwide fiber network. We believe Lumen is one of the few companies with the resources and scale to provide the critical infrastructure for AI. And the partnerships we've announced represent a large and growing opportunity to provide private connectivity fabric solutions. We see a runway to growth as we transform telecom, and we believe this sets up a value-creation path for Lumen all as we continue to execute on our core strategic goals of commercial excellence, securing the base and innovating for growth.\\n\\nAs Kate mentioned, our sales growth engines within our large and mid-market enterprise channels in our business segment, along with our mass market segment showed solid performance this quarter with large enterprise and mid-market sales both up over 26% year over year. Additionally, Quantum Fiber broadband net additions of 40,000 again, sets an all-time record, and we passed the 1 million total fiber subscriber mark in July. Outstanding work by the team. While consolidated revenue and adjusted EBITDA still feels the impact of legacy declines, we are encouraged by improvements we're making in the business.\\n\\nNow let's move to the discussion of financial results for the second quarter. On a year-over-year basis, total reported revenue declined 10.7% to $3.268 billion. 36% of the decline was due to the impact of divestitures, commercial agreements and the sale of the CDN business. Business segment revenue declined 11.4% to $2.577 billion, and approximately 42% of that decline was due to the impact of divestitures and commercial agreements.\\n\\nMass markets segment revenue declined 8.2% to $691 million. Adjusted EBITDA was $1.011 billion, with a 30.9% margin and free cash flow was negative $156 million. Next, I'll review our detailed revenue results for the quarter on a year-over-year basis. Within our North America enterprise channels, which is our business segment, excluding wholesale, international and other, revenue declined 3.6%.\\n\\nWe continue to expect public sector to be the first channel to pivot to sustainable growth later this year, followed by mid-market and then large enterprise. Overall, North American business declined 5.5%. Large enterprise revenue declined 6.9% in the second quarter. Our grow revenue was approximately flat year over year with continued pressure in nurture and harvest product revenue.\\n\\nWe expect continued variability in trends as we drive toward overall stabilization. Mid-market revenue declined approximately 7% year over year, with improvement in growth offset by near-term harvest. Public sector revenue increased 8% year over year, driven by strength in our grow and other product revenue and partially offset by declines in return harvest. We continue to see traction with large bookings in this space.\\n\\nwhich take time to ramp to revenue, and these wins give us continued confidence that public sector will be the first sales channel to return to sustainable growth this year. Wholesale revenue declined approximately 10% year over year. The harvest portion of the wholesale portfolio, which is comprised of products like TDM voice and private line, saw revenue contract by 17.9% year over year in the second quarter. This is primarily driven by telco partners that are selling legacy services.\\n\\nOur harvest product revenue will likely continue to decline over time and is an area that we will manage for cash. International and other revenue declined 67.1%, driven primarily by the divestiture of our EMEA business and the sale of select CDN contracts in the fourth quarter of last year. Moving to our business product life cycle reporting our reference results based on our North America enterprise channel. The 3.6% year-over-year decrease was due to declines in our nurture and harvest segments partially offset by growth, particularly enterprise broadband, dark fiber and IT.\\n\\nWhile results can vary in any quarter, we expect sustained strength in the grow product revenue as we execute on our core turnarounds. Within Nurture and Harvest, we continue to expect headwinds in these markets, declining categories. However, we continue to take proactive steps to migrate customers to newer technologies and these actions improve our customers' experience and will provide an uplift in customer lifetime value for Lumen. Additionally, we will continue to pursue opportunities for cost optimization when we help customers migrate from off-net legacy and TDM-based services onto Lumen's network.\\n\\nWithin North American enterprise channels, grow Products revenue increased 1.5% year over year. Grow now represents approximately 43% of our North America enterprise revenue. And for our total business segment carried an approximate 80% direct margin this quarter. Nurture Products revenue decreased 12.1% year over year.\\n\\nNurture represents 30% of our North American enterprise revenue and for our total business segment carried an approximate 66% direct margin this quarter. Harvest products revenue decreased 10.6% year over year and continues to be negatively impacted by declines in TDM-based voice and private line. Harvest represented approximately 16% of our North America enterprise revenue in the second quarter. For our total business segment, it carried an approximate 77% direct margin this quarter.\\n\\nOther product revenue improved 18.5% year over year. As a reminder, other product revenue tends to experience fluctuations due to the variable nature of these products. Now moving on to mass markets. Our fiber broadband revenue grew 14.6% year over year and represents approximately 38% of mass markets broadband revenue.\\n\\nDuring the quarter, Fiber broadband-enabled location adds were 136,000, bringing our total to over 3.9 million as of June 30th and pacing toward our targeted annual 500,000 build target this year. We also added 40,000 Quantum Fiber customers, which is our best fiber net add quarter reported to date, and this brings our total to 992,000. Fiber ARPU was $62, up slightly, both sequentially and year over year. Importantly, we reached a significant milestone of 1 million fiber broadband subscribers in July.\\n\\nAt the end of the second quarter, our penetration of legacy copper broadband was approximately 9% and our quantum fiber penetration stood at approximately 25%. As we look ahead, we will continue our market-by-market assessment of the mass markets business as we explore a range of strategic options to maximize its value. Those options include potential joint ventures, wholesaling arrangements or future divestitures to generate incremental cash. Now turning to adjusted EBITDA.\\n\\nFor the second quarter of 2024, adjusted EBITDA was $1.011 billion, compared to $1.229 billion in the year-ago quarter. Second quarter EBITDA was positively impacted our strong first quarter sales bookings, as well as efficiency improvements from our second quarter cost actions and overall margin management. Special items impacting adjusted EBITDA totaled $136 million. The majority special items in the quarter were related to severance.\\n\\nFor the second quarter of 2024, our adjusted EBITDA margin was 30.9%, Capital expenditures were $753 million, and free cash flow, excluding special items, was negative $156 million. As we've previously stated, we're leaning into our network investments to support the rapid growth in demand our customers are facing. Now before I provide an update on our 2024 financial outlook, I'd like to provide some color around the near-term impact of our PCF sales and the additional liquidity and flexibility we have. As Kate mentioned, we're moving full speed ahead in investing in our transformation, which includes additional spending on network and systems unification that will ultimately lead to more efficient operations and a better customer experience.\\n\\nGiven our improving liquidity profile, we intend to pull forward some expenses from '26 and '27 into '25, accelerating the time line of our cost takeout goals. With the investments in transformation and costs associated with recent PCF sales and in conjunction with continued legacy revenue declines, directionally, we see 2025 EBITDA below 2024 levels with a significant rebound in 2026 and growing thereafter. We will provide more detailed 2025 guidance on our fourth quarter 2024 call in February. Now moving on to our financial outlook.\\n\\nWe now estimate fiscal year '24 EBITDA to be in the range of $3.9 billion to $4 billion. Capex in the range of $3.1 billion to $3.3 billion. Cash interest in the range of $1.15 billion to $1.25 billion and free cash flow in the range of $1 billion to $1.2 billion. This guidance includes some incremental opex, capex, and cash flows associated with our PCF sales growth, the gain on a sale of an investment, as well as incremental spending to ultimately improve our cost structure and margins.\\n\\nThis additional opex and capex will be fully funded upfront by the incremental PCF cash flow. And with that, I'll turn it back to Kate for closing remarks.\\n\\nKate Johnson -- President and Chief Executive Officer\\n\\nThanks, Chris. Before we open up the call for questions, I wanted to pause to acknowledge where we are. AI represents one of the most significant technology shifts in history. Every person and every organization on earth will be impacted.\\n\\nAI needs data, data needs data centers and data centers need to be connected. What was once an overbuilt fiber network is shifting from commodity to something much more valuable. At Lumen, we aren't streamlining and digitizing our operations to try to find growth in legacy telco markets. Instead, we're building a digital platform to help us become the trusted network for AI so we can capitalize on the markets that will likely see explosive growth for decades.\\n\\nThis is Lumen's moment. We are playing to win. This is the business that we are in. Operator, we're ready for questions.\\n\\nQuestions & Answers:\\n\\nOperator\\n\\nThank you. [Operator instructions] Your first question comes from the line of Michael Rollins with Citi. Please go ahead.\\n\\nMichael Rollins -- Analyst\\n\\nThanks, and good afternoon. First, with respect to the $5 billion of sales, Curious if you could give some additional color on the competitiveness of that process? Are these customers using single vendors for the solution or multiple vendors. So this is something that's not just helping Lumen but maybe the ecosystem. And then, for women specifically, can you share the mix of assets that are existing fiber, existing conduit, leveraging assets that are already out there from you versus what you're building as new infrastructure.\\n\\nAnd as you consume some of those fiber inventories such that investment mix or margin mix might look differently over time as you continue to sell within this new PFC segment? Thanks.\\n\\nKate Johnson -- President and Chief Executive Officer\\n\\nMike, so I'll take the first the first part, let Chris handle the second part. So first part, what does the competitive landscape look like? Look, obviously, I'm a little bit biased, but here's my observation. Our network is the crown jewel that we always thought it was. It's got great coverage, unique routes, it's diverse and it's got state-of-the-art fiber because we've been taking care of it for a long time.\\n\\nAnd that's giving us great positioning with our customers. They're looking at -- sometimes building some routes by themselves. Most of the time understanding that we can get them there faster with higher quality and better service and that's the observation across the deals we've won so far.\\n\\nChristopher David Stansbury -- Executive Vice President and Chief Financial Officer\\n\\nYeah. And just on the economics, it's a really good question, and then, I'm not gonna be evasive with you. But the reality is it's really complicated. So it's a deal to deal, every deal is different in terms of where they want to go from and to, how much capacity they need.\\n\\nAnd inevitably, you will end up with a combination of existing fiber, new fiber, existing conduit, new conduit. It really does vary deal to deal. Now on that, we'll never disclose it because -- these are called private connectivity fabric for a reason. And our customers want to keep it private because it's a competitive -- a secret that they have as is it a competitive secret for us.\\n\\nSo it will vary deal to deal, but the video that we released, I think, gives a good flavor on average.\\n\\nOperator\\n\\nYour next question comes from the line of Sebastiano Petti. Please go ahead.\\n\\nSebastiano Petti -- Analyst\\n\\nHi. Thanks for taking the question. Just had a quick question on the free cash flow guidance. Can you help us think about, is that fully just driven by the customer deposits from the just private custom fiber -- fabric AI? Or is that also reflective of the -- I think, Chris, you said gain on the sale? And in addition to that, can you help us maybe think -- does the free cash flow uplift that you're seeing here? Is that something that we should expect to stay on the balance sheet in 2024 or is this something that will probably get spent as you probably get to fund the increase in capex that you've guided to today? Just trying to help think about the commencement of the build-out in pacing?\\n\\nChristopher David Stansbury -- Executive Vice President and Chief Financial Officer\\n\\nI'll give you credit because you asked one of the great questions on the call early. So the cash flow guidance for this year is driven by both some of the upfront cash received. We haven't received all of it, obviously, for the PCF deals, and it is also related to the asset sale that we did, so both of those things contributed to the free cash flow. As it relates to where we go from here, and again, I want to really be really careful because we are not giving 2025 guidance yet.\\n\\nBut we haven't received all the cash yet. That will be received some this year, some next year, some the following year because again, these are massive construction projects. They take time. And we will start to spend the capex as evidenced by our guidance.\\n\\nThis year and have more next year. But the point is on these deals, we're not financing the build, so we get paid in advance of the construction. The only thing that is kind of hanging as you go out 12 months is we pay tax on the cash received. So even though the revenue is amortized, the IRS likes to get paid on a cash basis for these deals.\\n\\nSo that will be something that we deal with, and we'll get more color on that as we move through. But high level, I would say that next year, free cash flow looks good.\\n\\nOperator\\n\\nYour next question comes from the line of Batya Levi from UBS. Please go ahead.\\n\\nBatya Levi -- Analyst\\n\\nGreat. Thank you. Looking at the EBITDA guidance change for the year, is that purely related to the incremental opex for getting ready for these networks? Or -- is there any change in terms of the underlying trend? And can you just go over the $1 billion cost savings you expect over the next three years the pacing of that? I think you mentioned some of the expenses will be pulled forward. And then, is there any incremental cost to achieve that savings through the next three years?\\n\\nChristopher David Stansbury -- Executive Vice President and Chief Financial Officer\\n\\nYes. So as it relates to this year, the vast majority of the -- there's, obviously, a lot of things that go on inside of EBITDA. But the main driver here are the TCF deals. And the opex investments we need to make to get that construction factory up and running in a more scaled way.\\n\\nIt's a group that exists. It's one of Lumen's core competencies, but the size of that group needs to get substantially larger to support just the quantum of these deals. And so, that's the key driver. As it relates to the $1 billion cost takeout, we haven't -- again, I want to stay away from 2025 guidance as much as I can.\\n\\nWe're not expecting those savings to start until next year. There will be some investment next year, and we'll disclose that when we give guidance for next year. But my comments on just trying to dimensionalize where we go from here, are really around the fact that we're taking the opportunity near term balance of this year and '25 to really pull forward investments we were going to have to make in '26 and '27 to get to a place where our IT systems, as Kate mentioned, are more consolidated, simplified to drive a customer experience. And I would say, if there's one key driver in that, it's going from what our four enterprise networks today to one.\\n\\nAnd that is a legacy that exist today that needs to be cleaned up because it just drives a much more seamless customer experience as we go forward.\\n\\nOperator\\n\\nYour next question comes from the line of David Barden from Bank of America. Please go ahead.\\n\\nDavid Barden -- Analyst\\n\\nHey, guys, thanks so much for taking the question. Chris, I guess, it's not so much a question is I want to put forward a hypothesis and I want you to tell -- it would be, I think, super helpful for people to share what you think is right or wrong about it. So we've got this $5 billion deal, but the majority of the cash is coming in, in the next three to four years, and the majority of the cash -- of that cash is also going out the door in the next three to four years. So any kind of cash inflow we're getting is kind of a timing benefit relative to the capex that's required under the contract.\\n\\nAnd if it's a $5 billion contract and the majority of it is related to the construction piece, let's just call it $3 billion round numbers. That means that the actual IRU sale piece is about $2 billion. And as you shared in your video, that IRU revenue doesn't start until after the build is done, which would be probably Year 4 or 5, over a 20-year period, $2 billion dollar, a $100 million in revenue a year, very high margin, maybe $85 million in EBITDA, tax affected, as you've mentioned in your video, maybe, again, the taxes will be timing related, but let's just call it $65 million of tax-affected cash flow over a 20-year period. So a $5 billion deal announcement turns into $65 million of cash flow five years from now, what's right and what's wrong about that assessment?\\n\\nChristopher David Stansbury -- Executive Vice President and Chief Financial Officer\\n\\nI'd say most of it is wrong. The -- Yes, I think, David, here's where we go, so again, it's multiple deals that added up to the $5 billion. Not just one. And in the video, we talked about a cash contribution margin, which is effectively the EBITDA less the capex, pre-tax that's roughly in the ballpark of our existing EBITDA margin.\\n\\nSo you do the math on that that will give you the pre-tax free cash flow associated with these deals. And that cash flow, to your point, does come largely at the front end. Now there's ongoing payments for space and power, for operating and maintenance if they want us to run the networks for them that gives us nice cash flow over the years, but the tax would also be front-end loaded. So the key thing here is that in one set of deals in those $5 billion deals that the net after-tax cash generated from that fully fund the liquidity gap that we've talked about for so long on these calls.\\n\\nIt's older, it's behind us. And we're not done. So as we said, there's another $7 billion of discussions underway right now. And this trend will continue.\\n\\nThe demand isn't one and done. So that's the key difference, so there's more cash in the deal than you've laid out and there's more to come.\\n\\nKate Johnson -- Presi\", \"Read the article below and provide summary and insight\\nOperator\\n\\nGreetings, and welcome to Lumen Technologies' second quarter 2024 earnings call. [Operator instructions] And as a reminder, this conference is being recorded. I would now like to turn the conference over to Jim Breen, senior vice president, investor relations. Jim, please go ahead.\\n\\nJim Breen -- Senior Vice President, Investor Relations\\n\\nGood afternoon, everyone, and thank you for joining Lumen Technologies' second quarter 2024 earnings call. On the call today are: Kate Johnson, president and chief executive officer; and Chris Stansbury, executive vice president and chief financial officer. Before we begin, I need to call your attention to our safe harbor statement on Slide 1 of our second quarter 2024 presentation, which notes that this conference call may include forward-looking statements subject to certain risks and uncertainties. All forward-looking statements should be considered in conjunction with the cautionary statements and the risk factors in our SEC filings.\\n\\nWe will be referring to certain non-GAAP financial measures reconciled to the most comparable GAAP measures, which can be found in our earnings press release. In addition, certain metrics discussed today exclude costs for special items as detailed in our earnings materials, which can be found on our investor relations section of our Lumen website. With that, I'll turn the call over to Kate.\\n\\nKate Johnson -- President and Chief Executive Officer\\n\\nThanks, Jim. Good afternoon, everyone. Thanks for joining. I'm cognizant of the timing of this call because over the past two days, the markets have been a bit noisy with lots of uncertainty about the health of the economy in the next six to 12 to 18 months.\\n\\nIn contrast, the announcement we made last night about Lumen's pivot to growth is all about building critical infrastructure to support the AI economy for the next several decades. And to net it out, there are three key takeaways from our call today. First, Lumen's enterprise operational turnaround is progressing well. With continued sales momentum across our growth portfolio and further improvement in customer satisfaction.\\n\\nWe are also executing extremely well in our quantum fiber business. Second, Lumen has been anointed as the trusted network for AI by some of the most important technology companies on earth with over $5 billion in major partnerships in to date and visibility to nearly $7 billion more in opportunities, we see the market for Lumen's private connectivity fabric as providing a major positive momentum shift for this company. Third, given our success in forging these partnerships, we're seeing a significant improvement in our overall liquidity profile, further securing our ability to transform the company and pivot to growth. Let me give some detail on the operational turnaround part first.\\n\\nAs I've described on prior calls, we're focusing on delivering dramatically improved customer experiences from quote to cash, giving customers a reason to choose Lumen for core network services. The best way to measure that progress is to look at three areas: sales, customer SAP and securing the base. I'm delighted to share our progress across the fundamentals. After a blockbuster Q1, we continue to see strong sales performance in the second quarter, with North American large enterprise and mid-market sales up nearly 26% year over year.\\n\\nAdditionally, large and mid-market new logo sales increased 10% and net total contract value for all channels was up nearly 40% year over year. Two notable wins are Uber, who is leveraging custom fiber waves from Lumen to ensure unparalleled activity between their data centers and the State of New Mexico who is using Lumen to build its first statewide education network. To complement these sales results, we saw another step function improvement in customer SAP in our service delivery process with year-over-year transaction Net Promoter Scores rising 10 points for large enterprise, 35 points for wholesale, 37 points for mid-market and a whopping 42 points for public sector. Once again, every one of our enterprise customer channels showed significant year-over-year improvement, which should manifest in lower churn, higher growth sales and improved overall revenue growth over time.\\n\\nFinally, we're making progress securing the base with our relentless focus on five key levers: installs, renewals, migration, usage and disconnect. We think the best way to measure our progress here is to compare ourselves to market trends. And once again, we saw less revenue declines this quarter than our industry peers. We continue to fine tune in our motions, developing and launching new product bundles and educating our customers on the best migration path from legacy to modern technologies.\\n\\nAnd while we're excited by the progress of our operational turnaround in legacy core network services, the real breakthrough to share with you is how we're repositioning the company for the future in the growing market of AI. Two ways that we're repositioning Lumen. First, we're cloudifying telecom by delivering a digital platform to enable enterprise customers to digitally design, price, order and consume secure network services. quickly, securely and effortlessly.\\n\\nWe're thrilled with our progress driving adoption of our Lumen digital flagship network as a service offering with companies like Versa, T Marzetti and DXC Technologies, as well as many other companies across the industry. OK. The second way we're repositioning the company for growth is with Lumen's private connectivity fabric. To summarize what's happening.\\n\\nThe dramatic rise in AI innovation spring explosive growth in data center build-outs, and data centers simply have to be connected. We're honored that technology powerhouses like Microsoft and several other big technology firms are choosing Lumen to build their AI backbone, and they're choosing us for two reasons. First, our world-class fiber network with its unique route vast coverage and state-of-the-art fiber solutions from our strategic partnership with Corning. And second, the digital platform we're building that makes consumption quick, secure and effortless.\\n\\nWith $5 billion in closed deals so far and the active discussions we're having with a long list of additional customers, we believe Lumen is becoming known as the trusted network for AI. The growth in this type of sale will be meaningful and accretive to our cash position in the short term and positions us for long-term predictable revenue growth in the future. Looking ahead, I'm sure you, like everybody else on Planet Earth, is wondering how big are these networking deals gonna be for AI and what's the market look like, so I'm going to share our early hypothesis with you. We think there are likely to be three distinct phases.\\n\\nThe first phase as evidenced by our closed deals is with huge technology companies, cloud providers, social platforms, etc., who are AI thought leaders and are building and training AI models responsible for the explosive growth in data center build-outs. They were the first to recognize that today's Internet simply won't serve tomorrow's AI economy, and they're partnering with Lumen to massively expand their connectivity infrastructure. We think the next tranche of demand is likely to come from the AI model inference phase, probably with forward-thinking enterprises who see AI as a way to transform their businesses, think financial services, healthcare and retailers to start. And finally, in the third phase, we suspect breakout growth in demand for connectivity and digital on-demand network services will come when AI starts talking to AI in rings and exchanges.\\n\\nWe're in very early discussions with strategic partners who are helping shape our view in this space. Please note that these recent announcements, which were not included in our 2024 guidance, fund the necessary upfront opex and capex to ramp and scale these new AI workloads. Additionally, these deals provide funding for continued innovation and strategic cost takeouts. And that leads me to my next important piece of news.\\n\\nToday, we're announcing that we see a path to creating $1 billion in cost takeout by the end of 2027. This next cost wave of efficiency will come from deeply strategic infrastructure simplification in three major areas: network, product portfolio and IT. These infrastructure projects are rooted in network standardization. We're now integrating the network from four different architectures, engineering them into one simplified standardized and unified network fabric.\\n\\nThis move provides a step function change in the level of simplification that we can drive inside the company, providing breakthrough improvements in our customer and employee experiences. Let me provide just a little bit more color on the impact of the plan. Our target is to ensure that the majority of our net new services are on this unified network fabric by the end of 2025. This will enable massive simplification in our product portfolio, enabling us to significantly reduce our product count from thousands of product codes to a target of around 300, of massive simplification enabler across Lumen and our ecosystem.\\n\\nOnce we unify the network and simplify the product portfolio in our enterprise business, we'll go after technical cost savings in IT. For example, we'd like to compress our 24 order management systems to a target number of one and reduce our 17 billing systems to, well, you guessed it a target of one, this work is going to take a few years to complete, but it will yield material and enduring bottom-line benefits. To reemphasize, the work wouldn't be possible without the additional liquidity gained from our private connectivity fabric sales, which also allows us to self-fund a spending increase in key areas to drive out these costs for the long term. To summarize our enterprise business transformation efforts, we've got the cash, we've got the assets.\\n\\nAnd we've got a world-class leadership team needed to execute on the next phase of our transformation, unlocking breakthrough growth opportunities and strategic cost savings moving forward. And finally, I'm really delighted to share that our mass market segment is showing steady results improvement. We continue to opportunistically deploy capital, enabling 136,000 locations in Q2, on track to deliver 500,000 new fiber-enabled locations this year. We also continued our strong fiber sales momentum from first quarter '24 as indicated by our record level of 2Q fiber net adds of 40,000.\\n\\nAnd we're happy to announce we've reached over 1 million fiber subscribers in July. This is a significant milestone and reinforces the value of the product we're delivering to the consumer. And it also shows our mass markets team who really knows how to execute well. With that, I'll turn the call over to Chris.\\n\\nChristopher David Stansbury -- Executive Vice President and Chief Financial Officer\\n\\nThanks, Kate. Before I discuss the quarter, I want to take a moment to reflect back to Q2 earnings last year. Since that time, we successfully completed a refinancing that addressed over $15 billion of our debt and extended over $10 billion of our maturities, and we secured access to over $2.3 billion in new liquidity. And we launched our PCS solutions, as well as our suite of new digital offerings.\\n\\nAnd we've generated early growth in our public sector in the growth segment of our large enterprise business. And as of yesterday, we announced the largest sales in the company's history, totaling nearly $5 billion fueled by our AI hyperscaler customers. This is all as we drive in network unification from four discrete enterprise networks to one, resulting in over $1 billion in cost efficiencies. None of this would be possible without a world-class management team who's executing on our vision.\\n\\nWe're moving with pace and we're not done. The recent developments in our business reflect major proof points in terms of early and material execution on women's transformation path forward, and we are pleased the market is starting to value this opportunity. We believe we're in the first inning of the AI growth opportunity for our fiber infrastructure and Lumen digital services. Accordingly, the positive impact these private connectivity fabric sales will have on our financials are powerful and clear.\\n\\nFirst, we believe the progress we've made on driving PCF sales these past few months is just the beginning of a vast new TAM, which brings long-term sticky revenue offsetting higher churn legacy declines. Second, we estimate that the cash received from PCF sales will close any free cash flow deficit between now and when we reach sustainable positive free cash flow growth. Third, we will have ample free cash flow to invest in our transformation and reduce debt. And finally, in our view, PCF sales are significant and incremental to the overall value of Lumen's business.\\n\\nThe building blocks of our value creation are clear, starting with our nationwide fiber network. We believe Lumen is one of the few companies with the resources and scale to provide the critical infrastructure for AI. And the partnerships we've announced represent a large and growing opportunity to provide private connectivity fabric solutions. We see a runway to growth as we transform telecom, and we believe this sets up a value-creation path for Lumen all as we continue to execute on our core strategic goals of commercial excellence, securing the base and innovating for growth.\\n\\nAs Kate mentioned, our sales growth engines within our large and mid-market enterprise channels in our business segment, along with our mass market segment showed solid performance this quarter with large enterprise and mid-market sales both up over 26% year over year. Additionally, Quantum Fiber broadband net additions of 40,000 again, sets an all-time record, and we passed the 1 million total fiber subscriber mark in July. Outstanding work by the team. While consolidated revenue and adjusted EBITDA still feels the impact of legacy declines, we are encouraged by improvements we're making in the business.\\n\\nNow let's move to the discussion of financial results for the second quarter. On a year-over-year basis, total reported revenue declined 10.7% to $3.268 billion. 36% of the decline was due to the impact of divestitures, commercial agreements and the sale of the CDN business. Business segment revenue declined 11.4% to $2.577 billion, and approximately 42% of that decline was due to the impact of divestitures and commercial agreements.\\n\\nMass markets segment revenue declined 8.2% to $691 million. Adjusted EBITDA was $1.011 billion, with a 30.9% margin and free cash flow was negative $156 million. Next, I'll review our detailed revenue results for the quarter on a year-over-year basis. Within our North America enterprise channels, which is our business segment, excluding wholesale, international and other, revenue declined 3.6%.\\n\\nWe continue to expect public sector to be the first channel to pivot to sustainable growth later this year, followed by mid-market and then large enterprise. Overall, North American business declined 5.5%. Large enterprise revenue declined 6.9% in the second quarter. Our grow revenue was approximately flat year over year with continued pressure in nurture and harvest product revenue.\\n\\nWe expect continued variability in trends as we drive toward overall stabilization. Mid-market revenue declined approximately 7% year over year, with improvement in growth offset by near-term harvest. Public sector revenue increased 8% year over year, driven by strength in our grow and other product revenue and partially offset by declines in return harvest. We continue to see traction with large bookings in this space.\\n\\nwhich take time to ramp to revenue, and these wins give us continued confidence that public sector will be the first sales channel to return to sustainable growth this year. Wholesale revenue declined approximately 10% year over year. The harvest portion of the wholesale portfolio, which is comprised of products like TDM voice and private line, saw revenue contract by 17.9% year over year in the second quarter. This is primarily driven by telco partners that are selling legacy services.\\n\\nOur harvest product revenue will likely continue to decline over time and is an area that we will manage for cash. International and other revenue declined 67.1%, driven primarily by the divestiture of our EMEA business and the sale of select CDN contracts in the fourth quarter of last year. Moving to our business product life cycle reporting our reference results based on our North America enterprise channel. The 3.6% year-over-year decrease was due to declines in our nurture and harvest segments partially offset by growth, particularly enterprise broadband, dark fiber and IT.\\n\\nWhile results can vary in any quarter, we expect sustained strength in the grow product revenue as we execute on our core turnarounds. Within Nurture and Harvest, we continue to expect headwinds in these markets, declining categories. However, we continue to take proactive steps to migrate customers to newer technologies and these actions improve our customers' experience and will provide an uplift in customer lifetime value for Lumen. Additionally, we will continue to pursue opportunities for cost optimization when we help customers migrate from off-net legacy and TDM-based services onto Lumen's network.\\n\\nWithin North American enterprise channels, grow Products revenue increased 1.5% year over year. Grow now represents approximately 43% of our North America enterprise revenue. And for our total business segment carried an approximate 80% direct margin this quarter. Nurture Products revenue decreased 12.1% year over year.\\n\\nNurture represents 30% of our North American enterprise revenue and for our total business segment carried an approximate 66% direct margin this quarter. Harvest products revenue decreased 10.6% year over year and continues to be negatively impacted by declines in TDM-based voice and private line. Harvest represented approximately 16% of our North America enterprise revenue in the second quarter. For our total business segment, it carried an approximate 77% direct margin this quarter.\\n\\nOther product revenue improved 18.5% year over year. As a reminder, other product revenue tends to experience fluctuations due to the variable nature of these products. Now moving on to mass markets. Our fiber broadband revenue grew 14.6% year over year and represents approximately 38% of mass markets broadband revenue.\\n\\nDuring the quarter, Fiber broadband-enabled location adds were 136,000, bringing our total to over 3.9 million as of June 30th and pacing toward our targeted annual 500,000 build target this year. We also added 40,000 Quantum Fiber customers, which is our best fiber net add quarter reported to date, and this brings our total to 992,000. Fiber ARPU was $62, up slightly, both sequentially and year over year. Importantly, we reached a significant milestone of 1 million fiber broadband subscribers in July.\\n\\nAt the end of the second quarter, our penetration of legacy copper broadband was approximately 9% and our quantum fiber penetration stood at approximately 25%. As we look ahead, we will continue our market-by-market assessment of the mass markets business as we explore a range of strategic options to maximize its value. Those options include potential joint ventures, wholesaling arrangements or future divestitures to generate incremental cash. Now turning to adjusted EBITDA.\\n\\nFor the second quarter of 2024, adjusted EBITDA was $1.011 billion, compared to $1.229 billion in the year-ago quarter. Second quarter EBITDA was positively impacted our strong first quarter sales bookings, as well as efficiency improvements from our second quarter cost actions and overall margin management. Special items impacting adjusted EBITDA totaled $136 million. The majority special items in the quarter were related to severance.\\n\\nFor the second quarter of 2024, our adjusted EBITDA margin was 30.9%, Capital expenditures were $753 million, and free cash flow, excluding special items, was negative $156 million. As we've previously stated, we're leaning into our network investments to support the rapid growth in demand our customers are facing. Now before I provide an update on our 2024 financial outlook, I'd like to provide some color around the near-term impact of our PCF sales and the additional liquidity and flexibility we have. As Kate mentioned, we're moving full speed ahead in investing in our transformation, which includes additional spending on network and systems unification that will ultimately lead to more efficient operations and a better customer experience.\\n\\nGiven our improving liquidity profile, we intend to pull forward some expenses from '26 and '27 into '25, accelerating the time line of our cost takeout goals. With the investments in transformation and costs associated with recent PCF sales and in conjunction with continued legacy revenue declines, directionally, we see 2025 EBITDA below 2024 levels with a significant rebound in 2026 and growing thereafter. We will provide more detailed 2025 guidance on our fourth quarter 2024 call in February. Now moving on to our financial outlook.\\n\\nWe now estimate fiscal year '24 EBITDA to be in the range of $3.9 billion to $4 billion. Capex in the range of $3.1 billion to $3.3 billion. Cash interest in the range of $1.15 billion to $1.25 billion and free cash flow in the range of $1 billion to $1.2 billion. This guidance includes some incremental opex, capex, and cash flows associated with our PCF sales growth, the gain on a sale of an investment, as well as incremental spending to ultimately improve our cost structure and margins.\\n\\nThis additional opex and capex will be fully funded upfront by the incremental PCF cash flow. And with that, I'll turn it back to Kate for closing remarks.\\n\\nKate Johnson -- President and Chief Executive Officer\\n\\nThanks, Chris. Before we open up the call for questions, I wanted to pause to acknowledge where we are. AI represents one of the most significant technology shifts in history. Every person and every organization on earth will be impacted.\\n\\nAI needs data, data needs data centers and data centers need to be connected. What was once an overbuilt fiber network is shifting from commodity to something much more valuable. At Lumen, we aren't streamlining and digitizing our operations to try to find growth in legacy telco markets. Instead, we're building a digital platform to help us become the trusted network for AI so we can capitalize on the markets that will likely see explosive growth for decades.\\n\\nThis is Lumen's moment. We are playing to win. This is the business that we are in. Operator, we're ready for questions.\\n\\nQuestions & Answers:\\n\\nOperator\\n\\nThank you. [Operator instructions] Your first question comes from the line of Michael Rollins with Citi. Please go ahead.\\n\\nMichael Rollins -- Analyst\\n\\nThanks, and good afternoon. First, with respect to the $5 billion of sales, Curious if you could give some additional color on the competitiveness of that process? Are these customers using single vendors for the solution or multiple vendors. So this is something that's not just helping Lumen but maybe the ecosystem. And then, for women specifically, can you share the mix of assets that are existing fiber, existing conduit, leveraging assets that are already out there from you versus what you're building as new infrastructure.\\n\\nAnd as you consume some of those fiber inventories such that investment mix or margin mix might look differently over time as you continue to sell within this new PFC segment? Thanks.\\n\\nKate Johnson -- President and Chief Executive Officer\\n\\nMike, so I'll take the first the first part, let Chris handle the second part. So first part, what does the competitive landscape look like? Look, obviously, I'm a little bit biased, but here's my observation. Our network is the crown jewel that we always thought it was. It's got great coverage, unique routes, it's diverse and it's got state-of-the-art fiber because we've been taking care of it for a long time.\\n\\nAnd that's giving us great positioning with our customers. They're looking at -- sometimes building some routes by themselves. Most of the time understanding that we can get them there faster with higher quality and better service and that's the observation across the deals we've won so far.\\n\\nChristopher David Stansbury -- Executive Vice President and Chief Financial Officer\\n\\nYeah. And just on the economics, it's a really good question, and then, I'm not gonna be evasive with you. But the reality is it's really complicated. So it's a deal to deal, every deal is different in terms of where they want to go from and to, how much capacity they need.\\n\\nAnd inevitably, you will end up with a combination of existing fiber, new fiber, existing conduit, new conduit. It really does vary deal to deal. Now on that, we'll never disclose it because -- these are called private connectivity fabric for a reason. And our customers want to keep it private because it's a competitive -- a secret that they have as is it a competitive secret for us.\\n\\nSo it will vary deal to deal, but the video that we released, I think, gives a good flavor on average.\\n\\nOperator\\n\\nYour next question comes from the line of Sebastiano Petti. Please go ahead.\\n\\nSebastiano Petti -- Analyst\\n\\nHi. Thanks for taking the question. Just had a quick question on the free cash flow guidance. Can you help us think about, is that fully just driven by the customer deposits from the just private custom fiber -- fabric AI? Or is that also reflective of the -- I think, Chris, you said gain on the sale? And in addition to that, can you help us maybe think -- does the free cash flow uplift that you're seeing here? Is that something that we should expect to stay on the balance sheet in 2024 or is this something that will probably get spent as you probably get to fund the increase in capex that you've guided to today? Just trying to help think about the commencement of the build-out in pacing?\\n\\nChristopher David Stansbury -- Executive Vice President and Chief Financial Officer\\n\\nI'll give you credit because you asked one of the great questions on the call early. So the cash flow guidance for this year is driven by both some of the upfront cash received. We haven't received all of it, obviously, for the PCF deals, and it is also related to the asset sale that we did, so both of those things contributed to the free cash flow. As it relates to where we go from here, and again, I want to really be really careful because we are not giving 2025 guidance yet.\\n\\nBut we haven't received all the cash yet. That will be received some this year, some next year, some the following year because again, these are massive construction projects. They take time. And we will start to spend the capex as evidenced by our guidance.\\n\\nThis year and have more next year. But the point is on these deals, we're not financing the build, so we get paid in advance of the construction. The only thing that is kind of hanging as you go out 12 months is we pay tax on the cash received. So even though the revenue is amortized, the IRS likes to get paid on a cash basis for these deals.\\n\\nSo that will be something that we deal with, and we'll get more color on that as we move through. But high level, I would say that next year, free cash flow looks good.\\n\\nOperator\\n\\nYour next question comes from the line of Batya Levi from UBS. Please go ahead.\\n\\nBatya Levi -- Analyst\\n\\nGreat. Thank you. Looking at the EBITDA guidance change for the year, is that purely related to the incremental opex for getting ready for these networks? Or -- is there any change in terms of the underlying trend? And can you just go over the $1 billion cost savings you expect over the next three years the pacing of that? I think you mentioned some of the expenses will be pulled forward. And then, is there any incremental cost to achieve that savings through the next three years?\\n\\nChristopher David Stansbury -- Executive Vice President and Chief Financial Officer\\n\\nYes. So as it relates to this year, the vast majority of the -- there's, obviously, a lot of things that go on inside of EBITDA. But the main driver here are the TCF deals. And the opex investments we need to make to get that construction factory up and running in a more scaled way.\\n\\nIt's a group that exists. It's one of Lumen's core competencies, but the size of that group needs to get substantially larger to support just the quantum of these deals. And so, that's the key driver. As it relates to the $1 billion cost takeout, we haven't -- again, I want to stay away from 2025 guidance as much as I can.\\n\\nWe're not expecting those savings to start until next year. There will be some investment next year, and we'll disclose that when we give guidance for next year. But my comments on just trying to dimensionalize where we go from here, are really around the fact that we're taking the opportunity near term balance of this year and '25 to really pull forward investments we were going to have to make in '26 and '27 to get to a place where our IT systems, as Kate mentioned, are more consolidated, simplified to drive a customer experience. And I would say, if there's one key driver in that, it's going from what our four enterprise networks today to one.\\n\\nAnd that is a legacy that exist today that needs to be cleaned up because it just drives a much more seamless customer experience as we go forward.\\n\\nOperator\\n\\nYour next question comes from the line of David Barden from Bank of America. Please go ahead.\\n\\nDavid Barden -- Analyst\\n\\nHey, guys, thanks so much for taking the question. Chris, I guess, it's not so much a question is I want to put forward a hypothesis and I want you to tell -- it would be, I think, super helpful for people to share what you think is right or wrong about it. So we've got this $5 billion deal, but the majority of the cash is coming in, in the next three to four years, and the majority of the cash -- of that cash is also going out the door in the next three to four years. So any kind of cash inflow we're getting is kind of a timing benefit relative to the capex that's required under the contract.\\n\\nAnd if it's a $5 billion contract and the majority of it is related to the construction piece, let's just call it $3 billion round numbers. That means that the actual IRU sale piece is about $2 billion. And as you shared in your video, that IRU revenue doesn't start until after the build is done, which would be probably Year 4 or 5, over a 20-year period, $2 billion dollar, a $100 million in revenue a year, very high margin, maybe $85 million in EBITDA, tax affected, as you've mentioned in your video, maybe, again, the taxes will be timing related, but let's just call it $65 million of tax-affected cash flow over a 20-year period. So a $5 billion deal announcement turns into $65 million of cash flow five years from now, what's right and what's wrong about that assessment?\\n\\nChristopher David Stansbury -- Executive Vice President and Chief Financial Officer\\n\\nI'd say most of it is wrong. The -- Yes, I think, David, here's where we go, so again, it's multiple deals that added up to the $5 billion. Not just one. And in the video, we talked about a cash contribution margin, which is effectively the EBITDA less the capex, pre-tax that's roughly in the ballpark of our existing EBITDA margin.\\n\\nSo you do the math on that that will give you the pre-tax free cash flow associated with these deals. And that cash flow, to your point, does come largely at the front end. Now there's ongoing payments for space and power, for operating and maintenance if they want us to run the networks for them that gives us nice cash flow over the years, but the tax would also be front-end loaded. So the key thing here is that in one set of deals in those $5 billion deals that the net after-tax cash generated from that fully fund the liquidity gap that we've talked about for so long on these calls.\\n\\nIt's older, it's behind us. And we're not done. So as we said, there's another $7 billion of discussions underway right now. And this trend will continue.\\n\\nThe demand isn't one and done. So that's the key difference, so there's more cash in the deal than you've laid out and there's more to come.\\n\\nKate Johnson -- President and Chief Executive Officer\\n\\nAnd additionally, it's not one deal. The $5 billion represents multiple customers, and each contract is very different. I think that's importa\", 'Who are LUMN competitors in the world. Rank them by revenue and location', 'Who is the likely acquirer of SNAP should a company decide to acquire?', 'analyze SNAP and identify best entry point from risk reward perspective?', 'Analyze TSLA. What is the best entry point for TSLA', 'Analyze CLOV and identify the best entry point from risk reward perspecive', 'Given today Nasdaq is down and given current market situation including middle east, What are the chances that market would fall further tomorrow?', 'What might trigger a turn around in the current market situation specially Nasdaq', 'How many bitcoins IBIT holds?', 'Analyze MDB, what is the perfect entry point? Who is the likely acquirer of this company?', 'What is one technology stock that I should buy that is likely to double or triple in the next few years and is relatively safer to invest in?', 'What is one technology stock that I should buy that is likely to double or triple in the next few years other than Nvidia and is relatively safer to invest in?', 'What is one technology stock that I should buy that is likely to double or triple in the next few years other than Nvidia and SMCI ', 'Who is manufacturing Serv robotics robots in the United States?', 'Explain in english. Identify red flags\\nSjulsveien 17 er en tradisjonell landbrukseiendom med flott beliggenhet i Modum kommune, med utsikt mot nord og vest over Tyrifjorden. Gårdsbruket ligger usjenert og fint til for seg selv i et typisk landbruksområde med noe spredt boligbebyggelse, og med flott utsyn over et vakkert kulturlandskap. En koselig liten gård med litt dyrket mark, noe skogarealer og eldre bebyggelse. Eiendommen grenser ned mot Tyrifjorden mot nord, og har egen strandlinje. Den dyrkede marken utgjør 34,8 dekar, og er lett tilgjengelig for maskiner, har god arrondering og gir gode årvisse avlinger. Mesteparten av jordbruksarealet ligger i tilknytning til tunet. Det produktivet skogsarealet utgjør 60,6 dekar. Skogsteigene ligger i litt avstand fra tunet, men har god adkomst. Det er satt i gang planting av ny skog. Selve tunet ligger høyt med utsikt over Tyrifjorden, og rammes fint inn av bygningsmassen. Parkeringsforholdene er meget gode og det er enkel tilgang til både låve, garasje og våningshus. Hageanlegget er pent og består av romslige plenarealer med diverse beplantninger. Solforholdene er svært gode og fra tunet er det vidstrakt utsikt over det omkringliggende kulturlandskapet.\\n\\nDen eldste delen av våningshuset er fra 1930, og tilbygget i vinkel på det gamle huset med entré, kjøkken, bad, vaskerom, soverom og stuer i 1981. Det ble også bygget terrasseplatting mot sør og terrasse mot nord/vest.  Våningshuset går over to plan og samlet BRA er 337 kvm. Utstyr og innredninger holder en god standard, og vedlikeholdet er gjennomgående gått, men huset har imidlertid behov for oppgraderinger for å tilfredsstille dagens bostandard. Eldste del av låven har ukjent byggeår, med påbygning  ca. 1957. Grunnflaten er på 181 kvm. Eldre låve som benyttes som lager og maskinlager. Bygningen inneholder en fjøsdel, som er omgjort til garasje og lagerrom med gjødselkjeller under. Det er gulv over hele 2. etg., som benyttes som lager. Dobbellgarasjen er oppført i 2000, og inneholder i tillegg isolert lagerrom og uisolert vedskjul/lager i forlengelse mot nord. Det er også en enkel lekestue og et eldre sommerfjøs, som er ombygget på tomten.\\n\\nBeliggenhet, adkomst\\nEiendommen har en flott beliggenhet på østsiden av Tyrifjorden. Solrikt, naturskjønt med nydelig utsikt mot fjorden. Det er ca. 7 km til Vikersund sentrum med godt og variert forretningstilbud, som apotek, kafeer, kjøpesenter m.m. Kort vei til flotte tur og rekreasjonsområder på Furumo og Moane med landskjente Frisklivs-sentralen, Furumo svømmehall, idrettsanlegg, kunstfrossen skøytebane og fotballbane. Eiendommen har nærhet til Vikersund hoppsenter med verdens største skiflyvningsbakke og Øståsen med flotte turområder sommer som vinter og med fine skiløyper inn i Finnemarka. Tyrifjorden er en innsjø i Ringerike, Hole, Lier og Modum kommuner i Buskerud fylke. Rundt Tyrifjorden finnes det en rekke flotte badeplasser, som eksempelvis Onsakervika og Røsholmstranda innerst i Nordfjorden.\\n\\nModum kommune har et meget godt fritids- og aktivitetstilbud for store og små. I tillegg kan man gå den fine og naturskjønne turstien \"Modum på langs\" som går fra Kongsfossen i Åmot oppover Drammenselva, videre til Kattfoss i Geithus, bort til Delinga, videre langs Bergsjø, opptil Tyrifjorden dertil enten bort til Tyrifjord Hotell eller opptil Kleiva og videre til Gamle Kirkevei, fram til Heggen Kirke. Kommunen har et mangfoldig og rikt kulturtilbud. Modum Kulturhus med kino, bibliotek, musikkskole og jevnlig besøk av Riksteateret. Sammen med Åmot kirke, Olavskirken, Festsalen på Modum Bad og Glasshytta på Blaafarveværket tiltrekker bygda seg høyt profilerte aktører i alle sjangere.\\n\\nDet er ca. 38 km til Hønefoss, ca. 48 km til Drammen og ca. 72 km til Oslo. Kun ca. en time med bil til Norefjell Ski & Spa med flotte skiløyper og skianlegg. Se for øvrig vedlagt Nabolagsprofil for flere avstander og ytterligere informasjon om området.\\n\\nType, eierform og byggeår\\nLandbrukseiendom Selveier, oppført i 1930\\n\\nVåningshus:\\n1. etg.;\\nEntré, kjøkken m/spiseplass, bad m/dusj og wc, vaskerom, 3 soverom, toalettrom og stue/spisestue m/utg. til terrasse\\n2. etg.:\\nSoverom, loftstue og kott\\nKjeller:\\nDiverse kjellerrom og boder\\n\\nLåve:\\n1. etg.:\\nFjøsrom, som er omgjort til garasje/lagerrom\\n2. etg.:\\nLagerrom\\nKjeller:\\nGjødselkjeller\\n\\nGarasje:\\n1. etg.:\\nDobbelgarasje m/lagerrom og vedskjul/lager\\n\\nLekestue og sommerfjøs/uthus, som er ombygget.\\n\\nBygninger og byggemåte\\nVåningshus:\\nUtvendig:\\nOppført i 1930, og er oppført i tømmer på mur. Tilbygget er oppført i bindingsverk i 1981. Det er kjeller under tilbygget i betongvegger, som har innvendig adkomst. Det er saltak og valmet tak, som er tekket med betongtakstein. Beslag, takrenner og nedløp er i plastbelagt stål. Takkonstruksjonen på den eldste delen har sperrekonstruksjon i tre. takkonstruksjonen på tilbygget er W-takstoler i tre. Vegger på den eldste delen har tømmerkonstruksjon. Veggen på tilbygget har bindingsverkonstruksjon. Fasade/kledning har stående bordkledning, som er godt vedlikeholdt. Bygningen har hovedsakelig malte trevinduer med 2-lags glass. Noen vinduer er byttet i den eldste delen. Det er malt hovedytterdør og balkongdør i tre, og skyvebalkongdør i malt tre. Terrasse på to sider av boligen.\\nInnvendig:\\nGulv av parkett, tepper, laminat og heltre furugulv i 1. etg. og teppe i 2. etg. Gulv i kjeller er av betong, og har teppe ved trapp. Veggene har tapet og trepanel. Innvendig tak har malte plater, trepanel og himlingsplater. Betong/mur og plater i kjeller. Innvendige finédører og malte fyllingsdører. Boligen har ubehandlet tretrapp til 2. etg. i den eldste delen, og ned til kjeller i tilbygget. Pipe i tilbygget er fra byggeår. Nytt ildsted i 2023. Pipe i det opprinnelige huset er tettet og er ikke i bruk. Etasjeskilllere er av trebjelkelag.\\nVåtrom:\\nFlislagt bad og trepanel i himlingen. Ingen dokumentert membran. Tilpasset baderomsinnredning med servant i innredningen. Innebygget dusjhjørne og toalett. Elektriske varmekabler på gulv. Sluk er montert i dusjsonen, og er målt til 1:20. Det er plastsluk og smøremembran med ukjent utførelse. Ingen synlig membran i sluk. Rommet har naturlig ventilering.\\nVaskerom:\\nEtt ombygd soverom, og ingen dokumentert membran. Manglende oppkant ved dører og manglende fall mot sluk. Gjenværende brukstid er usikker, grunnet alder og byggemåte.\\nKjøkken:\\nKjøkkeninnredning med profilerte fronter, og laminat benkeplate, med nedfelt oppvaskkom. Det er integrert kjøleskap, oppvvaskmaskin, induksjonstopp, dampovn og stekovn. Kjøkkenventilator med avtrekk ut.\\nSpesialrom:\\nToalett er frakoblet vann, og ikke i bruk per dags dato. Avløp er av ukjent påkobling.\\nTekniske installasjoner:\\nInnvendige vannledninger er av kobber med plastkappe. Alle vannrør er synlige. Stakeluke i vegg i kjeller. Boligen har naturlig ventilasjon. Varmtvannsbereder på 200 liter. 2 stk luft til luft varmepumper, som er montert i 1. og 2. etg. Elektrisk anlegg i det opprinnelige våningshuset fra ca. 1930, og i tilbygget fra 1981. Det elektriske er veldig begrenset kontrollert i forbindelse med tilstandsrapporten, da dette krever Nemko godkjent kompetanse. På generelt grunnlag anbefales det kontroll av el-anleggget av autorisert firma. Byggerket skal være tilrettelagt for effektiv manuell slokking av brann. Slokkeutstyr skal kunne benyttes av personer i byggverket for slokking av et branntilløp i en tidlig fase. Bygningene har forskriftsmessig brannvarslere og slokkeutstyr.\\n\\nLåve:\\nByggeåret er ukjent, men er påbygd i 1957, i følge Norske gårdsbruk. Eldre utidsmessig kjørebrulåve, som benyttes som redskapslager. Oppført i grovt bindingsverk på fundamenter/mur av granittstein. Fjøsdelen er oppført i mur, som er pusset og malt. Kjeller under fjøsdel. Deler av grunnmuren under denne delen har begynt å gli noe ut. Saltak, åstak, som er tekket med gammel betongtakstein, med beslag av stål på mønet. En bit av mønebeslaget har løsnet, og falt av. Det er montert stålplater på vegg mot nord på slutten av 1980-/begynnelsen av 1990-tallet. Bygningen benyttes i dag som lager, og har begrenset bruksverdi. Fjøsdelen er gjort om til garasje/lager, med en stor skyveport inn mot tunet. Det er behov for en del vedlikehold og overflatebehandling av bygningen, og ombygninger dersom den skal utnyttes mer.\\n\\nGarasje:\\nDobbel bilgarasje med lagringsrom og vedskjul. Oppført i 2000 på støpt plate på mark. Vegger i oppført bindingsverk, utvendig kledd med stående trepaneler. Saltak, W-takstoler, som er tekket med betongtakstein. Garasjen er isolert, og har to isolerte leddporter, og plass til to biler i tillegg til innvendige lagerrom. Garasjen har gangdør mot vedbod/lager, som er tilbygget mot nord, og har vinder mot øst. Vedbod/lager mot nord er oppført på støpt plate. Bindingsverk med stående trekledning. Denne delen er ikke isolert.\\n\\nLekestue:\\nLita lekestue i hagen, oppført i rammeverk på pilarer/blokker. Vegger i bindingsverk med trekledning og innvendig kledt med panel. Saltak, som er tekket med steinbelagte ståltakplater av typen Decra el. med taksteinmønsker. Bygningen er normalt godt vedlikeholdt.\\n\\nSommerfjøs-uthus:\\nEldre sommerfjøs med bruksendring til uthus i 1994, og er tidligere benyttet som hytte. Bygningen står på grunnmuren til det gamle sommerfjøset, og er oppført i bindingsverk med utvendig trekledning. Det er saltak. Strøm er frakoblet. Bygningen er ikke besiktiget av takstmann, og er i følge hjemmelsinnehaver kondemnabelt. Det er mugg og råte. Bygningen har tidligere vært brukt som fritidsbolig, men har ikke vært i bruk på flere år, og har et meget stort vedlikeholdsetterslep.\\n\\nSkogshytte - hvilebrakke:\\nDet er satt opp en brakke i skogen ved Abbortjernåsen. Denne ble satt der i 1989, og kledd inn. Den er bl.a. benyttet som hvilekoie under skogsdrift. Hytta er ikke omsøkt.\\n\\nOppvarming\\nDet er installert varmepumper i huset; en i 1. etasje fra 2017 og en eldre en som går til 2. etasje. Disse er vedlikeholdt. Ellers er oppvarming med ved og elektrisitet i form av varmekabler (våtrom) og panelovner. Ny vedovn i 2023.\\n\\nAreal og eierform (opplysninger om evt. feste)\\nAreal: 105 200 kvm, Eierform: Eiet tomt\\n\\nEn tomt er skilt ut ved Tyrifjorden, men den er ikke bebygget, da man ikke får lov til å bygge noe i strandsonen. Her er en sak i gang med å tilbakeføre denne tomta til gården. Det ventes på at tomten skal tinglyses tilbake til gården i disse dager.\\n\\nTun, jord og skog\\nTun:\\nSelve tunet har en enkel og grei adkomst. Tunet rammes fint inn av bygningsmassene og har gode parkeringsforhold. Tilgang adkomst til både låve, garasje og våningshus er enkel. Hageanlegget er opparbeidet med romslig plen med diverse beplantninger. Solforholdene er svært gode og er vidstrakt utsikt mot Tyrifjorden og ut over det omkringliggende kulturlandskapet. Eiendommen grenser ned mot Tyrifjorden mot nord, og har egen strandlinje.\\n\\nDyrket mark:\\nDyrket mark er på 34,8 dekar totalt, og er lett tilgjengelig for maskiner, har god arrondering og gir gode årvisse avlinger. Mulighet for vanning fra Tyrifjorden. Den dyrkede marka leies ut. Leieperioden går ut i 2025. Årlige leieinntekter er kr. 8.000,-\\n\\nSkog:\\nDet er ingen skogbruksplan laget for gården. I følge NIBIO gårdskart er det følgende bonitetsfordeling i skogen:\\n\\nSkog høy bonitet: 42,8 dekar\\nSkog middels bonitet: 17,8 dekar\\nUproduktiv skog: 2,0 dekar\\n\\nBeregnet tilvekst i forhold til bonitetsfordelingen er 32 kbm/ år. Tilveksten avhenger av hogstklassefordelingen, og vil variere i forhold til hvor gammel skog som står på arealene. Mye skog\\ni hogstklasse I og II vil gi en lavere tilvekst noen år, før skogen går over i hogstklasse III og IV, hvor skogen har høyest tilvekst, for så å stagnere gradvis ved overgang til hogstklasse V.\\n\\nInnestående på skogavgiftskonto pr. 31.12.2023 var kr. 28 185,-. Det er satt i gang planting av ny skog, og det forventes at store deler av skogavgiftskontoen går med til å dekke dette arbeidet.\\n\\nEiendommen er tilknyttet grunneierlag for elgjakt og rådyrjakt.\\n\\nVei, vann og avløp\\nEiendommen har adkomst fra offentlig vei, via felles privat vei. Felles adkomstvei med 3 naboer. Det deles på utgifter til vedlikehold, men det foreligger ingen avtaler.\\n\\nVannforsyning fra privat borebrønn. Borebrønn eies sammen med to naboer. Pumpestasjon har egen strømmåler og det deles på strømregningen for denne. Det er utvendig stoppekran for alle tre i en kum rett ovenfor gården. Det er filter på vannforsyningen inn. Vannprøver viser god vannstandard. Innvendig stoppekran er i kjeller i boligen, hvor også VV-berederen står.\\nHjemmelshaver har utbedret vei opp til borebrønn og opp til gammelt sommerfjøs - syd for eiendommen. Her er vei breddet ut, og grøfter utbedret. Det er mulighet for jordbruksvanning fra Tyrifjorden. Her har gården et enkelt anlegg med pumpe og slanger. Det har ikke vært i bruk de senere årene.\\n\\nEiendommen har privat avløp til lukket septiktank og gråvannstank, med kommunal tømming.\\n\\nReguleringsplan\\nEiendommen er uregulert, og ligger i et område avsatt til LNF-område i kommuneplanen (landbruk, natur, fritid). Kommuneplanens navn er kommuneplanens arealdel 2016 - 2027. Endelig vedtatt arealplan: 04.02.2019. Arealbruk for delarealer: LNFR areal for nødvendig tiltak for landbruk og reindrift og gårdstilknyttet næringsvirksomhet basert på gårdens ressursgrunnlag, nåværende.\\n\\nServitutter og heftelser i grunnbok\\n1952/3451-1/25  Bestemmelse om veg   \\n10.10.1952  \\nrettighetshaver:Knr:3316 Gnr:12 Bnr:4   \\nMed flere bestemmelser\\nDokumentet finnes ikke i arkivet til Statens kartverk\\n\\n1958/922-1/25  Erklæring/avtale   \\n31.03.1958  \\nBestemmelse om kloakkledning\\nVegvesenets betingelser vedtatt\\nMeglers kommentar: Eier gir tillatelse til å føre kloakkledning over d.e.\\n\\n1982/5850-1/25  Bestemmelse om bebyggelse   \\n07.07.1982  \\nrettighetshaver:Knr:3316 Gnr:13 Bnr:31   \\nByggetillatelse gitt etter dispensasjon i henhold\\ntil Bygningsloven\\nMeglers kommentar: Erklæring for rett til å bygge på nabotomten etter bygningsloven §70\\n\\n1982/7087-1/25  Erklæring/avtale   \\n23.08.1982  \\nrettighetshaver:Knr:3316 Gnr:13 Bnr:31   \\nBESTEMMELSE OM DRENSVANN/STIKKRENNER M.V.\\nMeglers kommentar: Eier gir naboeiendommen rett til å anlegge sandfilter grøft med tilstøtende ledninger og inspeksjonkom over d.e.\\n\\n1999/62-2/25  Bestemmelse om veg   \\n05.01.1999  \\nrettighetshaver:Knr:3316 Gnr:13 Bnr:33 \\nMeglers kommentar: Eiendommen gis veirett over hovedbølet og vannrett fra felles brønn på hovedbølet\\n\\n2006/5930-1/25  Elektriske kraftlinjer   \\n13.06.2006  \\nRettighetshaver: Midt Nett Buskerud AS\\nMed flere bestemmelser\\nMeglers kommentar: Eier gir Midt Nett Buskerud AS å føre høyspennings luftledning fra Vikersund i Modum kommune til Kusandvika i Modum kommune\\n\\n2014/934335-1/200  Jordskifte   \\n29.10.2014  \\nJordskiftesak 0600-2012-0019 Øst Modum Skogsveiforening.\\nDiverse bestemmelser om tiltak og rettigheter vedr. Breiliveien.\\nGjelder denne registerenheten med flere\\nJordskiftedom fra 2014 mellom flere eiendommer, med bestemmelser om bl.a. gjerde, drenering og areal ved bom\\n\\n2017/541622-1/200  Elektriske kraftlinjer   \\n23.05.2017 21:00  \\nRettighetshaver:KRAFTIA ENERGI AS   \\nOrg.nr: 980283976\\nMeglers kommentar: Avtale om rettigheter og plikter ved bygging, drift og vedlikehold av høspenningskabelanlegg.\\n\\nFerdigattest\\nFerdigattest for tilbygg, datert 01.10.1979 for tilbygg til våningshuset, og ferdigattest, datert 22.09.1994 for uthuset.\\n\\nDet foreligger godkjente og byggemeldte tegninger, men de stemmer ikke med dagens bruk Det er etablert vaskerom der hvor soverom er tegnet inn - til høyre for inngang i tilbygg fra 1981.. Den eldste delen av huset er fra ca. 1930. Her foreligger ikke tegninger.\\n\\nKjøper påtar seg risikoen for både fremtidig, fortsatt bruk og eventuelle pålegg, for alle ovennevnte ulovlige forhold, herunder risikoen for om bruken lar seg godkjenne, og alle kostnader forbundet med dette. Det kan være krav i lov, forskrift og planverk som ikke er eller kan oppfylles, i så fall vil kommunen kunne kreve at rommet/ rommene/ bygningsdelene settes tilbake til opprinnelig godkjent stand.\\n\\nInformasjon om odelsloven\\nDet er ikke odel på eiendommen, da den ikke har et areal på over 35 dekar full- og/eller overflatedyrket jord - og/eller har over 500 dekar produktiv skog.\\n\\nInformasjon om konsesjonsloven og jordloven\\nDenne eiendommen er blant annet underlagt jordloven og konsesjonsloven.\\n\\nDet er konsesjonsplikt i henhold til konsesjonsloven på eiendommen. Det er den enkelte kommune som er rett konsesjonsmyndighet. I konsesjonsbehandlingen legges det særlig vekt på:\\n\\n1. At kjøper skal oppfylle sin boplikt. Innflytting i boligen på gården skal skje senest ett år etter tinglysning av skjøte og deretter må man bebo eiendommen i minst 5 år etter dette.\\n\\n2. At eiendommen driftes. Arealer med dyrket mark skal holdes i hevd, altså dyrkes videre hvilket også følger av jordlova. Ofte vil utleie av jorda til en nabobonde vil være tilstrekkelig.\\n\\n3. Den avtalte prisen må tilgodese en samfunnsmessig forsvarlig prisutvikling. Det er dette som kalles priskontroll hvor kommunen i praksis vurderer hvorvidt den avtalte prisen mellom kjøper og selger ligger på et riktig nivå.\\n\\nDet er konsesjonsplikt i henhold til konsesjonsloven på eiendommen. Det er den enkelte kommune som er rett konsesjonsmyndighet. \\n\\nSjulsveien 17 er unntatt priskontroll på grunnlag av eiendommens sammensetning. Det er Kjøpers risiko og kostnad å oppnå konsesjon for de øvrige vilkår etter konsesjonslovens §9. Bud med forbehold om konsesjon vil ikke bli godtatt av selger. Etter budaksept og påfølgende kontraktsmøte forplikter kjøper seg til samtidig å søke konsesjon på eiendommen. Dersom konsesjonsmyndigheten i den respektive kommunen fatter et negativt konsesjonsvedtak plikter kjøper å påklage dette til Statsforvalter, alternativt å rette seg etter de pålegg konsesjonsmyndighetene gir. Uavhengig av hva endelig konsesjonsvedtak blir, har kjøper ingen rett til å kreve prisavslag fra selger. Avtalt kjøpesum må betales fullt ut og overtakelse må finne sted i henhold til kjøpekontraktens overtakelsesdato. I ytterste konsekvens kan et negativt konsesjonsvedtak innebære at kjøper må foreta et videresalg av eiendommen. Kjøper vil da måtte må bære et eventuelt økonomisk tap.\\nSe utfyllende informasjon på landbruksdirektoratets hjemmeside: https://www.landbruksdirektoratet.no/no/eiendom-og-skog/eiendom/konsesjon/konsesjon#hva-maa-jeg-gjoere-hvis-jeg-ikke-faar-konsesjon-\\n\\nAlle landbrukseiendommer er også omfattet av jordlovens bestemmelser. Dette innebærer at det er driveplikt og at det gjelder et forbud mot deling av gården. Dette gjelder også selv om eiendommen består av flere bruksnumre.\\n\\nSpørsmål om odel, bo- og driveplikt, konsesjon og delingsforbudet kan rettes til oppdragsansvarlig.\\n\\nPrisantydning\\n6 200 000,-\\n\\nTakst / tilstandsrapport\\nUtført av : Thor B. Grønnerød\\nTakstdato : 24.07.2024\\nMarkedsverdi : 6 200 000,-\\nFormuesverdi : 1 212 048,-\\nFormuesverdi pr. : 31.12.2024\\n\\nLandbrukseiendom med våningshus oppført i 1930, det er våningshuset tilstandsrapporten er på. Andre bygg på gården er kun enkelt vurdert og beskrevet i salgsoppgaven, se også landbrukstakst for gården generelt. \\n\\nPå denne eiendommen har følgende fått tilstandsgrad 2 (TG2) og tilstandsgrad 3 (TG3):\\n\\nEnebolig\\nUtvendig\\nTaktekking,TG2\\nTaktekkingen på tilbygg fra 1981 er av betongtakstein. Taket er besiktiget fra bakkenivå.\\nVurdering av avvik:\\n- Mer enn halvparten av forventet brukstid er passert på taktekkingen.\\n- Mer enn halvparten av forventet brukstid er passert på undertak.\\n- Mer enn halvparten av forventet brukstid er oppbrukt på undertak.\\n\\nTiltak\\n- Tidspunkt for utskiftning av taktekking nærmer seg.\\n- Tidspunkt for utskiftning av undertak nærmer seg.\\n\\nTaktekking - 2,TG2\\nTaktekkingen på opprinnelig våningshus ble lagt om i 2015, og er av betongtakstein. Taket er besiktiget fra bakkenivå.\\nVurdering av avvik:\\n- Mer enn halvparten av forventet brukstid er passert på taktekkingen.\\n- Mer enn halvparten av forventet brukstid er oppbrukt på undertak.\\n\\nTiltak\\n- Overvåk tilstanden jevnlig. For å få tilstandsgrad 0 eller 1 må tekkingen skiftes ut, men tidspunktet for når dette er nødvendig er vanskelig å si noe om.\\nKostnadsestimat : Ingen umiddelbar kostnad\\n\\nNedløp og beslag,TG2\\nRenner, nedløp og beslag i plast fra byggeår på tilbygget fra 1981. På den eldste bygningskroppen fra ca. 1930 ble disse byttet ut i forbindelse med omlegging av tak i 2015. Her er beslag, renner og nedløp i plastbelagt stål. Vann ledes til terreng.\\n\\nVurdering av avvik:\\n- Det er ikke tilfredsstillende bortledning av vann fra taknedløp ved grunnmur.\\n- Det mangler snøfangere på hele eller deler av taket, men det var ikke krav om dette på byggemeldingstidspunktet.\\n\\nTiltak\\n- Det bør lages system for bortledning av vann fra taknedløp ved grunnmur.\\n- Vann fra nedløp må ledes vekk fra grunnmur.\\nTakkonstruksjon/Loft - 2,TG2\\n\\nTakkonstruksjonen på den eldste delen har sperrekonstruksjon.\\nVurdering av avvik:\\n- Det er påvist fuktskjolder/skader i takkonstruksjonen.\\n- Noe fuktskjolder kan sees i takkonstruksjonen. Dette skyldes antagelig tidligere lekkasjer fra før takomlegging i 2015.\\n\\nTiltak\\n- Det må gjøres nærmere undersøkelser.\\nKostnadsestimat : Ingen umiddelbar kostnad\\n\\nDører,TG3\\nBygningen har malt hovedytterdør, malt balkongdør i tre og skyvebalkongdør i malt tre.\\nVurdering av avvik:\\n- Det er påvist dør(er) med fukt/råteskader.\\n- Det er påvist dører med punkterte eller sprukne glassruter.\\n- Det er påvist dører som er vanskelig å åpne eller lukke.\\n- Det er påvist utetthet/åpning mellom dørblad og dørkarm. Dvs. at kald trekk kan oppstå.\\n- Karmene i dører er værslitte utvendig og det er sprekker i trevirket.\\n- Skyvedør i stue må justeres. Det er oppdaget råte og punkterte glass i balkong dører.\\n\\nTiltak\\n- Det må foretas lokal utbedring.\\n- Dører må justeres.\\n- Dører med punkterte/sprukne glass må påregnes skiftes ut, enten hele døren eller kun selve glassene.\\n- Lokal utbedring/utskifting av fukt/råteskadet treverk.\\nKostnadsestimat : 10 000 - 50 000\\n\\nBalkonger, terrasser og rom under balkonger,TG3\\nDet er terrasse på to sider av boligen.\\nVurdering av avvik:\\n- Åpninger i rekkverk er ikke i henhold til krav i dagens forskrifter.\\n- Det er ikke montert rekkverk.\\n- Rekkverket er for lavt i forhold til dagens krav til rekkverkshøyder.\\n\\nTiltak\\n- Åpninger i rekkverk må endres for å tilfredsstille krav på byggemeldingstidspunktet.\\n- Rekkverkshøyde må endres for å tilfredsstille krav på byggemeldingstidspunktet.\\nKostnadsestimat : 10 000 - 50 000\\n\\nUtvendige trapper,TG3\\nVurdering av avvik:\\n- Åpninger i rekkverk er ikke i henhold til krav i dagens forskrifter.\\n- Det er ikke montert rekkverk.\\n- Rekkverkshøyder er under dagens forskriftskrav til rekkverk i trapper.\\n\\nTiltak\\n- Rekkverk må monteres for å lukke avviket.\\nKostnadsestimat : Under 10 000\\n\\nInnvendig\\nEtasjeskille/gulv mot grunn,TG2\\nEtasjeskiller er av trebjelkelag i den eldste delen.\\nVurdering av avvik:\\n- Målt høydeforskjell på mellom 10 - 20 mm innenfor en lengde på 2 meter. Tilstandsgrad 2 gis med bakgrunn i standardens krav til godkjente måleavvik.\\n- Målt høydeforskjell på over 15 mm gjennom hele rommet. Tilstandsgrad 2 gis med bakgrunn i standardens krav til godkjente måleavvik.\\n\\nTiltak\\n- For å få tilstandsgrad 0 eller 1 må høydeforskjeller rettes opp. Det vil imidlertid sjelden være økonomisk rasjonelt som et enkeltstående tiltak i en bolig som dette. Dersom boligen en gang skal renoveres, kan man vurdere slike tiltak.\\n\\nEtasjeskille/gulv mot grunn - 2,TG2\\nEtasjeskiller er av trebjelkelag i tilbygg fra 1981.\\nVurdering av avvik:\\n- Målt høydeforskjell på mellom 10 - 20 mm innenfor en lengde på 2 meter. Tilstandsgrad 2 gis med bakgrunn i standardens krav til godkjente måleavvik.\\n- Målt høydeforskjell på over 15 mm gjennom hele rommet. Tilstandsgrad 2 gis med bakgrunn i standardens krav til godkjente måleavvik.\\n\\nTiltak\\n- For å få tilstandsgrad 0 eller 1 må høydeforskjeller rettes opp. Det vil imidlertid sjelden være økonomisk rasjonelt som et enkeltstående tiltak i en bolig som dette. Dersom boligen en gang skal renoveres, kan man vurdere slike tiltak.\\n\\nRom Under Terreng,TG2\\nGulvet er av betong og har teppe ved trapp. Veggene har betong/mur og plater. Hulltaking er foretatt uten å påvise unormale forhold.\\nVurdering av avvik:\\n- Det er påvist indikasjoner på noe fuktgjennomtrenging i kjellergulv.\\n- Det er påvist indikasjoner på noe fuktgjennomtrenging inn i kjellermur.\\n\\nTiltak\\n- Det påviste fuktnivå gir grunn til å overvåke konstruksjonen jevnlig for å se utvikling over tid, og eventuelt foreta tiltak for å unngå fuktskader.\\n\\nInnvendige trapper,TG2\\nUbehandlet tretrapp\\nVurdering av avvik:\\n- Åpninger i rekkverk er større enn dagens forskriftskrav til rekkverk i trapper.\\n- Åpninger mellom trinn i innvendig trapp er større enn dagens forskriftskrav.\\n- Det mangler håndløper på vegg i trappeløpet.\\n- Rekkverkshøyder er under dagens forskriftskrav til rekkverk i trapper.\\n\\nTiltak\\n- Håndløper bør monteres, men det var ikke krav på byggetidspunktet.\\n- Åpninger er såpass store at det ut ifra sikkerhetsmessige forhold anbefales å lage mindre åpninger.\\nKostnadsestimat : Ingen umiddelbar kostnad\\n\\nInnvendige trapper - 2,TG3\\nBoligen har ubehandlet tretrapp opp til 2. etasje i den eldste delen.\\nVurdering av avvik:\\n- Åpninger i rekkverk er større enn dagens forskriftskrav til rekkverk i trapper.\\n- Åpninger mellom trinn i innvendig trapp er større enn dagens forskriftskrav.\\n- Det er ikke montert rekkverk.\\n- Rekkverkshøyder er under dagens forskriftskrav til rekkverk i trapper.\\n\\nTiltak\\n- Åpninger er såpass store at det ut ifra sikkerhetsmessige forhold anbefales å lage mindre åpninger.\\n- Rekkverk må monteres for å lukke avviket.\\nKostnadsestimat : 10 000 - 50 000\\n\\nVåtrom\\nEtasje 1 > Bad\\nOverflater Gulv,TG2\\nGulvet er flislagt. Rommet har elektriske varmekabler. Fall mot sluk er målt til 1:20 i dusjsone. Det er målt ca 35 mm høydeforskjell på gulv fra dørterskel til topp slukrist. Sluk er montert i lukket dusjsone.\\n\\nVurdering av avvik:\\n- Det er mulighet for at det kan forekomme vannlekkasje på våtrommet hvor vann ikke vil gå til sluk.\\n\\nTiltak\\n- Det bør etableres avrenning inn til sluk for hele våtrommet.\\nKostnadsestimat : Ingen umiddelbar kostnad\\n\\nEtasje 1 > Bad\\nSluk, membran og tettesjikt,TG2\\nDet er plastsluk og smøremembran med ukjent utførelse. Ingen synlig membran i sluk. Det gjøres spesielt oppmerksom på at tekking (membran og mansjetter) ikke er kontrollerbare fordi dette bare gjøres ved demontering av fliser. Denne typen destruktive undersøkelser blir aldri foretatt ved en tilstandskontroll. Det dusjes direkte på vegger og gulv. Direkte vannbelastning på vegger og gulv kan på generelt grunnlag gi økt risiko for fukt i bakenforliggende/tilstøtende konstruksjoner.\\n\\nVurdering av avvik:\\n- Det er rundt sluk påvist en ikke-fagmessig utførelse av membran/tettesjikt/klemring.\\n\\nTiltak\\n- Overvåk konstruksjonen jevnlig. For å få tilstandsgrad 0 eller 1 må løsningen utbedres, men tidspunktet for når dette er nødvendig er vanskelig å si noe om.\\nKostnadsestimat : Ingen umiddelbar kostnad\\n\\nEtasje 1 > Bad\\nVentilasjon,TG2\\nDet er naturlig ventilering.\\nVurdering av avvik:\\n- Rommet har kun naturlig ventilasjon.\\n- Våtrommet mangler tilluftsventilering, f.eks. spalte/ventil ved dør.\\n\\nTiltak\\n- Det bør etableres tilfredsstillende tilluft til våtrom f.eks. luftespalte ved dør e.l.\\n- Elektrisk avtrekksvifte bør monteres for å lukke avviket.\\n\\nEtasje 1 > Vaskerom\\nGenerell,TG3\\nAktuell byggeforskrift er byggeforskrifter fra før 1997. Dokumentasjon: ingen dokumentasjon. Vaskerom er et ombygd soverom. Ingen dokumentert membran. Manglende oppkant ved dører og manglende fall mot sluk. TG:3 gis som en automatisk konsekvens av alder på våtrommet og at forskriftskrav ikke er oppfylt. Gjenværende brukstid er usikker grunnet alder og byggemåte. Bruken av rommet vil være avgjørende.\\n\\nVurdering av avvik:\\n- Våtrommet må oppgraderes for å tåle normal bruk etter dagens krav.\\n\\nTiltak\\n- Våtrommet må totalrenoveres. Alle forhold med tettesjikt, våtsone, sluk m.m. må dokumenteres.\\n- Eldre vaskerom som fungerer i dag til formålet, og kan fungere i mange år til. Det bemerkes at det er en risiko med eldre materialer og konstruksjoner, og konstruksjonen må overvåkes i forhold til funksjonssvikt. Det må påregnes utbedring av rommet.\\nKostnadsestimat : Ingen umiddelbar kostnad\\n\\nKjøkken\\nEtasje 1 > Kjøkken\\nOverflater og innredning,TG2\\n\\nInnredning med profilert fronter og laminat benkeplate med nedfelt oppvaskkum. Integrerte hvitevarer: kjøleskap, oppvaskmaskin, induksjonstopp, dampovn og stekeovn.\\nVurdering av avvik:\\n- Det er påvist at overflater har noe skader.\\n- Noe skader/ merker i gulv, og noe knirk i gulv.\\n\\nTiltak\\n- Det må foretas lokal utbedring.\\n- Overfladiske merker i gulv bør utbedres.\\nKostnadsestimat : Ingen umiddelbar kostnad\\n\\nSpesialrom\\nEtasje 1 > Toalettrom\\nOverflater og konstruksjon,TG3\\nToalett er frakoblet vann og ikke i bruk per dags dato. Avløp er av ukjent påkobling.\\nVurdering av avvik:\\n- Toalettrom har ingen ventilering fra rommet, NS 3600 krever mekanisk avtrekk for å kunne gi TG 0/1.\\n- Toalettrom mangler tilluftsventilering, f.eks. spalte/ventil ved dør.\\n\\nTiltak\\n- Mekanisk avtrekk bør etableres på toalettrom.\\n- Vann og avløp må undersøkes og tilkobles/ utbedres om rommet skal tas i bruk.\\nKostnadsestimat : 10 000 - 50 000\\n\\nTekniske installasjoner\\nVannledninger,TG2\\n\\nInnvendige vannledninger er av kobber med plastkappe. Alle vannrør er synlige.\\nVurdering av avvik:\\n- Det er påvist ufagmessig utførelse av vannledninger.\\n- Mer enn halvparten av forventet brukstid er passert på innvendige vannledninger.\\n\\nTiltak\\n- Anlegget bør sjekkes av fagperson.\\n- Det er ikke behov for utbedringstiltak siden anlegget fungerer i dag, men ut ifra alder kan skader plutselig oppstå på eldre anlegg.\\n\\nVentilasjon,TG2\\nBoligen har naturlig ventilasjon.\\nVurdering av avvik:\\n- Det er påvist mangelfull ventilasjon på ett eller flere rom i boligen.\\n\\nTiltak\\n- Det bør etableres veggventiler/vindusventiler i alle oppholdsrom som ikke har det.\\n\\nVarmtvannstank,TG2\\nVarmtvannstanken er på ca. 200 liter. Kan med fordel direkte kobles og ikke være tilkoblet stikkontakt.\\nVurdering av avvik:\\n- Det er ikke påvist tilfredsstillende el-tilkobling av varmtvannstank iht. gjeldende forskrift.\\n- Det er påvist at varmtvannstank er over 20 år\\n- Det var vanlig å koble varmtvannsbereder i stikkontakt når denne varmtvannsberederen ble tilkoblet. I dag er det krav om at nye varmtvannsberedere med effekt på 1500 W eller mer skal være fast tilkoblet. Kravet ble opprinnelig innført i 2010 og gjaldt da varmtvannsberedere med effekt over 2000 watt, men ble endret til 1500 watt i 2014. Som for det meste annet av regelverk, har ikke kravene tilbakevirkende kraft. Dersom du har en bereder som ikke tilfredsstiller dagens krav, men ble montert før de ble innført, må du altså ikke foreta deg noe med det elektriske anlegget.\\n\\nTiltak\\n- Det bør etableres tilfredsstillende el-tilkobling etter gjeldende forskrift.\\n- Det er ikke behov for utbedringstiltak siden tanken fungerer i dag, men ut ifra alder kan skader plutselig oppstå på eldre tanker.\\n- Du er ikke pålagt å bygge om anlegget. Det er viktig at man jevnlig tar ut støpselet og ser etter varmegang. Det anbefales å bruke fast tilkobling.\\nElektrisk anlegg,TG3\\nElektrisk anlegg i det opprinnelige våningshuset fra ca. 1930. Det elektriske er veldig begrenset kontrollert i forbindelse med rapporten da dette krever Nemko godkjent kompetanse. På generelt grunnlag anbefales det kontroll av el-anlegget av autorisert firma.\\n21.Foreligger det eltilsynsrapport de siste 5 år, og det er ikke foretatt arbeid på anlegget etter denne, utenom retting av eventuelle avvik i eltilsynsrapport (dvs en el-tilsynsrapport uten avvik)?\\nNei\\nKommentar:\\n1.Når ble det elektriske anlegget installert eller sist gang totalt rehabilitert (årstall)? Med totalt rehabilitert menes fullstendig utskiftet anlegg fra inntakssikring og videre.\\nKommentar: Det er to el-anlegg og sikringsskap i huset. Ny del fra byggeår 1981. Gammel del fra byggeår ca. 1930.\\n2.Er alle elektriske arbeider/anlegg i boligen utført av en registrert elektroinstallasjonsvirksomhet?\\nNei\\nKommentar:\\n3.Er det elektriske anlegget utført eller er det foretatt tilleggsarbeider på det elektriske anlegget etter 1.1.1999?\\nJa\\nKommentar: Kjeller i \"ny del\" av bolig har blitt renovert i forbindelse med vann i kjeller. Deler av el-anlegget er byttet ut av elektriker.\\n5.Foreligger det kontrollrapport fra offentlig myndighet - Det Lokale Eltilsyn (DLE) eller eventuelt andre tilsvarende kontrollinstanser med avvik som ikke er utbedret eller kontrollen er over 5 år?\\nNei\\nKommentar:\\n6.Forekommer det ofte at sikringene løses ut?\\nNei\\nKommentar:\\n7.Har det vært brann, branntilløp eller varmgang (for eksempel termiske skader på deksler, kontaktpunkter e', 'What are the top 20 AI players whose stock price is under $ 30?', 'Analyze and compare Shopify, Wix, Big commerce and Squarespace. Which stock is best to own from risk rewrad perspective ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 34951/35267 [26:22<00:14, 22.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 40455 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Failed ['PUEDES AGREGAR A ESTE INDICADOR  UN  VIX OSCILADOR SMOOTH /@version=5 \\nindicator(\"VIX Oscillator Smooth\", shorttitle=\"VIX Osc Sm\", overlay=false) \\n\\n// User Inputs \\ntimeframe = input.timeframe(\"\", title=\"Period Timeframe\")  \\nAvgLength = input.int(14, title=\"Average Length\") \\nSmoothLength = input.int(3, title=\"Smooth Length\") \\n\\n// Securities \\nticker = ticker.new(syminfo.prefix, syminfo.ticker, session.regular) \\nvix = request.security(\"CBOE:VIX\", timeframe, close)  \\ncl = request.security(ticker, timeframe, close, lookahead=barmerge.lookahead_on) \\n\\n// Z-Score Calculation \\n\\n// Z Score VIX  \\nvixhx = ta.sma(vix, AvgLength)  \\nvixsd = ta.stdev(vix, AvgLength)  \\nz_raw = (vix - vixhx) / vixsd  \\nz = ta.sma(z_raw, SmoothLength) // Smoothed Z Score VIX line \\n\\n// Z Score Stock \\nstockhx = ta.sma(cl, AvgLength)  \\nstocksd = ta.stdev(cl, AvgLength) \\nzs_raw = (cl - stockhx) / stocksd  \\nzs = ta.sma(zs_raw, SmoothLength) // Smoothed Z Score Stock line \\n\\n// User Inputs for background levels and colors \\nupper_level = input.float(2.5, title=\"Upper Level for Hline Background\", step=0.1) \\nupper_color = input.color(color.gray, title=\"Color for Upper Level Background\") \\nlower_level = input.float(-2.5, title=\"Lower Level for Hline Background\", step=0.1) \\nlower_color = input.color(#ff525275, title=\"Color for Lower Level Background\") \\nzero_level = input.float(0, title=\"Zero Level for Hline Background\", step=0.1) \\nzero_color = input.color(#2195f38a, title=\"Color for Zero Level Background\") \\n\\n// Background color \\nbgcolor_condition = (z >= upper_level and zs <= lower_level) or (z <= lower_level and zs >= upper_level) ? upper_color :  \\n                   (z >= zero_level and zs <= zero_level) or (z <= zero_level and zs >= zero_level) ? zero_color : \\n                   lower_color \\nbgcolor(bgcolor_condition) \\n\\n// Plots \\nplot(z, \"VIX Smooth\", color=color.purple, linewidth=3) // Cambiado a violeta\\nplot(zs, \"Ticker Smooth\", color=color.blue, linewidth=3) // Cambiado a azul\\n\\n// Horizontal lines for levels \\nhline(upper_level, \"Upper Level\", color=color.green)\\nhline(zero_level, \"Zero Level\", color=color.gray)  \\nhline(lower_level, \"Lower Level\", color=color.red) \\n\\n// Draw horizontal lines at upper and zero levels and store their IDs\\nhline_upper = hline(upper_level, \"Upper Level\", color=color.green) \\nhline_zero = hline(zero_level, \"Zero Level\", color=color.gray) \\nhline_lower = hline(lower_level, \"Upper Level\", color=#d61f1f) \\n\\n// Fill the area between zero level and upper level\\nfill(hline_zero, hline_upper, color=#4caf4f69)\\n// Fill the area between zero level and lower level\\nfill(hline_zero, hline_lower, color=#af4c4c69)\\n \\n// Alert conditions for hline background \\nalertcondition(z >= upper_level and zs <= lower_level, title=\"Upper Level Background Alert\", message=\"VIX is at upper level and Ticker is at lower level\") \\nalertcondition(z <= lower_level and zs >= upper_level, title=\"Lower Level Background Alert\", message=\"VIX is at lower level and Ticker is at upper level\") \\n \\n// Signal for crossover at level 0 \\ncrossover_signal = ta.crossover(z, zs) and zs[1] < 0 or ta.crossunder(z, zs) and zs[1] > 0 \\nplotshape(series=crossover_signal, title=\"Crossover Signal\", location=location.absolute, style=shape.circle, color=#ffae0083, size=size.tiny, offset=-AvgLength/2)\\n', 'PUEDES AGREGAR A ESTE INDICADOR UN VIX OSCILADOR SMOOTH DANDOME LA OPCION PARA CONFIGURARLO //@version=5\\nindicator(\\'Matrix Series and Vix Fix with VWAP CCI and QQE Signals\\', shorttitle=\\'Matrix\\', precision=2)\\n\\n// Function to select the type of source\\nget_src(Type) =>\\n    switch Type \\n        \"VWAP\" => ta.vwap\\n        \"Close\" => close\\n        \"Open\" => open\\n        \"HL2\" => hl2\\n        \"HLC3\" => hlc3\\n        \"OHLC4\" => ohlc4\\n        \"HLCC4\" => hlcc4\\n        \"High\" => high\\n        \"Low\" => low\\n        \"TR\" => ta.tr\\n        \"vwap(Close)\" => ta.vwap(close)\\n        \"vwap(Open)\" => ta.vwap(open)\\n        \"vwap(High)\" => ta.vwap(high)\\n        \"vwap(Low)\" => ta.vwap(low)\\n        \"AVG(vwap(H,L))\" => math.avg(ta.vwap(high), ta.vwap(low))\\n        \"AVG(vwap(O,C))\" => math.avg(ta.vwap(open), ta.vwap(close))        \\n//__________________________________________________________________\\n// Based on \"Matrix Series\" - Author: @glaz\\n// https://www.tradingview.com/script/2X2cVLhb-Matrix-Series/\\n// https://www.wisestocktrader.com/indicators/2739-flower-indicator\\n//__________________________________________________________________\\nalert_Buy_Matrix  = input.bool(false, \"? Alert: [Buy - Matrix]\",  inline=\"alertMatrix\")\\nalert_Sell_Matrix = input.bool(false, \"? Alert: [Sell - Matrix]\", inline=\"alertMatrix\")\\nshow_buy_signal   = input.bool(true,  \"Show buy signal\",  inline=\"show_signal\")\\nshow_sell_signal  = input.bool(true,  \"Show sell signal\", inline=\"show_signal\")\\nshow_dot          = input.bool(true,  \"Show Watch/Warning Point\", inline=\"show_signal\")\\n//--- Trend Bought/Sold Detail\\ndynamic      = input.bool(true,  \"Show Dynamic Zones ??? \", inline=\"show_limit\")\\nOBOS         = input.bool(false, \"Show OB/OS\", inline=\"show_limit\")\\nshow_candles = input.bool(true,  \"Show Matrix Candles  ??? \", inline=\"s_matrix\")\\nshow_hist    = input.bool(false, \"Show Matrix Histogram\", inline=\"s_matrix\")\\n//--- Sup/Res Detail\\nPricePeriod      = input.int(16,  \"Price Period\", 1, inline=\"period\")\\nSmoother         = input.int(5,   \"Smoother\", 2, inline=\"period\")\\nSupResPeriod     = input.int(50,  \"Superior Resolution Period\", 1, inline=\"Superior Resolution\")\\nSupResPercentage = input.int(100, \"Percentage\", 1, inline=\"Superior Resolution\")\\n//--- Line Detail \\nob = input.int(200,  \"OverBought Above\", inline=\"line\")\\nos = input.int(-200, \"OverSold Bellow\",  inline=\"line\")\\n\\nys1  = (high + low + close * 2) / 4\\nrk3  = ta.ema(ys1, Smoother)\\nrk4  = ta.stdev(ys1, Smoother)\\nrk5  = (ys1 - rk3) * 200 / rk4\\nrk6  = ta.ema(rk5, Smoother)\\nup   = ta.ema(rk6, Smoother)\\ndown = ta.ema(up, Smoother)\\nOo   = up < down ? up : down\\nHh   = Oo\\nLl   = up < down ? down : up\\nCc   = Ll\\nsell_matrix = ta.cross(up, ob) == 1 and up[1] > up \\nbuy_matrix  = ta.cross(up, os) == 1 and up[1] < up \\n\\ncoral     = #FF8080\\nlavender  = #8080FF\\namber     = color.new(#FFE500, 60)\\nvcolor = show_candles and (Oo > Cc) ? coral : show_candles and (up > down) ? lavender : show_candles ? coral : na\\n\\nplotcandle(Oo, Hh, Ll, Cc, \"Matrix Candles\", color = vcolor, wickcolor = vcolor, bordercolor = vcolor)\\nplotshape(show_sell_signal and sell_matrix, \"Sell\", shape.triangledown, location.top, coral)\\nbgcolor(show_sell_signal and sell_matrix ? color.new(coral, 70) : na, title=\"Sell\")\\nplotshape(show_buy_signal and buy_matrix, \"Buy\", shape.triangleup, location.bottom, lavender)\\nbgcolor(show_buy_signal and buy_matrix ? color.new(lavender, 70) : na, title=\"Buy\")\\n\\n//-------S/R Zones------\\nLookback = SupResPeriod\\nPerCent  = SupResPercentage\\n\\nValue1 = ta.cci(close, PricePeriod)\\nValue2 = ta.highest(Value1, Lookback)\\nValue3 = ta.lowest(Value1, Lookback)\\nValue4 = Value2 - Value3\\nValue5 = Value4 * (PerCent / 100)\\nResistanceLine = Value3 + Value5\\nSupportLine    = Value2 - Value5\\nplot(dynamic ? ResistanceLine : na, \"Resistance Line\", color.new(coral, 5))\\nplot(dynamic ? ResistanceLine : na, \"Resistance Line\", color.new(coral, 90), 7)\\nplot(dynamic ? SupportLine    : na, \"Support Line\",    color.new(lavender, 5))\\nplot(dynamic ? SupportLine    : na, \"Support Line\",    color.new(lavender, 90), 7)\\n\\n//--Overbought/Oversold/Warning Detail\\nh01 = ta.highest(up, 1)   + 20\\nh02 = ta.highest(down, 1) + 20\\nl01 = ta.lowest(down, 1)  - 20\\nl02 = ta.lowest(up, 1)    - 20\\nUPshape   = up   > ob and up > down ? h01 : up   > ob and up < down ? h02 : na\\nDOWNshape = down < os and up > down ? l01 : down < os and up < down ? l02 : na\\n\\nplot(show_dot ? UPshape   : na, \"UP Shape\",   amber, 4, plot.style_circles)\\nplot(show_dot ? DOWNshape : na, \"DOWN Shape\", amber, 4, plot.style_circles)\\nhline(OBOS ? ob : na, \"OverBought\")\\nhline(OBOS ? os : na, \"OverSold\")\\n\\nAccumulationColor = color.new(#FFA07A, 60)     //\"Accumulation Zone\"\\nWarningColor      = color.new(color.white, 60) //\"Warning/Watch Signal\"\\nDistributionColor = color.new(#3CB371, 60)     //\"Distribution Zone\"\\n\\nrk5_210  = ( ys1 - rk3 ) * 210 / rk4\\nrk6_210  = ta.ema(rk5_210, Smoother)\\nUP_210   = ta.ema(rk6_210, Smoother)\\nDOWN_210 = ta.ema(UP_210, Smoother)\\nUPColor  = (UP_210 >= 210 ? DistributionColor : (UP_210 <= -210 ? AccumulationColor : WarningColor))\\nplot(show_hist ? UP_210   : na, \"UP\",         UPColor, 1, plot.style_columns)\\nplot(show_hist ? DOWN_210 : na, \"Signal(UP)\", color.aqua)\\n\\nif alert_Buy_Matrix and buy_matrix\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Buy [Matrix Series].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Sell_Matrix and sell_matrix\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Sell [Matrix Series].-\\', alert.freq_once_per_bar_close)\\n\\n//_____________________________________________________________________________________________\\n// Based on \"Williams Vix Fix\" - Author: @ChrisMoody\\n// https://www.tradingview.com/script/pJpXG5JH-CM-Williams-Vix-Fix-V3-Ultimate-Filtered-Alerts/\\n// https://www.ireallytrade.com/newsletters/VIXFix.pdf\\n//_____________________________________________________________________________________________\\nshows_Vix_Fix = input(false, \\'Show Vix Fix Histogram ???\\', inline=\"histVF\", group=\\'Williams Vix Fix\\')\\nswvfm    = input.float(1.0, \"Multiplier (Values 0.5 to 2)\", minval = 0.5, maxval = 2, inline=\"histVF\", group=\\'Williams Vix Fix\\')\\nsbcFilt  = input.bool(true, \\'Show Signal For Filtered Entry [? FE ]\\', group=\\'Williams Vix Fix\\') // Use FILTERED Criteria\\nsbcAggr  = input.bool(true, \\'Show Signal For AGGRESSIVE Filtered Entry [? AE ]\\', group=\\'Williams Vix Fix\\') // Use FILTERED Criteria\\nalert_AE = input.bool(false, \\'? Alert: [AGGRESSIVE Entry]\\', inline=\\'AlertVF\\', group=\\'Williams Vix Fix\\')\\nalert_FE = input.bool(false, \\'? Alert: [Filtered Entry]\\', inline=\\'AlertVF\\', group=\\'Williams Vix Fix\\')\\npd       = input.int(22, \\'LookBack Period Standard Deviation High\\', 1, group=\\'Williams Vix Fix\\')\\nbbl      = input.int(20, \\'Bolinger Band Length\\', 1, group=\\'Williams Vix Fix\\')\\nmult     = input.float(2.0, \\'Bollinger Band Standard Devaition Up\\', minval=1, maxval=5, group=\\'Williams Vix Fix\\')\\nlb       = input.int(50, \\'Look Back Period Percentile High\\', 1, group=\\'Williams Vix Fix\\')\\nph       = input.float(.85, \\'Highest Percentile - 0.90=90%, 0.95=95%, 0.99=99%\\', minval=.05, maxval=1, group=\\'Williams Vix Fix\\')\\nltLB     = input.int(40, minval=25, maxval=99, title=\\'Long-Term Look Back Current Bar Has To Close Below This Value OR Medium Term--Default=40\\', group=\\'Williams Vix Fix\\')\\nmtLB     = input.int(14, minval=10, maxval=20, title=\\'Medium-Term Look Back Current Bar Has To Close Below This Value OR Long Term--Default=14\\', group=\\'Williams Vix Fix\\')\\nstr      = input.int(3,  minval=1,  maxval=9,  title=\\'Entry Price Action Strength--Close > X Bars Back---Default=3\\', group=\\'Williams Vix Fix\\')\\n\\n// Williams Vix Fix Formula\\nwvf       = (ta.highest(close, pd) - low) / ta.highest(close, pd) * 100\\nsDev      = mult * ta.stdev(wvf, bbl)\\nmidLine   = ta.sma(wvf, bbl)\\nlowerBand = midLine - sDev\\nupperBand = midLine + sDev\\nrangeHigh = ta.highest(wvf, lb) * ph\\n\\n// Filtered Criteria\\nupRange      = low > low[1] and close > high[1]\\nupRange_Aggr = close > close[1] and close > open[1]\\n// Filtered Criteria\\nfiltered      = (wvf[1] >= upperBand[1] or wvf[1] >= rangeHigh[1]) and wvf < upperBand and wvf < rangeHigh\\nfiltered_Aggr = (wvf[1] >= upperBand[1] or wvf[1] >= rangeHigh[1]) and not(wvf < upperBand and wvf < rangeHigh)\\n\\n// Alerts Criteria\\nalert1  = wvf >= upperBand or wvf >= rangeHigh ? 1 : 0\\nalert2  = (wvf[1] >= upperBand[1] or wvf[1] >= rangeHigh[1]) and wvf < upperBand and wvf < rangeHigh ? 1 : 0\\ncond_FE = upRange and close > close[str] and (close < close[ltLB] or close < close[mtLB]) and filtered \\nalert3  = cond_FE ? 1 : 0\\ncond_AE = upRange_Aggr and close > close[str] and (close < close[ltLB] or close < close[mtLB]) and filtered_Aggr\\nalert4  = cond_AE ? 1 : 0\\n\\ncolor_VF = alert1 ? color.new(color.lime, 60) : alert2 ? color.new(color.teal, 60) : color.new(color.silver, 80)\\nswvfm_precent = swvfm * (ResistanceLine - SupportLine) / (2.2 * rangeHigh)\\n\\nplot(shows_Vix_Fix ? wvf * -1 * swvfm_precent : na, \\'Williams Vix Fix\\', color_VF, 4, plot.style_columns)\\n\\nplotshape(sbcAggr and alert4 ? alert4 : na, \\'Aggressive Entry\\', shape.triangleup, location.bottom, color.new(#80FF00, 0), text=\\'AE\\', textcolor=color.new(#80FF00, 0))\\nbgcolor(sbcAggr and alert4 ? color.new(#80FF00, 80) : na, title=\\'Aggressive Entry\\')\\nplotshape(sbcFilt and alert3 ? alert3 : na, \\'Filtered Entry\\', shape.triangleup, location.bottom, color.new(#15FF00, 0), text=\\'FE\\', textcolor=color.new(#15FF00, 0))\\nbgcolor(sbcFilt and alert3 ? color.new(#15FF00, 80) : na, title=\\'Filtered Entry\\')\\n\\nif alert_AE and cond_AE\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Aggressive Entry [VixFix].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_FE and cond_FE\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Filtered Entry [VixFix].-\\', alert.freq_once_per_bar_close)\\n\\n//_____________________________________________________________\\n// Based on \"QQE signals\" - Author: @colinmck\\n// https://www.tradingview.com/script/7R6ZxyZu-QQE-signals/\\n//_____________________________________________________________\\nshows_QQE       = input.bool(false, \"????????? Show QQE Signals ?????????\")\\nalert_Long_QQE  = input.bool(false, \"? Alert: [Long - QQE]\",  inline=\"alertQQE\")\\nalert_Short_QQE = input.bool(false, \"? Alert: [Short - QQE]\", inline=\"alertQQE\")\\nRSI_Period = input.int(14,      \"RSI Length\", 1)\\nSF         = input.int(5,       \"RSI Smoothing\", 2)\\nQQE        = input.float(4.238, \"Fast QQE Factor\", 1)\\n\\nWilders_Period = RSI_Period * 2 - 1\\nlongband = 0.0\\nshortband = 0.0\\ntrend = 0\\n\\nRsi      = ta.rsi(close, RSI_Period)\\nRSIndex  = ta.ema(Rsi, SF)\\nAtrRsi   = math.abs(RSIndex[1] - RSIndex)\\nMaAtrRsi = ta.ema(AtrRsi, Wilders_Period)\\n\\nDeltaFastAtrRsi = ta.ema(MaAtrRsi, Wilders_Period) * QQE\\nnewshortband = RSIndex + DeltaFastAtrRsi\\nnewlongband = RSIndex - DeltaFastAtrRsi\\nlongband := RSIndex[1] > longband[1] and RSIndex > longband[1] ? math.max(longband[1], newlongband) : newlongband\\nshortband := RSIndex[1] < shortband[1] and RSIndex < shortband[1] ? math.min(shortband[1], newshortband) : newshortband\\ncross_1 = ta.cross(longband[1], RSIndex)\\ntrend := ta.cross(RSIndex, shortband[1]) ? 1 : cross_1 ? -1 : nz(trend[1], 1)\\nFastAtrRsiTL = trend == 1 ? longband : shortband\\n\\n// Find all the QQE Crosses\\nQQExlong = 0\\nQQExlong := nz(QQExlong[1])\\nQQExshort = 0\\nQQExshort := nz(QQExshort[1])\\nQQExlong := FastAtrRsiTL < RSIndex ? QQExlong + 1 : 0\\nQQExshort := FastAtrRsiTL > RSIndex ? QQExshort + 1 : 0\\n\\n// Conditions\\nqqeLong = QQExlong == 1 ? FastAtrRsiTL[1] - 50 : na\\nqqeShort = QQExshort == 1 ? FastAtrRsiTL[1] - 50 : na\\n\\n// Plotting\\nredorange  = color.new(#FF4000, 0)\\nchartreuse = color.new(#80FF00, 0)\\nplotshape(shows_QQE and qqeLong,  \"QQE Long\",  shape.labelup,   location.bottom, color.new(chartreuse, 25), text=\"QQE\", size=size.tiny, textcolor=color.new(#000000, 5))\\nplotshape(shows_QQE and qqeShort, \"QQE Short\", shape.labeldown, location.top,    color.new(redorange, 25),  text=\"QQE\", size=size.tiny, textcolor=color.new(color.white, 5))\\n\\nif alert_Long_QQE  and qqeLong\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Long [Quantitative Qualitative Estimation (QQE)].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Short_QQE and qqeShort\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Short [Quantitative Qualitative Estimation (QQE)].-\\', alert.freq_once_per_bar_close)\\n\\n//___________________________________________________________________________\\n// VWAP CCI Signals\\n//___________________________________________________________________________\\nshows_VCCI = input.bool(false, \"????????? Show VWAP CCI Signals ?????????\")\\nlen_cci    = input.int(21, \"Length\", 1, inline=\"cci\")\\nType_cci   = input.string(\"vwap(Close)\", \"Source\", inline=\"cci\", options=[\"vwap(Close)\", \"vwap(Open)\", \"vwap(High)\", \"vwap(Low)\", \"AVG(vwap(H,L))\", \"AVG(vwap(O,C))\", \"VWAP\", \"Close\", \"Open\", \"HL2\", \"HLC3\", \"OHLC4\", \"HLCC4\", \"High\", \"Low\", \"TR\"])\\nalert_Long_cci  = input.bool(false, \"? Alert: [Long - CCI]\",  inline=\"alertCCI\")\\nalert_Short_cci = input.bool(false, \"? Alert: [Short - CCI]\", inline=\"alertCCI\")\\n\\n// ALMA - Arnaud Legoux Moving Average of @kurtsmock\\nenhanced_alma(_series, _length, _offset, _sigma) =>\\n    length      = int(_length) // Floating point protection\\n    numerator   = 0.0\\n    denominator = 0.0 \\n    m = _offset * (length - 1)\\n    s = length / _sigma\\n    for i=0 to length-1\\n        weight       = math.exp(-((i-m)*(i-m)) / (2 * s * s))\\n        numerator   := numerator   + weight * _series[length - 1 - i]\\n        denominator := denominator + weight\\n    numerator / denominator\\n\\n// Calculate vwap cci\\nsrc_cci = get_src(Type_cci)\\ncci = (src_cci - enhanced_alma(src_cci, len_cci, 0.85, 6.0)) / (0.015 * ta.dev(src_cci, len_cci))\\n\\n// Define long and short entry signal\\nlong_cci  = ta.crossover(cci, -200)\\nshort_cci = ta.crossunder(cci, 200)\\n\\nplotshape(shows_VCCI and long_cci,  \"VWAP CCI Long\",  shape.labelup,   location.bottom, color.new(chartreuse, 25), text=\"CCI\", size=size.tiny, textcolor=color.new(#000000, 5))\\nplotshape(shows_VCCI and short_cci, \"VWAP CCI Short\", shape.labeldown, location.top,    color.new(redorange, 25),  text=\"CCI\", size=size.tiny, textcolor=color.new(color.white, 5))\\n\\nif alert_Long_cci and long_cci\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Long [Commodity Channel Index (CCI)].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Short_cci and short_cci\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Short [Commodity Channel Index (CCI)].-\\', alert.freq_once_per_bar_close)\\n\\n//___________________________________________________________________________________________\\n// Based on \"Squeeze Momentum Indicator\" - Author: @LazyBear\\n// @KivancOzbilgic: https://www.tradingview.com/script/NVzKGYFJ/\\n// @capissimo: https://www.tradingview.com/script/Ejx2tEHY-Squeeze-Momentum-Indicator-mod-3/\\n// Indicator description: http://www.forextrading-pips.com/squeeze-indicator/\\n//___________________________________________________________________________________________\\nshow_SMI   = input.bool(false,     \"Show Squeeze Momentum Indicator\", group = \"Squeeze Momentum Indicator\")\\nalert_Buy  = input.bool(false,     \"? Alert: [Buy - Mom]\",  inline = \"alertMom\", group = \"Squeeze Momentum Indicator\")\\nalert_Sell = input.bool(false,     \"? Alert: [Sell - Mom]\", inline = \"alertMom\", group = \"Squeeze Momentum Indicator\")\\nType_src   = input.string(\"Close\", \"Source\", inline = \"s1\", group = \"Squeeze Momentum Indicator\", options = [\"VWAP\", \"Close\", \"Open\", \"HL2\", \"HLC3\", \"OHLC4\", \"HLCC4\", \"High\", \"Low\", \"vwap(Close)\", \"vwap(Open)\", \"vwap(High)\", \"vwap(Low)\", \"AVG(vwap(H,L))\", \"AVG(vwap(O,C))\", \"TR\"])\\nsrc0       = get_src(Type_src)\\nshow_sb    = input.bool(false, \"Show buy/sell signal\", inline = \"s1\", group = \"Squeeze Momentum Indicator\")\\nlengthMom  = input.int(20,     \"Momentum Length\", 1, inline = \"mom\", group = \"Squeeze Momentum Indicator\")\\nsiglen     = input.int(5,      \"Signal\", 2, inline = \"mom\", group = \"Squeeze Momentum Indicator\")\\nlengthSQZ  = input.int(20,     \"KC/BB Length\", 1, inline = \"sqz\", group = \"Squeeze Momentum Indicator\")\\nmultBB     = input.float(2.0,  \"BB MultFactor\", 0.5, step=0.05, inline = \"sqz\", group = \"Squeeze Momentum Indicator\")\\nmultKC     = input.float(1.3,  \"KC MultFactor\", 0.5, step=0.05, inline = \"kc\", group = \"Squeeze Momentum Indicator\")\\nuse_tr     = input.bool(true,  \"Use TrueRange (KC)\", inline = \"kc\", group = \"Squeeze Momentum Indicator\")\\n\\n// Calculate BB\\nbasis   = ta.sma(src0, lengthSQZ)\\ndev     = ta.stdev(src0, lengthSQZ)  \\nupperBB = basis + multBB * dev \\nlowerBB = basis - multBB * dev  \\n\\n// Calculate KC\\nmaKC    = ta.sma(src0, lengthSQZ)\\nrange_1 = use_tr ? ta.tr : high - low\\nrangema = ta.sma(range_1, lengthSQZ)\\nupperKC = maKC + rangema * multKC\\nlowerKC = maKC - rangema * multKC\\n\\n// When both the upper and lower Bollinger Bands go inside the Keltner Channel, the squeeze is on.\\n// When the Bollinger Bands (BOTH lines) start to come out of the Keltner Channel, the squeeze has been released (off). \\n// When one of the Bollinger Bands is out of Keltner Channel, no highlighting is done.\\nsqzOn  = lowerBB > lowerKC  and upperBB < upperKC\\nsqzOff = lowerBB <= lowerKC and upperBB >= upperKC\\nnoSqz  = sqzOn == false and sqzOff == false\\n\\nmom = ta.linreg(src0 - math.avg(math.avg(ta.highest(high, lengthMom), ta.lowest(low, lengthMom)), ta.sma(close, lengthMom)), lengthMom, 0)\\nsig = ta.sma(mom, siglen)\\n\\nlongSM  = ta.crossover(mom, sig)\\nshortSM = ta.crossunder(mom, sig)\\nsqz_pos = (sqzOff and mom >= sig)\\nsqz_neg = (sqzOff and mom <  sig)\\n\\nplotshape(show_sb and longSM,  \"Long Momentum\",  shape.triangleup,   location.bottom, #05FFA6, text=\\'M\\', textcolor=#05FFA6)\\nplotshape(show_sb and shortSM, \"Short Momentum\", shape.triangledown, location.top,    #FF0AE2, text=\\'M\\', textcolor=#FF0AE2)\\nplotshape(show_SMI and (sqzOn  or  noSqz) ? true : na, \"In Squeeze\", shape.circle, location.top, color.silver)\\nplotshape(show_SMI and sqz_pos ? true : na, \"Squeeze Release\", shape.triangleup,   location.top, #05FFA6)\\nplotshape(show_SMI and sqz_neg ? true : na, \"Squeeze Release\", shape.triangledown, location.top, #FF0AE2)\\n\\nif alert_Buy and longSM\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Buy [Momentum].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Sell and shortSM\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Sell [Momentum].-\\', alert.freq_once_per_bar_close)\\n\\nsqueeze_ON = \\'{\"content\": \"@everyone\" ,\"embeds\": [{\"title\": \"{{ticker}} \",\"description\": \"In Squeeze | Price ${{close}}\", \"color\": \"2092045\",\"author\": {\"name\":null},\"timestamp\": \"{{timenow}}\"}],\"username\": \"Alertbot\"}\\'\\nsqueeze_Long = \\'{\"content\": \"@everyone\" ,\"embeds\": [{\"title\": \"{{ticker}} \",\"description\": \"Squeeze Release Long | Price ${{close}}\", \"color\": \"2092045\",\"author\": {\"name\":null},\"timestamp\": \"{{timenow}}\"}],\"username\": \"Alertbot\"}\\'\\nsqueeze_Short = \\'{\"content\": \"@everyone\" ,\"embeds\": [{\"title\": \"{{ticker}} \",\"description\": \"Squeeze Release Short | Price ${{close}}\", \"color\": \"2092045\",\"author\": {\"name\":null},\"timestamp\": \"{{timenow}}\"}],\"username\": \"Alertbot\"}\\'\\n\\nalertcondition(sqzOn or noSqz, title=\"In Squeeze\", message=squeeze_ON)\\nalertcondition(sqz_pos, title=\"Squeeze Release | Long\",  message=squeeze_Long)\\nalertcondition(sqz_neg, title=\"Squeeze Release | Short\", message=squeeze_Short)', 'ESTA BIEN EL INDICADOR PERO AUN NO ME HAS AGREGADO EL VIX OSCILADOR SMOOTH QUE TENGA OPCION PARA CONFIGURARLO SO  RECONSIDERA', \"CORRIGE ESTE ERROR Y TODOS LOS EXISTENTES Mismatched input 'end of line without line continuation' expecting ')'\", \"CORREGIR ESTE ERROR Y TODOS LOS EXISTENTES mismatched character '\\\\n' expecting '''\", 'QUIERO ME CONBINES ESTOS DOS INDICADORES EN UNO  DE LA MEJOR FORMA TE SEA POSIBLE  AQUI PARTES IMPORTANTES TENER MUY EN CUENTA Y SEAN BISIBLES ES CANDELS MATRX , BUY ,SELL  Y EL VIX  OSCILADOR SMOOTH CON SUS RESPECTIVAS CONFIGURACIONES //@version=5\\nindicator(\\'Matrix Series and Vix Fix with VWAP CCI and QQE Signals\\', shorttitle=\\'Matrix\\', precision=2)\\n\\n// Function to select the type of source\\nget_src(Type) =>\\n    switch Type \\n        \"VWAP\" => ta.vwap\\n        \"Close\" => close\\n        \"Open\" => open\\n        \"HL2\" => hl2\\n        \"HLC3\" => hlc3\\n        \"OHLC4\" => ohlc4\\n        \"HLCC4\" => hlcc4\\n        \"High\" => high\\n        \"Low\" => low\\n        \"TR\" => ta.tr\\n        \"vwap(Close)\" => ta.vwap(close)\\n        \"vwap(Open)\" => ta.vwap(open)\\n        \"vwap(High)\" => ta.vwap(high)\\n        \"vwap(Low)\" => ta.vwap(low)\\n        \"AVG(vwap(H,L))\" => math.avg(ta.vwap(high), ta.vwap(low))\\n        \"AVG(vwap(O,C))\" => math.avg(ta.vwap(open), ta.vwap(close))        \\n//__________________________________________________________________\\n// Based on \"Matrix Series\" - Author: @glaz\\n// https://www.tradingview.com/script/2X2cVLhb-Matrix-Series/\\n// https://www.wisestocktrader.com/indicators/2739-flower-indicator\\n//__________________________________________________________________\\nalert_Buy_Matrix  = input.bool(false, \"? Alert: [Buy - Matrix]\",  inline=\"alertMatrix\")\\nalert_Sell_Matrix = input.bool(false, \"? Alert: [Sell - Matrix]\", inline=\"alertMatrix\")\\nshow_buy_signal   = input.bool(true,  \"Show buy signal\",  inline=\"show_signal\")\\nshow_sell_signal  = input.bool(true,  \"Show sell signal\", inline=\"show_signal\")\\nshow_dot          = input.bool(true,  \"Show Watch/Warning Point\", inline=\"show_signal\")\\n//--- Trend Bought/Sold Detail\\ndynamic      = input.bool(true,  \"Show Dynamic Zones ??? \", inline=\"show_limit\")\\nOBOS         = input.bool(false, \"Show OB/OS\", inline=\"show_limit\")\\nshow_candles = input.bool(true,  \"Show Matrix Candles  ??? \", inline=\"s_matrix\")\\nshow_hist    = input.bool(false, \"Show Matrix Histogram\", inline=\"s_matrix\")\\n//--- Sup/Res Detail\\nPricePeriod      = input.int(16,  \"Price Period\", 1, inline=\"period\")\\nSmoother         = input.int(5,   \"Smoother\", 2, inline=\"period\")\\nSupResPeriod     = input.int(50,  \"Superior Resolution Period\", 1, inline=\"Superior Resolution\")\\nSupResPercentage = input.int(100, \"Percentage\", 1, inline=\"Superior Resolution\")\\n//--- Line Detail \\nob = input.int(200,  \"OverBought Above\", inline=\"line\")\\nos = input.int(-200, \"OverSold Bellow\",  inline=\"line\")\\n\\nys1  = (high + low + close * 2) / 4\\nrk3  = ta.ema(ys1, Smoother)\\nrk4  = ta.stdev(ys1, Smoother)\\nrk5  = (ys1 - rk3) * 200 / rk4\\nrk6  = ta.ema(rk5, Smoother)\\nup   = ta.ema(rk6, Smoother)\\ndown = ta.ema(up, Smoother)\\nOo   = up < down ? up : down\\nHh   = Oo\\nLl   = up < down ? down : up\\nCc   = Ll\\nsell_matrix = ta.cross(up, ob) == 1 and up[1] > up \\nbuy_matrix  = ta.cross(up, os) == 1 and up[1] < up \\n\\ncoral     = #FF8080\\nlavender  = #8080FF\\namber     = color.new(#FFE500, 60)\\nvcolor = show_candles and (Oo > Cc) ? coral : show_candles and (up > down) ? lavender : show_candles ? coral : na\\n\\nplotcandle(Oo, Hh, Ll, Cc, \"Matrix Candles\", color = vcolor, wickcolor = vcolor, bordercolor = vcolor)\\nplotshape(show_sell_signal and sell_matrix, \"Sell\", shape.triangledown, location.top, coral)\\nbgcolor(show_sell_signal and sell_matrix ? color.new(coral, 70) : na, title=\"Sell\")\\nplotshape(show_buy_signal and buy_matrix, \"Buy\", shape.triangleup, location.bottom, lavender)\\nbgcolor(show_buy_signal and buy_matrix ? color.new(lavender, 70) : na, title=\"Buy\")\\n\\n//-------S/R Zones------\\nLookback = SupResPeriod\\nPerCent  = SupResPercentage\\n\\nValue1 = ta.cci(close, PricePeriod)\\nValue2 = ta.highest(Value1, Lookback)\\nValue3 = ta.lowest(Value1, Lookback)\\nValue4 = Value2 - Value3\\nValue5 = Value4 * (PerCent / 100)\\nResistanceLine = Value3 + Value5\\nSupportLine    = Value2 - Value5\\nplot(dynamic ? ResistanceLine : na, \"Resistance Line\", color.new(coral, 5))\\nplot(dynamic ? ResistanceLine : na, \"Resistance Line\", color.new(coral, 90), 7)\\nplot(dynamic ? SupportLine    : na, \"Support Line\",    color.new(lavender, 5))\\nplot(dynamic ? SupportLine    : na, \"Support Line\",    color.new(lavender, 90), 7)\\n\\n//--Overbought/Oversold/Warning Detail\\nh01 = ta.highest(up, 1)   + 20\\nh02 = ta.highest(down, 1) + 20\\nl01 = ta.lowest(down, 1)  - 20\\nl02 = ta.lowest(up, 1)    - 20\\nUPshape   = up   > ob and up > down ? h01 : up   > ob and up < down ? h02 : na\\nDOWNshape = down < os and up > down ? l01 : down < os and up < down ? l02 : na\\n\\nplot(show_dot ? UPshape   : na, \"UP Shape\",   amber, 4, plot.style_circles)\\nplot(show_dot ? DOWNshape : na, \"DOWN Shape\", amber, 4, plot.style_circles)\\nhline(OBOS ? ob : na, \"OverBought\")\\nhline(OBOS ? os : na, \"OverSold\")\\n\\nAccumulationColor = color.new(#FFA07A, 60)     //\"Accumulation Zone\"\\nWarningColor      = color.new(color.white, 60) //\"Warning/Watch Signal\"\\nDistributionColor = color.new(#3CB371, 60)     //\"Distribution Zone\"\\n\\nrk5_210  = ( ys1 - rk3 ) * 210 / rk4\\nrk6_210  = ta.ema(rk5_210, Smoother)\\nUP_210   = ta.ema(rk6_210, Smoother)\\nDOWN_210 = ta.ema(UP_210, Smoother)\\nUPColor  = (UP_210 >= 210 ? DistributionColor : (UP_210 <= -210 ? AccumulationColor : WarningColor))\\nplot(show_hist ? UP_210   : na, \"UP\",         UPColor, 1, plot.style_columns)\\nplot(show_hist ? DOWN_210 : na, \"Signal(UP)\", color.aqua)\\n\\nif alert_Buy_Matrix and buy_matrix\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Buy [Matrix Series].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Sell_Matrix and sell_matrix\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Sell [Matrix Series].-\\', alert.freq_once_per_bar_close)\\n\\n//_____________________________________________________________________________________________\\n// Based on \"Williams Vix Fix\" - Author: @ChrisMoody\\n// https://www.tradingview.com/script/pJpXG5JH-CM-Williams-Vix-Fix-V3-Ultimate-Filtered-Alerts/\\n// https://www.ireallytrade.com/newsletters/VIXFix.pdf\\n//_____________________________________________________________________________________________\\nshows_Vix_Fix = input(false, \\'Show Vix Fix Histogram ???\\', inline=\"histVF\", group=\\'Williams Vix Fix\\')\\nswvfm    = input.float(1.0, \"Multiplier (Values 0.5 to 2)\", minval = 0.5, maxval = 2, inline=\"histVF\", group=\\'Williams Vix Fix\\')\\nsbcFilt  = input.bool(true, \\'Show Signal For Filtered Entry [? FE ]\\', group=\\'Williams Vix Fix\\') // Use FILTERED Criteria\\nsbcAggr  = input.bool(true, \\'Show Signal For AGGRESSIVE Filtered Entry [? AE ]\\', group=\\'Williams Vix Fix\\') // Use FILTERED Criteria\\nalert_AE = input.bool(false, \\'? Alert: [AGGRESSIVE Entry]\\', inline=\\'AlertVF\\', group=\\'Williams Vix Fix\\')\\nalert_FE = input.bool(false, \\'? Alert: [Filtered Entry]\\', inline=\\'AlertVF\\', group=\\'Williams Vix Fix\\')\\npd       = input.int(22, \\'LookBack Period Standard Deviation High\\', 1, group=\\'Williams Vix Fix\\')\\nbbl      = input.int(20, \\'Bolinger Band Length\\', 1, group=\\'Williams Vix Fix\\')\\nmult     = input.float(2.0, \\'Bollinger Band Standard Devaition Up\\', minval=1, maxval=5, group=\\'Williams Vix Fix\\')\\nlb       = input.int(50, \\'Look Back Period Percentile High\\', 1, group=\\'Williams Vix Fix\\')\\nph       = input.float(.85, \\'Highest Percentile - 0.90=90%, 0.95=95%, 0.99=99%\\', minval=.05, maxval=1, group=\\'Williams Vix Fix\\')\\nltLB     = input.int(40, minval=25, maxval=99, title=\\'Long-Term Look Back Current Bar Has To Close Below This Value OR Medium Term--Default=40\\', group=\\'Williams Vix Fix\\')\\nmtLB     = input.int(14, minval=10, maxval=20, title=\\'Medium-Term Look Back Current Bar Has To Close Below This Value OR Long Term--Default=14\\', group=\\'Williams Vix Fix\\')\\nstr      = input.int(3,  minval=1,  maxval=9,  title=\\'Entry Price Action Strength--Close > X Bars Back---Default=3\\', group=\\'Williams Vix Fix\\')\\n\\n// Williams Vix Fix Formula\\nwvf       = (ta.highest(close, pd) - low) / ta.highest(close, pd) * 100\\nsDev      = mult * ta.stdev(wvf, bbl)\\nmidLine   = ta.sma(wvf, bbl)\\nlowerBand = midLine - sDev\\nupperBand = midLine + sDev\\nrangeHigh = ta.highest(wvf, lb) * ph\\n\\n// Filtered Criteria\\nupRange      = low > low[1] and close > high[1]\\nupRange_Aggr = close > close[1] and close > open[1]\\n// Filtered Criteria\\nfiltered      = (wvf[1] >= upperBand[1] or wvf[1] >= rangeHigh[1]) and wvf < upperBand and wvf < rangeHigh\\nfiltered_Aggr = (wvf[1] >= upperBand[1] or wvf[1] >= rangeHigh[1]) and not(wvf < upperBand and wvf < rangeHigh)\\n\\n// Alerts Criteria\\nalert1  = wvf >= upperBand or wvf >= rangeHigh ? 1 : 0\\nalert2  = (wvf[1] >= upperBand[1] or wvf[1] >= rangeHigh[1]) and wvf < upperBand and wvf < rangeHigh ? 1 : 0\\ncond_FE = upRange and close > close[str] and (close < close[ltLB] or close < close[mtLB]) and filtered \\nalert3  = cond_FE ? 1 : 0\\ncond_AE = upRange_Aggr and close > close[str] and (close < close[ltLB] or close < close[mtLB]) and filtered_Aggr\\nalert4  = cond_AE ? 1 : 0\\n\\ncolor_VF = alert1 ? color.new(color.lime, 60) : alert2 ? color.new(color.teal, 60) : color.new(color.silver, 80)\\nswvfm_precent = swvfm * (ResistanceLine - SupportLine) / (2.2 * rangeHigh)\\n\\nplot(shows_Vix_Fix ? wvf * -1 * swvfm_precent : na, \\'Williams Vix Fix\\', color_VF, 4, plot.style_columns)\\n\\nplotshape(sbcAggr and alert4 ? alert4 : na, \\'Aggressive Entry\\', shape.triangleup, location.bottom, color.new(#80FF00, 0), text=\\'AE\\', textcolor=color.new(#80FF00, 0))\\nbgcolor(sbcAggr and alert4 ? color.new(#80FF00, 80) : na, title=\\'Aggressive Entry\\')\\nplotshape(sbcFilt and alert3 ? alert3 : na, \\'Filtered Entry\\', shape.triangleup, location.bottom, color.new(#15FF00, 0), text=\\'FE\\', textcolor=color.new(#15FF00, 0))\\nbgcolor(sbcFilt and alert3 ? color.new(#15FF00, 80) : na, title=\\'Filtered Entry\\')\\n\\nif alert_AE and cond_AE\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Aggressive Entry [VixFix].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_FE and cond_FE\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Filtered Entry [VixFix].-\\', alert.freq_once_per_bar_close)\\n\\n//_____________________________________________________________\\n// Based on \"QQE signals\" - Author: @colinmck\\n// https://www.tradingview.com/script/7R6ZxyZu-QQE-signals/\\n//_____________________________________________________________\\nshows_QQE       = input.bool(false, \"????????? Show QQE Signals ?????????\")\\nalert_Long_QQE  = input.bool(false, \"? Alert: [Long - QQE]\",  inline=\"alertQQE\")\\nalert_Short_QQE = input.bool(false, \"? Alert: [Short - QQE]\", inline=\"alertQQE\")\\nRSI_Period = input.int(14,      \"RSI Length\", 1)\\nSF         = input.int(5,       \"RSI Smoothing\", 2)\\nQQE        = input.float(4.238, \"Fast QQE Factor\", 1)\\n\\nWilders_Period = RSI_Period * 2 - 1\\nlongband = 0.0\\nshortband = 0.0\\ntrend = 0\\n\\nRsi      = ta.rsi(close, RSI_Period)\\nRSIndex  = ta.ema(Rsi, SF)\\nAtrRsi   = math.abs(RSIndex[1] - RSIndex)\\nMaAtrRsi = ta.ema(AtrRsi, Wilders_Period)\\n\\nDeltaFastAtrRsi = ta.ema(MaAtrRsi, Wilders_Period) * QQE\\nnewshortband = RSIndex + DeltaFastAtrRsi\\nnewlongband = RSIndex - DeltaFastAtrRsi\\nlongband := RSIndex[1] > longband[1] and RSIndex > longband[1] ? math.max(longband[1], newlongband) : newlongband\\nshortband := RSIndex[1] < shortband[1] and RSIndex < shortband[1] ? math.min(shortband[1], newshortband) : newshortband\\ncross_1 = ta.cross(longband[1], RSIndex)\\ntrend := ta.cross(RSIndex, shortband[1]) ? 1 : cross_1 ? -1 : nz(trend[1], 1)\\nFastAtrRsiTL = trend == 1 ? longband : shortband\\n\\n// Find all the QQE Crosses\\nQQExlong = 0\\nQQExlong := nz(QQExlong[1])\\nQQExshort = 0\\nQQExshort := nz(QQExshort[1])\\nQQExlong := FastAtrRsiTL < RSIndex ? QQExlong + 1 : 0\\nQQExshort := FastAtrRsiTL > RSIndex ? QQExshort + 1 : 0\\n\\n// Conditions\\nqqeLong = QQExlong == 1 ? FastAtrRsiTL[1] - 50 : na\\nqqeShort = QQExshort == 1 ? FastAtrRsiTL[1] - 50 : na\\n\\n// Plotting\\nredorange  = color.new(#FF4000, 0)\\nchartreuse = color.new(#80FF00, 0)\\nplotshape(shows_QQE and qqeLong,  \"QQE Long\",  shape.labelup,   location.bottom, color.new(chartreuse, 25), text=\"QQE\", size=size.tiny, textcolor=color.new(#000000, 5))\\nplotshape(shows_QQE and qqeShort, \"QQE Short\", shape.labeldown, location.top,    color.new(redorange, 25),  text=\"QQE\", size=size.tiny, textcolor=color.new(color.white, 5))\\n\\nif alert_Long_QQE  and qqeLong\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Long [Quantitative Qualitative Estimation (QQE)].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Short_QQE and qqeShort\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Short [Quantitative Qualitative Estimation (QQE)].-\\', alert.freq_once_per_bar_close)\\n\\n//___________________________________________________________________________\\n// VWAP CCI Signals\\n//___________________________________________________________________________\\nshows_VCCI = input.bool(false, \"????????? Show VWAP CCI Signals ?????????\")\\nlen_cci    = input.int(21, \"Length\", 1, inline=\"cci\")\\nType_cci   = input.string(\"vwap(Close)\", \"Source\", inline=\"cci\", options=[\"vwap(Close)\", \"vwap(Open)\", \"vwap(High)\", \"vwap(Low)\", \"AVG(vwap(H,L))\", \"AVG(vwap(O,C))\", \"VWAP\", \"Close\", \"Open\", \"HL2\", \"HLC3\", \"OHLC4\", \"HLCC4\", \"High\", \"Low\", \"TR\"])\\nalert_Long_cci  = input.bool(false, \"? Alert: [Long - CCI]\",  inline=\"alertCCI\")\\nalert_Short_cci = input.bool(false, \"? Alert: [Short - CCI]\", inline=\"alertCCI\")\\n\\n// ALMA - Arnaud Legoux Moving Average of @kurtsmock\\nenhanced_alma(_series, _length, _offset, _sigma) =>\\n    length      = int(_length) // Floating point protection\\n    numerator   = 0.0\\n    denominator = 0.0 \\n    m = _offset * (length - 1)\\n    s = length / _sigma\\n    for i=0 to length-1\\n        weight       = math.exp(-((i-m)*(i-m)) / (2 * s * s))\\n        numerator   := numerator   + weight * _series[length - 1 - i]\\n        denominator := denominator + weight\\n    numerator / denominator\\n\\n// Calculate vwap cci\\nsrc_cci = get_src(Type_cci)\\ncci = (src_cci - enhanced_alma(src_cci, len_cci, 0.85, 6.0)) / (0.015 * ta.dev(src_cci, len_cci))\\n\\n// Define long and short entry signal\\nlong_cci  = ta.crossover(cci, -200)\\nshort_cci = ta.crossunder(cci, 200)\\n\\nplotshape(shows_VCCI and long_cci,  \"VWAP CCI Long\",  shape.labelup,   location.bottom, color.new(chartreuse, 25), text=\"CCI\", size=size.tiny, textcolor=color.new(#000000, 5))\\nplotshape(shows_VCCI and short_cci, \"VWAP CCI Short\", shape.labeldown, location.top,    color.new(redorange, 25),  text=\"CCI\", size=size.tiny, textcolor=color.new(color.white, 5))\\n\\nif alert_Long_cci and long_cci\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Long [Commodity Channel Index (CCI)].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Short_cci and short_cci\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Short [Commodity Channel Index (CCI)].-\\', alert.freq_once_per_bar_close)\\n\\n//___________________________________________________________________________________________\\n// Based on \"Squeeze Momentum Indicator\" - Author: @LazyBear\\n// @KivancOzbilgic: https://www.tradingview.com/script/NVzKGYFJ/\\n// @capissimo: https://www.tradingview.com/script/Ejx2tEHY-Squeeze-Momentum-Indicator-mod-3/\\n// Indicator description: http://www.forextrading-pips.com/squeeze-indicator/\\n//___________________________________________________________________________________________\\nshow_SMI   = input.bool(false,     \"Show Squeeze Momentum Indicator\", group = \"Squeeze Momentum Indicator\")\\nalert_Buy  = input.bool(false,     \"? Alert: [Buy - Mom]\",  inline = \"alertMom\", group = \"Squeeze Momentum Indicator\")\\nalert_Sell = input.bool(false,     \"? Alert: [Sell - Mom]\", inline = \"alertMom\", group = \"Squeeze Momentum Indicator\")\\nType_src   = input.string(\"Close\", \"Source\", inline = \"s1\", group = \"Squeeze Momentum Indicator\", options = [\"VWAP\", \"Close\", \"Open\", \"HL2\", \"HLC3\", \"OHLC4\", \"HLCC4\", \"High\", \"Low\", \"vwap(Close)\", \"vwap(Open)\", \"vwap(High)\", \"vwap(Low)\", \"AVG(vwap(H,L))\", \"AVG(vwap(O,C))\", \"TR\"])\\nsrc0       = get_src(Type_src)\\nshow_sb    = input.bool(false, \"Show buy/sell signal\", inline = \"s1\", group = \"Squeeze Momentum Indicator\")\\nlengthMom  = input.int(20,     \"Momentum Length\", 1, inline = \"mom\", group = \"Squeeze Momentum Indicator\")\\nsiglen     = input.int(5,      \"Signal\", 2, inline = \"mom\", group = \"Squeeze Momentum Indicator\")\\nlengthSQZ  = input.int(20,     \"KC/BB Length\", 1, inline = \"sqz\", group = \"Squeeze Momentum Indicator\")\\nmultBB     = input.float(2.0,  \"BB MultFactor\", 0.5, step=0.05, inline = \"sqz\", group = \"Squeeze Momentum Indicator\")\\nmultKC     = input.float(1.3,  \"KC MultFactor\", 0.5, step=0.05, inline = \"kc\", group = \"Squeeze Momentum Indicator\")\\nuse_tr     = input.bool(true,  \"Use TrueRange (KC)\", inline = \"kc\", group = \"Squeeze Momentum Indicator\")\\n\\n// Calculate BB\\nbasis   = ta.sma(src0, lengthSQZ)\\ndev     = ta.stdev(src0, lengthSQZ)  \\nupperBB = basis + multBB * dev \\nlowerBB = basis - multBB * dev  \\n\\n// Calculate KC\\nmaKC    = ta.sma(src0, lengthSQZ)\\nrange_1 = use_tr ? ta.tr : high - low\\nrangema = ta.sma(range_1, lengthSQZ)\\nupperKC = maKC + rangema * multKC\\nlowerKC = maKC - rangema * multKC\\n\\n// When both the upper and lower Bollinger Bands go inside the Keltner Channel, the squeeze is on.\\n// When the Bollinger Bands (BOTH lines) start to come out of the Keltner Channel, the squeeze has been released (off). \\n// When one of the Bollinger Bands is out of Keltner Channel, no highlighting is done.\\nsqzOn  = lowerBB > lowerKC  and upperBB < upperKC\\nsqzOff = lowerBB <= lowerKC and upperBB >= upperKC\\nnoSqz  = sqzOn == false and sqzOff == false\\n\\nmom = ta.linreg(src0 - math.avg(math.avg(ta.highest(high, lengthMom), ta.lowest(low, lengthMom)), ta.sma(close, lengthMom)), lengthMom, 0)\\nsig = ta.sma(mom, siglen)\\n\\nlongSM  = ta.crossover(mom, sig)\\nshortSM = ta.crossunder(mom, sig)\\nsqz_pos = (sqzOff and mom >= sig)\\nsqz_neg = (sqzOff and mom <  sig)\\n\\nplotshape(show_sb and longSM,  \"Long Momentum\",  shape.triangleup,   location.bottom, #05FFA6, text=\\'M\\', textcolor=#05FFA6)\\nplotshape(show_sb and shortSM, \"Short Momentum\", shape.triangledown, location.top,    #FF0AE2, text=\\'M\\', textcolor=#FF0AE2)\\nplotshape(show_SMI and (sqzOn  or  noSqz) ? true : na, \"In Squeeze\", shape.circle, location.top, color.silver)\\nplotshape(show_SMI and sqz_pos ? true : na, \"Squeeze Release\", shape.triangleup,   location.top, #05FFA6)\\nplotshape(show_SMI and sqz_neg ? true : na, \"Squeeze Release\", shape.triangledown, location.top, #FF0AE2)\\n\\nif alert_Buy and longSM\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Buy [Momentum].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Sell and shortSM\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Sell [Momentum].-\\', alert.freq_once_per_bar_close)\\n\\nsqueeze_ON = \\'{\"content\": \"@everyone\" ,\"embeds\": [{\"title\": \"{{ticker}} \",\"description\": \"In Squeeze | Price ${{close}}\", \"color\": \"2092045\",\"author\": {\"name\":null},\"timestamp\": \"{{timenow}}\"}],\"username\": \"Alertbot\"}\\'\\nsqueeze_Long = \\'{\"content\": \"@everyone\" ,\"embeds\": [{\"title\": \"{{ticker}} \",\"description\": \"Squeeze Release Long | Price ${{close}}\", \"color\": \"2092045\",\"author\": {\"name\":null},\"timestamp\": \"{{timenow}}\"}],\"username\": \"Alertbot\"}\\'\\nsqueeze_Short = \\'{\"content\": \"@everyone\" ,\"embeds\": [{\"title\": \"{{ticker}} \",\"description\": \"Squeeze Release Short | Price ${{close}}\", \"color\": \"2092045\",\"author\": {\"name\":null},\"timestamp\": \"{{timenow}}\"}],\"username\": \"Alertbot\"}\\'\\n\\nalertcondition(sqzOn or noSqz, title=\"In Squeeze\", message=squeeze_ON)\\nalertcondition(sqz_pos, title=\"Squeeze Release | Long\",  message=squeeze_Long)\\nalertcondition(sqz_neg, title=\"Squeeze Release | Short\", message=squeeze_Short)//@version=5 \\nindicator(\"VIX Oscillator Smooth\", shorttitle=\"VIX Osc Sm\", overlay=false) \\n\\n// User Inputs \\ntimeframe = input.timeframe(\"\", title=\"Period Timeframe\")  \\nAvgLength = input.int(14, title=\"Average Length\") \\nSmoothLength = input.int(3, title=\"Smooth Length\") \\n\\n// Securities \\nticker = ticker.new(syminfo.prefix, syminfo.ticker, session.regular) \\nvix = request.security(\"CBOE:VIX\", timeframe, close)  \\ncl = request.security(ticker, timeframe, close, lookahead=barmerge.lookahead_on) \\n\\n// Z-Score Calculation \\n\\n// Z Score VIX  \\nvixhx = ta.sma(vix, AvgLength)  \\nvixsd = ta.stdev(vix, AvgLength)  \\nz_raw = (vix - vixhx) / vixsd  \\nz = ta.sma(z_raw, SmoothLength) // Smoothed Z Score VIX line \\n\\n// Z Score Stock \\nstockhx = ta.sma(cl, AvgLength)  \\nstocksd = ta.stdev(cl, AvgLength) \\nzs_raw = (cl - stockhx) / stocksd  \\nzs = ta.sma(zs_raw, SmoothLength) // Smoothed Z Score Stock line \\n\\n// User Inputs for background levels and colors \\nupper_level = input.float(2.5, title=\"Upper Level for Hline Background\", step=0.1) \\nupper_color = input.color(color.gray, title=\"Color for Upper Level Background\") \\nlower_level = input.float(-2.5, title=\"Lower Level for Hline Background\", step=0.1) \\nlower_color = input.color(#ff525275, title=\"Color for Lower Level Background\") \\nzero_level = input.float(0, title=\"Zero Level for Hline Background\", step=0.1) \\nzero_color = input.color(#2195f38a, title=\"Color for Zero Level Background\") \\n\\n// Background color \\nbgcolor_condition = (z >= upper_level and zs <= lower_level) or (z <= lower_level and zs >= upper_level) ? upper_color :  \\n                   (z >= zero_level and zs <= zero_level) or (z <= zero_level and zs >= zero_level) ? zero_color : \\n                   lower_color \\nbgcolor(bgcolor_condition) \\n\\n// Plots \\nplot(z, \"VIX Smooth\", color=color.purple, linewidth=3) // Cambiado a violeta\\nplot(zs, \"Ticker Smooth\", color=color.blue, linewidth=3) // Cambiado a azul\\n\\n// Horizontal lines for levels \\nhline(upper_level, \"Upper Level\", color=color.green)\\nhline(zero_level, \"Zero Level\", color=color.gray)  \\nhline(lower_level, \"Lower Level\", color=color.red) \\n\\n// Draw horizontal lines at upper and zero levels and store their IDs\\nhline_upper = hline(upper_level, \"Upper Level\", color=color.green) \\nhline_zero = hline(zero_level, \"Zero Level\", color=color.gray) \\nhline_lower = hline(lower_level, \"Upper Level\", color=#d61f1f) \\n\\n// Fill the area between zero level and upper level\\nfill(hline_zero, hline_upper, color=#4caf4f69)\\n// Fill the area between zero level and lower level\\nfill(hline_zero, hline_lower, color=#af4c4c69)\\n \\n// Alert conditions for hline background \\nalertcondition(z >= upper_level and zs <= lower_level, title=\"Upper Level Background Alert\", message=\"VIX is at upper level and Ticker is at lower level\") \\nalertcondition(z <= lower_level and zs >= upper_level, title=\"Lower Level Background Alert\", message=\"VIX is at lower level and Ticker is at upper level\") \\n \\n// Signal for crossover at level 0 \\ncrossover_signal = ta.crossover(z, zs) and zs[1] < 0 or ta.crossunder(z, zs) and zs[1] > 0 \\nplotshape(series=crossover_signal, title=\"Crossover Signal\", location=location.absolute, style=shape.circle, color=#ffae0083, size=size.tiny, offset=-AvgLength/2)\\n RECHEQUELO Y DE UNA VEZ  CUALQUIER ERROR LO ENCUENTRES CORRIGE ES PARA TRADING VIEW', \"CORREGIR ESTE ERROR Y TODOS LOS EXISTENTES TRATA DE LOS ERRORES NO SE VUELVAN A REPETIR mismatched character '\\\\n' expecting '''\", \"CORREGIR ESTE ERROR Syntax error at input 'alert'Y TODOS LOS EXISTENTES  Y EL overlay=false\", \"tienes muchos errores lee el pine script sede los originales y corrige todos los errores mismatched character '\\\\n' expecting '''\", 'puedes simplificar este indicador no mas lo mas importante //@version=5\\nindicator(\\'Matrix Series and Vix Fix with VWAP CCI and QQE Signals\\', shorttitle=\\'Matrix\\', precision=2)\\n\\n// Function to select the type of source\\nget_src(Type) =>\\n    switch Type \\n        \"VWAP\" => ta.vwap\\n        \"Close\" => close\\n        \"Open\" => open\\n        \"HL2\" => hl2\\n        \"HLC3\" => hlc3\\n        \"OHLC4\" => ohlc4\\n        \"HLCC4\" => hlcc4\\n        \"High\" => high\\n        \"Low\" => low\\n        \"TR\" => ta.tr\\n        \"vwap(Close)\" => ta.vwap(close)\\n        \"vwap(Open)\" => ta.vwap(open)\\n        \"vwap(High)\" => ta.vwap(high)\\n        \"vwap(Low)\" => ta.vwap(low)\\n        \"AVG(vwap(H,L))\" => math.avg(ta.vwap(high), ta.vwap(low))\\n        \"AVG(vwap(O,C))\" => math.avg(ta.vwap(open), ta.vwap(close))        \\n\\n//__________________________________________________________________\\n// Based on \"Matrix Series\" - Author: @glaz\\n// https://www.tradingview.com/script/2X2cVLhb-Matrix-Series/\\n// https://www.wisestocktrader.com/indicators/2739-flower-indicator\\n//__________________________________________________________________\\nalert_Buy_Matrix  = input.bool(false, \"? Alert: [Buy - Matrix]\",  inline=\"alertMatrix\")\\nalert_Sell_Matrix = input.bool(false, \"? Alert: [Sell - Matrix]\", inline=\"alertMatrix\")\\nshow_buy_signal   = input.bool(true,  \"Show buy signal\",  inline=\"show_signal\")\\nshow_sell_signal  = input.bool(true,  \"Show sell signal\", inline=\"show_signal\")\\nshow_dot          = input.bool(true,  \"Show Watch/Warning Point\", inline=\"show_signal\")\\n//--- Trend Bought/Sold Detail\\ndynamic      = input.bool(true,  \"Show Dynamic Zones ??? \", inline=\"show_limit\")\\nOBOS         = input.bool(false, \"Show OB/OS\", inline=\"show_limit\")\\nshow_candles = input.bool(true,  \"Show Matrix Candles  ??? \", inline=\"s_matrix\")\\nshow_hist    = input.bool(false, \"Show Matrix Histogram\", inline=\"s_matrix\")\\n//--- Sup/Res Detail\\nPricePeriod      = input.int(16,  \"Price Period\", 1, inline=\"period\")\\nSmoother         = input.int(5,   \"Smoother\", 2, inline=\"period\")\\nSupResPeriod     = input.int(50,  \"Superior Resolution Period\", 1, inline=\"Superior Resolution\")\\nSupResPercentage = input.int(100, \"Percentage\", 1, inline=\"Superior Resolution\")\\n//--- Line Detail \\nob = input.int(200,  \"OverBought Above\", inline=\"line\")\\nos = input.int(-200, \"OverSold Bellow\",  inline=\"line\")\\n\\nys1  = (high + low + close * 2) / 4\\nrk3  = ta.ema(ys1, Smoother)\\nrk4  = ta.stdev(ys1, Smoother)\\nrk5  = (ys1 - rk3) * 200 / rk4\\nrk6  = ta.ema(rk5, Smoother)\\nup   = ta.ema(rk6, Smoother)\\ndown = ta.ema(up, Smoother)\\nOo   = up < down ? up : down\\nHh   = Oo\\nLl   = up < down ? down : up\\nCc   = Ll\\nsell_matrix = ta.cross(up, ob) == 1 and up[1] > up \\nbuy_matrix  = ta.cross(up, os) == 1 and up[1] < up \\n\\ncoral     = #FF8080\\nlavender  = #8080FF\\namber     = color.new(#FFE500, 60)\\nvcolor = show_candles and (Oo > Cc) ? coral : show_candles and (up > down) ? lavender : show_candles ? coral : na\\n\\nplotcandle(Oo, Hh, Ll, Cc, \"Matrix Candles\", color = vcolor, wickcolor = vcolor, bordercolor = vcolor)\\nplotshape(show_sell_signal and sell_matrix, \"Sell\", shape.triangledown, location.top, coral)\\nbgcolor(show_sell_signal and sell_matrix ? color.new(coral, 70) : na, title=\"Sell\")\\nplotshape(show_buy_signal and buy_matrix, \"Buy\", shape.triangleup, location.bottom, lavender)\\nbgcolor(show_buy_signal and buy_matrix ? color.new(lavender, 70) : na, title=\"Buy\")\\n\\n//-------S/R Zones------\\nLookback = SupResPeriod\\nPerCent  = SupResPercentage\\n\\nValue1 = ta.cci(close, PricePeriod)\\nValue2 = ta.highest(Value1, Lookback)\\nValue3 = ta.lowest(Value1, Lookback)\\nValue4 = Value2 - Value3\\nValue5 = Value4 * (PerCent / 100)\\nResistanceLine = Value3 + Value5\\nSupportLine    = Value2 - Value5\\nplot(dynamic ? ResistanceLine : na, \"Resistance Line\", color.new(coral, 5))\\nplot(dynamic ? ResistanceLine : na, \"Resistance Line\", color.new(coral, 90), 7)\\nplot(dynamic ? SupportLine    : na, \"Support Line\",    color.new(lavender, 5))\\nplot(dynamic ? SupportLine    : na, \"Support Line\",    color.new(lavender, 90), 7)\\n\\n//--Overbought/Oversold/Warning Detail\\nh01 = ta.highest(up, 1)   + 20\\nh02 = ta.highest(down, 1) + 20\\nl01 = ta.lowest(down, 1)  - 20\\nl02 = ta.lowest(up, 1)    - 20\\nUPshape   = up   > ob and up > down ? h01 : up   > ob and up < down ? h02 : na\\nDOWNshape = down < os and up > down ? l01 : down < os and up < down ? l02 : na\\n\\nplot(show_dot ? UPshape   : na, \"UP Shape\",   amber, 4, plot.style_circles)\\nplot(show_dot ? DOWNshape : na, \"DOWN Shape\", amber, 4, plot.style_circles)\\nhline(OBOS ? ob : na, \"OverBought\")\\nhline(OBOS ? os : na, \"OverSold\")\\n\\nAccumulationColor = color.new(#FFA07A, 60)     //\"Accumulation Zone\"\\nWarningColor      = color.new(color.white, 60) //\"Warning/Watch Signal\"\\nDistributionColor = color.new(#3CB371, 60)     //\"Distribution Zone\"\\n\\nrk5_210  = ( ys1 - rk3 ) * 210 / rk4\\nrk6_210  = ta.ema(rk5_210, Smoother)\\nUP_210   = ta.ema(rk6_210, Smoother)\\nDOWN_210 = ta.ema(UP_210, Smoother)\\nUPColor  = (UP_210 >= 210 ? DistributionColor : (UP_210 <= -210 ? AccumulationColor : WarningColor))\\nplot(show_hist ? UP_210   : na, \"UP\",         UPColor, 1, plot.style_columns)\\nplot(show_hist ? DOWN_210 : na, \"Signal(UP)\", color.aqua)\\n\\nif alert_Buy_Matrix and buy_matrix\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Buy [Matrix Series].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Sell_Matrix and sell_matrix\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Sell [Matrix Series].-\\', alert.freq_once_per_bar_close)\\n\\n//_____________________________________________________________________________________________\\n// Based on \"Williams Vix Fix\" - Author: @ChrisMoody\\n// https://www.tradingview.com/script/pJpXG5JH-CM-Williams-Vix-Fix-V3-Ultimate-Filtered-Alerts/\\n// https://www.ireallytrade.com/newsletters/VIXFix.pdf\\n//_____________________________________________________________________________________________\\nshows_Vix_Fix = input(false, \\'Show Vix Fix Histogram ???\\', inline=\"histVF\", group=\\'Williams Vix Fix\\')\\nswvfm    = input.float(1.0, \"Multiplier (Values 0.5 to 2)\", minval = 0.5, maxval = 2, inline=\"histVF\", group=\\'Williams Vix Fix\\')\\nsbcFilt  = input.bool(true, \\'Show Signal For Filtered Entry [? FE ]\\', group=\\'Williams Vix Fix\\') // Use FILTERED Criteria\\nsbcAggr  = input.bool(true, \\'Show Signal For AGGRESSIVE Filtered Entry [? AE ]\\', group=\\'Williams Vix Fix\\') // Use FILTERED Criteria\\nalert_AE = input.bool(false, \\'? Alert: [AGGRESSIVE Entry]\\', inline=\\'AlertVF\\', group=\\'Williams Vix Fix\\')\\nalert_FE = input.bool(false, \\'? Alert: [Filtered Entry]\\', inline=\\'AlertVF\\', group=\\'Williams Vix Fix\\')\\npd       = input.int(22, \\'LookBack Period Standard Deviation High\\', 1, group=\\'Williams Vix Fix\\')\\nbbl      = input.int(20, \\'Bolinger Band Length\\', 1, group=\\'Williams Vix Fix\\')\\nmult     = input.float(2.0, \\'Bollinger Band Standard Devaition Up\\', minval=1, maxval=5, group=\\'Williams Vix Fix\\')\\nlb       = input.int(50, \\'Look Back Period Percentile High\\', 1, group=\\'Williams Vix Fix\\')\\nph       = input.float(.85, \\'Highest Percentile - 0.90=90%, 0.95=95%, 0.99=99%\\', minval=.05, maxval=1, group=\\'Williams Vix Fix\\')\\nltLB     = input.int(40, minval=25, maxval=99, title=\\'Long-Term Look Back Current Bar Has To Close Below This Value OR Medium Term--Default=40\\', group=\\'Williams Vix Fix\\')\\nmtLB     = input.int(14, minval=10, maxval=20, title=\\'Medium-Term Look Back Current Bar Has To Close Below This Value OR Long Term--Default=14\\', group=\\'Williams Vix Fix\\')\\nstr      = input.int(3,  minval=1,  maxval=9,  title=\\'Entry Price Action Strength--Close > X Bars Back---Default=3\\', group=\\'Williams Vix Fix\\')\\n\\n// Williams Vix Fix Formula\\nwvf       = (ta.highest(close, pd) - low) / ta.highest(close, pd) * 100\\nsDev      = mult * ta.stdev(wvf, bbl)\\nmidLine   = ta.sma(wvf, bbl)\\nlowerBand = midLine - sDev\\nupperBand = midLine + sDev\\nrangeHigh = ta.highest(wvf, lb) * ph\\n\\n// Filtered Criteria\\nupRange      = low > low[1] and close > high[1]\\nupRange_Aggr = close > close[1] and close > open[1]\\n// Filtered Criteria\\nfiltered      = (wvf[1] >= upperBand[1] or wvf[1] >= rangeHigh[1]) and wvf < upperBand and wvf < rangeHigh\\nfiltered_Aggr = (wvf[1] >= upperBand[1] or wvf[1] >= rangeHigh[1]) and not(wvf < upperBand and wvf < rangeHigh)\\n\\n// Alerts Criteria\\nalert1  = wvf >= upperBand or wvf >= rangeHigh ? 1 : 0\\nalert2  = (wvf[1] >= upperBand[1] or wvf[1] >= rangeHigh[1]) and wvf < upperBand and wvf < rangeHigh ? 1 : 0\\ncond_FE = upRange and close > close[str] and (close < close[ltLB] or close < close[mtLB]) and filtered \\nalert3  = cond_FE ? 1 : 0\\ncond_AE = upRange_Aggr and close > close[str] and (close < close[ltLB] or close < close[mtLB]) and filtered_Aggr\\nalert4  = cond_AE ? 1 : 0\\n\\ncolor_VF = alert1 ? color.new(color.lime, 60) : alert2 ? color.new(color.teal, 60) : color.new(color.silver, 80)\\nswvfm_precent = swvfm * (ResistanceLine - SupportLine) / (2.2 * rangeHigh)\\n\\nplot(shows_Vix_Fix ? wvf * -1 * swvfm_precent : na, \\'Williams Vix Fix\\', color_VF, 4, plot.style_columns)\\n\\nplotshape(sbcAggr and alert4 ? alert4 : na, \\'Aggressive Entry\\', shape.triangleup, location.bottom, color.new(#80FF00, 0), text=\\'AE\\', textcolor=color.new(#80FF00, 0))\\nbgcolor(sbcAggr and alert4 ? color.new(#80FF00, 80) : na, title=\\'Aggressive Entry\\')\\nplotshape(sbcFilt and alert3 ? alert3 : na, \\'Filtered Entry\\', shape.triangleup, location.bottom, color.new(#15FF00, 0), text=\\'FE\\', textcolor=color.new(#15FF00, 0))\\nbgcolor(sbcFilt and alert3 ? color.new(#15FF00, 80) : na, title=\\'Filtered Entry\\')\\n\\nif alert_AE and cond_AE\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Aggressive Entry [VixFix].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_FE and cond_FE\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Filtered Entry [VixFix].-\\', alert.freq_once_per_bar_close)\\n\\n//_____________________________________________________________\\n// Based on \"QQE signals\" - Author: @colinmck\\n// https://www.tradingview.com/script/7R6ZxyZu-QQE-signals/\\n//_____________________________________________________________\\nshows_QQE       = input.bool(false, \"????????? Show QQE Signals ?????????\")\\nalert_Long_QQE  = input.bool(false, \"? Alert: [Long - QQE]\",  inline=\"alertQQE\")\\nalert_Short_QQE = input.bool(false, \"? Alert: [Short - QQE]\", inline=\"alertQQE\")\\nRSI_Period = input.int(14,      \"RSI Length\", 1)\\nSF         = input.int(5,       \"RSI Smoothing\", 2)\\nQQE        = input.float(4.238, \"Fast QQE Factor\", 1)\\n\\nWilders_Period = RSI_Period * 2 - 1\\nlongband = 0.0\\nshortband = 0.0\\ntrend = 0\\n\\nRsi      = ta.rsi(close, RSI_Period)\\nRSIndex  = ta.ema(Rsi, SF)\\nAtrRsi   = math.abs(RSIndex[1] - RSIndex)\\nMaAtrRsi = ta.ema(AtrRsi, Wilders_Period)\\n\\nDeltaFastAtrRsi = ta.ema(MaAtrRsi, Wilders_Period) * QQE\\nnewshortband = RSIndex + DeltaFastAtrRsi\\nnewlongband = RSIndex - DeltaFastAtrRsi\\nlongband := RSIndex[1] > longband[1] and RSIndex > longband[1] ? math.max(longband[1], newlongband) : newlongband\\nshortband := RSIndex[1] < shortband[1] and RSIndex < shortband[1] ? math.min(shortband[1], newshortband) : newshortband\\ncross_1 = ta.cross(longband[1], RSIndex)\\ntrend := ta.cross(RSIndex, shortband[1]) ? 1 : cross_1 ? -1 : nz(trend[1], 1)\\nFastAtrRsiTL = trend == 1 ? longband : shortband\\n\\n// Find all the QQE Crosses\\nQQExlong = 0\\nQQExlong := nz(QQExlong[1])\\nQQExshort = 0\\nQQExshort := nz(QQExshort[1])\\nQQExlong := FastAtrRsiTL < RSIndex ? QQExlong + 1 : 0\\nQQExshort := FastAtrRsiTL > RSIndex ? QQExshort + 1 : 0\\n\\n// Conditions\\nqqeLong = QQExlong == 1 ? FastAtrRsiTL[1] - 50 : na\\nqqeShort = QQExshort == 1 ? FastAtrRsiTL[1] - 50 : na\\n\\n// Plotting\\nredorange  = color.new(#FF4000, 0)\\nchartreuse = color.new(#80FF00, 0)\\nplotshape(shows_QQE and qqeLong,  \"QQE Long\",  shape.labelup,   location.bottom, color.new(chartreuse, 25), text=\"QQE\", size=size.tiny, textcolor=color.new(#000000, 5))\\nplotshape(shows_QQE and qqeShort, \"QQE Short\", shape.labeldown, location.top,    color.new(redorange, 25),  text=\"QQE\", size=size.tiny, textcolor=color.new(color.white, 5))\\n\\nif alert_Long_QQE  and qqeLong\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Long [Quantitative Qualitative Estimation (QQE)].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Short_QQE and qqeShort\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Short [Quantitative Qualitative Estimation (QQE)].-\\', alert.freq_once_per_bar_close)\\n\\n//___________________________________________________________________________\\n// VWAP C', 'a este mismo indicador pudieras agregar el vix  oscilador smooth representado con lineas', 'en este indicador podemos eliminar los backgrounds //@version=5\\nindicator(title=\\'Williams Vix Fix OHLC candles plot indicator (Rigo)\\', shorttitle=\\'VIXFIX_CANDLES_RIGO\\', overlay=false, timeframe=\"\", timeframe_gaps=false)\\n\\n// AUX FUNCTIONS\\nna_skip_highest_or_lowest(series float src, int len, int mode) =>\\n    highest_or_lowest = src\\n    i = 1\\n    j = 1\\n    while i < len and j <= len*2\\n        val = src[j]\\n        if not na(val)\\n            i += 1\\n            if (mode == 0 and highest_or_lowest < val) or (mode == 1 and highest_or_lowest > val)\\n                highest_or_lowest := val\\n        j += 1\\n    highest_or_lowest\\n\\nna_skip_highest(series float src, int len) =>\\n    na_skip_highest_or_lowest(src, len, 0)\\n\\nna_skip_lowest(series float src, int len) =>\\n    na_skip_highest_or_lowest(src, len, 1)\\n\\nf_featurescale(x, a, b, min_x, max_x) =>\\n    a + ((x - min_x) * (b - a) / (max_x - min_x))\\n\\nf_logfscale(x, b, min_x=0.0, max_x=10.0) =>\\n    x_log = math.log(x+1)\\n    f_featurescale(x_log, 0.0, b, min_x, max_x)\\n\\n// INPUT PARAMETERS\\ngrp1=\\'Core VixFix parameters\\'\\nindex_current = input.bool(true, title=\\'Use current chart symbol instead of the selected one below?\\', group=grp1)\\nindex_symbol = input.symbol(\\'SP:SPX\\', \\'Index symbol\\', group=grp1)\\nsym = index_current ? syminfo.tickerid : index_symbol\\npd = input(50, title=\\'LookBack Period\\', group=grp1)\\nskip_off_days = input.bool(false, title=\\'Skip off days?\\', group=grp1)\\n\\ngrp2=\\'VixFix OHLC bars plot\\'\\nshowvixfixbars = input.bool(true, title=\\'Show classical VixFix as an OHLC bars plot\\', group=grp2)\\nvixfixbars_display_mode = input.string(\\'candlesplot\\', title=\\'Show as a bars plot or candles plot?\\', group=grp2, options=[\\'candlesplot\\', \\'barsplot\\'])\\nvixfix_log = input.bool(false, title=\\'Logscale?\\', group=grp2)\\nswvf = input(false, title=\\'Show Williams Vix Fix as a Histogram too?\\', group=grp2)\\n\\ngrp3=\\'Price/VixFix OHLC candles plot\\'\\nshowpricediv = input.bool(false, title=\\'Show Price Divided By VixFix\\', group=grp3)\\npricedivscale = input.float(0.0, title=\"Price Divided by VixFix Rescaling\", group=grp3)\\npricedivscale_log = input.bool(true, title=\\'Logscale?\\', group=grp3)\\npricedivafterscale = input.float(1.0, title=\\'Visual scaling factor (will be applied after the previous rescaling)\\', minval=0.000001, step=0.5, group=grp3)\\npricedivadaptiverescaling = input.int(0, title=\\'Adaptive Rescaling Lookback Period\\', group=grp3)\\n\\ngrp4 = \\'VixFix Oscillator (alternative to Stochastic VIXfix SVIX)\\'\\noscdisp = input.bool(false, \\'Display VIXfix Oscillator\\', group=grp4)\\nosclp = input.int(50, \\'Oscillator Lookback Period\\', group=grp4)\\nosclog = input.bool(false, \\'Log-transform input to oscillator\\', group=grp4)\\noscrescale = input.int(300, \\'Adaptive rescale period to fit with VIXfix candles plot?\\', group=grp4)\\noschlight = input.bool(false, \\'Highlight when oversold/overbought?\\', group=grp4)\\noschlight_bthr = input.int(80, \\'Overbought threshold\\', group=grp4, minval=0, maxval=100)\\noschlight_sthr = input.int(20, \\'Oversold threshold\\', group=grp4, minval=0, maxval=100)\\noschlight_bg = input.bool(false, \\'Display overbought/oversold as background highlight?\\', group=grp4)\\n\\ngrp5 = \\'Price / VIXfix Oscillator\\'\\noscpvdisp = input.bool(false, \\'Display Price Divided By VIXfix Oscillator\\', group=grp5)\\noscpvlp = input.int(50, \\'Oscillator Lookback Period\\', group=grp5)\\noscpvlog = input.bool(true, \\'Log-transform input to oscillator\\', group=grp5)\\noscpvrescale = input.int(300, \\'Adaptive rescale period to fit with VIXfix candles plot?\\', group=grp5)\\noscpvhlight = input.bool(false, \\'Highlight when oversold/overbought?\\', group=grp5)\\noscpvhlight_bthr = input.int(80, \\'Overbought threshold\\', group=grp5, minval=0, maxval=100)\\noscpvhlight_sthr = input.int(20, \\'Oversold threshold\\', group=grp5, minval=0, maxval=100)\\noscpvhlight_bg = input.bool(false, \\'Display overbought/oversold as background highlight?\\', group=grp5)\\n\\n// VIX FIX CALCULATIONS\\n//\\n\\n// Load data\\nsource_close = request.security(sym, timeframe.period, close, gaps=barmerge.gaps_on, lookahead=barmerge.lookahead_off)\\nsource_open = request.security(sym, timeframe.period, open, gaps=barmerge.gaps_on, lookahead=barmerge.lookahead_off)\\nsource_high = request.security(sym, timeframe.period, high, gaps=barmerge.gaps_on, lookahead=barmerge.lookahead_off)\\nsource_low = request.security(sym, timeframe.period, low, gaps=barmerge.gaps_on, lookahead=barmerge.lookahead_off)\\nhighest_close = not skip_off_days ? ta.highest(source_close, pd) : na_skip_highest(source_close, pd) // there is one difference with the real VIX: when the target market has closed days, such as SPX being closed on weekends, the calculation will be off because length window pd will include less open days bars because off days will be included, which are just duplicate values. Not sure there is an easy fix unless pinescript gets modified to include a way to fetch only open days bars regardless on the current symbol opened in the chart. The next equation, skipping na bars, is trying to workaround this limitation.\\n\\n// Williams Vix Fix Formula\\nwvf = (highest_close - source_low) / highest_close * 100\\n\\n// WILLIAM\\'S VIXFIX TYPICAL HISTOGRAM PLOT\\n//\\nwvf_display = (vixfix_log ? math.log(wvf+1) : wvf)  // add +1 to ensure we don\\'t get any value between 0 and 1, which would result in a negative value\\nplot(swvf and wvf ? wvf_display : na, title=\\'Williams Vix Fix Histogram\\', style=plot.style_columns, linewidth=4, color=color.new(color.gray, 50), histbase=0.0)\\n\\n// WILLIAM\\'S VIXFIX CANDLES/BARS PLOT\\n//\\nwvf_open_mode = true\\nhighest_open = not skip_off_days ? ta.highest(source_open, pd) : na_skip_highest(source_open, pd)\\nhighest_high = not skip_off_days ? ta.highest(source_high, pd) : na_skip_highest(source_high, pd)\\nhighest_low = not skip_off_days ? ta.highest(source_low, pd) : na_skip_highest(source_low, pd)\\nwvf_close = (highest_close - source_low) / highest_close * 100\\nwvf_open = wvf_open_mode ? nz(wvf_close[1]) : (highest_open - source_low) / highest_open * 100 // the latter equation is useless in general because unless there are lots of gaps, open == close of the previous day, hence candles will most often have the same open and close values\\nwvf_high = (highest_high - source_low) / highest_high * 100\\nwvf_low = (highest_low - source_low) / highest_low * 100\\n// rescale on a logscale if necessary\\nwvf_close_rescaled = vixfix_log ? math.max(0.0, math.log(wvf_close+1)) : wvf_close  // add +1 to ensure we don\\'t get any value between 0 and 1, which would result in a negative value\\nwvf_open_rescaled = vixfix_log ? math.max(0.0, math.log(wvf_open+1)) : wvf_open\\nwvf_high_rescaled = vixfix_log ? math.max(0.0, math.log(wvf_high+1)) : wvf_high\\nwvf_low_rescaled = vixfix_log ? math.max(0.0, math.log(wvf_low+1)) : wvf_low // cap to 0.0 because log of values < 1.0 will produce negative values, which we don\\'t want as they will go towards the inverse vixfix part of the graph and are not meaningful\\n// bars color\\nwvf_bars_color = (wvf_close >= wvf_open) ? color.lime : color.red\\n// display bars or candles plot\\n// candles plot\\nplotcandle(wvf_open_rescaled, wvf_high_rescaled, wvf_low_rescaled, wvf_close_rescaled, title=\"Williams VixFix OHLC Candles Plot\", color=wvf_bars_color, wickcolor=wvf_bars_color, bordercolor=wvf_bars_color, display=showvixfixbars and vixfixbars_display_mode == \\'candlesplot\\' ? display.all : display.none)\\n// bars plot\\nplotbar(wvf_open_rescaled, wvf_high_rescaled, wvf_low_rescaled, wvf_close_rescaled, title=\"Williams VixFix OHLC Bars Plot\", color=wvf_bars_color, display=showvixfixbars and vixfixbars_display_mode == \\'barsplot\\' ? display.all : display.none)\\n\\n// PRICE DIVISION\\n//\\nprice_div_wvf_close = close / wvf_close\\nprice_div_wvf_open = open / wvf_open\\nprice_div_wvf_low = high / math.max(wvf_high, wvf_close, wvf_open, wvf_low)  // low and high are inverted since we divide\\nprice_div_wvf_high = high / math.min(wvf_low, wvf_close, wvf_open, wvf_high)  // wvf_low can be 0.0, then division by zero and hence na value\\nif na(price_div_wvf_high) or price_div_wvf_high > 2*(math.max(price_div_wvf_close, price_div_wvf_open))\\n    // Since we divide by lowest prices, we can get a division by a very tiny number like 0.01, then we can get a huge wick that does not represent anything.\\n    // In this case, we just revert to divide to another more reasonable value.\\n    price_div_wvf_high := nz(high / math.min(wvf_close, wvf_open, wvf_high))\\n    //price_div_wvf_high := nz(math.max(price_div_wvf_close, price_div_wvf_open, price_div_wvf_high))\\n    //price_div_wvf_high := 1.0\\n\\n// Plot divided price on a log scale, otherwise because of division we can only see some big candles and the rest is flat\\n// Also we featurescale so that we can plot price alongside the vixfix histogram\\n//max_wvf_high = math.max(wvf_high, nz(wvf_high[1]))\\nmax_wvf_high = ta.max(wvf_high_rescaled) // get the max price over the historic bars, to rescale price divided by vixfix\\nif nz(pricedivscale) == 0.0 // autoscaling by finding the max high value of current series\\n    pricedivscale := max_wvf_high\\n// Find max and min target value to rescale to\\n// If adaptive rescaling, then use a sliding window, otherwise get the min/max over the whole history\\nprice_div_wvf_high_max = pricedivadaptiverescaling > 0 ? ta.highest(price_div_wvf_high, pricedivadaptiverescaling) : ta.max(price_div_wvf_high)\\nprice_div_wvf_low_min = pricedivadaptiverescaling > 0 ? ta.highest(price_div_wvf_low, pricedivadaptiverescaling) : ta.max(price_div_wvf_low)\\n// Rescale on a log scale if enabled\\nmax_price_div_wvf_high = pricedivscale_log ? math.log(price_div_wvf_high_max) : price_div_wvf_high_max\\nmin_price_div_wvf_low = pricedivscale_log ? math.log(price_div_wvf_low_min) : price_div_wvf_low_min\\n// Apply rescaling (we use two different functions depending on if x is rescaled on a log scale or not)\\nprice_div_wvf_open_rescaled = pricedivscale_log ? f_logfscale(price_div_wvf_open, pricedivscale, min_price_div_wvf_low, max_price_div_wvf_high) * pricedivafterscale : f_featurescale(price_div_wvf_open, 0.0, pricedivscale, min_price_div_wvf_low, max_price_div_wvf_high)\\nprice_div_wvf_high_rescaled = pricedivscale_log ? f_logfscale(price_div_wvf_high, pricedivscale, min_price_div_wvf_low, max_price_div_wvf_high) * pricedivafterscale : f_featurescale(price_div_wvf_high, 0.0, pricedivscale, min_price_div_wvf_low, max_price_div_wvf_high)\\nprice_div_wvf_low_rescaled = pricedivscale_log ? f_logfscale(price_div_wvf_low, pricedivscale, min_price_div_wvf_low, max_price_div_wvf_high) * pricedivafterscale : f_featurescale(price_div_wvf_low, 0.0, pricedivscale, min_price_div_wvf_low, max_price_div_wvf_high)\\nprice_div_wvf_close_rescaled = pricedivscale_log ? f_logfscale(price_div_wvf_close, pricedivscale, min_price_div_wvf_low, max_price_div_wvf_high) * pricedivafterscale : f_featurescale(price_div_wvf_close, 0.0, pricedivscale, min_price_div_wvf_low, max_price_div_wvf_high)\\n// Bar colors\\nprice_div_wvf_color = (price_div_wvf_close >= price_div_wvf_open) ? color.aqua : color.yellow\\n// Plot!\\nplotcandle(price_div_wvf_open_rescaled, price_div_wvf_high_rescaled, price_div_wvf_low_rescaled, price_div_wvf_close_rescaled, title=\"Price Divided By William\\'s VixFix (OHLC Candles Plot)\", color=price_div_wvf_color, wickcolor=price_div_wvf_color, bordercolor=price_div_wvf_color, display=showpricediv ? display.all : display.none) // uncomment this when ready\\n//plotcandle(price_div_wvf_open, price_div_wvf_high, price_div_wvf_low, price_div_wvf_close, title=\"Price Divided By William\\'s VixFix\", color=price_div_wvf_color, wickcolor=price_div_wvf_color, bordercolor=price_div_wvf_color) // debug\\n\\n// Plot Oscillator of VixFix\\n// Log rescale if enabled\\nosc_open = osclog ? math.log(wvf_close) : wvf_close\\nosc_high = osclog ? math.log(wvf_high) : wvf_high\\nosc_low = osclog ? math.log(wvf_low) : wvf_low\\n// Convert Price / VixFix into a stochastic oscillator\\nwvf_stoch = ta.stoch(osc_open, osc_high, osc_low, osclp)\\n// Visually rescale if we want to plot alongside VixFix\\nwvf_high_highest_osc = oscrescale > 0 ? ta.highest(wvf_high_rescaled, oscrescale) : na\\nwvf_stoch_disp = oscrescale > 0 ? f_featurescale(wvf_stoch, 0.0, wvf_high_highest_osc, 0.0, 100.0) : wvf_stoch\\n// Plot stochastic oscillator\\nplot(wvf_stoch_disp, title=\\'Price / VixFix Stoch\\', color=color.new(color.olive, 0), linewidth=2, display=oscdisp ? display.all : display.none)\\n// Draw limits\\nhline(100, title=\"Price / VixFix Stoch upper limit (100%)\", color=color.gray, linestyle=hline.style_dashed, display=oscdisp and oscrescale <= 0 ? display.all : display.none)\\nplot(wvf_high_highest_osc, title=\"Price / VixFix Stoch upper limit (unbounded, dynamically rescaled when plotted alongside VixFix)\", color=color.gray, display=oscdisp and oscrescale > 0 ? display.all : display.none)\\nhline(50, title=\"Price / VixFix Stoch neutral level (50%)\", color=color.gray, linestyle=hline.style_dotted, display=oscdisp and oscrescale <= 0 ? display.all : display.none)\\nplot(wvf_high_highest_osc / 2, title=\"Price / VixFix Stoch neutral limit (unbounded, dynamically rescaled when plotted alongside VixFix)\", color=color.gray, display=oscdisp and oscrescale > 0 ? display.all : display.none)\\nh1 = hline(0, title=\"Price / VixFix Stoch lower limit (0%)\", color=color.gray, linestyle=oscrescale <= 0 ? hline.style_dashed : hline.style_solid, display=oscdisp ? display.all : display.none)\\n// Overbought/oversold highlight in a bar (because the background would get too cluttered otherwise with a background highlight)\\nh2 = hline(-5, title=\\'VIXfix Stoch bar highlight lower line\\', display=display.none)\\noschlight_condition = not oschlight ? na : wvf_stoch > oschlight_bthr ? color.new(color.green, 50) : wvf_stoch < oschlight_sthr ? color.new(color.red, 50) : na\\nfill(h1, h2, color=oschlight_condition, title=\\'VIXfix Stochastic Oscillator Overbought/Oversold Background Highlight\\', display=not oschlight_bg ? display.all : display.none)\\n// Or as background\\nbgcolor(oschlight_condition, title=\\'Price / VIXfix Stochastic Oscillator Overbought/Oversold Background Highlight\\', display=oschlight_bg ? display.all : display.none)\\n\\n// Plot Oscillator of Price / VixFix\\n// Log rescale if enabled\\noscpv_open = oscpvlog ? math.log(price_div_wvf_close) : price_div_wvf_close\\noscpv_high = oscpvlog ? math.log(price_div_wvf_high) : price_div_wvf_high\\noscpv_low = oscpvlog ? math.log(price_div_wvf_low) : price_div_wvf_low\\n// Convert Price / VixFix into a stochastic oscillator\\nprice_div_wvf_stoch = ta.stoch(oscpv_open, oscpv_high, oscpv_low, oscpvlp)\\n// Visually rescale if we want to plot alongside VixFix\\nwvf_high_highest_oscpv = oscpvrescale > 0 ? ta.highest(wvf_high_rescaled, oscpvrescale) : na\\nprice_div_wvf_stoch_disp = oscpvrescale > 0 ? f_featurescale(price_div_wvf_stoch, 0.0, wvf_high_highest_oscpv, 0.0, 100.0) : price_div_wvf_stoch\\n// Plot stochastic oscillator\\nplot(price_div_wvf_stoch_disp, title=\\'Price / VixFix Stoch\\', color=color.new(color.purple, 0), linewidth=2, display=oscpvdisp ? display.all : display.none)\\n// Draw limits\\nhline(100, title=\"Price / VixFix Stoch upper limit (100%)\", color=color.gray, linestyle=hline.style_dashed, display=oscpvdisp and oscpvrescale <= 0 ? display.all : display.none)\\nplot(wvf_high_highest_oscpv, title=\"Price / VixFix Stoch upper limit (unbounded, dynamically rescaled when plotted alongside VixFix)\", color=color.gray, display=oscpvdisp and oscpvrescale > 0 ? display.all : display.none)\\nhline(50, title=\"Price / VixFix Stoch neutral level (50%)\", color=color.gray, linestyle=hline.style_dotted, display=oscpvdisp and oscpvrescale <= 0 ? display.all : display.none)\\nplot(wvf_high_highest_oscpv / 2, title=\"Price / VixFix Stoch neutral limit (unbounded, dynamically rescaled when plotted alongside VixFix)\", color=color.gray, display=oscpvdisp and oscpvrescale > 0 ? display.all : display.none)\\nhline(0, title=\"Price / VixFix Stoch lower limit (0%)\", color=color.gray, linestyle=oscpvrescale <= 0 ? hline.style_dashed : hline.style_solid, display=oscpvdisp ? display.all : display.none)\\n// Overbought/oversold bar highlight\\nh3 = hline(-10, title=\\'VIXfix Stoch bar highlight lower line\\', display=display.none)\\noscpvhlight_condition = not oscpvhlight ? na : price_div_wvf_stoch > oscpvhlight_bthr ? color.new(color.yellow, 50) : price_div_wvf_stoch < oscpvhlight_sthr ? color.new(color.aqua, 50) : na\\nfill(h2, h3, color=oscpvhlight_condition, title=\\'Price / VIXfix Stochastic Oscillator Overbought/Oversold Bar Highlight\\', display=not oscpvhlight_bg ? display.all : display.none)\\n// Or as a background highlight\\nbgcolor(oscpvhlight_condition, title=\\'Price / VIXfix Stochastic Oscillator Overbought/Oversold Background Highlight\\', display=oscpvhlight_bg ? display.all : display.none)\\n', ' //@version=5 \\nindicator(\"VIX Oscillator Smooth\", shorttitle=\"VIX Osc Sm\", overlay=false) \\n\\n// User Inputs \\ntimeframe = input.timeframe(\"\", title=\"Period Timeframe\")  \\nAvgLength = input.int(14, title=\"Average Length\") \\nSmoothLength = input.int(3, title=\"Smooth Length\") \\n\\n// Securities \\nticker = ticker.new(syminfo.prefix, syminfo.ticker, session.regular) \\nvix = request.security(\"CBOE:VIX\", timeframe, close)  \\ncl = request.security(ticker, timeframe, close, lookahead=barmerge.lookahead_on) \\n\\n// Z-Score Calculation \\n\\n// Z Score VIX  \\nvixhx = ta.sma(vix, AvgLength)  \\nvixsd = ta.stdev(vix, AvgLength)  \\nz_raw = (vix - vixhx) / vixsd  \\nz = ta.sma(z_raw, SmoothLength) // Smoothed Z Score VIX line \\n\\n// Z Score Stock \\nstockhx = ta.sma(cl, AvgLength)  \\nstocksd = ta.stdev(cl, AvgLength) \\nzs_raw = (cl - stockhx) / stocksd  \\nzs = ta.sma(zs_raw, SmoothLength) // Smoothed Z Score Stock line \\n\\n// User Inputs for background levels and colors \\nupper_level = input.float(2.5, title=\"Upper Level for Hline Background\", step=0.1) \\nupper_color = input.color(color.gray, title=\"Color for Upper Level Background\") \\nlower_level = input.float(-2.5, title=\"Lower Level for Hline Background\", step=0.1) \\nlower_color = input.color(#ff525275, title=\"Color for Lower Level Background\") \\nzero_level = input.float(0, title=\"Zero Level for Hline Background\", step=0.1) \\nzero_color = input.color(#2195f38a, title=\"Color for Zero Level Background\") \\n\\n// Background color \\nbgcolor_condition = (z >= upper_level and zs <= lower_level) or (z <= lower_level and zs >= upper_level) ? upper_color :  \\n                   (z >= zero_level and zs <= zero_level) or (z <= zero_level and zs >= zero_level) ? zero_color : \\n                   lower_color \\nbgcolor(bgcolor_condition) \\n\\n// Plots \\nplot(z, \"VIX Smooth\", color=color.purple, linewidth=3) // Cambiado a violeta\\nplot(zs, \"Ticker Smooth\", color=color.blue, linewidth=3) // Cambiado a azul\\n\\n// Horizontal lines for levels \\nhline(upper_level, \"Upper Level\", color=color.green)\\nhline(zero_level, \"Zero Level\", color=color.gray)  \\nhline(lower_level, \"Lower Level\", color=color.red) \\n\\n// Draw horizontal lines at upper and zero levels and store their IDs\\nhline_upper = hline(upper_level, \"Upper Level\", color=color.green) \\nhline_zero = hline(zero_level, \"Zero Level\", color=color.gray) \\nhline_lower = hline(lower_level, \"Upper Level\", color=#d61f1f) \\n\\n// Fill the area between zero level and upper level\\nfill(hline_zero, hline_upper, color=#4caf4f69)\\n// Fill the area between zero level and lower level\\nfill(hline_zero, hline_lower, color=#af4c4c69)\\n \\n// Alert conditions for hline background \\nalertcondition(z >= upper_level and zs <= lower_level, title=\"Upper Level Background Alert\", message=\"VIX is at upper level and Ticker is at lower level\") \\nalertcondition(z <= lower_level and zs >= upper_level, title=\"Lower Level Background Alert\", message=\"VIX is at lower level and Ticker is at upper level\") \\n \\n// Signal for crossover at level 0 \\ncrossover_signal = ta.crossover(z, zs) and zs[1] < 0 or ta.crossunder(z, zs) and zs[1] > 0 \\nplotshape(series=crossover_signal, title=\"Crossover Signal\", location=location.absolute, style=shape.circle, color=#ffae0083, size=size.tiny, offset=-AvgLength/2)\\n/@version=5\\nindicator(\\'Matrix Series and Vix Fix with VWAP CCI and QQE Signals\\', shorttitle=\\'Matrix\\', precision=2)\\n\\n// Function to select the type of source\\nget_src(Type) =>\\n    switch Type \\n        \"VWAP\" => ta.vwap\\n        \"Close\" => close\\n        \"Open\" => open\\n        \"HL2\" => hl2\\n        \"HLC3\" => hlc3\\n        \"OHLC4\" => ohlc4\\n        \"HLCC4\" => hlcc4\\n        \"High\" => high\\n        \"Low\" => low\\n        \"TR\" => ta.tr\\n        \"vwap(Close)\" => ta.vwap(close)\\n        \"vwap(Open)\" => ta.vwap(open)\\n        \"vwap(High)\" => ta.vwap(high)\\n        \"vwap(Low)\" => ta.vwap(low)\\n        \"AVG(vwap(H,L))\" => math.avg(ta.vwap(high), ta.vwap(low))\\n        \"AVG(vwap(O,C))\" => math.avg(ta.vwap(open), ta.vwap(close))        \\n//__________________________________________________________________\\n// Based on \"Matrix Series\" - Author: @glaz\\n// https://www.tradingview.com/script/2X2cVLhb-Matrix-Series/\\n// https://www.wisestocktrader.com/indicators/2739-flower-indicator\\n//__________________________________________________________________\\nalert_Buy_Matrix  = input.bool(false, \"? Alert: [Buy - Matrix]\",  inline=\"alertMatrix\")\\nalert_Sell_Matrix = input.bool(false, \"? Alert: [Sell - Matrix]\", inline=\"alertMatrix\")\\nshow_buy_signal   = input.bool(true,  \"Show buy signal\",  inline=\"show_signal\")\\nshow_sell_signal  = input.bool(true,  \"Show sell signal\", inline=\"show_signal\")\\nshow_dot          = input.bool(true,  \"Show Watch/Warning Point\", inline=\"show_signal\")\\n//--- Trend Bought/Sold Detail\\ndynamic      = input.bool(true,  \"Show Dynamic Zones ??? \", inline=\"show_limit\")\\nOBOS         = input.bool(false, \"Show OB/OS\", inline=\"show_limit\")\\nshow_candles = input.bool(true,  \"Show Matrix Candles  ??? \", inline=\"s_matrix\")\\nshow_hist    = input.bool(false, \"Show Matrix Histogram\", inline=\"s_matrix\")\\n//--- Sup/Res Detail\\nPricePeriod      = input.int(16,  \"Price Period\", 1, inline=\"period\")\\nSmoother         = input.int(5,   \"Smoother\", 2, inline=\"period\")\\nSupResPeriod     = input.int(50,  \"Superior Resolution Period\", 1, inline=\"Superior Resolution\")\\nSupResPercentage = input.int(100, \"Percentage\", 1, inline=\"Superior Resolution\")\\n//--- Line Detail \\nob = input.int(200,  \"OverBought Above\", inline=\"line\")\\nos = input.int(-200, \"OverSold Bellow\",  inline=\"line\")\\n\\nys1  = (high + low + close * 2) / 4\\nrk3  = ta.ema(ys1, Smoother)\\nrk4  = ta.stdev(ys1, Smoother)\\nrk5  = (ys1 - rk3) * 200 / rk4\\nrk6  = ta.ema(rk5, Smoother)\\nup   = ta.ema(rk6, Smoother)\\ndown = ta.ema(up, Smoother)\\nOo   = up < down ? up : down\\nHh   = Oo\\nLl   = up < down ? down : up\\nCc   = Ll\\nsell_matrix = ta.cross(up, ob) == 1 and up[1] > up \\nbuy_matrix  = ta.cross(up, os) == 1 and up[1] < up \\n\\ncoral     = #FF8080\\nlavender  = #8080FF\\namber     = color.new(#FFE500, 60)\\nvcolor = show_candles and (Oo > Cc) ? coral : show_candles and (up > down) ? lavender : show_candles ? coral : na\\n\\nplotcandle(Oo, Hh, Ll, Cc, \"Matrix Candles\", color = vcolor, wickcolor = vcolor, bordercolor = vcolor)\\nplotshape(show_sell_signal and sell_matrix, \"Sell\", shape.triangledown, location.top, coral)\\nbgcolor(show_sell_signal and sell_matrix ? color.new(coral, 70) : na, title=\"Sell\")\\nplotshape(show_buy_signal and buy_matrix, \"Buy\", shape.triangleup, location.bottom, lavender)\\nbgcolor(show_buy_signal and buy_matrix ? color.new(lavender, 70) : na, title=\"Buy\")\\n\\n//-------S/R Zones------\\nLookback = SupResPeriod\\nPerCent  = SupResPercentage\\n\\nValue1 = ta.cci(close, PricePeriod)\\nValue2 = ta.highest(Value1, Lookback)\\nValue3 = ta.lowest(Value1, Lookback)\\nValue4 = Value2 - Value3\\nValue5 = Value4 * (PerCent / 100)\\nResistanceLine = Value3 + Value5\\nSupportLine    = Value2 - Value5\\nplot(dynamic ? ResistanceLine : na, \"Resistance Line\", color.new(coral, 5))\\nplot(dynamic ? ResistanceLine : na, \"Resistance Line\", color.new(coral, 90), 7)\\nplot(dynamic ? SupportLine    : na, \"Support Line\",    color.new(lavender, 5))\\nplot(dynamic ? SupportLine    : na, \"Support Line\",    color.new(lavender, 90), 7)\\n\\n//--Overbought/Oversold/Warning Detail\\nh01 = ta.highest(up, 1)   + 20\\nh02 = ta.highest(down, 1) + 20\\nl01 = ta.lowest(down, 1)  - 20\\nl02 = ta.lowest(up, 1)    - 20\\nUPshape   = up   > ob and up > down ? h01 : up   > ob and up < down ? h02 : na\\nDOWNshape = down < os and up > down ? l01 : down < os and up < down ? l02 : na\\n\\nplot(show_dot ? UPshape   : na, \"UP Shape\",   amber, 4, plot.style_circles)\\nplot(show_dot ? DOWNshape : na, \"DOWN Shape\", amber, 4, plot.style_circles)\\nhline(OBOS ? ob : na, \"OverBought\")\\nhline(OBOS ? os : na, \"OverSold\")\\n\\nAccumulationColor = color.new(#FFA07A, 60)     //\"Accumulation Zone\"\\nWarningColor      = color.new(color.white, 60) //\"Warning/Watch Signal\"\\nDistributionColor = color.new(#3CB371, 60)     //\"Distribution Zone\"\\n\\nrk5_210  = ( ys1 - rk3 ) * 210 / rk4\\nrk6_210  = ta.ema(rk5_210, Smoother)\\nUP_210   = ta.ema(rk6_210, Smoother)\\nDOWN_210 = ta.ema(UP_210, Smoother)\\nUPColor  = (UP_210 >= 210 ? DistributionColor : (UP_210 <= -210 ? AccumulationColor : WarningColor))\\nplot(show_hist ? UP_210   : na, \"UP\",         UPColor, 1, plot.style_columns)\\nplot(show_hist ? DOWN_210 : na, \"Signal(UP)\", color.aqua)\\n\\nif alert_Buy_Matrix and buy_matrix\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Buy [Matrix Series].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Sell_Matrix and sell_matrix\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Sell [Matrix Series].-\\', alert.freq_once_per_bar_close)\\n\\n//_____________________________________________________________________________________________\\n// Based on \"Williams Vix Fix\" - Author: @ChrisMoody\\n// https://www.tradingview.com/script/pJpXG5JH-CM-Williams-Vix-Fix-V3-Ultimate-Filtered-Alerts/\\n// https://www.ireallytrade.com/newsletters/VIXFix.pdf\\n//_____________________________________________________________________________________________\\nshows_Vix_Fix = input(false, \\'Show Vix Fix Histogram ???\\', inline=\"histVF\", group=\\'Williams Vix Fix\\')\\nswvfm    = input.float(1.0, \"Multiplier (Values 0.5 to 2)\", minval = 0.5, maxval = 2, inline=\"histVF\", group=\\'Williams Vix Fix\\')\\nsbcFilt  = input.bool(true, \\'Show Signal For Filtered Entry [? FE ]\\', group=\\'Williams Vix Fix\\') // Use FILTERED Criteria\\nsbcAggr  = input.bool(true, \\'Show Signal For AGGRESSIVE Filtered Entry [? AE ]\\', group=\\'Williams Vix Fix\\') // Use FILTERED Criteria\\nalert_AE = input.bool(false, \\'? Alert: [AGGRESSIVE Entry]\\', inline=\\'AlertVF\\', group=\\'Williams Vix Fix\\')\\nalert_FE = input.bool(false, \\'? Alert: [Filtered Entry]\\', inline=\\'AlertVF\\', group=\\'Williams Vix Fix\\')\\npd       = input.int(22, \\'LookBack Period Standard Deviation High\\', 1, group=\\'Williams Vix Fix\\')\\nbbl      = input.int(20, \\'Bolinger Band Length\\', 1, group=\\'Williams Vix Fix\\')\\nmult     = input.float(2.0, \\'Bollinger Band Standard Devaition Up\\', minval=1, maxval=5, group=\\'Williams Vix Fix\\')\\nlb       = input.int(50, \\'Look Back Period Percentile High\\', 1, group=\\'Williams Vix Fix\\')\\nph       = input.float(.85, \\'Highest Percentile - 0.90=90%, 0.95=95%, 0.99=99%\\', minval=.05, maxval=1, group=\\'Williams Vix Fix\\')\\nltLB     = input.int(40, minval=25, maxval=99, title=\\'Long-Term Look Back Current Bar Has To Close Below This Value OR Medium Term--Default=40\\', group=\\'Williams Vix Fix\\')\\nmtLB     = input.int(14, minval=10, maxval=20, title=\\'Medium-Term Look Back Current Bar Has To Close Below This Value OR Long Term--Default=14\\', group=\\'Williams Vix Fix\\')\\nstr      = input.int(3,  minval=1,  maxval=9,  title=\\'Entry Price Action Strength--Close > X Bars Back---Default=3\\', group=\\'Williams Vix Fix\\')\\n\\n// Williams Vix Fix Formula\\nwvf       = (ta.highest(close, pd) - low) / ta.highest(close, pd) * 100\\nsDev      = mult * ta.stdev(wvf, bbl)\\nmidLine   = ta.sma(wvf, bbl)\\nlowerBand = midLine - sDev\\nupperBand = midLine + sDev\\nrangeHigh = ta.highest(wvf, lb) * ph\\n\\n// Filtered Criteria\\nupRange      = low > low[1] and close > high[1]\\nupRange_Aggr = close > close[1] and close > open[1]\\n// Filtered Criteria\\nfiltered      = (wvf[1] >= upperBand[1] or wvf[1] >= rangeHigh[1]) and wvf < upperBand and wvf < rangeHigh\\nfiltered_Aggr = (wvf[1] >= upperBand[1] or wvf[1] >= rangeHigh[1]) and not(wvf < upperBand and wvf < rangeHigh)\\n\\n// Alerts Criteria\\nalert1  = wvf >= upperBand or wvf >= rangeHigh ? 1 : 0\\nalert2  = (wvf[1] >= upperBand[1] or wvf[1] >= rangeHigh[1]) and wvf < upperBand and wvf < rangeHigh ? 1 : 0\\ncond_FE = upRange and close > close[str] and (close < close[ltLB] or close < close[mtLB]) and filtered \\nalert3  = cond_FE ? 1 : 0\\ncond_AE = upRange_Aggr and close > close[str] and (close < close[ltLB] or close < close[mtLB]) and filtered_Aggr\\nalert4  = cond_AE ? 1 : 0\\n\\ncolor_VF = alert1 ? color.new(color.lime, 60) : alert2 ? color.new(color.teal, 60) : color.new(color.silver, 80)\\nswvfm_precent = swvfm * (ResistanceLine - SupportLine) / (2.2 * rangeHigh)\\n\\nplot(shows_Vix_Fix ? wvf * -1 * swvfm_precent : na, \\'Williams Vix Fix\\', color_VF, 4, plot.style_columns)\\n\\nplotshape(sbcAggr and alert4 ? alert4 : na, \\'Aggressive Entry\\', shape.triangleup, location.bottom, color.new(#80FF00, 0), text=\\'AE\\', textcolor=color.new(#80FF00, 0))\\nbgcolor(sbcAggr and alert4 ? color.new(#80FF00, 80) : na, title=\\'Aggressive Entry\\')\\nplotshape(sbcFilt and alert3 ? alert3 : na, \\'Filtered Entry\\', shape.triangleup, location.bottom, color.new(#15FF00, 0), text=\\'FE\\', textcolor=color.new(#15FF00, 0))\\nbgcolor(sbcFilt and alert3 ? color.new(#15FF00, 80) : na, title=\\'Filtered Entry\\')\\n\\nif alert_AE and cond_AE\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Aggressive Entry [VixFix].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_FE and cond_FE\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Filtered Entry [VixFix].-\\', alert.freq_once_per_bar_close)\\n\\n//_____________________________________________________________\\n// Based on \"QQE signals\" - Author: @colinmck\\n// https://www.tradingview.com/script/7R6ZxyZu-QQE-signals/\\n//_____________________________________________________________\\nshows_QQE       = input.bool(false, \"????????? Show QQE Signals ?????????\")\\nalert_Long_QQE  = input.bool(false, \"? Alert: [Long - QQE]\",  inline=\"alertQQE\")\\nalert_Short_QQE = input.bool(false, \"? Alert: [Short - QQE]\", inline=\"alertQQE\")\\nRSI_Period = input.int(14,      \"RSI Length\", 1)\\nSF         = input.int(5,       \"RSI Smoothing\", 2)\\nQQE        = input.float(4.238, \"Fast QQE Factor\", 1)\\n\\nWilders_Period = RSI_Period * 2 - 1\\nlongband = 0.0\\nshortband = 0.0\\ntrend = 0\\n\\nRsi      = ta.rsi(close, RSI_Period)\\nRSIndex  = ta.ema(Rsi, SF)\\nAtrRsi   = math.abs(RSIndex[1] - RSIndex)\\nMaAtrRsi = ta.ema(AtrRsi, Wilders_Period)\\n\\nDeltaFastAtrRsi = ta.ema(MaAtrRsi, Wilders_Period) * QQE\\nnewshortband = RSIndex + DeltaFastAtrRsi\\nnewlongband = RSIndex - DeltaFastAtrRsi\\nlongband := RSIndex[1] > longband[1] and RSIndex > longband[1] ? math.max(longband[1], newlongband) : newlongband\\nshortband := RSIndex[1] < shortband[1] and RSIndex < shortband[1] ? math.min(shortband[1], newshortband) : newshortband\\ncross_1 = ta.cross(longband[1], RSIndex)\\ntrend := ta.cross(RSIndex, shortband[1]) ? 1 : cross_1 ? -1 : nz(trend[1], 1)\\nFastAtrRsiTL = trend == 1 ? longband : shortband\\n\\n// Find all the QQE Crosses\\nQQExlong = 0\\nQQExlong := nz(QQExlong[1])\\nQQExshort = 0\\nQQExshort := nz(QQExshort[1])\\nQQExlong := FastAtrRsiTL < RSIndex ? QQExlong + 1 : 0\\nQQExshort := FastAtrRsiTL > RSIndex ? QQExshort + 1 : 0\\n\\n// Conditions\\nqqeLong = QQExlong == 1 ? FastAtrRsiTL[1] - 50 : na\\nqqeShort = QQExshort == 1 ? FastAtrRsiTL[1] - 50 : na\\n\\n// Plotting\\nredorange  = color.new(#FF4000, 0)\\nchartreuse = color.new(#80FF00, 0)\\nplotshape(shows_QQE and qqeLong,  \"QQE Long\",  shape.labelup,   location.bottom, color.new(chartreuse, 25), text=\"QQE\", size=size.tiny, textcolor=color.new(#000000, 5))\\nplotshape(shows_QQE and qqeShort, \"QQE Short\", shape.labeldown, location.top,    color.new(redorange, 25),  text=\"QQE\", size=size.tiny, textcolor=color.new(color.white, 5))\\n\\nif alert_Long_QQE  and qqeLong\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Long [Quantitative Qualitative Estimation (QQE)].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Short_QQE and qqeShort\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Short [Quantitative Qualitative Estimation (QQE)].-\\', alert.freq_once_per_bar_close)\\n\\n//___________________________________________________________________________\\n// VWAP CCI Signals\\n//___________________________________________________________________________\\nshows_VCCI = input.bool(false, \"????????? Show VWAP CCI Signals ?????????\")\\nlen_cci    = input.int(21, \"Length\", 1, inline=\"cci\")\\nType_cci   = input.string(\"vwap(Close)\", \"Source\", inline=\"cci\", options=[\"vwap(Close)\", \"vwap(Open)\", \"vwap(High)\", \"vwap(Low)\", \"AVG(vwap(H,L))\", \"AVG(vwap(O,C))\", \"VWAP\", \"Close\", \"Open\", \"HL2\", \"HLC3\", \"OHLC4\", \"HLCC4\", \"High\", \"Low\", \"TR\"])\\nalert_Long_cci  = input.bool(false, \"? Alert: [Long - CCI]\",  inline=\"alertCCI\")\\nalert_Short_cci = input.bool(false, \"? Alert: [Short - CCI]\", inline=\"alertCCI\")\\n\\n// ALMA - Arnaud Legoux Moving Average of @kurtsmock\\nenhanced_alma(_series, _length, _offset, _sigma) =>\\n    length      = int(_length) // Floating point protection\\n    numerator   = 0.0\\n    denominator = 0.0 \\n    m = _offset * (length - 1)\\n    s = length / _sigma\\n    for i=0 to length-1\\n        weight       = math.exp(-((i-m)*(i-m)) / (2 * s * s))\\n        numerator   := numerator   + weight * _series[length - 1 - i]\\n        denominator := denominator + weight\\n    numerator / denominator\\n\\n// Calculate vwap cci\\nsrc_cci = get_src(Type_cci)\\ncci = (src_cci - enhanced_alma(src_cci, len_cci, 0.85, 6.0)) / (0.015 * ta.dev(src_cci, len_cci))\\n\\n// Define long and short entry signal\\nlong_cci  = ta.crossover(cci, -200)\\nshort_cci = ta.crossunder(cci, 200)\\n\\nplotshape(shows_VCCI and long_cci,  \"VWAP CCI Long\",  shape.labelup,   location.bottom, color.new(chartreuse, 25), text=\"CCI\", size=size.tiny, textcolor=color.new(#000000, 5))\\nplotshape(shows_VCCI and short_cci, \"VWAP CCI Short\", shape.labeldown, location.top,    color.new(redorange, 25),  text=\"CCI\", size=size.tiny, textcolor=color.new(color.white, 5))\\n\\nif alert_Long_cci and long_cci\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Long [Commodity Channel Index (CCI)].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Short_cci and short_cci\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Short [Commodity Channel Index (CCI)].-\\', alert.freq_once_per_bar_close)\\n\\n//___________________________________________________________________________________________\\n// Based on \"Squeeze Momentum Indicator\" - Author: @LazyBear\\n// @KivancOzbilgic: https://www.tradingview.com/script/NVzKGYFJ/\\n// @capissimo: https://www.tradingview.com/script/Ejx2tEHY-Squeeze-Momentum-Indicator-mod-3/\\n// Indicator description: http://www.forextrading-pips.com/squeeze-indicator/\\n//___________________________________________________________________________________________\\nshow_SMI   = input.bool(false,     \"Show Squeeze Momentum Indicator\", group = \"Squeeze Momentum Indicator\")\\nalert_Buy  = input.bool(false,     \"? Alert: [Buy - Mom]\",  inline = \"alertMom\", group = \"Squeeze Momentum Indicator\")\\nalert_Sell = input.bool(false,     \"? Alert: [Sell - Mom]\", inline = \"alertMom\", group = \"Squeeze Momentum Indicator\")\\nType_src   = input.string(\"Close\", \"Source\", inline = \"s1\", group = \"Squeeze Momentum Indicator\", options = [\"VWAP\", \"Close\", \"Open\", \"HL2\", \"HLC3\", \"OHLC4\", \"HLCC4\", \"High\", \"Low\", \"vwap(Close)\", \"vwap(Open)\", \"vwap(High)\", \"vwap(Low)\", \"AVG(vwap(H,L))\", \"AVG(vwap(O,C))\", \"TR\"])\\nsrc0       = get_src(Type_src)\\nshow_sb    = input.bool(false, \"Show buy/sell signal\", inline = \"s1\", group = \"Squeeze Momentum Indicator\")\\nlengthMom  = input.int(20,     \"Momentum Length\", 1, inline = \"mom\", group = \"Squeeze Momentum Indicator\")\\nsiglen     = input.int(5,      \"Signal\", 2, inline = \"mom\", group = \"Squeeze Momentum Indicator\")\\nlengthSQZ  = input.int(20,     \"KC/BB Length\", 1, inline = \"sqz\", group = \"Squeeze Momentum Indicator\")\\nmultBB     = input.float(2.0,  \"BB MultFactor\", 0.5, step=0.05, inline = \"sqz\", group = \"Squeeze Momentum Indicator\")\\nmultKC     = input.float(1.3,  \"KC MultFactor\", 0.5, step=0.05, inline = \"kc\", group = \"Squeeze Momentum Indicator\")\\nuse_tr     = input.bool(true,  \"Use TrueRange (KC)\", inline = \"kc\", group = \"Squeeze Momentum Indicator\")\\n\\n// Calculate BB\\nbasis   = ta.sma(src0, lengthSQZ)\\ndev     = ta.stdev(src0, lengthSQZ)  \\nupperBB = basis + multBB * dev \\nlowerBB = basis - multBB * dev  \\n\\n// Calculate KC\\nmaKC    = ta.sma(src0, lengthSQZ)\\nrange_1 = use_tr ? ta.tr : high - low\\nrangema = ta.sma(range_1, lengthSQZ)\\nupperKC = maKC + rangema * multKC\\nlowerKC = maKC - rangema * multKC\\n\\n// When both the upper and lower Bollinger Bands go inside the Keltner Channel, the squeeze is on.\\n// When the Bollinger Bands (BOTH lines) start to come out of the Keltner Channel, the squeeze has been released (off). \\n// When one of the Bollinger Bands is out of Keltner Channel, no highlighting is done.\\nsqzOn  = lowerBB > lowerKC  and upperBB < upperKC\\nsqzOff = lowerBB <= lowerKC and upperBB >= upperKC\\nnoSqz  = sqzOn == false and sqzOff == false\\n\\nmom = ta.linreg(src0 - math.avg(math.avg(ta.highest(high, lengthMom), ta.lowest(low, lengthMom)), ta.sma(close, lengthMom)), lengthMom, 0)\\nsig = ta.sma(mom, siglen)\\n\\nlongSM  = ta.crossover(mom, sig)\\nshortSM = ta.crossunder(mom, sig)\\nsqz_pos = (sqzOff and mom >= sig)\\nsqz_neg = (sqzOff and mom <  sig)\\n\\nplotshape(show_sb and longSM,  \"Long Momentum\",  shape.triangleup,   location.bottom, #05FFA6, text=\\'M\\', textcolor=#05FFA6)\\nplotshape(show_sb and shortSM, \"Short Momentum\", shape.triangledown, location.top,    #FF0AE2, text=\\'M\\', textcolor=#FF0AE2)\\nplotshape(show_SMI and (sqzOn  or  noSqz) ? true : na, \"In Squeeze\", shape.circle, location.top, color.silver)\\nplotshape(show_SMI and sqz_pos ? true : na, \"Squeeze Release\", shape.triangleup,   location.top, #05FFA6)\\nplotshape(show_SMI and sqz_neg ? true : na, \"Squeeze Release\", shape.triangledown, location.top, #FF0AE2)\\n\\nif alert_Buy and longSM\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Buy [Momentum].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Sell and shortSM\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Sell [Momentum].-\\', alert.freq_once_per_bar_close)\\n\\nsqueeze_ON = \\'{\"content\": \"@everyone\" ,\"embeds\": [{\"title\": \"{{ticker}} \",\"description\": \"In Squeeze | Price ${{close}}\", \"color\": \"2092045\",\"author\": {\"name\":null},\"timestamp\": \"{{timenow}}\"}],\"username\": \"Alertbot\"}\\'\\nsqueeze_Long = \\'{\"content\": \"@everyone\" ,\"embeds\": [{\"title\": \"{{ticker}} \",\"description\": \"Squeeze Release Long | Price ${{close}}\", \"color\": \"2092045\",\"author\": {\"name\":null},\"timestamp\": \"{{timenow}}\"}],\"username\": \"Alertbot\"}\\'\\nsqueeze_Short = \\'{\"content\": \"@everyone\" ,\"embeds\": [{\"title\": \"{{ticker}} \",\"description\": \"Squeeze Release Short | Price ${{close}}\", \"color\": \"2092045\",\"author\": {\"name\":null},\"timestamp\": \"{{timenow}}\"}],\"username\": \"Alertbot\"}\\'\\n\\nalertcondition(sqzOn or noSqz, title=\"In Squeeze\", message=squeeze_ON)\\nalertcondition(sqz_pos, title=\"Squeeze Release | Long\",  message=squeeze_Long)\\nalertcondition(sqz_neg, title=\"Squeeze Release | Short\", message=squeeze_Short)', '//@version=5 \\nindicator(\"VIX Oscillator Smooth\", shorttitle=\"VIX Osc Sm\", overlay=false) \\n\\n// User Inputs \\ntimeframe = input.timeframe(\"\", title=\"Period Timeframe\")  \\nAvgLength = input.int(14, title=\"Average Length\") \\nSmoothLength = input.int(3, title=\"Smooth Length\") \\n\\n// Securities \\nticker = ticker.new(syminfo.prefix, syminfo.ticker, session.regular) \\nvix = request.security(\"CBOE:VIX\", timeframe, close)  \\ncl = request.security(ticker, timeframe, close, lookahead=barmerge.lookahead_on) \\n\\n// Z-Score Calculation \\n\\n// Z Score VIX  \\nvixhx = ta.sma(vix, AvgLength)  \\nvixsd = ta.stdev(vix, AvgLength)  \\nz_raw = (vix - vixhx) / vixsd  \\nz = ta.sma(z_raw, SmoothLength) // Smoothed Z Score VIX line \\n\\n// Z Score Stock \\nstockhx = ta.sma(cl, AvgLength)  \\nstocksd = ta.stdev(cl, AvgLength) \\nzs_raw = (cl - stockhx) / stocksd  \\nzs = ta.sma(zs_raw, SmoothLength) // Smoothed Z Score Stock line \\n\\n// User Inputs for background levels and colors \\nupper_level = input.float(2.5, title=\"Upper Level for Hline Background\", step=0.1) \\nupper_color = input.color(color.gray, title=\"Color for Upper Level Background\") \\nlower_level = input.float(-2.5, title=\"Lower Level for Hline Background\", step=0.1) \\nlower_color = input.color(#ff525275, title=\"Color for Lower Level Background\") \\nzero_level = input.float(0, title=\"Zero Level for Hline Background\", step=0.1) \\nzero_color = input.color(#2195f38a, title=\"Color for Zero Level Background\") \\n\\n// Background color \\nbgcolor_condition = (z >= upper_level and zs <= lower_level) or (z <= lower_level and zs >= upper_level) ? upper_color :  \\n                   (z >= zero_level and zs <= zero_level) or (z <= zero_level and zs >= zero_level) ? zero_color : \\n                   lower_color \\nbgcolor(bgcolor_condition) \\n\\n// Plots \\nplot(z, \"VIX Smooth\", color=color.purple, linewidth=3) // Cambiado a violeta\\nplot(zs, \"Ticker Smooth\", color=color.blue, linewidth=3) // Cambiado a azul\\n\\n// Horizontal lines for levels \\nhline(upper_level, \"Upper Level\", color=color.green)\\nhline(zero_level, \"Zero Level\", color=color.gray)  \\nhline(lower_level, \"Lower Level\", color=color.red) \\n\\n// Draw horizontal lines at upper and zero levels and store their IDs\\nhline_upper = hline(upper_level, \"Upper Level\", color=color.green) \\nhline_zero = hline(zero_level, \"Zero Level\", color=color.gray) \\nhline_lower = hline(lower_level, \"Upper Level\", color=#d61f1f) \\n\\n// Fill the area between zero level and upper level\\nfill(hline_zero, hline_upper, color=#4caf4f69)\\n// Fill the area between zero level and lower level\\nfill(hline_zero, hline_lower, color=#af4c4c69)\\n \\n// Alert conditions for hline background \\nalertcondition(z >= upper_level and zs <= lower_level, title=\"Upper Level Background Alert\", message=\"VIX is at upper level and Ticker is at lower level\") \\nalertcondition(z <= lower_level and zs >= upper_level, title=\"Lower Level Background Alert\", message=\"VIX is at lower level and Ticker is at upper level\") \\n \\n// Signal for crossover at level 0 \\ncrossover_signal = ta.crossover(z, zs) and zs[1] < 0 or ta.crossunder(z, zs) and zs[1] > 0 \\nplotshape(series=crossover_signal, title=\"Crossover Signal\", location=location.absolute, style=shape.circle, color=#ffae0083, size=size.tiny, offset=-AvgLength/2)\\npodemos agregar a este indicador los matrix candles aqui te lo escribo este segundo indicador para que lo tomes como referencia  lo puedes sacar de aqui  los matrix candles y  las senales mas importantes de buy y sell //@version=5\\nindicator(\\'Matrix Series and Vix Fix with VWAP CCI and QQE Signals\\', shorttitle=\\'Matrix\\', precision=2)\\n\\n// Function to select the type of source\\nget_src(Type) =>\\n    switch Type \\n        \"VWAP\" => ta.vwap\\n        \"Close\" => close\\n        \"Open\" => open\\n        \"HL2\" => hl2\\n        \"HLC3\" => hlc3\\n        \"OHLC4\" => ohlc4\\n        \"HLCC4\" => hlcc4\\n        \"High\" => high\\n        \"Low\" => low\\n        \"TR\" => ta.tr\\n        \"vwap(Close)\" => ta.vwap(close)\\n        \"vwap(Open)\" => ta.vwap(open)\\n        \"vwap(High)\" => ta.vwap(high)\\n        \"vwap(Low)\" => ta.vwap(low)\\n        \"AVG(vwap(H,L))\" => math.avg(ta.vwap(high), ta.vwap(low))\\n        \"AVG(vwap(O,C))\" => math.avg(ta.vwap(open), ta.vwap(close))        \\n//__________________________________________________________________\\n// Based on \"Matrix Series\" - Author: @glaz\\n// https://www.tradingview.com/script/2X2cVLhb-Matrix-Series/\\n// https://www.wisestocktrader.com/indicators/2739-flower-indicator\\n//__________________________________________________________________\\nalert_Buy_Matrix  = input.bool(false, \"? Alert: [Buy - Matrix]\",  inline=\"alertMatrix\")\\nalert_Sell_Matrix = input.bool(false, \"? Alert: [Sell - Matrix]\", inline=\"alertMatrix\")\\nshow_buy_signal   = input.bool(true,  \"Show buy signal\",  inline=\"show_signal\")\\nshow_sell_signal  = input.bool(true,  \"Show sell signal\", inline=\"show_signal\")\\nshow_dot          = input.bool(true,  \"Show Watch/Warning Point\", inline=\"show_signal\")\\n//--- Trend Bought/Sold Detail\\ndynamic      = input.bool(true,  \"Show Dynamic Zones ??? \", inline=\"show_limit\")\\nOBOS         = input.bool(false, \"Show OB/OS\", inline=\"show_limit\")\\nshow_candles = input.bool(true,  \"Show Matrix Candles  ??? \", inline=\"s_matrix\")\\nshow_hist    = input.bool(false, \"Show Matrix Histogram\", inline=\"s_matrix\")\\n//--- Sup/Res Detail\\nPricePeriod      = input.int(16,  \"Price Period\", 1, inline=\"period\")\\nSmoother         = input.int(5,   \"Smoother\", 2, inline=\"period\")\\nSupResPeriod     = input.int(50,  \"Superior Resolution Period\", 1, inline=\"Superior Resolution\")\\nSupResPercentage = input.int(100, \"Percentage\", 1, inline=\"Superior Resolution\")\\n//--- Line Detail \\nob = input.int(200,  \"OverBought Above\", inline=\"line\")\\nos = input.int(-200, \"OverSold Bellow\",  inline=\"line\")\\n\\nys1  = (high + low + close * 2) / 4\\nrk3  = ta.ema(ys1, Smoother)\\nrk4  = ta.stdev(ys1, Smoother)\\nrk5  = (ys1 - rk3) * 200 / rk4\\nrk6  = ta.ema(rk5, Smoother)\\nup   = ta.ema(rk6, Smoother)\\ndown = ta.ema(up, Smoother)\\nOo   = up < down ? up : down\\nHh   = Oo\\nLl   = up < down ? down : up\\nCc   = Ll\\nsell_matrix = ta.cross(up, ob) == 1 and up[1] > up \\nbuy_matrix  = ta.cross(up, os) == 1 and up[1] < up \\n\\ncoral     = #FF8080\\nlavender  = #8080FF\\namber     = color.new(#FFE500, 60)\\nvcolor = show_candles and (Oo > Cc) ? coral : show_candles and (up > down) ? lavender : show_candles ? coral : na\\n\\nplotcandle(Oo, Hh, Ll, Cc, \"Matrix Candles\", color = vcolor, wickcolor = vcolor, bordercolor = vcolor)\\nplotshape(show_sell_signal and sell_matrix, \"Sell\", shape.triangledown, location.top, coral)\\nbgcolor(show_sell_signal and sell_matrix ? color.new(coral, 70) : na, title=\"Sell\")\\nplotshape(show_buy_signal and buy_matrix, \"Buy\", shape.triangleup, location.bottom, lavender)\\nbgcolor(show_buy_signal and buy_matrix ? color.new(lavender, 70) : na, title=\"Buy\")\\n\\n//-------S/R Zones------\\nLookback = SupResPeriod\\nPerCent  = SupResPercentage\\n\\nValue1 = ta.cci(close, PricePeriod)\\nValue2 = ta.highest(Value1, Lookback)\\nValue3 = ta.lowest(Value1, Lookback)\\nValue4 = Value2 - Value3\\nValue5 = Value4 * (PerCent / 100)\\nResistanceLine = Value3 + Value5\\nSupportLine    = Value2 - Value5\\nplot(dynamic ? ResistanceLine : na, \"Resistance Line\", color.new(coral, 5))\\nplot(dynamic ? ResistanceLine : na, \"Resistance Line\", color.new(coral, 90), 7)\\nplot(dynamic ? SupportLine    : na, \"Support Line\",    color.new(lavender, 5))\\nplot(dynamic ? SupportLine    : na, \"Support Line\",    color.new(lavender, 90), 7)\\n\\n//--Overbought/Oversold/Warning Detail\\nh01 = ta.highest(up, 1)   + 20\\nh02 = ta.highest(down, 1) + 20\\nl01 = ta.lowest(down, 1)  - 20\\nl02 = ta.lowest(up, 1)    - 20\\nUPshape   = up   > ob and up > down ? h01 : up   > ob and up < down ? h02 : na\\nDOWNshape = down < os and up > down ? l01 : down < os and up < down ? l02 : na\\n\\nplot(show_dot ? UPshape   : na, \"UP Shape\",   amber, 4, plot.style_circles)\\nplot(show_dot ? DOWNshape : na, \"DOWN Shape\", amber, 4, plot.style_circles)\\nhline(OBOS ? ob : na, \"OverBought\")\\nhline(OBOS ? os : na, \"OverSold\")\\n\\nAccumulationColor = color.new(#FFA07A, 60)     //\"Accumulation Zone\"\\nWarningColor      = color.new(color.white, 60) //\"Warning/Watch Signal\"\\nDistributionColor = color.new(#3CB371, 60)     //\"Distribution Zone\"\\n\\nrk5_210  = ( ys1 - rk3 ) * 210 / rk4\\nrk6_210  = ta.ema(rk5_210, Smoother)\\nUP_210   = ta.ema(rk6_210, Smoother)\\nDOWN_210 = ta.ema(UP_210, Smoother)\\nUPColor  = (UP_210 >= 210 ? DistributionColor : (UP_210 <= -210 ? AccumulationColor : WarningColor))\\nplot(show_hist ? UP_210   : na, \"UP\",         UPColor, 1, plot.style_columns)\\nplot(show_hist ? DOWN_210 : na, \"Signal(UP)\", color.aqua)\\n\\nif alert_Buy_Matrix and buy_matrix\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Buy [Matrix Series].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Sell_Matrix and sell_matrix\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Sell [Matrix Series].-\\', alert.freq_once_per_bar_close)\\n\\n//_____________________________________________________________________________________________\\n// Based on \"Williams Vix Fix\" - Author: @ChrisMoody\\n// https://www.tradingview.com/script/pJpXG5JH-CM-Williams-Vix-Fix-V3-Ultimate-Filtered-Alerts/\\n// https://www.ireallytrade.com/newsletters/VIXFix.pdf\\n//_____________________________________________________________________________________________\\nshows_Vix_Fix = input(false, \\'Show Vix Fix Histogram ???\\', inline=\"histVF\", group=\\'Williams Vix Fix\\')\\nswvfm    = input.float(1.0, \"Multiplier (Values 0.5 to 2)\", minval = 0.5, maxval = 2, inline=\"histVF\", group=\\'Williams Vix Fix\\')\\nsbcFilt  = input.bool(true, \\'Show Signal For Filtered Entry [? FE ]\\', group=\\'Williams Vix Fix\\') // Use FILTERED Criteria\\nsbcAggr  = input.bool(true, \\'Show Signal For AGGRESSIVE Filtered Entry [? AE ]\\', group=\\'Williams Vix Fix\\') // Use FILTERED Criteria\\nalert_AE = input.bool(false, \\'? Alert: [AGGRESSIVE Entry]\\', inline=\\'AlertVF\\', group=\\'Williams Vix Fix\\')\\nalert_FE = input.bool(false, \\'? Alert: [Filtered Entry]\\', inline=\\'AlertVF\\', group=\\'Williams Vix Fix\\')\\npd       = input.int(22, \\'LookBack Period Standard Deviation High\\', 1, group=\\'Williams Vix Fix\\')\\nbbl      = input.int(20, \\'Bolinger Band Length\\', 1, group=\\'Williams Vix Fix\\')\\nmult     = input.float(2.0, \\'Bollinger Band Standard Devaition Up\\', minval=1, maxval=5, group=\\'Williams Vix Fix\\')\\nlb       = input.int(50, \\'Look Back Period Percentile High\\', 1, group=\\'Williams Vix Fix\\')\\nph       = input.float(.85, \\'Highest Percentile - 0.90=90%, 0.95=95%, 0.99=99%\\', minval=.05, maxval=1, group=\\'Williams Vix Fix\\')\\nltLB     = input.int(40, minval=25, maxval=99, title=\\'Long-Term Look Back Current Bar Has To Close Below This Value OR Medium Term--Default=40\\', group=\\'Williams Vix Fix\\')\\nmtLB     = input.int(14, minval=10, maxval=20, title=\\'Medium-Term Look Back Current Bar Has To Close Below This Value OR Long Term--Default=14\\', group=\\'Williams Vix Fix\\')\\nstr      = input.int(3,  minval=1,  maxval=9,  title=\\'Entry Price Action Strength--Close > X Bars Back---Default=3\\', group=\\'Williams Vix Fix\\')\\n\\n// Williams Vix Fix Formula\\nwvf       = (ta.highest(close, pd) - low) / ta.highest(close, pd) * 100\\nsDev      = mult * ta.stdev(wvf, bbl)\\nmidLine   = ta.sma(wvf, bbl)\\nlowerBand = midLine - sDev\\nupperBand = midLine + sDev\\nrangeHigh = ta.highest(wvf, lb) * ph\\n\\n// Filtered Criteria\\nupRange      = low > low[1] and close > high[1]\\nupRange_Aggr = close > close[1] and close > open[1]\\n// Filtered Criteria\\nfiltered      = (wvf[1] >= upperBand[1] or wvf[1] >= rangeHigh[1]) and wvf < upperBand and wvf < rangeHigh\\nfiltered_Aggr = (wvf[1] >= upperBand[1] or wvf[1] >= rangeHigh[1]) and not(wvf < upperBand and wvf < rangeHigh)\\n\\n// Alerts Criteria\\nalert1  = wvf >= upperBand or wvf >= rangeHigh ? 1 : 0\\nalert2  = (wvf[1] >= upperBand[1] or wvf[1] >= rangeHigh[1]) and wvf < upperBand and wvf < rangeHigh ? 1 : 0\\ncond_FE = upRange and close > close[str] and (close < close[ltLB] or close < close[mtLB]) and filtered \\nalert3  = cond_FE ? 1 : 0\\ncond_AE = upRange_Aggr and close > close[str] and (close < close[ltLB] or close < close[mtLB]) and filtered_Aggr\\nalert4  = cond_AE ? 1 : 0\\n\\ncolor_VF = alert1 ? color.new(color.lime, 60) : alert2 ? color.new(color.teal, 60) : color.new(color.silver, 80)\\nswvfm_precent = swvfm * (ResistanceLine - SupportLine) / (2.2 * rangeHigh)\\n\\nplot(shows_Vix_Fix ? wvf * -1 * swvfm_precent : na, \\'Williams Vix Fix\\', color_VF, 4, plot.style_columns)\\n\\nplotshape(sbcAggr and alert4 ? alert4 : na, \\'Aggressive Entry\\', shape.triangleup, location.bottom, color.new(#80FF00, 0), text=\\'AE\\', textcolor=color.new(#80FF00, 0))\\nbgcolor(sbcAggr and alert4 ? color.new(#80FF00, 80) : na, title=\\'Aggressive Entry\\')\\nplotshape(sbcFilt and alert3 ? alert3 : na, \\'Filtered Entry\\', shape.triangleup, location.bottom, color.new(#15FF00, 0), text=\\'FE\\', textcolor=color.new(#15FF00, 0))\\nbgcolor(sbcFilt and alert3 ? color.new(#15FF00, 80) : na, title=\\'Filtered Entry\\')\\n\\nif alert_AE and cond_AE\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Aggressive Entry [VixFix].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_FE and cond_FE\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Filtered Entry [VixFix].-\\', alert.freq_once_per_bar_close)\\n\\n//_____________________________________________________________\\n// Based on \"QQE signals\" - Author: @colinmck\\n// https://www.tradingview.com/script/7R6ZxyZu-QQE-signals/\\n//_____________________________________________________________\\nshows_QQE       = input.bool(false, \"????????? Show QQE Signals ?????????\")\\nalert_Long_QQE  = input.bool(false, \"? Alert: [Long - QQE]\",  inline=\"alertQQE\")\\nalert_Short_QQE = input.bool(false, \"? Alert: [Short - QQE]\", inline=\"alertQQE\")\\nRSI_Period = input.int(14,      \"RSI Length\", 1)\\nSF         = input.int(5,       \"RSI Smoothing\", 2)\\nQQE        = input.float(4.238, \"Fast QQE Factor\", 1)\\n\\nWilders_Period = RSI_Period * 2 - 1\\nlongband = 0.0\\nshortband = 0.0\\ntrend = 0\\n\\nRsi      = ta.rsi(close, RSI_Period)\\nRSIndex  = ta.ema(Rsi, SF)\\nAtrRsi   = math.abs(RSIndex[1] - RSIndex)\\nMaAtrRsi = ta.ema(AtrRsi, Wilders_Period)\\n\\nDeltaFastAtrRsi = ta.ema(MaAtrRsi, Wilders_Period) * QQE\\nnewshortband = RSIndex + DeltaFastAtrRsi\\nnewlongband = RSIndex - DeltaFastAtrRsi\\nlongband := RSIndex[1] > longband[1] and RSIndex > longband[1] ? math.max(longband[1], newlongband) : newlongband\\nshortband := RSIndex[1] < shortband[1] and RSIndex < shortband[1] ? math.min(shortband[1], newshortband) : newshortband\\ncross_1 = ta.cross(longband[1], RSIndex)\\ntrend := ta.cross(RSIndex, shortband[1]) ? 1 : cross_1 ? -1 : nz(trend[1], 1)\\nFastAtrRsiTL = trend == 1 ? longband : shortband\\n\\n// Find all the QQE Crosses\\nQQExlong = 0\\nQQExlong := nz(QQExlong[1])\\nQQExshort = 0\\nQQExshort := nz(QQExshort[1])\\nQQExlong := FastAtrRsiTL < RSIndex ? QQExlong + 1 : 0\\nQQExshort := FastAtrRsiTL > RSIndex ? QQExshort + 1 : 0\\n\\n// Conditions\\nqqeLong = QQExlong == 1 ? FastAtrRsiTL[1] - 50 : na\\nqqeShort = QQExshort == 1 ? FastAtrRsiTL[1] - 50 : na\\n\\n// Plotting\\nredorange  = color.new(#FF4000, 0)\\nchartreuse = color.new(#80FF00, 0)\\nplotshape(shows_QQE and qqeLong,  \"QQE Long\",  shape.labelup,   location.bottom, color.new(chartreuse, 25), text=\"QQE\", size=size.tiny, textcolor=color.new(#000000, 5))\\nplotshape(shows_QQE and qqeShort, \"QQE Short\", shape.labeldown, location.top,    color.new(redorange, 25),  text=\"QQE\", size=size.tiny, textcolor=color.new(color.white, 5))\\n\\nif alert_Long_QQE  and qqeLong\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Long [Quantitative Qualitative Estimation (QQE)].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Short_QQE and qqeShort\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Short [Quantitative Qualitative Estimation (QQE)].-\\', alert.freq_once_per_bar_close)\\n\\n//___________________________________________________________________________\\n// VWAP CCI Signals\\n//___________________________________________________________________________\\nshows_VCCI = input.bool(false, \"????????? Show VWAP CCI Signals ?????????\")\\nlen_cci    = input.int(21, \"Length\", 1, inline=\"cci\")\\nType_cci   = input.string(\"vwap(Close)\", \"Source\", inline=\"cci\", options=[\"vwap(Close)\", \"vwap(Open)\", \"vwap(High)\", \"vwap(Low)\", \"AVG(vwap(H,L))\", \"AVG(vwap(O,C))\", \"VWAP\", \"Close\", \"Open\", \"HL2\", \"HLC3\", \"OHLC4\", \"HLCC4\", \"High\", \"Low\", \"TR\"])\\nalert_Long_cci  = input.bool(false, \"? Alert: [Long - CCI]\",  inline=\"alertCCI\")\\nalert_Short_cci = input.bool(false, \"? Alert: [Short - CCI]\", inline=\"alertCCI\")\\n\\n// ALMA - Arnaud Legoux Moving Average of @kurtsmock\\nenhanced_alma(_series, _length, _offset, _sigma) =>\\n    length      = int(_length) // Floating point protection\\n    numerator   = 0.0\\n    denominator = 0.0 \\n    m = _offset * (length - 1)\\n    s = length / _sigma\\n    for i=0 to length-1\\n        weight       = math.exp(-((i-m)*(i-m)) / (2 * s * s))\\n        numerator   := numerator   + weight * _series[length - 1 - i]\\n        denominator := denominator + weight\\n    numerator / denominator\\n\\n// Calculate vwap cci\\nsrc_cci = get_src(Type_cci)\\ncci = (src_cci - enhanced_alma(src_cci, len_cci, 0.85, 6.0)) / (0.015 * ta.dev(src_cci, len_cci))\\n\\n// Define long and short entry signal\\nlong_cci  = ta.crossover(cci, -200)\\nshort_cci = ta.crossunder(cci, 200)\\n\\nplotshape(shows_VCCI and long_cci,  \"VWAP CCI Long\",  shape.labelup,   location.bottom, color.new(chartreuse, 25), text=\"CCI\", size=size.tiny, textcolor=color.new(#000000, 5))\\nplotshape(shows_VCCI and short_cci, \"VWAP CCI Short\", shape.labeldown, location.top,    color.new(redorange, 25),  text=\"CCI\", size=size.tiny, textcolor=color.new(color.white, 5))\\n\\nif alert_Long_cci and long_cci\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Long [Commodity Channel Index (CCI)].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Short_cci and short_cci\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Short [Commodity Channel Index (CCI)].-\\', alert.freq_once_per_bar_close)\\n\\n//___________________________________________________________________________________________\\n// Based on \"Squeeze Momentum Indicator\" - Author: @LazyBear\\n// @KivancOzbilgic: https://www.tradingview.com/script/NVzKGYFJ/\\n// @capissimo: https://www.tradingview.com/script/Ejx2tEHY-Squeeze-Momentum-Indicator-mod-3/\\n// Indicator description: http://www.forextrading-pips.com/squeeze-indicator/\\n//___________________________________________________________________________________________\\nshow_SMI   = input.bool(false,     \"Show Squeeze Momentum Indicator\", group = \"Squeeze Momentum Indicator\")\\nalert_Buy  = input.bool(false,     \"? Alert: [Buy - Mom]\",  inline = \"alertMom\", group = \"Squeeze Momentum Indicator\")\\nalert_Sell = input.bool(false,     \"? Alert: [Sell - Mom]\", inline = \"alertMom\", group = \"Squeeze Momentum Indicator\")\\nType_src   = input.string(\"Close\", \"Source\", inline = \"s1\", group = \"Squeeze Momentum Indicator\", options = [\"VWAP\", \"Close\", \"Open\", \"HL2\", \"HLC3\", \"OHLC4\", \"HLCC4\", \"High\", \"Low\", \"vwap(Close)\", \"vwap(Open)\", \"vwap(High)\", \"vwap(Low)\", \"AVG(vwap(H,L))\", \"AVG(vwap(O,C))\", \"TR\"])\\nsrc0       = get_src(Type_src)\\nshow_sb    = input.bool(false, \"Show buy/sell signal\", inline = \"s1\", group = \"Squeeze Momentum Indicator\")\\nlengthMom  = input.int(20,     \"Momentum Length\", 1, inline = \"mom\", group = \"Squeeze Momentum Indicator\")\\nsiglen     = input.int(5,      \"Signal\", 2, inline = \"mom\", group = \"Squeeze Momentum Indicator\")\\nlengthSQZ  = input.int(20,     \"KC/BB Length\", 1, inline = \"sqz\", group = \"Squeeze Momentum Indicator\")\\nmultBB     = input.float(2.0,  \"BB MultFactor\", 0.5, step=0.05, inline = \"sqz\", group = \"Squeeze Momentum Indicator\")\\nmultKC     = input.float(1.3,  \"KC MultFactor\", 0.5, step=0.05, inline = \"kc\", group = \"Squeeze Momentum Indicator\")\\nuse_tr     = input.bool(true,  \"Use TrueRange (KC)\", inline = \"kc\", group = \"Squeeze Momentum Indicator\")\\n\\n// Calculate BB\\nbasis   = ta.sma(src0, lengthSQZ)\\ndev     = ta.stdev(src0, lengthSQZ)  \\nupperBB = basis + multBB * dev \\nlowerBB = basis - multBB * dev  \\n\\n// Calculate KC\\nmaKC    = ta.sma(src0, lengthSQZ)\\nrange_1 = use_tr ? ta.tr : high - low\\nrangema = ta.sma(range_1, lengthSQZ)\\nupperKC = maKC + rangema * multKC\\nlowerKC = maKC - rangema * multKC\\n\\n// When both the upper and lower Bollinger Bands go inside the Keltner Channel, the squeeze is on.\\n// When the Bollinger Bands (BOTH lines) start to come out of the Keltner Channel, the squeeze has been released (off). \\n// When one of the Bollinger Bands is out of Keltner Channel, no highlighting is done.\\nsqzOn  = lowerBB > lowerKC  and upperBB < upperKC\\nsqzOff = lowerBB <= lowerKC and upperBB >= upperKC\\nnoSqz  = sqzOn == false and sqzOff == false\\n\\nmom = ta.linreg(src0 - math.avg(math.avg(ta.highest(high, lengthMom), ta.lowest(low, lengthMom)), ta.sma(close, lengthMom)), lengthMom, 0)\\nsig = ta.sma(mom, siglen)\\n\\nlongSM  = ta.crossover(mom, sig)\\nshortSM = ta.crossunder(mom, sig)\\nsqz_pos = (sqzOff and mom >= sig)\\nsqz_neg = (sqzOff and mom <  sig)\\n\\nplotshape(show_sb and longSM,  \"Long Momentum\",  shape.triangleup,   location.bottom, #05FFA6, text=\\'M\\', textcolor=#05FFA6)\\nplotshape(show_sb and shortSM, \"Short Momentum\", shape.triangledown, location.top,    #FF0AE2, text=\\'M\\', textcolor=#FF0AE2)\\nplotshape(show_SMI and (sqzOn  or  noSqz) ? true : na, \"In Squeeze\", shape.circle, location.top, color.silver)\\nplotshape(show_SMI and sqz_pos ? true : na, \"Squeeze Release\", shape.triangleup,   location.top, #05FFA6)\\nplotshape(show_SMI and sqz_neg ? true : na, \"Squeeze Release\", shape.triangledown, location.top, #FF0AE2)\\n\\nif alert_Buy and longSM\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Buy [Momentum].-\\', alert.freq_once_per_bar_close)\\n\\nif alert_Sell and shortSM\\n    alert(\\'Symbol = (\\' + syminfo.tickerid + \\') \\\\n TimeFrame = (\\' + timeframe.period + \\') \\\\n Current Price (\\' + str.tostring(close) + \\') \\\\n Sell [Momentum].-\\', alert.freq_once_per_bar_close)\\n\\nsqueeze_ON = \\'{\"content\": \"@everyone\" ,\"embeds\": [{\"title\": \"{{ticker}} \",\"description\": \"In Squeeze | Price ${{close}}\", \"color\": \"2092045\",\"author\": {\"name\":null},\"timestamp\": \"{{timenow}}\"}],\"username\": \"Alertbot\"}\\'\\nsqueeze_Long = \\'{\"content\": \"@everyone\" ,\"embeds\": [{\"title\": \"{{ticker}} \",\"description\": \"Squeeze Release Long | Price ${{close}}\", \"color\": \"2092045\",\"author\": {\"name\":null},\"timestamp\": \"{{timenow}}\"}],\"username\": \"Alertbot\"}\\'\\nsqueeze_Short = \\'{\"content\": \"@everyone\" ,\"embeds\": [{\"title\": \"{{ticker}} \",\"description\": \"Squeeze Release Short | Price ${{close}}\", \"color\": \"2092045\",\"author\": {\"name\":null},\"timestamp\": \"{{timenow}}\"}],\"username\": \"Alertbot\"}\\'\\n\\nalertcondition(sqzOn or noSqz, title=\"In Squeeze\", message=squeeze_ON)\\nalertcondition(sqz_pos, title=\"Squeeze Release | Long\",  message=squeeze_Long)\\nalertcondition(sqz_neg, title=\"Squeeze Release | Short\", message=squeeze_Short)', 'en el mismo indicador puedes configurar para que sean visibles al mismo tiempo el ticker  vix smooth y matrix candles si las escalas son problemas ignoralas']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35267/35267 [26:36<00:00, 22.09it/s]\n"
     ]
    }
   ],
   "source": [
    "output = await main(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "2887a64c-92d3-4511-a976-73e7ee4b590e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35267"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e18370e-c2d5-4cfc-a6ec-ab1302c2df74",
   "metadata": {},
   "source": [
    "#### - post data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "5e9ff5f0-4e8d-4a94-882b-0f80cfea36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(output, columns = ['id','sentiment'])\n",
    "test.to_csv(\"C:\\\\Users\\\\Liu Shi Peng\\\\Documents\\\\sentiment_analysis\\\\100_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "57333fe5-ef41-4990-895e-aa66c9c680db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Negative', 'Neutral', 'Positive']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def map_to_sentiment(word):\n",
    "    # Lowercase the word for easier comparison\n",
    "    word = str(word).lower()\n",
    "    \n",
    "    # Regular expressions for matching similar words\n",
    "    positive_patterns = r'\\b(pos|posit|positive)\\b'\n",
    "    negative_patterns = r'\\b(neg|negat|negative)\\b'\n",
    "    neutral_patterns = r'\\b(neu|neutral)\\b'\n",
    "\n",
    "    # Check for matches and map accordingly\n",
    "    if re.search(positive_patterns, word):\n",
    "        return 'Positive'\n",
    "    elif re.search(negative_patterns, word):\n",
    "        return 'Negative'\n",
    "    elif re.search(neutral_patterns, word):\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Neutral'  # If no match, return None or handle accordingly\n",
    "\n",
    "words = ['Positive.', 'negativity', 'Neutralize', 'unrelated', 'POSIT', 'NEG', 'neu', 'POSITIVE']\n",
    "mapped_words = [map_to_sentiment(word) for word in words]\n",
    "print(mapped_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "a8169e52-2ffa-46cb-aedd-276840bd3d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35267, 2)"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output = pd.DataFrame(output, columns = ['id','sentiment'])\n",
    "df_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "f6c00657-6679-491b-9848-acef7ab05ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment: Positive\\nReasoning: The user expre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment: Neutral\\nReasoning: The user's quer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sentiment: Neutral\\nReasoning: The user is see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sentiment: Neutral\\nReasoning: The user's quer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Sentiment: Neutral\\nReasoning: The user is inq...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          sentiment\n",
       "0   0  Sentiment: Positive\\nReasoning: The user expre...\n",
       "1   1  Sentiment: Neutral\\nReasoning: The user's quer...\n",
       "2   2  Sentiment: Neutral\\nReasoning: The user is see...\n",
       "3   3  Sentiment: Neutral\\nReasoning: The user's quer...\n",
       "4   4  Sentiment: Neutral\\nReasoning: The user is inq..."
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "ecb8b060-d706-4893-8765-9f1629c165e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment: Positive\\nReasoning: The user expre...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>The user expresses satisfaction and gratitude ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment: Neutral\\nReasoning: The user's quer...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>The user's query \"Ciao\" does not provide enoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sentiment: Neutral\\nReasoning: The user is see...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>The user is seeking specific trading advice an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sentiment: Neutral\\nReasoning: The user's quer...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>The user's query does not convey a clear senti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Sentiment: Neutral\\nReasoning: The user is inq...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>The user is inquiring about specific stocks an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          sentiment Sentiment  \\\n",
       "0   0  Sentiment: Positive\\nReasoning: The user expre...  Positive   \n",
       "1   1  Sentiment: Neutral\\nReasoning: The user's quer...   Neutral   \n",
       "2   2  Sentiment: Neutral\\nReasoning: The user is see...   Neutral   \n",
       "3   3  Sentiment: Neutral\\nReasoning: The user's quer...   Neutral   \n",
       "4   4  Sentiment: Neutral\\nReasoning: The user is inq...   Neutral   \n",
       "\n",
       "                                           Reasoning  \n",
       "0  The user expresses satisfaction and gratitude ...  \n",
       "1  The user's query \"Ciao\" does not provide enoug...  \n",
       "2  The user is seeking specific trading advice an...  \n",
       "3  The user's query does not convey a clear senti...  \n",
       "4  The user is inquiring about specific stocks an...  "
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output[['Sentiment', 'Reasoning']] = df_output['sentiment'].str.extract(r'Sentiment:\\s*([^\\n]+)\\nReasoning:\\s*(.*)')\n",
    "df_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "bd81ab1a-1b35-4d7f-a96a-c723dea5d243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Neutral     32047\n",
       "Negative     1716\n",
       "Positive     1231\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "fcf1c9bd-9206-486f-b16a-ab8faeb88533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35267, 3)"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_map = df_output['Sentiment'].apply(map_to_sentiment).reset_index(name='map_to_sentiment')\n",
    "df_map['Reasoning'] = df_output['Reasoning']\n",
    "df_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "9bc29839-7ee2-431f-aa90-e15b427b46d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>quer_list</th>\n",
       "      <th>session_time</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00010369-2ee4-459b-9080-43c6793c32ff</td>\n",
       "      <td>[What's the least expensive crypto coin to tra...</td>\n",
       "      <td>8/3/2024 2:01</td>\n",
       "      <td>Positive</td>\n",
       "      <td>The user expresses satisfaction and gratitude ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0003c604-45a5-4f1d-8b87-b684ef3641e4</td>\n",
       "      <td>[Ciao]</td>\n",
       "      <td>8/22/2024 17:08</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>The user's query \"Ciao\" does not provide enoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00054a16-b0cb-4c2e-84c1-505a7e859899</td>\n",
       "      <td>[Get me trades for today to buy for weekend ga...</td>\n",
       "      <td>8/9/2024 19:25</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>The user is seeking specific trading advice an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0006538f-33de-4fb9-8f57-19835fb2e110</td>\n",
       "      <td>[Top 3 Call option contracts in the AI hardwar...</td>\n",
       "      <td>9/1/2024 4:59</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>The user's query does not convey a clear senti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0007bc3e-d9ec-4a4f-b79c-b2f7069d0a4e</td>\n",
       "      <td>[What stock made the most money today, What pe...</td>\n",
       "      <td>8/23/2024 20:45</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>The user is inquiring about specific stocks an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             session_id  \\\n",
       "0  00010369-2ee4-459b-9080-43c6793c32ff   \n",
       "1  0003c604-45a5-4f1d-8b87-b684ef3641e4   \n",
       "2  00054a16-b0cb-4c2e-84c1-505a7e859899   \n",
       "3  0006538f-33de-4fb9-8f57-19835fb2e110   \n",
       "4  0007bc3e-d9ec-4a4f-b79c-b2f7069d0a4e   \n",
       "\n",
       "                                           quer_list     session_time  \\\n",
       "0  [What's the least expensive crypto coin to tra...    8/3/2024 2:01   \n",
       "1                                             [Ciao]  8/22/2024 17:08   \n",
       "2  [Get me trades for today to buy for weekend ga...   8/9/2024 19:25   \n",
       "3  [Top 3 Call option contracts in the AI hardwar...    9/1/2024 4:59   \n",
       "4  [What stock made the most money today, What pe...  8/23/2024 20:45   \n",
       "\n",
       "  sentiment                                          reasoning  \n",
       "0  Positive  The user expresses satisfaction and gratitude ...  \n",
       "1   Neutral  The user's query \"Ciao\" does not provide enoug...  \n",
       "2   Neutral  The user is seeking specific trading advice an...  \n",
       "3   Neutral  The user's query does not convey a clear senti...  \n",
       "4   Neutral  The user is inquiring about specific stocks an...  "
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_session[['sentiment', 'reasoning']] = df_map[['map_to_sentiment', 'Reasoning']]\n",
    "df_session.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "54c4f431-782c-4c84-b716-89da140d1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_session.to_csv(\"C:\\\\Users\\\\Liu Shi Peng\\\\Documents\\\\sentiment_analysis\\\\aug_session_sentiment0830.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "dcfdab70-2a5d-466d-9e38-6fa386227659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "06fcf366-70aa-4f70-b000-b29592648ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>topic_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00005a40-17f6-4f75-af23-91464e5dedb7</td>\n",
       "      <td>Stock Market Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000c413-41cf-4181-af22-6ff72b5d74e7</td>\n",
       "      <td>Options Trading Strategies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000c413-41cf-4181-af22-6ff72b5d74e7</td>\n",
       "      <td>Market Sentiment &amp; Trends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000c413-41cf-4181-af22-6ff72b5d74e7</td>\n",
       "      <td>Non-Financial/Unrelated Topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000c413-41cf-4181-af22-6ff72b5d74e7</td>\n",
       "      <td>Stock Market Analysis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             session_id                      topic_list\n",
       "0  00005a40-17f6-4f75-af23-91464e5dedb7           Stock Market Analysis\n",
       "1  0000c413-41cf-4181-af22-6ff72b5d74e7      Options Trading Strategies\n",
       "1  0000c413-41cf-4181-af22-6ff72b5d74e7       Market Sentiment & Trends\n",
       "1  0000c413-41cf-4181-af22-6ff72b5d74e7  Non-Financial/Unrelated Topics\n",
       "1  0000c413-41cf-4181-af22-6ff72b5d74e7           Stock Market Analysis"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_list = df.groupby(['session_id'])['topic_y'].apply(list).reset_index(name = 'topic_list')\n",
    "df_topic_list_explode = df_topic_list.explode('topic_list')\n",
    "df_topic_list_explode = df_topic_list_explode[['session_id', 'topic_list']].drop_duplicates()\n",
    "df_topic_list_explode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "0b591896-2637-40ca-84f6-3f00ef974eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_list_explode.to_csv(\"C:\\\\Users\\\\Liu Shi Peng\\\\Documents\\\\sentiment_analysis\\\\aug_session_topics0830.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "405b0baa-3abe-43f0-9d6f-0d4e5d4d290d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88431, 4)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df = pd.merge(df_session, df_session_sentiment, left_on = 'session_id', right_on = 'session_id', how = 'left')\n",
    "merge_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d190fd47-5aa3-41f9-a255-c7d4edfe9ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>quer_list</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00005a40-17f6-4f75-af23-91464e5dedb7</td>\n",
       "      <td>[What should I invest on to male $1,000,000 of...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[Stock Market Analysis]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000c413-41cf-4181-af22-6ff72b5d74e7</td>\n",
       "      <td>[Top 3 Call option contracts related to EV wit...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[Options Trading Strategies, Market Sentiment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001bf62-dc63-40ef-b940-7e3419384860</td>\n",
       "      <td>[analyze hnd, analyze hnd.to, DO you have data...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[Technical Indicators &amp; Analysis, Non-Financia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00027d51-b756-4dfd-8070-50ea23c93bdc</td>\n",
       "      <td>[Find me an options trade to make right now fo...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[Options Trading Strategies, Options Trading S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00032951-f74d-4f67-ac8f-3655f9f2d153</td>\n",
       "      <td>[What are you, Explain the Greeks as if you wo...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[Non-Financial/Unrelated Topics, Options Tradi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             session_id  \\\n",
       "0  00005a40-17f6-4f75-af23-91464e5dedb7   \n",
       "1  0000c413-41cf-4181-af22-6ff72b5d74e7   \n",
       "2  0001bf62-dc63-40ef-b940-7e3419384860   \n",
       "3  00027d51-b756-4dfd-8070-50ea23c93bdc   \n",
       "4  00032951-f74d-4f67-ac8f-3655f9f2d153   \n",
       "\n",
       "                                           quer_list sentiment  \\\n",
       "0  [What should I invest on to male $1,000,000 of...  Positive   \n",
       "1  [Top 3 Call option contracts related to EV wit...   Neutral   \n",
       "2  [analyze hnd, analyze hnd.to, DO you have data...   Neutral   \n",
       "3  [Find me an options trade to make right now fo...  Negative   \n",
       "4  [What are you, Explain the Greeks as if you wo...  Positive   \n",
       "\n",
       "                                          topic_list  \n",
       "0                            [Stock Market Analysis]  \n",
       "1  [Options Trading Strategies, Market Sentiment ...  \n",
       "2  [Technical Indicators & Analysis, Non-Financia...  \n",
       "3  [Options Trading Strategies, Options Trading S...  \n",
       "4  [Non-Financial/Unrelated Topics, Options Tradi...  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2ac76296-e635-4e6d-8658-38f9383cbb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.to_csv(\"C:\\\\Users\\\\Liu Shi Peng\\\\Documents\\\\sentiment_analysis\\\\fen_jul_session_sentiment_with_topic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b31a81d-d10f-4a70-87e5-f9ed3cd3c720",
   "metadata": {},
   "source": [
    "### tool extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "c6ae851e-6c48-4a95-bd33-b1a15d03e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tool = pd.read_csv(\"C:\\\\Users\\\\Liu Shi Peng\\\\Documents\\\\chat_messages_202409161505.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "c596868b-e389-41ab-a2e1-8e80158d6e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>tools_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85f5186b-4a1a-414e-8f76-2abde35f54f0</td>\n",
       "      <td>5454e817-c32d-4c1a-a9ef-0fc0d41fa02e</td>\n",
       "      <td>[{\"get_option_strategy_recommendation\": {\"inpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7e6a5c6f-ec03-4b32-aad8-67d99363760e</td>\n",
       "      <td>ef0763bf-4059-41c3-b974-ed951dfd423c</td>\n",
       "      <td>[{\"get_asset_price_info\": {\"input\": {\"ticker\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3194fade-8a75-4abc-b0c6-be20c4db52e1</td>\n",
       "      <td>361858ce-1d9f-45fa-8a6b-a79035a4d555</td>\n",
       "      <td>[{\"get_stock_related_news\": {\"input\": {\"ticker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e05a0e53-246c-4e2c-bc5f-d6c107bbe99a</td>\n",
       "      <td>5f0de6ae-1741-4d88-894e-37032c34ae92</td>\n",
       "      <td>[{\"get_option_strategy_recommendation\": {\"inpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ead8e8f8-3308-49f6-8c61-a4b8f3942c95</td>\n",
       "      <td>1b3c4e72-c13c-47f8-8a9e-0452f62cacd9</td>\n",
       "      <td>[{\"get_asset_price_info\": {\"input\": {\"ticker\":...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             message_id                            session_id  \\\n",
       "0  85f5186b-4a1a-414e-8f76-2abde35f54f0  5454e817-c32d-4c1a-a9ef-0fc0d41fa02e   \n",
       "1  7e6a5c6f-ec03-4b32-aad8-67d99363760e  ef0763bf-4059-41c3-b974-ed951dfd423c   \n",
       "2  3194fade-8a75-4abc-b0c6-be20c4db52e1  361858ce-1d9f-45fa-8a6b-a79035a4d555   \n",
       "3  e05a0e53-246c-4e2c-bc5f-d6c107bbe99a  5f0de6ae-1741-4d88-894e-37032c34ae92   \n",
       "4  ead8e8f8-3308-49f6-8c61-a4b8f3942c95  1b3c4e72-c13c-47f8-8a9e-0452f62cacd9   \n",
       "\n",
       "                                          tools_used  \n",
       "0  [{\"get_option_strategy_recommendation\": {\"inpu...  \n",
       "1  [{\"get_asset_price_info\": {\"input\": {\"ticker\":...  \n",
       "2  [{\"get_stock_related_news\": {\"input\": {\"ticker...  \n",
       "3  [{\"get_option_strategy_recommendation\": {\"inpu...  \n",
       "4  [{\"get_asset_price_info\": {\"input\": {\"ticker\":...  "
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tool.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "5f22042f-1d87-4ea6-9da1-e58dc4f6f81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46630"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_tool['session_id'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "36596a13-7bda-4f12-bc35-2690dda8e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df[['session_id', 'message_id', 'tools_used']]\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "17c81fad-98b5-4a2c-bc0f-446f5e7d720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "\n",
    "# def extract_tools(json_array_str):\n",
    "#     json_array = json.loads(json_array_str)\n",
    "#     return [list(item.keys())[0] for item in json_array]\n",
    "\n",
    "\n",
    "def extract_tools(tools_str):\n",
    "    try:\n",
    "        # Use ast.literal_eval to safely evaluate the string representation of the list\n",
    "        tools_list = ast.literal_eval(tools_str)\n",
    "        return [list(item.keys())[0] for item in tools_list]\n",
    "    except (ValueError, SyntaxError):\n",
    "        # Return an empty list or None if parsing fails\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "56fcbc17-8f23-429c-9b0d-8918b408c1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120759, 2)"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tool['tools'] = df_tool['tools_used'].apply(extract_tools)\n",
    "df2_tools = df_tool.explode('tools')\n",
    "df2_tools = df2_tools[['session_id', 'tools']].drop_duplicates()\n",
    "df2_tools.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "16c914bd-b99d-4ade-b78f-a5a1486e2218",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_tools.to_csv(\"C:\\\\Users\\\\Liu Shi Peng\\\\Documents\\\\sentiment_analysis\\\\aug_session_tools0830.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21fd0fa-0120-4edb-93bd-780bbd6bbaea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e50af0b-7003-425f-824e-eb164c62602f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f2daee-146a-4a2a-b2df-9abe8607c098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa443f3-5b1d-4ef8-8969-76ba64fdc455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8f442-06b0-4ec6-af31-3d0d0b26073b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7469de7-2257-48e9-8a12-476380abde79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
